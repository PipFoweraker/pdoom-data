[
  {
    "id": "alignmentforum_143490a8465619ca",
    "title": "$1000 USD prize - Circular Dependency of Counterfactuals",
    "year": 2022,
    "category": "public_awareness",
    "description": "**Congrats to the winner TailCalled with their post** [**Some thoughts on \"The Nature of Counterfactuals\"**](https://www.lesswrong.com/posts/euDWQDKRwMkdrpfGr/some-thoughts-on-the-nature-of-counterfactuals)**. See the** [**winner announcement post**](https://www.lesswrong.com/posts/qMfHFdDKbxzku2vpo/results-circular-dependency-of-counterfactuals-prize)**.**",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Gzw6FwPD9FeL4GTWC/usd1000-usd-prize-circular-dependency-of-counterfactuals"
    ],
    "tags": [
      "bounties & prizes (active)",
      "bounties (closed)",
      "decision theory",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_40e96310daaafff5",
    "title": "How an alien theory of mind might be unlearnable",
    "year": 2022,
    "category": "policy_development",
    "description": "**EDIT**: *This is a post about an alien mind being unlearnable in practice. As a reminder, theory of mind is unlearnable in theory, as stated [here](https://arxiv.org/abs/1712.05812) - there is more information in \"preferences + (ir)rationality\" than there is in \"behaviour\", \"policy\", or even \"[complete internal brain structure](https://www.lesswrong.com/posts/9rjW9rhyhJijHTM92/learning-human-preferences-black-box-white-box-and)\". This information gap must be covered by assumptions (or \"labelled data\", in CS terms) of one form or another - assumptions that cannot be deduced from observation. It is unclear whether we need only a few trivial assumptions or a lot of detailed and subtle ones. Hence posts like this one, looking at the practicality angle.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/kMJxwCZ4mc9w4ezbs/how-an-alien-theory-of-mind-might-be-unlearnable"
    ],
    "tags": [
      "ai",
      "value learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_452d2efe6ce62654",
    "title": "Prizes for ELK proposals",
    "year": 2022,
    "category": "public_awareness",
    "description": "**We are no longer accepting submissions. We'll get in touch with winners and make a post about winning proposals sometime in the next month.**",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals"
    ],
    "tags": [
      "ai",
      "bounties (closed)",
      "community",
      "eliciting latent knowledge (elk)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_038288d015e3e6cd",
    "title": "Apply for research internships at ARC!",
    "year": 2022,
    "category": "technical_research_breakthrough",
    "description": "**(Update: we are no longer accepting applications for interns.)**",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/BRsxztzkTzScFQfDW/apply-for-research-internships-at-arc"
    ],
    "tags": [
      "ai",
      "community"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_fb34441353afe5f6",
    "title": "Promising posts on AF that have fallen through the cracks",
    "year": 2022,
    "category": "public_awareness",
    "description": "Yesterday on the [Weekly Alignment Research Coffee Time](https://www.alignmentforum.org/posts/cysgh8zpmvt56f6Qw/event-weekly-alignment-research-coffee-time-01-03) call as people were sharing updates on their recent work, Vanessa Kosoy brought up her and Diffractor's post [Infra-Bayesian physicalism: a formal theory of naturalized induction](https://www.alignmentforum.org/posts/gHgs2e2J5azvGFatb/infra-bayesian-physicalism-a-formal-theory-of-naturalized) which she was interested in getting feedback on.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/WerwgmeYZYGC2hKXN/promising-posts-on-af-that-have-fallen-through-the-cracks"
    ],
    "tags": [
      "ai",
      "community",
      "site meta"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f4620b358a20ccb4",
    "title": "More Is Different for AI",
    "year": 2022,
    "category": "public_awareness",
    "description": "Machine learning is touching increasingly many aspects of our society, and its effect will only continue to grow. Given this, I and many others care about risks from future ML systems and how to mitigate them.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Lp4Q9kSGsJHLfoHX3/more-is-different-for-ai"
    ],
    "tags": [
      "ai",
      "ai risk"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ba9d44fa118f2468",
    "title": "Importance of foresight evaluations within ELK",
    "year": 2022,
    "category": "policy_development",
    "description": "This post makes a few basic observations regarding the importance of foresight-based evaluations within ELK [[Christiano et al 2021](https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge)]:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/mvGNKQ6iSDf3d4gCi/importance-of-foresight-evaluations-within-elk"
    ],
    "tags": [
      "ai",
      "eliciting latent knowledge (elk)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0c440cf859e29ce0",
    "title": "Future ML Systems Will Be Qualitatively Different",
    "year": 2022,
    "category": "public_awareness",
    "description": "In 1972, the Nobel prize-winning physicist Philip Anderson wrote the essay \"[More Is Different](https://science.sciencemag.org/content/177/4047/393)\". In it, he argues that quantitative changes can lead to qualitatively different and unexpected phenomena. While he focused on physics, one can find many examples of More is Different in other domains as well, including biology, economics, and computer science. Some examples of More is Different include:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pZaPhGg2hmmPwByHc/future-ml-systems-will-be-qualitatively-different"
    ],
    "tags": [
      "ai",
      "machine learning  (ml)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_27ca82db681522f7",
    "title": "Understanding the two-head strategy for teaching ML to answer questions honestly",
    "year": 2022,
    "category": "public_awareness",
    "description": "This post is the result of my attempts to understand what's going on in these two posts from summer 2021:",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Ntmbm79zQakr29XLw/understanding-the-two-head-strategy-for-teaching-ml-to"
    ],
    "tags": [
      "ai",
      "eliciting latent knowledge (elk)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2f28d6796bb74b7b",
    "title": "New year, new research agenda post",
    "year": 2022,
    "category": "policy_development",
    "description": "*Thanks to Steve Byrnes, Adam Shimi, John Wentworth, and Peter Barnett for feedback.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/zuHezdoGr2KtM2n43/new-year-new-research-agenda-post"
    ],
    "tags": [
      "ai",
      "research agendas"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cb1dc86a7f0eac21",
    "title": "The Greedy Doctor Problem... turns out to be relevant to the ELK problem?",
    "year": 2022,
    "category": "policy_development",
    "description": "The following post was published on [my Substack](https://universalprior.substack.com/p/the-greedy-doctor-problem) and discussed on [HackerNews](https://news.ycombinator.com/item?id=29269973) about 2 months ago. I originally planned it as an accessible introduction to [Vinge's principle](https://arbital.com/p/Vinge_principle/) and the [Principal-Agent-Problem](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem). Despite having equations and simulations to support my argument, I originally did not think it was sufficiently novel or relevant for the Alignment Forum. However, now that I got a chance to read the new work from ARC on the [ELK problem](https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge), I think the post might be relevant (or at least thought-provoking) for the community after all. The Greedy Doctor Problem overlaps quite a lot with the ELK problem (just replace the coin flip with the presence of the d...",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/dDzHJGmyeQa2tGmqH/the-greedy-doctor-problem-turns-out-to-be-relevant-to-the"
    ],
    "tags": [
      "ai",
      "eliciting latent knowledge (elk)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3b85cfd2624314fa",
    "title": "Challenges with Breaking into MIRI-Style Research",
    "year": 2022,
    "category": "public_awareness",
    "description": "Trying to break into MIRI-style[[1]](#fnbumcs1si1j) research seems to be much, much harder than trying to break into ML-style safety research. This is worrying if you believe this research to be important[[2]](#fn13410j7g67vg). I'll examine two kinds of causes: those which come from MIRI-style research being a niche area and those which go beyond this:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Kcbo4rXu3jYPnauoK/challenges-with-breaking-into-miri-style-research"
    ],
    "tags": [
      "agent foundations",
      "ai",
      "ai risk",
      "machine intelligence research institute (miri)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8a8b23acae6d90f2",
    "title": "Truthful LMs as a warm-up for aligned AGI",
    "year": 2022,
    "category": "policy_development",
    "description": "*This post is heavily informed by prior work, most notably that of Owain Evans, Owen Cotton-Barratt and others (*[*Truthful AI*](https://www.alignmentforum.org/posts/aBixCPqSnTsPsTJBQ/truthful-ai-developing-and-governing-ai-that-does-not-lie)*), Beth Barnes (*[*Risks from AI persuasion*](https://www.alignmentforum.org/posts/5cWtwATHL6KyzChck/risks-from-ai-persuasion)*), Paul Christiano (unpublished) and Dario Amodei (unpublished), but was written by me and is not necessarily endorsed by those people. I am also very grateful to Paul Christiano, Leo Gao, Beth Barnes, William Saunders, Owain Evans, Owen Cotton-Barratt, Holly Mandel and Daniel Ziegler for invaluable feedback.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/jWkqACmDes6SoAiyE/truthful-lms-as-a-warm-up-for-aligned-agi"
    ],
    "tags": [
      "ai",
      "ai risk",
      "gpt",
      "honesty",
      "language models",
      "machine learning  (ml)",
      "outer alignment",
      "truth, semantics, & meaning"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_167820cdf9d60223",
    "title": "Scalar reward is not enough for aligned AGI",
    "year": 2022,
    "category": "policy_development",
    "description": "This post was authored by Peter Vamplew and Cameron Foale (Federation University), and Richard Dazeley (Deakin University)",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/eeEEgNeTepZb6F6NF/scalar-reward-is-not-enough-for-aligned-agi"
    ],
    "tags": [
      "ai",
      "reinforcement learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d27e65673b7c4e1b",
    "title": "What's Up With Confusingly Pervasive Goal Directedness?",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Fictionalized/Paraphrased version of a real dialog between me and John Wentworth.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/DJnvFsZ2maKxPi7v7/what-s-up-with-confusingly-pervasive-goal-directedness"
    ],
    "tags": [
      "ai",
      "consequentialism",
      "rationality"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e7814785ac8aeae3",
    "title": "[AN #171]: Disagreements between alignment \"optimists\" and \"pessimists\"",
    "year": 2022,
    "category": "policy_development",
    "description": "Listen to this newsletter on **[The Alignment Newsletter Podcast](http://alignment-newsletter.libsyn.com/)**.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/3vFmQhHBosnjZXuAJ/an-171-disagreements-between-alignment-optimists-and"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_28ecf8fe9effa635",
    "title": "Instrumental Convergence For Realistic Agent Objectives",
    "year": 2022,
    "category": "policy_development",
    "description": "**Edit, 5/16/23: I think this post is beautiful, correct in its narrow technical claims, and practically irrelevant to alignment. This post presents an unrealistic picture of the role of reward functions in reinforcement learning, conflating \"utility\" with \"reward.\" Reward functions are not \"goals\", reward functions are not \"objectives\" of the policy network, real-world policies are not \"optimal\", and the mechanistic function of reward is (usually) to provide policy gradients to update the policy network.**",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/W22Btd7NmGuucFejc/instrumental-convergence-for-realistic-agent-objectives"
    ],
    "tags": [
      "ai",
      "instrumental convergence"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_fd6a4a14ac33c548",
    "title": "ELK First Round Contest Winners",
    "year": 2022,
    "category": "public_awareness",
    "description": "Thank you to all those who have submitted proposals to the ELK proposal competition. We have evaluated all proposals submitted before January 14th[[1]](#fn-5CZvwshYeWEKxSPpz-1). Decisions are still being made on proposals submitted after January 14th.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qXFbGzS3Sg2NhrNAu/elk-first-round-contest-winners"
    ],
    "tags": [
      "eliciting latent knowledge (elk)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_03533f0c68cafeed",
    "title": "[Intro to brain-like-AGI safety] 1. What's the problem & Why work on it now?",
    "year": 2022,
    "category": "policy_development",
    "description": "1.1 Post summary / Table of contents\n====================================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/4basF9w9jaPZpoC8R/intro-to-brain-like-agi-safety-1-what-s-the-problem-and-why"
    ],
    "tags": [
      "ai",
      "has diagram",
      "neuromorphic ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_51ec01ffe57cc162",
    "title": "Arguments about Highly Reliable Agent Designs as a Useful Path to Artificial Intelligence Safety",
    "year": 2022,
    "category": "public_awareness",
    "description": "This paper is a revised and expanded version of my blog post [Plausible cases for HRAD work, and locating the crux in the \"realism about rationality\" debate](https://www.lesswrong.com/posts/BGxTpdBGbwCWrGiCL/plausible-cases-for-hrad-work-and-locating-the-crux-in-the), now with David Manheim as co-author.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/hWtpqjYXAvFExmAsD/arguments-about-highly-reliable-agent-designs-as-a-useful"
    ],
    "tags": [
      "agent foundations",
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4151937730386707",
    "title": "Causality, Transformative AI and alignment - part I",
    "year": 2022,
    "category": "public_awareness",
    "description": "**TL;DR:** transformative AI(TAI) plausibly requires causal models of the world. Thus, a component of AI safety is ensuring secure paths to generating these causal models. We think the lens of causal models might be undervalued within the current alignment research landscape and suggest possible research directions.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/oqzasmQ9Lye45QDMZ/causality-transformative-ai-and-alignment-part-i"
    ],
    "tags": [
      "ai",
      "causality",
      "transformative ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1de3d68b0622cd0c",
    "title": "[Intro to brain-like-AGI safety] 2. \"Learning from scratch\" in the brain",
    "year": 2022,
    "category": "policy_development",
    "description": "2.1 Post summary / Table of contents\n====================================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in"
    ],
    "tags": [
      "ai",
      "has diagram",
      "neuroscience"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_64ca59ffdee4ea1b",
    "title": "Thoughts on AGI safety from the top",
    "year": 2022,
    "category": "policy_development",
    "description": "In this post, I'll summarize my views on AGI safety after thinking about it for two years while on the Research Scholars Programme at FHI -- before which I was quite skeptical about the entire subject. [[1]](#fnd0ytybye60t)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ApLnWjgMwBTJt6buC/thoughts-on-agi-safety-from-the-top"
    ],
    "tags": [
      "ai",
      "ai risk",
      "ai timelines",
      "future of humanity institute (fhi)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_76b1ff95c97432fc",
    "title": "QNR prospects are important for AI alignment research",
    "year": 2022,
    "category": "public_awareness",
    "description": "***Attention conservation notice:** This discussion is intended for readers with an interest in prospects for knowledge-rich intelligent systems and potential applications of improved knowledge representations to AI capabilities and alignment. It contains no theorems.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FKE6cAzQxEK4QH9fC/qnr-prospects-are-important-for-ai-alignment-research"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)",
      "language models"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_37fcf702b81b7253",
    "title": "Alignment versus AI Alignment",
    "year": 2022,
    "category": "policy_development",
    "description": "*Financial status: This work is supported by individual donors and a grant from LTFF.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/hTfyX4823wKqnoFnS/alignment-versus-ai-alignment"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ce5edb27216aeef4",
    "title": "Paradigm-building: Introduction",
    "year": 2022,
    "category": "public_awareness",
    "description": "John Wentworth has described the current phase of AGI safety research as [preparadigmatic](https://www.alignmentforum.org/posts/3L46WGauGpr7nYubu/the-plan)--that is (courtesy of the [APA](https://dictionary.apa.org/preparadigmatic-science)), \"a science at a [very early] stage of development, before it has achieved a paradigm and established a consensus about the true nature of the subject matter and how to approach it.\" Here is my attempt to sketch this a bit more systematically:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/4TuzWEKysvYdhRXLd/paradigm-building-introduction"
    ],
    "tags": [
      "ai",
      "ai risk",
      "community",
      "research agendas"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_83f5dd75f5e4311f",
    "title": "How complex are myopic imitators?",
    "year": 2022,
    "category": "public_awareness",
    "description": "*This post was written under Evan Hubinger's direct guidance and mentorship, as a part of the* [*Stanford Existential Risks Institute ML Alignment Theory Scholars (MATS) program*](https://www.lesswrong.com/posts/FpokmCnbP3CEZ5h4t/ml-alignment-theory-program-under-evan-hubinger)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/2eRgFFeeS7pR4R8nD/how-complex-are-myopic-imitators-1"
    ],
    "tags": [
      "ai",
      "inner alignment",
      "myopia",
      "seri mats"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a44215fd8f7bdf56",
    "title": "Hypothesis: gradient descent prefers general circuits",
    "year": 2022,
    "category": "public_awareness",
    "description": "**Summary:** I discuss a potential mechanistic explanation for why SGD might prefer general circuits for generating model outputs. I use this preference to explain how models can learn to generalize even after overfitting to near zero training error (i.e., grokking). I also discuss other perspectives on grokking and deep learning generalization.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/JFibrXBewkSDmixuo/hypothesis-gradient-descent-prefers-general-circuits"
    ],
    "tags": [
      "ai",
      "gradient descent",
      "optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e4b033875f25df2d",
    "title": "[Intro to brain-like-AGI safety] 3. Two subsystems: Learning & Steering",
    "year": 2022,
    "category": "public_awareness",
    "description": "3.1 Post summary / Table of contents\n====================================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and"
    ],
    "tags": [
      "ai",
      "ai timelines",
      "has diagram",
      "neuroscience"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_02b9df000745fb39",
    "title": "Inferring utility functions from locally non-transitive preferences",
    "year": 2022,
    "category": "public_awareness",
    "description": "As part of the [AI Safety Camp](https://aisafety.camp/), I've been diving a bit deeper into the foundations of expected utility theory and preference learning. In this post, I am making explicit a connection between those two things that (I assume) many people already made implicitly. But I couldn't find a nice exposition of this argument so I wrote it up. Any feedback is of course highly welcome!",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/QZiGEDiobFz8ropA5/inferring-utility-functions-from-locally-non-transitive"
    ],
    "tags": [
      "ai",
      "utility functions"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_07736e961db61517",
    "title": "A summary of aligning narrowly superhuman models",
    "year": 2022,
    "category": "policy_development",
    "description": "*This post was written under Evan Hubinger's direct guidance and mentorship, as a part of the*[*Stanford Existential Risks Institute ML Alignment Theory Scholars (MATS) program*](https://www.lesswrong.com/posts/FpokmCnbP3CEZ5h4t/ml-alignment-theory-program-under-evan-hubinger)*. Where not indicated, ideas are summaries of the documents mentioned, not original contributions. I am thankful for the encouraging approach of the organizers of the program, especially Oliver Zhang.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/TSxAXeHHhgSxR5wGZ/a-summary-of-aligning-narrowly-superhuman-models"
    ],
    "tags": [
      "ai",
      "narrow ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d868fc0903598e62",
    "title": "Some Hacky ELK Ideas",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Credit to Adam Shimi, Alex Flint, and Rob Miles for discussions, counterexamples, and general input to the ideas here.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/3gAKoaziTXmvHusRv/some-hacky-elk-ideas"
    ],
    "tags": [
      "ai",
      "eliciting latent knowledge (elk)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_70350e2922fe0e78",
    "title": "Is ELK enough? Diamond, Matrix and Child AI",
    "year": 2022,
    "category": "public_awareness",
    "description": "Introduction\n============",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/XjDcwtgkHGWYA7stn/is-elk-enough-diamond-matrix-and-child-ai"
    ],
    "tags": [
      "ai",
      "eliciting latent knowledge (elk)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_85e66f347a8352ab",
    "title": "[Intro to brain-like-AGI safety] 4. The \"short-term predictor\"",
    "year": 2022,
    "category": "public_awareness",
    "description": "4.1 Post summary / Table of contents\n====================================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Y3bkJ59j4dciiLYyw/intro-to-brain-like-agi-safety-4-the-short-term-predictor"
    ],
    "tags": [
      "ai",
      "has diagram",
      "neuromorphic ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1faa081097385be7",
    "title": "Compute Trends Across Three eras of Machine Learning",
    "year": 2022,
    "category": "public_awareness",
    "description": "<https://arxiv.org/abs/2202.05924>",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/XKtybmbjhC6mXDm5z/compute-trends-across-three-eras-of-machine-learning"
    ],
    "tags": [
      "ai",
      "compute",
      "machine learning  (ml)",
      "moore's law",
      "scaling laws",
      "technological forecasting"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4db4f9ff307bd8f5",
    "title": "Implications of automated ontology identification",
    "year": 2022,
    "category": "policy_development",
    "description": "*Financial status: supported by individual donors and a grant from LTFF.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/LRgM9cuLNPbsjwEdN/implications-of-automated-ontology-identification"
    ],
    "tags": [
      "ai",
      "eliciting latent knowledge (elk)",
      "ontology"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_56296b0711612cc3",
    "title": "Alignment researchers, how useful is extra compute for you?",
    "year": 2022,
    "category": "public_awareness",
    "description": "Research publication: Alignment researchers, how useful is extra compute for you?",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/A4djH6sc9vZq2AYBD/alignment-researchers-how-useful-is-extra-compute-for-you-1"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7a0a8ad3a348feef",
    "title": "The Big Picture Of Alignment (Talk Part 1)",
    "year": 2022,
    "category": "public_awareness",
    "description": "I recently gave a two-part talk on the big picture of alignment, as I see it. The talk is not-at-all polished, but contains a lot of stuff for which I don't currently know of any good writeup. Major pieces in part one:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/xdSDFQs4aC5GrdHNZ/the-big-picture-of-alignment-talk-part-1"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b15af40d513cb277",
    "title": "Favorite / most obscure research on understanding DNNs?",
    "year": 2022,
    "category": "public_awareness",
    "description": "Research publication: Favorite / most obscure research on understanding DNNs?",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/269kZdpdKtmLwHni4/favorite-most-obscure-research-on-understanding-dnns"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4df030f09db83b15",
    "title": "Alignment research exercises",
    "year": 2022,
    "category": "policy_development",
    "description": "It's currently hard to know where to start when trying to get better at thinking about alignment. So below I've listed a few dozen exercises which I expect to be helpful. They assume a level of background alignment knowledge roughly equivalent to what's covered in the [technical alignment track of the AGI safety fundamentals course](https://forum.effectivealtruism.org/posts/BpAKCeGMtQqqty9ZJ/agi-safety-fundamentals-curriculum-and-application). They vary greatly in difficulty - some are standard knowledge in ML, some are open research questions. I've given the exercises star ratings from \\* to \\*\\*\\* for difficulty (note: *not* for length of time to complete - many require reading papers before engaging with them). However, I haven't tried to solve them all myself, so the star ratings may be significantly off.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/kj37Hzb2MsALwLqWt/alignment-research-exercises"
    ],
    "tags": [
      "ai",
      "exercises / problem-sets"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7b7a22177c6b6843",
    "title": "Ngo and Yudkowsky on scientific reasoning and pivotal acts",
    "year": 2022,
    "category": "policy_development",
    "description": "This is a transcript of a conversation between Richard Ngo and Eliezer Yudkowsky, facilitated by Nate Soares (and with some comments from Carl Shulman). This transcript continues the [Late 2021 MIRI Conversations](https://intelligence.org/late-2021-miri-conversations/) sequence, following [Ngo's view on alignment difficulty](https://www.lesswrong.com/posts/gf9hhmSvpZfyfS34B/ngo-s-view-on-alignment-difficulty).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/cCrpbZ4qTCEYXbzje/ngo-and-yudkowsky-on-scientific-reasoning-and-pivotal-acts"
    ],
    "tags": [
      "ai",
      "general intelligence"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9268d3aa134f1306",
    "title": "ELK Proposal: Thinking Via A Human Imitator",
    "year": 2022,
    "category": "public_awareness",
    "description": "I want to use the AI's intelligence to figure out how to translate into the human ontology. The hope is to route a smart entity's performance through a dumb entity's understanding and thereby get the smart entity to solve interpretability-by-the-dumb-entity. While my proposed current architecture overcomes a class of counterexamples in a manner which I find elegant, it is still broken by several plausible counterexamples.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/z3xTDPDsndJBmHLFH/elk-proposal-thinking-via-a-human-imitator"
    ],
    "tags": [
      "ai",
      "eliciting latent knowledge (elk)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9b6b2d018aa9ce78",
    "title": "[Intro to brain-like-AGI safety] 5. The \"long-term predictor\", and TD learning",
    "year": 2022,
    "category": "public_awareness",
    "description": "5.1 Post summary / Table of contents\n====================================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/F759WQ8iKjqBncDki/intro-to-brain-like-agi-safety-5-the-long-term-predictor-and"
    ],
    "tags": [
      "ai",
      "has diagram",
      "neuromorphic ai",
      "reinforcement learning"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2bf060b9b784c124",
    "title": "Transformer inductive biases & RASP",
    "year": 2022,
    "category": "public_awareness",
    "description": "This is a paper by Weiss et al. about the \"**Restricted Access Sequence Processing Language**\", which is a computational framework for what transformers can do.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/kwpvEpDXsivbmdYhr/transformer-inductive-biases-and-rasp"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0244595a63f37c60",
    "title": "The Big Picture Of Alignment (Talk Part 2)",
    "year": 2022,
    "category": "public_awareness",
    "description": "I recently gave a two-part talk on the big picture of alignment, as I see it. The talk is not-at-all polished, but contains a lot of stuff for which I don't currently know of any good writeup. Linkpost for the first part is [here](https://www.lesswrong.com/posts/xdSDFQs4aC5GrdHNZ/the-big-picture-of-alignment-talk-part-1); this linkpost is for the second part.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/aEtc5GgqJGFtTH2kQ/the-big-picture-of-alignment-talk-part-2-1"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8e1919afa0d740b0",
    "title": "How I Formed My Own Views About AI Safety",
    "year": 2022,
    "category": "public_awareness",
    "description": "***Disclaimer**: I work as a researcher at Anthropic, but this post entirely represents my own views, rather than the views of my own employer*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/JZrN4ckaCfd6J37cG/how-i-formed-my-own-views-about-ai-safety"
    ],
    "tags": [
      "ai",
      "ai risk",
      "humility",
      "inside/outside view",
      "rationality",
      "research taste"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cb15288e2438764f",
    "title": "Shah and Yudkowsky on alignment failures",
    "year": 2022,
    "category": "policy_development",
    "description": "This is the final discussion log in the [Late 2021 MIRI Conversations](https://www.lesswrong.com/s/n945eovrA3oDueqtq) sequence, featuring Rohin Shah and Eliezer Yudkowsky, with additional comments from Rob Bensinger, Nate Soares, Richard Ngo, and Jaan Tallinn.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/tcCxPLBrEXdxN5HCQ/shah-and-yudkowsky-on-alignment-failures"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cee6825497c3bc86",
    "title": "Late 2021 MIRI Conversations: AMA / Discussion",
    "year": 2022,
    "category": "public_awareness",
    "description": "With the release of [Rohin Shah and Eliezer Yudkowsky's conversation](https://www.lesswrong.com/posts/tcCxPLBrEXdxN5HCQ/shah-and-yudkowsky-on-alignment-plans), the **Late 2021 MIRI Conversations** sequence is now complete.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion"
    ],
    "tags": [
      "ai",
      "ama"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_eb138a2057e47522",
    "title": "Musings on the Speed Prior",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Thanks to Paul Christiano, Mark Xu, Abram Demski, Kate Woolverton, and Beth Barnes for some discussions which informed this post.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/GC69Hmc6ZQDM9xC3w/musings-on-the-speed-prior"
    ],
    "tags": [
      "ai",
      "eliciting latent knowledge (elk)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3ecb95080e1aad5d",
    "title": "[Intro to brain-like-AGI safety] 6. Big picture of motivation, decision-making, and RL",
    "year": 2022,
    "category": "public_awareness",
    "description": "6.1 Post summary / Table of contents\n====================================",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qNZSBqLEh4qLRqgWW/intro-to-brain-like-agi-safety-6-big-picture-of-motivation"
    ],
    "tags": [
      "ai",
      "has diagram",
      "neuromorphic ai",
      "reinforcement learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_fbe4d4e683cba0a7",
    "title": "Projecting compute trends in Machine Learning",
    "year": 2022,
    "category": "public_awareness",
    "description": "Research publication: Projecting compute trends in Machine Learning",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/3dBtgKCkJh5yCHbag/projecting-compute-trends-in-machine-learning-2"
    ],
    "tags": [
      "ai",
      "ai timelines",
      "forecasting & prediction",
      "moore's law"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e641343a3039cfe2",
    "title": "[MLSN #3]: NeurIPS Safety Paper Roundup",
    "year": 2022,
    "category": "policy_development",
    "description": "As part of a larger community building effort, I am writing a safety newsletter which is designed to cover empirical safety research and be palatable to the broader machine learning research community. You can [subscribe here](https://newsletter.mlsafety.org/) or follow the newsletter on [twitter](https://twitter.com/ml_safety) here.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/dhbLE8BqRvhBtsXhS/mlsn-3-neurips-safety-paper-roundup"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ac7c80db3284c055",
    "title": "Value extrapolation, concept extrapolation, model splintering",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Post written with Rebecca Gorman*.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/i8sHdLyGQeBTGwTqq/value-extrapolation-concept-extrapolation-model-splintering"
    ],
    "tags": [
      "ai",
      "value learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b23978374b53f6da",
    "title": "ELK prize results",
    "year": 2022,
    "category": "public_awareness",
    "description": "From January - February the Alignment Research Center [offered prizes](https://www.lesswrong.com/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals) for proposed algorithms for [eliciting latent knowledge](https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge). In total we received 197 proposals and are awarding 32 prizes of $5k-20k. We are also giving 24 proposals honorable mentions of $1k, for a total of $274,000.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/zjMKpSB2Xccn9qi5t/elk-prize-results"
    ],
    "tags": [
      "ai",
      "alignment research center",
      "eliciting latent knowledge (elk)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_75a22bf8d7940c36",
    "title": "[Intro to brain-like-AGI safety] 7. From hardcoded drives to foresighted plans: A worked example",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Part of the*[*\"Intro to brain-like-AGI safety\" post series*](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/zXibERtEWpKuG5XAC/intro-to-brain-like-agi-safety-7-from-hardcoded-drives-to"
    ],
    "tags": [
      "ai",
      "has diagram",
      "neuromorphic ai",
      "neuroscience"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e7e05580b55f85a2",
    "title": "It Looks Like You're Trying To Take Over The World",
    "year": 2022,
    "category": "public_awareness",
    "description": "*This story was originally posted as a response to* [*this thread*](https://forum.effectivealtruism.org/posts/DuPEzGJ5oscqxD5oh/shah-and-yudkowsky-on-alignment-failures?commentId=hjd7Z4AN6ToN2ebSm)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/a5e9arCnbDac9Doig/it-looks-like-you-re-trying-to-take-over-the-world"
    ],
    "tags": [
      "ai",
      "ai risk",
      "ai risk concrete stories",
      "ai takeoff",
      "fiction",
      "squiggle maximizer (formerly \"paperclip maximizer\")"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_dcb9a67af6d4e489",
    "title": "A Rephrasing Of and Footnote To An Embedded Agency Proposal",
    "year": 2022,
    "category": "technical_research_breakthrough",
    "description": "I wanted to clarify some comments made in [this post](https://www.alignmentforum.org/posts/TnkDtTAqCGetvLsgr/a-possible-resolution-to-spurious-counterfactuals), which proposed a way to resolve the issues brought up in the Action Counterfactuals subsection of [the Embedded Agency paper](https://arxiv.org/pdf/1902.09469.pdf). I noticed that my ideal edits would end up rewriting the post almost completely (it wasn't particularly clear), and I wanted to more coherently lay out the thinking.",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/mTuQDeuiXKnk972WR/a-rephrasing-of-and-footnote-to-an-embedded-agency-proposal"
    ],
    "tags": [
      "ai",
      "decision theory",
      "embedded agency"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9ae79b9bed10b091",
    "title": "A Longlist of Theories of Impact for Interpretability",
    "year": 2022,
    "category": "policy_development",
    "description": "I hear a lot of different arguments floating around for exactly how mechanistically interpretability research will reduce x-risk. As an interpretability researcher, forming clearer thoughts on this is pretty important to me! As a preliminary step, I've compiled a list with a longlist of 19 different arguments I've heard for why interpretability matters. These are pretty scattered and early stage thoughts (and emphatically my personal opinion than the official opinion of Anthropic!), but I'm sharing them in the hopes that this is interesting to people",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_87832a74ed5cbbe8",
    "title": "[Intro to brain-like-AGI safety] 8. Takeaways from neuro 1/2: On AGI development",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Part of the*[*\"Intro to brain-like-AGI safety\" post series*](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/fDPsYdDtkzhBp9A8D/intro-to-brain-like-agi-safety-8-takeaways-from-neuro-1-2-on"
    ],
    "tags": [
      "ai",
      "has diagram",
      "neuromorphic ai",
      "neuroscience"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d9de2a7b86a23c81",
    "title": "[Intro to brain-like-AGI safety] 9. Takeaways from neuro 2/2: On AGI motivation",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Part of the*[*\"Intro to brain-like-AGI safety\" post series*](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/vpdJz4k5BgGzuGo7A/intro-to-brain-like-agi-safety-9-takeaways-from-neuro-2-2-on"
    ],
    "tags": [
      "ai",
      "has diagram",
      "interpretability (ml & ai)",
      "the pointers problem",
      "wireheading"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5f1904bab40c1aa5",
    "title": "A survey of tool use and workflows in alignment research",
    "year": 2022,
    "category": "public_awareness",
    "description": "TL;DR: We are building language model powered tools to augment alignment researchers and accelerate alignment progress. We could use your feedback on what tools would be most useful. We've created **a short survey that can be filled out**[**here**](https://forms.office.com/r/mNBR7AKMBU)**.**",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ebYiodG3MAEqskCDG/a-survey-of-tool-use-and-workflows-in-alignment-research-1"
    ],
    "tags": [
      "ai",
      "ai safety camp",
      "ai-assisted/ai automated alignment",
      "surveys"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a386c6961dcaa577",
    "title": "Why Agent Foundations? An Overly Abstract Explanation",
    "year": 2022,
    "category": "public_awareness",
    "description": "Let's say you're relatively new to the field of AI alignment. You notice a certain cluster of people in the field who claim that no substantive progress is likely to be made on alignment without first solving various foundational questions of agency. These sound like a bunch of weird pseudophilosophical questions, like \"[what does it mean for some chunk of the world to do optimization?](https://www.lesswrong.com/posts/znfkdCoHMANwqc2WE/the-ground-of-optimization-1)\", or \"[how does an agent model a world bigger than itself?](https://www.lesswrong.com/posts/efWfvrWLgJmbBAs3m/embedded-world-models)\", or \"[how do we 'point' at things?](https://www.lesswrong.com/tag/the-pointers-problem)\", or in my case \"[how does abstraction work?](https://www.lesswrong.com/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro)\". You feel confused about why otherwise-smart-seeming people expect these weird pseudophilosophical questions to be unavoidable for engineering aligned...",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FWvzwCDRgcjb9sigb/why-agent-foundations-an-overly-abstract-explanation"
    ],
    "tags": [
      "agent foundations",
      "ai",
      "goodhart's law"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_98dda839ee97715c",
    "title": "Compute Governance: The Role of Commodity Hardware",
    "year": 2022,
    "category": "policy_development",
    "description": "*TL;DR: Thoughts on whether CPUs can make a comeback and become carriers of the next wave of machine learning progress (spoiler: they probably won't). Meditations on challenges in compute governance.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/z8BF9GwcCjeXShC4q/compute-governance-the-role-of-commodity-hardware"
    ],
    "tags": [
      "ai",
      "ai governance",
      "compute",
      "world optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4a5f891275734e56",
    "title": "[ASoT] Some ways ELK could still be solvable in practice",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Editor's note: I'm experimenting with having a lower quality threshold for just posting things even while I'm still confused and unconfident about my conclusions, but with this disclaimer at the top.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/SbxWdhhwJWCpifTst/asot-some-ways-elk-could-still-be-solvable-in-practice"
    ],
    "tags": [
      "ai",
      "eliciting latent knowledge (elk)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_bcb06697a2b39301",
    "title": "[ASoT] Searching for consequentialist structure",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Editor's note: I'm experimenting with having a lower quality threshold for just posting things even while I'm still confused and unconfident about my conclusions, but with this disclaimer at the top. Thanks to Kyle and Laria for discussions.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/f6ByNdGJYxR3Kwguy/asot-searching-for-consequentialist-structure"
    ],
    "tags": [
      "ai",
      "consequentialism"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a910b3113c82c3c4",
    "title": "[ASoT] Some thoughts about deceptive mesaoptimization",
    "year": 2022,
    "category": "technical_research_breakthrough",
    "description": "*Editor's note: I'm experimenting with having a lower quality threshold for just posting things even while I'm still confused and unconfident about my conclusions, but with this disclaimer at the top. Thanks to AI\\_WAIFU for discussions.*",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/oijJc8Mu2jPNgpuvy/asot-some-thoughts-about-deceptive-mesaoptimization"
    ],
    "tags": [
      "ai",
      "mesa-optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_291700ab0ada97d7",
    "title": "Vaniver's ELK Submission",
    "year": 2022,
    "category": "policy_development",
    "description": "*You can see the actual submission (including a more formalized model)* [*here*](https://docs.google.com/document/d/1VQcRtsPsf7-Yp6OfucyErpKm6YEdQR-mlNQciq5ZKC0/edit?usp=sharing)*, and the contest details* [*here*](https://www.lesswrong.com/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals)*. I've reordered things to be more natural as a blog post / explain the rationale / intuition a bit better. This didn't get a prize, tho it may have been because I didn't try to fit the ELK format.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/9f4zBjiFndqbR8y6e/vaniver-s-elk-submission"
    ],
    "tags": [
      "ai",
      "eliciting latent knowledge (elk)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f7ee2e2ac2ba5fa2",
    "title": "Towards a better circuit prior: Improving on ELK state-of-the-art",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Thanks to Paul Christiano for useful comments and feedback.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/7ygmXXGjXZaEktF6M/towards-a-better-circuit-prior-improving-on-elk-state-of-the"
    ],
    "tags": [
      "ai",
      "eliciting latent knowledge (elk)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4c6945df5983aa98",
    "title": "Gears-Level Mental Models of Transformer Interpretability",
    "year": 2022,
    "category": "public_awareness",
    "description": "This post aims to quickly break down and explain the dominant mental models interpretability researchers currently use when thinking about how transformers work.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/X26ksz4p3wSyycKNB/gears-level-mental-models-of-transformer-interpretability"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)",
      "language models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0a5cd119ae93fff6",
    "title": "[Intro to brain-like-AGI safety] 10. The alignment problem",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Part of the*[*\"Intro to brain-like-AGI safety\" post series*](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/wucncPjud27mLWZzQ/intro-to-brain-like-agi-safety-10-the-alignment-problem"
    ],
    "tags": [
      "ai",
      "goodhart's law",
      "has diagram",
      "inner alignment",
      "instrumental convergence",
      "outer alignment",
      "wireheading"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a6e977e32df2fa4c",
    "title": "[ASoT] Some thoughts about LM monologue limitations and ELK",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Editor's note: I'm experimenting with having a lower quality threshold for just posting things even while I'm still confused and unconfident about my conclusions, but with this disclaimer at the top. Thanks to Kyle and Laria for discussion.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Jiy3n5KMsGGJ6NNYH/asot-some-thoughts-about-lm-monologue-limitations-and-elk"
    ],
    "tags": [
      "ai",
      "language models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_96ba45b464357df6",
    "title": "Procedurally evaluating factual accuracy: a request for research",
    "year": 2022,
    "category": "policy_development",
    "description": "*I am grateful to Daniel Kokotajlo, Beth Barnes and John Schulman for feedback on this post.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/2zEeb36XL6HLnjDkj/procedurally-evaluating-factual-accuracy-a-request-for"
    ],
    "tags": [
      "ai",
      "language models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_96c7968a939062e3",
    "title": "ELK Computational Complexity: Three Levels of Difficulty",
    "year": 2022,
    "category": "policy_development",
    "description": "[ELK is described](https://www.lesswrong.com/tag/eliciting-latent-knowledge-elk) as a problem we wish to solve in the \"worst case\"; IE, for any potential failure case we can describe, we want to have a strong argument about why we won't run into that particular problem. The [ELK documen](https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge)t lists several cases we want to handle in that way. First in this list is the case where **the correct translation is arbitrarily computationally complex**.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ztnkDKD5odorWt5dB/elk-computational-complexity-three-levels-of-difficulty"
    ],
    "tags": [
      "ai",
      "eliciting latent knowledge (elk)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4f680eef68bcbcdd",
    "title": "AXRP Episode 13 - First Principles of AGI Safety with Richard Ngo",
    "year": 2022,
    "category": "policy_development",
    "description": "[Google Podcasts link](https://podcasts.google.com/feed/aHR0cHM6Ly9heHJwb2RjYXN0LmxpYnN5bi5jb20vcnNz/episode/OTlmYzM1ZjEtMDFkMi00ZTExLWExYjEtNTYwOTg2ZWNhOWNi)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/tEf8fEFCkFtPyg9pm/axrp-episode-13-first-principles-of-agi-safety-with-richard"
    ],
    "tags": [
      "ai",
      "ai risk",
      "audio",
      "axrp",
      "existential risk",
      "interviews"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_dce5c58a691f5dfa",
    "title": "Optimality is the tiger, and agents are its teeth",
    "year": 2022,
    "category": "policy_development",
    "description": "You've done it. You've built the machine.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/kpPnReyBC54KESiSn/optimality-is-the-tiger-and-agents-are-its-teeth"
    ],
    "tags": [
      "agency",
      "ai",
      "optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_90bc869be603f7e6",
    "title": "AI Governance across Slow/Fast Takeoff and Easy/Hard Alignment spectra",
    "year": 2022,
    "category": "policy_development",
    "description": "It has been suggested that in a rapid enough takeoff scenario, governance would not be useful, because the transition to superintelligence would be too rapid for human actors - whether governments, corporations, or individuals - to respond to. This seems to imply that we only care about takeoff speed. And if that is the only relevant factor, the case for governance only applies if you believe slow takeoff is likely. Of course, it also matters how long we have until takeoff - but even so, I think this leaves a fair amount on the table in terms of what governance could do, and I want to try to make the case that even in that world, governance (still defined broadly1) is important - though in different ways.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/xxMYFKLqiBJZRNoPj/ai-governance-across-slow-fast-takeoff-and-easy-hard"
    ],
    "tags": [
      "ai",
      "ai takeoff"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_db9bae3c1e93e771",
    "title": "On Agent Incentives to Manipulate Human Feedback in Multi-Agent Reward Learning Scenarios",
    "year": 2022,
    "category": "technical_research_breakthrough",
    "description": "This is an informal (i.e. *sans equations*) summary of a paper on which I have been working.",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/6TxmJRDGzDbwcLE3w/on-agent-incentives-to-manipulate-human-feedback-in-multi"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_fa2e8aab8b9144ca",
    "title": "Theories of Modularity in the Biological Literature",
    "year": 2022,
    "category": "technical_research_breakthrough",
    "description": "Introduction\n============",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/JzTfKrgC7Lfz3zcwM/theories-of-modularity-in-the-biological-literature"
    ],
    "tags": [
      "ai",
      "ai safety camp",
      "biology",
      "modularity"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_661b0abb05e48791",
    "title": "Project Intro: Selection Theorems for Modularity",
    "year": 2022,
    "category": "public_awareness",
    "description": "Introduction - what is modularity, and why should we care?\n==========================================================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/XKwKJCXgSKhSr9bZY/project-intro-selection-theorems-for-modularity"
    ],
    "tags": [
      "ai",
      "ai safety camp",
      "inner alignment",
      "modularity",
      "outer alignment",
      "selection theorems",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d27ed8bb825e58bb",
    "title": "Call For Distillers",
    "year": 2022,
    "category": "public_awareness",
    "description": "Many technical alignment researchers are bad-to-mediocre at writing up their ideas and results in a form intelligible to other people. And even for those who are reasonably good at it, writing up a good intuitive explanation still takes a lot of work, and that work lengthens the turn-time on publishing new results. For instance, a couple months ago I wrote [a post](https://www.lesswrong.com/posts/vvEebH5jEvxnJEvBC/abstractions-as-redundant-information) which formalized the idea of abstractions as redundant information, and argued that it's equivalent to abstractions as information relevant at a distance. That post came out about two months after I had the rough math worked out, because it took a lot of work to explain it decently - and I don't even think the end result was all that good an explanation! And I *still* don't have a post which explains well why that result is interesting.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/zo9zKcz47JxDErFzQ/call-for-distillers"
    ],
    "tags": [
      "ai",
      "distillation & pedagogy"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_13f4e825d0292ef3",
    "title": "Supervise Process, not Outcomes",
    "year": 2022,
    "category": "policy_development",
    "description": "We can think about machine learning systems on a spectrum from process-based to outcome-based:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes"
    ],
    "tags": [
      "ai",
      "factored cognition",
      "ought"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e94404eedd5bfad3",
    "title": "AXRP Episode 14 - Infra-Bayesian Physicalism with Vanessa Kosoy",
    "year": 2022,
    "category": "policy_development",
    "description": "[Google Podcasts link](https://podcasts.google.com/feed/aHR0cHM6Ly9heHJwb2RjYXN0LmxpYnN5bi5jb20vcnNz/episode/YjI2MDQ0YTUtYjAyMy00YmI3LWE3MWMtYmFhMTU3MmM4MzJl)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/YZ7xzwDrqrvnZjwdn/axrp-episode-14-infra-bayesian-physicalism-with-vanessa"
    ],
    "tags": [
      "ai",
      "audio",
      "axrp",
      "infra-bayesianism",
      "interviews"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cc440b9c70aa86c1",
    "title": "[Intro to brain-like-AGI safety] 11. Safety = alignment (but they're close!)",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Part of the* [*\"Intro to brain-like-AGI safety\" post series*](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/BeQcPCTAikQihhiaK/intro-to-brain-like-agi-safety-11-safety-alignment-but-they"
    ],
    "tags": [
      "ai",
      "ai boxing (containment)",
      "tool ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_07575b125e9f87f5",
    "title": "[Link] Why I'm excited about AI-assisted human feedback",
    "year": 2022,
    "category": "public_awareness",
    "description": "This is a link post for <https://aligned.substack.com/p/ai-assisted-human-feedback>",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qunrsimS2cECxyCKy/link-why-i-m-excited-about-ai-assisted-human-feedback"
    ],
    "tags": [
      "ai",
      "rlhf"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a4c5bd77b3172466",
    "title": "[Link] A minimal viable product for alignment",
    "year": 2022,
    "category": "public_awareness",
    "description": "This is a link post for <https://aligned.substack.com/p/alignment-mvp>",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/fYf9JAwa6BYMt8GBj/link-a-minimal-viable-product-for-alignment"
    ],
    "tags": [
      "ai",
      "ai assisted alignment",
      "ai-assisted/ai automated alignment"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_13146b6d46152bca",
    "title": "Truthfulness, standards and credibility",
    "year": 2022,
    "category": "policy_development",
    "description": "-1: Meta Prelude\n----------------",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Brr84ZmvK3kwy2eGJ/truthfulness-standards-and-credibility"
    ],
    "tags": [
      "ai",
      "truthful ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f4d4c222e7a274bc",
    "title": "How  BoMAI Might fail",
    "year": 2022,
    "category": "public_awareness",
    "description": "This post is me looking at [1905.12186.pdf](https://arxiv.org/pdf/1905.12186.pdf) and giving some ideas on how it might fail.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FebeDdcToayY6rHSf/how-bomai-might-fail"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b02f0ca8f89ce729",
    "title": "[ASoT] Some thoughts about imperfect world modeling",
    "year": 2022,
    "category": "public_awareness",
    "description": "So [a few posts ago](https://www.lesswrong.com/posts/oijJc8Mu2jPNgpuvy/some-thoughts-about-deceptive-mesaoptimization) I looked at the problem of not being able to anticipate all consequences of an action as being related to deceptive mesaoptimization, but also outer alignment too. This post digs more into some of the things I only touched on briefly in that post.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/bzLpXZMGAiMdfLNKy/asot-some-thoughts-about-imperfect-world-modeling"
    ],
    "tags": [
      "ai",
      "mesa-optimization",
      "outer alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7e18090be931e29f",
    "title": "Productive Mistakes, Not Perfect Answers",
    "year": 2022,
    "category": "public_awareness",
    "description": "*This post is part of the work done at* [*Conjecture*](https://conjecture.dev)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ADMWDDKGgivgghxWf/productive-mistakes-not-perfect-answers"
    ],
    "tags": [
      "ai",
      "intellectual progress (society-level)",
      "practice & philosophy of science"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_392847f52d00e3c1",
    "title": "Different perspectives on concept extrapolation",
    "year": 2022,
    "category": "policy_development",
    "description": "At the recent EAGx Oxford meetup, I ended up talking with a lot of people (18 people, back to back, on Sunday - for some reason, that day is a bit of a blur). Naturally, many of the conversations turned to [value extrapolation/concept extrapolation](https://www.lesswrong.com/posts/i8sHdLyGQeBTGwTqq/value-extrapolation-concept-extrapolation-model-splintering), the main current focus of our [Aligned AI startup](https://buildaligned.ai/). I explained the idea I explained multiple times and in multiple different ways. Different presentations were useful for people from different backgrounds.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/j9vCEjRFDwmH8FTKH/different-perspectives-on-concept-extrapolation"
    ],
    "tags": [
      "ai",
      "value learning"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0daa454cbd11a885",
    "title": "We Are Conjecture, A New Alignment Research Startup",
    "year": 2022,
    "category": "public_awareness",
    "description": "[Conjecture](https://www.conjecture.dev/) is a new alignment startup founded by Connor Leahy, Sid Black and Gabriel Alfour, which aims to scale alignment research. We have VC backing from, among others, Nat Friedman, Daniel Gross, Patrick and John Collison, Arthur Breitman, Andrej Karpathy, and Sam Bankman-Fried. Our founders and early staff are mostly [EleutherAI](https://www.eleuther.ai/) alumni and previously independent researchers like [Adam Shimi](https://www.alignmentforum.org/users/adamshimi). We are located in London.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/jfq2BH5kfQqu2vYv3/we-are-conjecture-a-new-alignment-research-startup"
    ],
    "tags": [
      "ai risk",
      "community",
      "conjecture (org)",
      "project announcement"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5eaa29342322bfa1",
    "title": "AIs should learn human preferences, not biases",
    "year": 2022,
    "category": "policy_development",
    "description": "A new paper by Rebecca Gorman and me, building on her ideas: [The dangers in algorithms learning humans' values and irrationalities](https://arxiv.org/abs/2202.13985).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/3PTfkdLLqZE9vppXC/ais-should-learn-human-preferences-not-biases"
    ],
    "tags": [
      "ai",
      "value learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_65749323dd1f3277",
    "title": "Language Model Tools for Alignment Research",
    "year": 2022,
    "category": "public_awareness",
    "description": "*I do not speak for the rest of the people working on this project*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/AhF8iXLu5PchsmyKf/language-model-tools-for-alignment-research"
    ],
    "tags": [
      "ai",
      "language models"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_57694468b5a27ae7",
    "title": "AMA Conjecture, A New Alignment Startup",
    "year": 2022,
    "category": "public_awareness",
    "description": "> [Conjecture](https://www.conjecture.dev/) is a new alignment startup founded by Connor Leahy, Sid Black and Gabriel Alfour, which aims to scale alignment research. We have VC backing from, among others, Nat Friedman, Daniel Gross, Patrick and John Collison, Arthur Breitman, Andrej Karpathy, and Sam Bankman-Fried. Our founders and early staff are mostly [EleutherAI](https://www.eleuther.ai/) alumni and previously independent researchers like [Adam Shimi](https://www.alignmentforum.org/users/adamshimi). We are located in London.\n> \n>",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/rtEtTybuCcDWLk7N9/ama-conjecture-a-new-alignment-startup"
    ],
    "tags": [
      "ai",
      "ai risk",
      "ama",
      "community",
      "conjecture (org)",
      "language models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5f63aa9bedd426bd",
    "title": "Elicit: Language Models as Research Assistants",
    "year": 2022,
    "category": "policy_development",
    "description": "[Ought](https://ought.org) is an applied machine learning lab. We're building [Elicit](https://elicit.org), the AI research assistant. Our mission is to automate and scale open-ended reasoning. To get there, we train language models by [supervising reasoning processes, not outcomes](https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes). This is better for reasoning capabilities in the short run and better for alignment in the long run.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/s5jrfbsGLyEexh4GT/elicit-language-models-as-research-assistants"
    ],
    "tags": [
      "ai",
      "language models",
      "ought",
      "research agendas"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9e211fe665c9947f",
    "title": "A broad basin of attraction around human values?",
    "year": 2022,
    "category": "public_awareness",
    "description": "Followup to: [Morality is Scary](https://www.lesswrong.com/posts/y5jAuKqkShdjMNZab/morality-is-scary), [AI design as opportunity and obligation to address human safety problems](https://www.lesswrong.com/posts/vbtvgNXkufFRSrx4j/three-ai-safety-related-ideas)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/TrvkWBwYvvJjSqSCj/a-broad-basin-of-attraction-around-human-values"
    ],
    "tags": [
      "ai",
      "corrigibility",
      "human values",
      "human-ai safety"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d4e69c85639a90dd",
    "title": "Reward model hacking as a challenge for reward learning",
    "year": 2022,
    "category": "policy_development",
    "description": "This post discusses an issue that could lead to catastrophically misaligned AI even when we have access to a perfect reward signal and there are no misaligned inner optimizers. Instead, the misalignment comes from the fact that our reward signal is too expensive to use directly for RL training, so we train a reward model, which is incorrect on some off-distribution transitions. The agent might then exploit these off-distribution deficiencies, which I'll refer to as *reward model hacking*.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/yyoKYmfFx7zpPyD99/reward-model-hacking-as-a-challenge-for-reward-learning"
    ],
    "tags": [
      "ai",
      "reward functions"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_095033de8cb4fb7a",
    "title": "A Small Negative Result on Debate",
    "year": 2022,
    "category": "public_awareness",
    "description": "Some context for this [new arXiv paper](https://arxiv.org/abs/2204.05212) from my group at NYU:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/CL8RFFCdsBwWWfKYS/a-small-negative-result-on-debate"
    ],
    "tags": [
      "ai",
      "debate (ai safety technique)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5d53e2b7b111b0f2",
    "title": "Another list of theories of impact for interpretability",
    "year": 2022,
    "category": "public_awareness",
    "description": "[Neel's post](https://www.alignmentforum.org/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability) on this is good. I thought I'd add my own list/framing. Somewhat rough.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/YQALrtMkeqemAF5GX/another-list-of-theories-of-impact-for-interpretability"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_206fb01d3475aa8d",
    "title": "What to include in a guest lecture on existential risks from AI?",
    "year": 2022,
    "category": "policy_development",
    "description": "A professor I'm friendly with has been teaching a course on AI ethics this semester, and he asked me if I could come give a guest lecture on \"AI apocalypse\" scenarios. What should I include in the lecture?",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/mwk8HGroo74WALLHy/what-to-include-in-a-guest-lecture-on-existential-risks-from"
    ],
    "tags": [
      "ai",
      "distillation & pedagogy"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3f249a0b9e72bb31",
    "title": "Takeoff speeds have a huge effect on what it means to work on AI x-risk",
    "year": 2022,
    "category": "public_awareness",
    "description": "The slow takeoff hypothesis predicts that AGI emerges in a world where powerful but non-AGI AI is already a really big deal. Whether AI is a big deal right before the emergence of AGI determines many super basic things about what we should think our current job is. I hadn't fully appreciated the size of this effect until a few days ago.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/hRohhttbtpY3SHmmD/takeoff-speeds-have-a-huge-effect-on-what-it-means-to-work-1"
    ],
    "tags": [
      "ai",
      "ai takeoff"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_953523a8e7b75c15",
    "title": "Early 2022 Paper Round-up",
    "year": 2022,
    "category": "policy_development",
    "description": "My students and collaborators have been doing some particularly awesome work over the past several months, and to highlight that I wanted to summarize their papers here, and explain why I'm excited about them. There's six papers in three categories.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qAhT2qvKXboXqLk4e/early-2022-paper-round-up"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_376d20301ca8bfab",
    "title": "Refine: An Incubator for Conceptual Alignment Research Bets",
    "year": 2022,
    "category": "public_awareness",
    "description": "I'm opening an incubator called **Refine**for [conceptual alignment](https://www.alignmentforum.org/posts/2Xfv3GQgo2kGER8vA/alignment-research-conceptual-alignment-research-applied) research in London, which will be hosted by [Conjecture](https://www.lesswrong.com/posts/jfq2BH5kfQqu2vYv3/we-are-conjecture-a-new-alignment-research-startup). The program is a three-month fully-paid fellowship for helping aspiring independent researchers find, formulate, and get funding for new conceptual alignment *research bets*, ideas that are promising enough to try out for a few months to see if they have more potential.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/D7epkkJb3CqDTYgX9/refine-an-incubator-for-conceptual-alignment-research-bets"
    ],
    "tags": [
      "ai",
      "community",
      "conjecture (org)",
      "refine"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_aca96838f38c2d64",
    "title": "Some reasons why a predictor wants to be a consequentialist",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Status: working notes*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/DQBpu6LweoXyxLSsf/some-reasons-why-a-predictor-wants-to-be-a-consequentialist"
    ],
    "tags": [
      "ai",
      "consequentialism",
      "oracle ai",
      "tool ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5bcf2b287c7f36ac",
    "title": "Everything I Need To Know About Takeoff Speeds I Learned From Air Conditioner Ratings On Amazon",
    "year": 2022,
    "category": "policy_development",
    "description": "I go to Amazon, search for \"air conditioner\", and sort by average customer rating. There's a couple pages of evaporative coolers (not what I'm looking for), one used window unit (?), and then [this](http://amazon.com/Pro-Breeze-10000-Portable-Conditioner/dp/B08Q7JVZ6B/):",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/MMAK6eeMCH3JGuqeZ/everything-i-need-to-know-about-takeoff-speeds-i-learned"
    ],
    "tags": [
      "ai takeoff",
      "air conditioning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_33569510e971b149",
    "title": "Concept extrapolation: key posts",
    "year": 2022,
    "category": "public_awareness",
    "description": "Concept extrapolation is the skill of taking a concept, a feature, or a goal that is defined in a narrow training situation... and extrapolating it safely to a more general situation. This more general situation might be very extreme, and the original concept might not make much sense (eg defining \"human beings\" in terms of quantum fields).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/rPWQzRRQbjtgYn7rE/concept-extrapolation-key-posts"
    ],
    "tags": [
      "ai",
      "coherent extrapolated volition"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2badc9b84e314ef3",
    "title": "\"Pivotal Act\" Intentions: Negative Consequences and Fallacious Arguments",
    "year": 2022,
    "category": "policy_development",
    "description": "**tl;dr:** I know a bunch of EA/rationality-adjacent people who argue -- sometimes jokingly and sometimes seriously -- that the only way or best way to reduce existential risk is to enable an \"aligned\" AGI development team to forcibly (even if nonviolently) shut down all other AGI projects, using safe AGI.  I find that the arguments for this conclusion are flawed, and that the conclusion itself causes harm to institutions who espouse it.   Fortunately (according to me), successful AI labs do not seem to espouse this \"pivotal act\" philosophy.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Jo89KvfAs9z7owoZp/pivotal-act-intentions-negative-consequences-and-fallacious"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ff403a63935a605c",
    "title": "[Intro to brain-like-AGI safety] 12. Two paths forward: \"Controlled AGI\" and \"Social-instinct AGI\"",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Part of the* [*\"Intro to brain-like-AGI safety\" post series*](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Sd4QvG4ZyjynZuHGt/intro-to-brain-like-agi-safety-12-two-paths-forward"
    ],
    "tags": [
      "ai",
      "has diagram",
      "neuromorphic ai",
      "neuroscience"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c7b7e6cf691a3b5c",
    "title": "For every choice of AGI difficulty, conditioning on gradual take-off implies shorter timelines.",
    "year": 2022,
    "category": "public_awareness",
    "description": "Alternate titles: Deconfusing Take-off; Taboo \"Fast\" and \"Slow\" Take-off.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/AiaAq5XeECg7MpTL7/for-every-choice-of-agi-difficulty-conditioning-on-gradual"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "ai timelines"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8f91ea40fad506ad",
    "title": "Infra-Miscellanea",
    "year": 2022,
    "category": "technical_research_breakthrough",
    "description": "This post is a writeup of some miscellaneous results that don't cohere into a single big post, but might possibly be of interest.",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/SYeuMzspmwoQABWdw/infra-miscellanea"
    ],
    "tags": [
      "infra-bayesianism"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_50b10948515ea829",
    "title": "Infra-Topology",
    "year": 2022,
    "category": "technical_research_breakthrough",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/PrYbdKcj89f8swCkr/infra-topology"
    ],
    "tags": [
      "logic & mathematics",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ca847eee207168fe",
    "title": "[ASoT] Consequentialist models as a superset of mesaoptimizers",
    "year": 2022,
    "category": "policy_development",
    "description": "**TL;DR**: I split out mesaoptimizers (models which do explicit search internally) from the superset of consequentialist models (which accomplish goals in the world, and may or may not use search internally). This resolves a bit of confusion I had about mesaoptimizers and whether things like GPT simulating an agent counted as mesaoptimization or not.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Rhbac7CfRodMrs77F/asot-consequentialist-models-as-a-superset-of-mesaoptimizers"
    ],
    "tags": [
      "ai",
      "mesa-optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4e4c231ec50e66f1",
    "title": "Intuitions about solving hard problems",
    "year": 2022,
    "category": "technical_research_breakthrough",
    "description": "**Solving hard scientific problems usually requires compelling insights**",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/GkXKvkLAcTm5ackCq/intuitions-about-solving-hard-problems"
    ],
    "tags": [
      "ai",
      "intellectual progress (society-level)",
      "practice & philosophy of science",
      "scholarship & learning",
      "world modeling",
      "world optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8f150d983a9c5977",
    "title": "Framings of Deceptive Alignment",
    "year": 2022,
    "category": "public_awareness",
    "description": "In this post I want to lay out some framings and thoughts about deception in misaligned AI systems.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/8whGos5JCdBzDbZhH/framings-of-deceptive-alignment"
    ],
    "tags": [
      "deceptive alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_399afd3da6f540c1",
    "title": "[$20K in Prizes] AI Safety Arguments Competition",
    "year": 2022,
    "category": "policy_development",
    "description": "TL;DR--We're distributing $20k in total as prizes for submissions that make effective arguments for the importance of AI safety. The goal is to generate short-form content for outreach to policymakers, management at tech companies, and ML researchers. This competition will be followed by another competition in around a month that focuses on long-form content.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/3eP8D5Sxih3NhPE6F/usd20k-in-prizes-ai-safety-arguments-competition"
    ],
    "tags": [
      "ai",
      "ai safety public materials",
      "bounties & prizes (active)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9ff9d35db58040fc",
    "title": "Why Copilot Accelerates Timelines",
    "year": 2022,
    "category": "public_awareness",
    "description": "> \"Say we have intelligences that are narrowly human / superhuman on every task you can think of (which, for what it's worth, I think will happen within 5-10 years). How long before we have self-replicating factories? Until foom? Until things are dangerously out of our control? Until GDP doubles within one year? In what order do these things happen?\" ([source](https://www.lesswrong.com/posts/qDoqwGs4Dhj27sbTj/what-are-the-numbers-in-mind-for-the-super-short-agi?commentId=9a2gZwy3mSa5awxpd))\n> \n>",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/aqTAd7KzsYmHWYdei/why-copilot-accelerates-timelines"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "ai timelines",
      "language models",
      "recursive self-improvement",
      "superintelligence"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8a02529f9bde6a78",
    "title": "SERI ML Alignment Theory Scholars Program 2022",
    "year": 2022,
    "category": "public_awareness",
    "description": "The Stanford Existential Risks Initiative ([SERI](https://cisac.fsi.stanford.edu/stanford-existential-risks-initiative/content/stanford-existential-risks-initiative)) recently opened [applications](https://airtable.com/shr3XO3xZVR9HNPS2) for the second iteration of the [ML Alignment Theory Scholars (MATS) Program](https://serimats.org/), which aims to help aspiring alignment researchers enter the field by pairing them with established research mentors and fostering an academic community in Berkeley, California over the summer. Current mentors include Alex Gray, Beth Barnes, Evan Hubinger, John Wentworth, Leo Gao and Stuart Armstrong. Applications close on May 15 and include a written response to mentor-specific selection questions, viewable on [our website](https://serimats.org/).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/8vLvpxzpc6ntfBWNo/seri-ml-alignment-theory-scholars-program-2022"
    ],
    "tags": [
      "ai",
      "community",
      "seri mats"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b684eb165fda0974",
    "title": "[Intro to brain-like-AGI safety] 13. Symbol grounding & human social instincts",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Part of the* [*\"Intro to brain-like-AGI safety\" post series*](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/5F5Tz3u6kJbTNMqsb/intro-to-brain-like-agi-safety-13-symbol-grounding-and-human"
    ],
    "tags": [
      "ai",
      "has diagram",
      "neuroscience",
      "symbol grounding"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2a46a8e18e80615e",
    "title": "Law-Following AI 1: Sequence Introduction and Structure",
    "year": 2022,
    "category": "policy_development",
    "description": "*This post is written in my personal capacity, and does not necessarily represent the views of OpenAI or any other organization. Cross-posted to the [Effective Altruism Forum](https://forum.effectivealtruism.org/posts/9RZodyypnWEtErFRM/law-following-ai-1-sequence-introduction-and-structure).*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/NrtbF3JHFnqBCztXC/law-following-ai-1-sequence-introduction-and-structure"
    ],
    "tags": [
      "ai",
      "ai governance",
      "law and legal systems"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3661f02bc5bd97f2",
    "title": "Law-Following AI 2: Intent Alignment + Superintelligence ? Lawless AI (By Default)",
    "year": 2022,
    "category": "policy_development",
    "description": "*This post is written in my personal capacity, and does not necessarily represent the views of OpenAI or any other organization. Cross-posted to the [Effective Altruism Forum](https://forum.effectivealtruism.org/s/3pyRzRQmcJNvHzf6J/p/cEj7o9rbPjmy7CDht).*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/9aSi7koXHCakb82Fz/law-following-ai-2-intent-alignment-superintelligence"
    ],
    "tags": [
      "ai",
      "ai governance",
      "law and legal systems"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_74bcf969ab264d29",
    "title": "Law-Following AI 3: Lawless AI Agents Undermine Stabilizing Agreements",
    "year": 2022,
    "category": "policy_development",
    "description": "*This post is written in my personal capacity, and does not necessarily represent the views of OpenAI or any other organization. Cross-posted to the [Effective Altruism Forum](https://forum.effectivealtruism.org/posts/ExHkFcNAL9cjqFmsF/law-following-ai-3-lawless-ai-agents-undermine-stabilizing)*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/DfcXaGH7XGYjW22C2/law-following-ai-3-lawless-ai-agents-undermine-stabilizing"
    ],
    "tags": [
      "ai",
      "ai governance",
      "law and legal systems"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f6ad2b95ec7ff1b3",
    "title": "The Speed + Simplicity Prior is probably anti-deceptive",
    "year": 2022,
    "category": "policy_development",
    "description": "*Thanks to Evan Hubinger for the extensive conversations that this post is based on, and for reviewing a draft.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/KSWSkxXJqWGd5jYLB/the-speed-simplicity-prior-is-probably-anti-deceptive"
    ],
    "tags": [
      "ai",
      "deception",
      "mesa-optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_28c50c4252cb8b26",
    "title": "Prize for Alignment Research Tasks",
    "year": 2022,
    "category": "policy_development",
    "description": "Can AI systems substantially help with alignment research before transformative AI? People disagree.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/XLx3mpdi7HSp4rytF/prize-for-alignment-research-tasks"
    ],
    "tags": [
      "ai",
      "ai-assisted/ai automated alignment",
      "bounties & prizes (active)",
      "ought"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f31244aa7efdb605",
    "title": "Learning the smooth prior",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Most of this document is composed of thoughts from Geoffrey Irving (safety researcher at DeepMind) written on January 15th, 2021 on [Learning the Prior](https://www.alignmentforum.org/posts/SL9mKhgdmDKXmxwE4/learning-the-prior)/[Imitative Generalization](https://www.alignmentforum.org/posts/JKj5Krff5oKMb8TjT/imitative-generalisation-aka-learning-the-prior-1), [cross-examination](https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1), and [AI safety via debate](https://arxiv.org/abs/1805.00899), plus some discussion between Geoffrey and Rohin and some extra commentary from me at the end. - Evan*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/9x5mtYjHYfr4T7KLj/learning-the-smooth-prior"
    ],
    "tags": [
      "ai",
      "debate (ai safety technique)",
      "market making (ai safety technique)",
      "outer alignment"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_caee676f4de2cb93",
    "title": "Introducing the ML Safety Scholars Program",
    "year": 2022,
    "category": "public_awareness",
    "description": "Program Overview\n----------------",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/CphfDP4ynz3QQ4AKY/introducing-the-ml-safety-scholars-program"
    ],
    "tags": [
      "ai",
      "community"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c8eefb6cc3a2d26f",
    "title": "High-stakes alignment via adversarial training [Redwood Research report]",
    "year": 2022,
    "category": "policy_development",
    "description": "***(Update: We think the tone of this post was overly positive considering our somewhat weak results. You can read our latest post with more takeaways and followup results*** [***here***](https://www.alignmentforum.org/posts/n3LAgnHg6ashQK3fF/takeaways-from-our-robust-injury-classifier-project-redwood)***.)***",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/A9tJFJY7DsGTFKKkh/high-stakes-alignment-via-adversarial-training-redwood"
    ],
    "tags": [
      "adversarial examples",
      "ai",
      "inner alignment",
      "redwood research"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_50963ff846fd21e4",
    "title": "Apply to the second iteration of the ML for Alignment Bootcamp (MLAB 2) in Berkeley [Aug 15 - Fri Sept 2]",
    "year": 2022,
    "category": "policy_development",
    "description": "Redwood Research is running another iteration of MLAB, our bootcamp aimed at helping people who are interested in AI alignment learn about machine learning, with a focus on ML skills and concepts that are relevant to doing the kinds of alignment research that we think seem most leveraged for reducing AI x-risk.  We co-organized the last iteration of the bootcamp with Lightcone in January, and there were 28 participants. The program was rated highly (see below for more), and several participants are now working full-time on alignment. We expect to start on Aug 15 but might push it back or forward by a week depending on applicant availability.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/3ouxBRRzjxarTukMW/apply-to-the-second-iteration-of-the-ml-for-alignment"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_93012c36052288ae",
    "title": "Open Problems in Negative Side Effect Minimization",
    "year": 2022,
    "category": "policy_development",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pnAxcABq9GBDG5BNW/open-problems-in-negative-side-effect-minimization"
    ],
    "tags": [
      "ai",
      "ai safety camp",
      "impact regularization",
      "open problems",
      "reinforcement learning"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2a5794e1ea93b2b3",
    "title": "The case for becoming a black-box investigator of language models",
    "year": 2022,
    "category": "public_awareness",
    "description": "Interpretability research is sometimes described as neuroscience for ML models. Neuroscience is one approach to understanding how human brains work. But empirical psychology research is another approach. I think more people should engage in the analogous activity for language models: trying to figure out how they work just by looking at their behavior, rather than trying to understand their internals.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/yGaw4NqRha8hgx5ny/the-case-for-becoming-a-black-box-investigator-of-language"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)",
      "prompt engineering"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_71f7e906aae63031",
    "title": "Elementary Infra-Bayesianism",
    "year": 2022,
    "category": "public_awareness",
    "description": "*TL;DR: I got nerd-sniped into working through some rather technical work in AI Safety. Here's my best guess of what is going on. Imprecise probabilities for handling catastrophic downside risk.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/9uj2Mto9CNdWZudyq/elementary-infra-bayesianism"
    ],
    "tags": [
      "infra-bayesianism",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ab6c549e4e9160d9",
    "title": "Updating Utility Functions",
    "year": 2022,
    "category": "public_awareness",
    "description": "This post will be about AIs that \"refine\" their utility function over time, and how it might be possible to construct such systems without giving them undesirable properties. The discussion relates to [corrigibility](https://arbital.com/p/corrigibility/), [value learning](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc), and (to a lesser extent) [wireheading](https://www.lesswrong.com/posts/vXzM5L6njDZSf4Ftk/defining-ai-wireheading).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/NjYdGP59Krhie4WBp/updating-utility-functions"
    ],
    "tags": [
      "ai",
      "convergence (org)",
      "corrigibility",
      "outer alignment",
      "the pointers problem",
      "utility functions"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1a076c9e9a8d1e6f",
    "title": "Jobs: Help scale up LM alignment research at NYU",
    "year": 2022,
    "category": "public_awareness",
    "description": "[NYU is hiring alignment-interested researchers!](https://docs.google.com/document/u/0/d/1r-ZsBQK07OHohLa9c9qCoLHZVV80EvnIOfpsdfVk0eg/edit?fromCopy=true)",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/5MHxjWgwWEoMrPXj8/jobs-help-scale-up-lm-alignment-research-at-nyu"
    ],
    "tags": [
      "ai",
      "community"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0899f3f56eed9eee",
    "title": "Introduction to Pragmatic AI Safety [Pragmatic AI Safety #1]",
    "year": 2022,
    "category": "policy_development",
    "description": "*This is the introduction to a sequence of posts that describe our models for Pragmatic AI Safety. Thanks to Oliver Zhang, Mantas Mazeika, Scott Emmons, Neel Nanda, Cameron Berg, Michael Chen, Vael Gates, Joe Kwon, Jacob Steinhardt, Steven Basart, and Jacob Hilton for feedback on this sequence (note: acknowledgements here may be updated as more reviewers are added to future posts).*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/bffA9WC9nEJhtagQi/introduction-to-pragmatic-ai-safety-pragmatic-ai-safety-1"
    ],
    "tags": [
      "ai",
      "emergent behavior ( emergence )"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_335dd68cf5f59664",
    "title": "A Bird's Eye View of the ML Field [Pragmatic AI Safety #2]",
    "year": 2022,
    "category": "policy_development",
    "description": "*This is the second post in* [*a sequence of posts*](https://www.alignmentforum.org/posts/bffA9WC9nEJhtagQi/introduction-to-pragmatic-ai-safety-pragmatic-ai-safety-1) *that describe our models for Pragmatic AI Safety.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/AtfQFj8umeyBBkkxa/a-bird-s-eye-view-of-the-ml-field-pragmatic-ai-safety-2"
    ],
    "tags": [
      "ai",
      "emergent behavior ( emergence )",
      "machine learning  (ml)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9ff3b9cf2addde5d",
    "title": "The limits of AI safety via debate",
    "year": 2022,
    "category": "public_awareness",
    "description": "The limits of AI safety via debate",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/kguLeJTt6LnGuYX4E/the-limits-of-ai-safety-via-debate"
    ],
    "tags": [
      "ai",
      "debate (ai safety technique)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3454e6614bf89839",
    "title": "[Intro to brain-like-AGI safety] 14. Controlled AGI",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Part of the* [*\"Intro to brain-like-AGI safety\" post series*](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/QpHewJvZJFaQYuLwH/intro-to-brain-like-agi-safety-14-controlled-agi"
    ],
    "tags": [
      "ai",
      "conservatism (ai)",
      "corrigibility",
      "has diagram"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_194f38b20c7386c4",
    "title": "Deepmind's Gato: Generalist Agent",
    "year": 2022,
    "category": "policy_development",
    "description": "[From the abstract](https://www.deepmind.com/publications/a-generalist-agent), emphasis mine:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/xxvKhjpcTAJwvtbWM/deepmind-s-gato-generalist-agent"
    ],
    "tags": [
      "ai",
      "ai timelines",
      "deepmind"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_538b9b748abe64b0",
    "title": "Introduction to the sequence: Interpretability Research for the Most Important Century",
    "year": 2022,
    "category": "public_awareness",
    "description": "This is the first post in a sequence exploring the argument that interpretability is a high-leverage research activity for solving the AI alignment problem.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/MygKP4iwdRL24eNsY/introduction-to-the-sequence-interpretability-research-for-1"
    ],
    "tags": [
      "ai",
      "ai success models",
      "interpretability (ml & ai)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9627c97065d81191",
    "title": "Interpretability's Alignment-Solving Potential: Analysis of 7 Scenarios",
    "year": 2022,
    "category": "policy_development",
    "description": "*This is the second post in the sequence \"Interpretability Research for the Most Important Century\". The first post, which introduces the sequence, defines several terms, and provides a comparison to existing works, can be found here:* [*Introduction to the sequence: Interpretability Research for the Most Important Century*](https://www.alignmentforum.org/posts/MygKP4iwdRL24eNsY/introduction-to-the-sequence-interpretability-research-for-1#Context_on_the_question_and_why_it_matters)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FrFZjkdRsmsbnQEm8/interpretability-s-alignment-solving-potential-analysis-of-7"
    ],
    "tags": [
      "ai",
      "ai risk",
      "ai success models",
      "debate (ai safety technique)",
      "eliciting latent knowledge (elk)",
      "inner alignment",
      "interpretability (ml & ai)",
      "iterated amplification",
      "market making (ai safety technique)",
      "myopia"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7d2c4c43fde53c4f",
    "title": "DeepMind is hiring for the Scalable Alignment and Alignment Teams",
    "year": 2022,
    "category": "policy_development",
    "description": "We are hiring for several roles in the Scalable Alignment and Alignment Teams at DeepMind, two of the subteams of DeepMind Technical AGI Safety trying to make artificial general intelligence go well.  In brief,",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/nzmCvRvPm4xJuqztv/deepmind-is-hiring-for-the-scalable-alignment-and-alignment"
    ],
    "tags": [
      "ai",
      "deepmind"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_744f58f340e54e5e",
    "title": "An observation about Hubinger et al.'s framework for learned optimization",
    "year": 2022,
    "category": "policy_development",
    "description": "The observations I make here have little consequence from the point of view of solving the alignment problem. If anything, they merely highlight the essential nature of the inner alignment problem. I will reject the idea that robust alignment, in the sense described in *Risks From Learned Optimization,* is possible at all. And I therefore also reject the related idea of 'internalization of the base objective', i.e. I do not think it is possible for a mesa-objective to \"agree\" with a base-objective or for a mesa-objective function to be \"adjusted towards the base objective function to the point where it is robustly aligned.\" I claim that whenever a learned algorithm is performing optimization, one needs to accept that an objective which one did not explicitly design is being pursued. At present, I refrain from attempting to propose my own adjustments to the framework, or to build on the existing literature or to develop my own theory. I am certainly not against doing any of those thi...",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/rAhJrdxjsXcngn3ip/an-observation-about-hubinger-et-al-s-framework-for-learned"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4bbe665bd64e3b5d",
    "title": "Agency As a Natural Abstraction",
    "year": 2022,
    "category": "policy_development",
    "description": "**Epistemic status:** Speculative attempt to synthesize findings from several distinct approaches to AI theory.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/n2urKnXbevj2ryvGY/agency-as-a-natural-abstraction"
    ],
    "tags": [
      "abstraction",
      "ai",
      "ai risk",
      "mesa-optimization",
      "natural abstraction"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4531836302574af0",
    "title": "Alignment as Constraints",
    "year": 2022,
    "category": "public_awareness",
    "description": "In order to find the most promising alignment research directions to pour resources into, we can go about it 3 ways",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/7GGRmAyMzqzidmBbi/alignment-as-constraints"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_de8a66243addc3e3",
    "title": "Frame for Take-Off Speeds to inform compute governance & scaling alignment",
    "year": 2022,
    "category": "policy_development",
    "description": "![](https://lh3.googleusercontent.com/tdHe9MrB4dj6v2tn6P9E0wKcd9Q-6t_FO2-0W1SyUJdOkPpBeilbAUjE0hx58XCrvNXfs73O7STdzN5Wfrujl4z5EOPsuamEBOuCj90yklG_nGSSwaATGsxCLuR_4r54ikUkhJ9eRGnHNSII_g)Figure 1: Something happens at future time t' that causes more resources to be poured into alignmentThe argument goes: there will be a time in the future, t', where e.g. a terrible AI accident occurs, alignment failures are documented (e.g. partial deception), or the majority of GDP is AI such that more people are pouring resources into aligning AI. Potentially to the point that >90% of alignment resources will be used in the years before x-catastrophe or a pivotal act (Figure 2)",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/XFL3vaA69mHxATWM7/frame-for-take-off-speeds-to-inform-compute-governance-and"
    ],
    "tags": [
      "ai",
      "ai takeoff"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7c44ebe3a82149e7",
    "title": "Clarifying the confusion around inner alignment",
    "year": 2022,
    "category": "policy_development",
    "description": "*Note 1: This article was written for the* [*EA UC Berkeley Distillation Contest*](https://eaberkeley.com/aims-distillation)*, and is also my capstone project for the* [*AGISF course*](https://www.eacambridge.org/agi-safety-fundamentals)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/xdtNd8xCdzpgfnGme/clarifying-the-confusion-around-inner-alignment"
    ],
    "tags": [
      "ai",
      "inner alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_6b30dd1ce01a0dd3",
    "title": "Proxy misspecification and the capabilities vs. value learning race",
    "year": 2022,
    "category": "technical_research_breakthrough",
    "description": "G Gordon Worley III [recently complained](https://www.alignmentforum.org/posts/sSpu2EABtTTDmBZ6T/g-gordon-worley-iii-s-shortform?commentId=tbj8R6K2q9rfhjtS7) about a lack of precision in discussions about whether Goodhart's Law will present a fatal problem for alignment in practice. After attending a talk in which Dylan Hadfield-Menell[[1]](#fntkb71ztt8d) presented the \"Goodhart's Law will be a big deal\" perspective, I came away with a relatively concrete formulation of where I disagree. In this post I'll try to explain my model for this, expanding on my short comment [here](https://www.lesswrong.com/posts/sSpu2EABtTTDmBZ6T/g-gordon-worley-iii-s-shortform?commentId=s4qu4hNykRmv8hALo).",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/tWpgtjRm9qwzxAZEi/proxy-misspecification-and-the-capabilities-vs-value"
    ],
    "tags": [
      "ai",
      "goodhart's law"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_fb49c8713c399e9c",
    "title": "[Intro to brain-like-AGI safety] 15. Conclusion: Open problems, how to help, AMA",
    "year": 2022,
    "category": "policy_development",
    "description": "15.1 Post summary / Table of contents\n=====================================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/tj8AC3vhTnBywdZoA/intro-to-brain-like-agi-safety-15-conclusion-open-problems-1"
    ],
    "tags": [
      "ai",
      "open problems"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f8f4ae1ea6ca8963",
    "title": "Actionable-guidance and roadmap recommendations for the NIST AI Risk Management Framework",
    "year": 2022,
    "category": "policy_development",
    "description": "*Updated 13 September 2022 with a link to our arXiv paper and corrections to out-of-date items*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/JNqXyEuKM4wbFZzpL/actionable-guidance-and-roadmap-recommendations-for-the-nist-1"
    ],
    "tags": [
      "ai",
      "ai governance",
      "ai risk",
      "community"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_da54f117676edbcc",
    "title": "Gato's Generalisation: Predictions and Experiments I'd Like to See",
    "year": 2022,
    "category": "policy_development",
    "description": "*I'm deliberately inhabiting a devil's advocate mindset because that perspective seems to be missing from the conversations I've witnessed. My actual fully-reflective median takeaway might differ.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/LnnMPNHEpqtaqonCM/gato-s-generalisation-predictions-and-experiments-i-d-like"
    ],
    "tags": [
      "agency",
      "ai",
      "ai capabilities",
      "ai timelines",
      "deepmind",
      "general intelligence"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_76e00c16673d9f05",
    "title": "How to get into AI safety research",
    "year": 2022,
    "category": "public_awareness",
    "description": "Recently, I had a conversation with someone from a math background, asking how they could get into AI safety research. Based on my own path from mathematics to AI alignment, I recommended the following sources. It may prove useful to others contemplating a similar change in career:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/WFnyxSL543c9bxGMm/how-to-get-into-ai-safety-research"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c33d8de28a3ee484",
    "title": "How RL Agents Behave When Their Actions Are Modified? [Distillation post]",
    "year": 2022,
    "category": "policy_development",
    "description": "Research publication: How RL Agents Behave When Their Actions Are Modified? [Distillation post]",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FeY4tXMYdTQSM4go3/how-rl-agents-behave-when-their-actions-are-modified"
    ],
    "tags": [
      "ai",
      "causality",
      "corrigibility",
      "distillation & pedagogy"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_387e01631b10ca26",
    "title": "Adversarial attacks and optimal control",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Meta: After a fun little motivating section, this post goes deep into the mathematical weeds. This thing aims to explore the mathematical properties of adversarial attacks from first principles. Perhaps other people are not as confused about this point as I was, but hopefully, the arguments are still useful and/or interesting to some. I'd be curious to hear if I'm reinventing the wheel.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/KtCJNw93KHg7MSSvw/adversarial-attacks-and-optimal-control"
    ],
    "tags": [
      "adversarial examples",
      "ai",
      "existential risk",
      "optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1cb8cfc11c9a75c5",
    "title": "AXRP Episode 15 - Natural Abstractions with John Wentworth",
    "year": 2022,
    "category": "policy_development",
    "description": "[Google Podcasts link](https://podcasts.google.com/feed/aHR0cHM6Ly9heHJwb2RjYXN0LmxpYnN5bi5jb20vcnNz/episode/MmJkNTgzMWEtZDFkMS00MWEwLWE4NzctMzg3OTcxMGVlYjZj)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/L896Fp8hLSbh8Ryei/axrp-episode-15-natural-abstractions-with-john-wentworth"
    ],
    "tags": [
      "abstraction",
      "agent foundations",
      "ai",
      "audio",
      "axrp",
      "interviews",
      "natural abstraction",
      "selection theorems"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_34ad7f86a999f44a",
    "title": "Complex Systems for AI Safety [Pragmatic AI Safety #3]",
    "year": 2022,
    "category": "policy_development",
    "description": "*This is the third post in* [*a sequence of posts*](https://www.alignmentforum.org/posts/bffA9WC9nEJhtagQi/introduction-to-pragmatic-ai-safety-pragmatic-ai-safety-1) *that describe our models for Pragmatic AI Safety.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/n767Q8HqbrteaPA25/complex-systems-for-ai-safety-pragmatic-ai-safety-3"
    ],
    "tags": [
      "ai",
      "ai risk"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_027507efc5639b69",
    "title": "The No Free Lunch theorems and their Razor",
    "year": 2022,
    "category": "technical_research_breakthrough",
    "description": "The No Free Lunch (NFL) family of theorems contains some of the most\nmisunderstood theorems of machine learning. They apply to learning[[1]](#fn-jLW5DsyqFrCmnZQv8-1) and\noptimization[[2]](#fn-jLW5DsyqFrCmnZQv8-2) and, in rough terms, they state:",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/sdrCBWpqyvNBJSZH5/the-no-free-lunch-theorems-and-their-razor"
    ],
    "tags": [
      "ai",
      "machine learning  (ml)",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_793adefe7a1eb5c7",
    "title": "autonomy: the missing AGI ingredient?",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Epistemic status: trying to feel out the shape of a concept and give it an appropriate name.  Trying to make explicit some things that I think exist implicitly in many people's minds. This post makes truth claims, but its main goal is to not to convince you that they are true.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/HSETWwdJnb45jsvT8/autonomy-the-missing-agi-ingredient"
    ],
    "tags": [
      "ai",
      "autonomy and choice"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9293c270df2306e2",
    "title": "RL with KL penalties is better seen as Bayesian inference",
    "year": 2022,
    "category": "policy_development",
    "description": "*This blog post is largely based on an [EMNLP paper](https://arxiv.org/abs/2205.11275) with Ethan Perez and Chris Buckley. It also benefited from discussions with and comments from Hady Elsahar, German Kruszewski, Marc Dymetman and Jeremy Scheurer.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/eoHbneGvqDu25Hasc/rl-with-kl-penalties-is-better-seen-as-bayesian-inference"
    ],
    "tags": [
      "ai",
      "bayes' theorem",
      "gpt",
      "language models",
      "outer alignment",
      "reinforcement learning"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ef1be1c5fe371110",
    "title": "The Problem With The Current State of AGI Definitions",
    "year": 2022,
    "category": "public_awareness",
    "description": "*The following includes a fictionalized account of a conversation had with professor* [*Viliam Lisy*](https://www.linkedin.com/in/viliam-lis%C3%BD-03801121/) *at* [*EAGx Prague*](https://www.eaglobal.org/events/eagxprague-2022/)*, with most of the details just plain made up because I forgot how it actually went. Special thanks to professor* [*Dusan D. Nesic*](https://www.linkedin.com/in/nesicdusan/)*, who I mistakenly thought I had this conversation with, and ended up providing useful feedback after a very confused discussion on WhatsApp. Credit also goes to Justis from LessWrong, who kindly provided some excellent feedback prior to publication. Any seemingly bad arguments presented are due to my flawed retelling, and are not Dusan's, Justis', or Viliam's.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/EpR5yTZMaJkDz4hhs/the-problem-with-the-current-state-of-agi-definitions"
    ],
    "tags": [
      "ai",
      "ai timelines",
      "definitions",
      "palm"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d508327264bdf440",
    "title": "Reshaping the AI Industry",
    "year": 2022,
    "category": "policy_development",
    "description": "The wider AI research community is an almost-optimal engine of apocalypse. The primary metric of a paper's success is how much it improves capabilities along concrete metrics, publish-or-perish dynamics supercharge that, the safety side of things is neglected to the tune of [1:49 rate of safety to other research](https://www.lesswrong.com/posts/AtfQFj8umeyBBkkxa/a-bird-s-eye-view-of-the-ml-field-pragmatic-ai-safety-2#Nearly_all_ML_research_is_unrelated_to_safety), and most results are made public so as to give everyone else in the world a fair shot at ending it too.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/mF8dkhZF9hAuLHXaD/reshaping-the-ai-industry"
    ],
    "tags": [
      "ai",
      "ai alignment fieldbuilding",
      "ai governance",
      "ai risk",
      "world optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_36502e8e71e35805",
    "title": "Six Dimensions of Operational Adequacy in AGI Projects",
    "year": 2022,
    "category": "policy_development",
    "description": "|  |\n| --- |\n| **Editor's note:** The following is a lightly edited copy of a document written by Eliezer Yudkowsky in November 2017. Since this is a snapshot of Eliezer's thinking at a specific time, we've sprinkled reminders throughout that this is from 2017.A background note:It's often the case that people are slow to abandon obsolete playbooks in response to a novel challenge. And AGI is certainly a very novel challenge.Italian general Luigi Cadorna offers a memorable historical example. In the Isonzo Offensive of World War I, Cadorna lost hundreds of thousands of men in futile frontal assaults against enemy trenches defended by barbed wire and machine guns.  As morale plummeted and desertions became epidemic, Cadorna began executing his own soldiers en masse, in an attempt to cure the rest of their \"cowardice.\" The offensive continued for *2.5 years*.Cadorna made many mistakes, but foremost among them was his refusal to recognize that this war was fundamentally unlike those tha...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects"
    ],
    "tags": [
      "ai",
      "ai governance",
      "organizational culture & design",
      "security mindset"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5fbbfaf6e8cc2dda",
    "title": "Perform Tractable Research While Avoiding Capabilities Externalities [Pragmatic AI Safety #4]",
    "year": 2022,
    "category": "technical_research_breakthrough",
    "description": "*This is the fourth post in* [*a sequence of posts*](https://www.alignmentforum.org/posts/bffA9WC9nEJhtagQi/introduction-to-pragmatic-ai-safety-pragmatic-ai-safety-1) *that describe our models for Pragmatic AI Safety.*",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/dfRtxWcFDupfWpLQo/perform-tractable-research-while-avoiding-capabilities"
    ],
    "tags": [
      "ai",
      "ai risk",
      "emergent behavior ( emergence )"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d285610e7c6d7e95",
    "title": "Paper: Teaching GPT3 to express uncertainty in words",
    "year": 2022,
    "category": "public_awareness",
    "description": "**Paper Authors:** Stephanie Lin (FHI, Oxford), [Jacob Hilton](https://www.lesswrong.com/users/jacob_hilton) (OpenAI), Owain Evans",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/vbfAwZqKs84agyGWC/paper-teaching-gpt3-to-express-uncertainty-in-words"
    ],
    "tags": [
      "ai",
      "calibration",
      "gpt",
      "honesty",
      "language models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_6c64a132b9aa65d0",
    "title": "Paradigms of AI alignment: components and enablers",
    "year": 2022,
    "category": "policy_development",
    "description": "*(Cross-posted from my* [*personal blog*](https://vkrakovna.wordpress.com/2022/06/02/paradigms-of-ai-alignment-components-and-enablers/)*. This post is based on an overview talk I gave at UCL EA and Oxford AI society (*[*recording here*](https://drive.google.com/file/d/1DXSum8dVnvmFCLGjLoz4Zmgb_l-KJkj-/view)*).* *Thanks to Janos Kramar for detailed feedback on this post and to Rohin Shah for feedback on the talk.)*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/JC7aJZjt2WvxxffGz/paradigms-of-ai-alignment-components-and-enablers"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7561f537b7801512",
    "title": "Confused why a \"capabilities research is good for alignment progress\" position isn't discussed more",
    "year": 2022,
    "category": "public_awareness",
    "description": "The predominant view on LW seems to be \"pure AI capabilities research is bad, because capabilities progress alone doesn't contribute to alignment progress, and capabilities progress without alignment progress means that we're doomed\".",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/EzAt4SbtQcXtDNhHK/confused-why-a-capabilities-research-is-good-for-alignment"
    ],
    "tags": [
      "ai",
      "ai risk",
      "outer alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cc6371f74b7d20fa",
    "title": "The prototypical catastrophic AI action is getting root access to its datacenter",
    "year": 2022,
    "category": "public_awareness",
    "description": "(I think Carl Shulman came up with the \"hacking the SSH server\" example, thanks to him for that. Thanks to Ryan Greenblatt, Jenny Nitishinskaya, and Ajeya Cotra for comments.)",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/BAzCGCys4BkzGDCWR/the-prototypical-catastrophic-ai-action-is-getting-root"
    ],
    "tags": [
      "ai",
      "existential risk"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_28ed16e008af5f70",
    "title": "Adversarial training, importance sampling, and anti-adversarial training for AI whistleblowing",
    "year": 2022,
    "category": "public_awareness",
    "description": "(Thanks to Ajeya Cotra and Ryan Greenblatt for comments.)",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/EFrsvnF6uZieZr3uG/adversarial-training-importance-sampling-and-anti"
    ],
    "tags": [
      "adversarial training",
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_111e80093c0d9a28",
    "title": "[MLSN #4]: Many New Interpretability Papers, Virtual Logit Matching, Rationalization Helps Robustness",
    "year": 2022,
    "category": "public_awareness",
    "description": "As part of a larger community building effort, I am writing a safety newsletter which is designed to cover empirical safety research and be palatable to the broader machine learning research community. You can [subscribe here](https://newsletter.mlsafety.org/), follow the newsletter on [twitter](https://twitter.com/ml_safety) here, or join the subreddit [here](https://www.reddit.com/r/mlsafety/).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/R39tGLeETfCZJ4FoE/mlsn-4-many-new-interpretability-papers-virtual-logit"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_013b2d33d1ecb284",
    "title": "Announcing the Alignment of Complex Systems Research Group",
    "year": 2022,
    "category": "policy_development",
    "description": "***tl;dr:** We're a new alignment research group based at Charles University, Prague. If you're interested in conceptual work on agency and the intersection of complex systems and AI alignment,* [*we want to hear from you*](https://humanalignedai.typeform.com/to/qrmnvRX7)*. Ideal for those who prefer an academic setting in Europe.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/H5iGhDhQBtoDpCBZ2/announcing-the-alignment-of-complex-systems-research-group"
    ],
    "tags": [
      "ai",
      "boundaries / membranes [technical]",
      "coordination / cooperation",
      "internal alignment (human)",
      "outer alignment",
      "research agendas",
      "subagents"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_16c2c71fedd15764",
    "title": "Deep Learning Systems Are Not Less Interpretable Than Logic/Probability/Etc",
    "year": 2022,
    "category": "public_awareness",
    "description": "There's a common perception that various non-deep-learning ML paradigms - like logic, probability, causality, etc - are very interpretable, whereas neural nets aren't. I claim this is wrong.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/gebzzEwn2TaA6rGkc/deep-learning-systems-are-not-less-interpretable-than-logic"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_06dad18a48c4a7d9",
    "title": "AGI Ruin: A List of Lethalities",
    "year": 2022,
    "category": "technical_research_breakthrough",
    "description": "Research publication: AGI Ruin: A List of Lethalities",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"
    ],
    "tags": [
      "ai",
      "ai risk",
      "threat models"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7112cbcd07b8c666",
    "title": "Epistemological Vigilance for Alignment",
    "year": 2022,
    "category": "public_awareness",
    "description": "*This post is part of the work done at* [*Conjecture*](https://conjecture.dev)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/72scWeZRta2ApsKja/epistemological-vigilance-for-alignment"
    ],
    "tags": [
      "ai risk",
      "conjecture (org)",
      "epistemology",
      "intellectual progress (society-level)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a9d7feb3d7eefa07",
    "title": "Why agents are powerful",
    "year": 2022,
    "category": "public_awareness",
    "description": "*[Written for Blog Post Day. Not super happy with it, it's too rambly and long, but I'm glad it exists.]*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/D5AzsRbRxZeqGuAZ4/why-agents-are-powerful"
    ],
    "tags": [
      "agency",
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_25ba01358b137ddc",
    "title": "Some ideas for follow-up projects to Redwood Research's recent paper",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Disclaimer: I originally wrote this list for myself and then decided it might be worth sharing. Very unpolished, not worth reading for most people. Some (many?) of these ideas were suggested in the paper already, I don't make any claims of novelty.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/7fBKErNKhtwB4nt4N/some-ideas-for-follow-up-projects-to-redwood-research-s"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8cbaad4d2cc7ae30",
    "title": "Reading the ethicists 2: Hunting for AI alignment papers",
    "year": 2022,
    "category": "policy_development",
    "description": "Introduction\n============",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/6DwprCdC7eErCRZkx/reading-the-ethicists-2-hunting-for-ai-alignment-papers"
    ],
    "tags": [
      "ai",
      "ethics & morality",
      "world optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5ae789eb8d7aaae3",
    "title": "Grokking \"Forecasting TAI with biological anchors\"",
    "year": 2022,
    "category": "policy_development",
    "description": "Research publication: Grokking \"Forecasting TAI with biological anchors\"",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/wgio8E758y9XWsi8j/grokking-forecasting-tai-with-biological-anchors"
    ],
    "tags": [
      "ai",
      "ai timelines"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f61d2ed27fabbff2",
    "title": "A descriptive, not prescriptive, overview of current AI Alignment Research",
    "year": 2022,
    "category": "policy_development",
    "description": "*TL;DR: In this project, we collected and cataloged AI alignment research literature and analyzed the resulting dataset in an unbiased way to identify major research directions. We found that the field is growing quickly, with several subfields emerging in parallel. We looked at the subfields and identified the prominent researchers, recurring topics, and different modes of communication in each. Furthermore, we found that a classifier trained on AI alignment research articles can detect relevant articles that we did not originally include in the dataset.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FgjcHiWvADgsocE34/a-descriptive-not-prescriptive-overview-of-current-ai"
    ],
    "tags": [
      "ai",
      "ai safety camp"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cb6ecbd5d316242e",
    "title": "Who models the models that model models? An exploration of GPT-3's in-context model fitting ability",
    "year": 2022,
    "category": "technical_research_breakthrough",
    "description": "Introduction\n------------",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/c2RzFadrxkzyRAFXa/who-models-the-models-that-model-models-an-exploration-of"
    ],
    "tags": [
      "ai",
      "gpt",
      "language models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_93b885673b0fae78",
    "title": "Eliciting Latent Knowledge (ELK) - Distillation/Summary",
    "year": 2022,
    "category": "policy_development",
    "description": "This post was inspired by the [AI safety distillation contest](https://forum.effectivealtruism.org/posts/ei4pYFJKcbGAdGnNb/calling-for-student-submissions-ai-safety-distillation). It turned out to be more of a summary than a distillation for two reasons. Firstly, I think that the main idea behind ELK is simple and can be explained in less than 2 minutes (see next section). Therefore, the main value comes from understanding the specific approaches and how they interact with each other. Secondly, I think some people shy away from reading a 50-page report but I expect they could get most of the understanding from reading/skimming this summary (I'm aware that the summary is longer than anticipated but it's still a >5x reduction of the original content).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/rxoBY9CMkqDsHt25t/eliciting-latent-knowledge-elk-distillation-summary"
    ],
    "tags": [
      "ai",
      "eliciting latent knowledge (elk)",
      "research agendas"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4fb3295bb57c7551",
    "title": "How Do Selection Theorems Relate To Interpretability?",
    "year": 2022,
    "category": "public_awareness",
    "description": "One pretty major problem with today's interpretability methods (e.g. work by [Chris Olah & co](https://transformer-circuits.pub/2021/framework/index.html)) is that we have to redo a bunch of work whenever a new net comes out, and even more work when a new architecture or data modality comes along (like e.g. transformers and language models).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/A7QgKwWvAkuXonAy5/how-do-selection-theorems-relate-to-interpretability"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)",
      "selection theorems"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_16386b40c311e716",
    "title": "You Only Get One Shot: an Intuition Pump for Embedded Agency",
    "year": 2022,
    "category": "public_awareness",
    "description": "*This is a short attempt to articulate a framing which I sometimes find useful for thinking about [embedded agency](https://www.lesswrong.com/tag/embedded-agency). I noticed that I wanted to refer to it a few times in conversations and other writings.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/HiufALieNbWHqR9en/you-only-get-one-shot-an-intuition-pump-for-embedded-agency"
    ],
    "tags": [
      "ai",
      "embedded agency",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_89f4f8ec618aa4f0",
    "title": "why assume AGIs will optimize for fixed goals?",
    "year": 2022,
    "category": "public_awareness",
    "description": "When I read posts about AI alignment on LW / AF/ Arbital, I almost always find a particular bundle of assumptions taken for granted:",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/dKTh9Td3KaJ8QW6gw/why-assume-agis-will-optimize-for-fixed-goals"
    ],
    "tags": [
      "ai",
      "goal-directedness"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0fc982ab48364fad",
    "title": "Open Problems in AI X-Risk [PAIS #5]",
    "year": 2022,
    "category": "policy_development",
    "description": "*This is the fifth post in* [*a sequence of posts*](https://www.alignmentforum.org/posts/bffA9WC9nEJhtagQi/introduction-to-pragmatic-ai-safety-pragmatic-ai-safety-1) *that describe our models for Pragmatic AI Safety.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5"
    ],
    "tags": [
      "ai",
      "ai risk",
      "open problems"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_695600f7b7881d8e",
    "title": "Godzilla Strategies",
    "year": 2022,
    "category": "public_awareness",
    "description": "> Clutching a bottle of whiskey in one hand and a shotgun in the other, John scoured the research literature for ideas... He discovered several papers that described software-assisted hardware recovery. The basic idea was simple: if hardware suffers more transient failures as it gets smaller, why not allow software to detect erroneous computations and re-execute them? This idea seemed promising until John realized THAT IT WAS THE WORST IDEA EVER. Modern software barely works when the hardware is correct, so relying on software to correct hardware errors is like asking Godzilla to prevent Mega-Godzilla from terrorizing Japan. THIS DOES NOT LEAD TO RISING PROPERTY VALUES IN TOKYO. It's better to stop scaling your transistors and avoid playing with monsters in the first place, instead of devising an elaborate series of monster checks-and-balances and then hoping that the monsters don't do what monsters are always going to do because if they didn't do those things, they'd be called dand...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/DwqgLXn5qYC7GqExF/godzilla-strategies"
    ],
    "tags": [
      "ai",
      "ai-assisted/ai automated alignment"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_32e4738221031a1d",
    "title": "Training Trace Priors",
    "year": 2022,
    "category": "public_awareness",
    "description": "I'm worried about scenarios involving deceptive models. We've failed at inner alignment so the model has goals that are not aligned with ours. It can somehow detect when it's in training, and during training it pretends to share our goals. During deployment, surprise! The model paperclips the universe.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/hjrqXjEpaw9ogScPh/training-trace-priors"
    ],
    "tags": [
      "ai",
      "deception"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_21cad79d1af5b88c",
    "title": "Continuity Assumptions",
    "year": 2022,
    "category": "public_awareness",
    "description": "This post will try to explain what I mean by continuity assumptions (and discontinuity assumptions), and why differences in these are upstream of many disagreements about AI safety. None of this is particularly new, but seemed worth reiterating in the form of a short post.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/cHJxSJ4jBmBRGtbaE/continuity-assumptions"
    ],
    "tags": [
      "ai",
      "ai risk",
      "ai takeoff"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_81ff36b0418b5dc5",
    "title": "Investigating causal understanding in LLMs",
    "year": 2022,
    "category": "public_awareness",
    "description": "This is a project by [Marius Hobbhahn](https://www.mariushobbhahn.com/aboutme/) and [Tom Lieberum](https://tomfrederik.github.io/). David Seiler contributed ideas and guidance.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/yZb5eFvDoaqB337X5/investigating-causal-understanding-in-llms"
    ],
    "tags": [
      "ai",
      "causality",
      "gpt",
      "language models",
      "outer alignment"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d147bb63590c948a",
    "title": "A central AI alignment problem: capabilities generalization, and the sharp left turn",
    "year": 2022,
    "category": "public_awareness",
    "description": "(*This post was factored out of a larger post that I (Nate Soares) wrote, with help from Rob Bensinger, who also rearranged some pieces and added some text to smooth things out. I'm not terribly happy with it, but am posting it anyway (or, well, having Rob post it on my behalf while I travel) on the theory that it's better than nothing.*)",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization"
    ],
    "tags": [
      "ai",
      "sharp left turn",
      "threat models"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a02ee8969b2a78b7",
    "title": "Ten experiments in modularity, which we'd like you to run!",
    "year": 2022,
    "category": "public_awareness",
    "description": "This is the third post describing our team's work on selection theorems for modularity, as part of a project mentored by John Wentworth ([see](https://www.lesswrong.com/posts/JzTfKrgC7Lfz3zcwM/theories-of-modularity-in-the-biological-literature) [here](https://www.lesswrong.com/posts/XKwKJCXgSKhSr9bZY/project-intro-selection-theorems-for-modularity) for the earlier posts). Although the theoretical and empirical parts of the project have both been going very well, we're currently bottlenecked on the empirical side: we have several theories and ideas for how to test them, but few experimental results. Right now, we only have one empiricist coding up experiments, so this overhang seems likely to persist.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run"
    ],
    "tags": [
      "ai",
      "experiments",
      "modularity",
      "request post"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_6351fc310549fe3b",
    "title": "Breaking Down Goal-Directed Behaviour",
    "year": 2022,
    "category": "public_awareness",
    "description": "When we speak about entities 'wanting' things, or having '[goal-directed](https://www.alignmentforum.org/tag/goal-directedness) behaviour', what do we mean?",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/WhETfFgkfNSTShc4y/breaking-down-goal-directed-behaviour"
    ],
    "tags": [
      "abstraction",
      "goal-directedness",
      "optimization",
      "rationality",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_06a11b91b971b277",
    "title": "Humans are very reliable agents",
    "year": 2022,
    "category": "policy_development",
    "description": "*This post has been recorded as part of the LessWrong Curated Podcast, and an be listened to on* [*Spotify*](https://open.spotify.com/episode/7qxVJWIR9YJuzFH9bLaTae?si=c29f23a272a74fd3)*,* [*Apple Podcasts*](https://podcasts.apple.com/us/podcast/humans-are-very-reliable-agents-by-alyssa-vance/id1630783021?i=1000569826102)*, and* [*Libsyn*](https://sites.libsyn.com/421877/humans-are-very-reliable-agents-by-alyssa-vance)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/28zsuPaJpKAGSX4zq/humans-are-very-reliable-agents"
    ],
    "tags": [
      "ai",
      "autonomous vehicles",
      "robust agents"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_bb0fa1be996dff69",
    "title": "A transparency and interpretability tech tree",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Thanks to Chris Olah, Neel Nanda, Kate Woolverton, Richard Ngo, Buck Shlegeris, Daniel Kokotajlo, Kyle McDonell, Laria Reynolds, Eliezer Yudkowksy, Mark Xu, and James Lucassen for useful comments, conversations, and feedback that informed this post.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/nbq2bWLcYmSGup9aF/a-transparency-and-interpretability-tech-tree"
    ],
    "tags": [
      "ai",
      "auditing games",
      "interpretability (ml & ai)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f22887c056cf14db",
    "title": "wrapper-minds are the enemy",
    "year": 2022,
    "category": "public_awareness",
    "description": "This post is a follow-up to \"[why assume AGIs will optimize for fixed goals?](https://www.lesswrong.com/posts/dKTh9Td3KaJ8QW6gw/why-assume-agis-will-optimize-for-fixed-goals)\".  I'll assume you've read that one first.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Mrz2srZWc7EzbADSo/wrapper-minds-are-the-enemy"
    ],
    "tags": [
      "ai",
      "goal-directedness"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_121703b9ca9475f6",
    "title": "Quantifying General Intelligence",
    "year": 2022,
    "category": "public_awareness",
    "description": "Introduction\n============",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/a47XLgmX5ecWmxruY/quantifying-general-intelligence-2"
    ],
    "tags": [
      "ai",
      "general intelligence",
      "optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3b70d44482207323",
    "title": "Pivotal outcomes and pivotal processes",
    "year": 2022,
    "category": "policy_development",
    "description": "***tl;dr:** If you think humanity is on a dangerous path, and needs to \"pivot\" toward a different future in order to achieve safety, consider how such a pivot could be achieved by multiple acts across multiple persons and institutions, rather than a single act.  Engaging more actors in the process is more costly in terms of coordination, but in the end may be a more practicable social process involving less extreme risk-taking than a single \"pivotal act\".*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/etNJcXCsKC6izQQZj/pivotal-outcomes-and-pivotal-processes"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9ed57de9a5430f86",
    "title": "Where I agree and disagree with Eliezer",
    "year": 2022,
    "category": "technical_research_breakthrough",
    "description": "(*Partially in response to*[*AGI Ruin: A list of Lethalities*](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities). *Written in the same rambling style. Not exhaustive.*)",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer"
    ],
    "tags": [
      "ai",
      "ai risk",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_129332414ecb8f7c",
    "title": "Causal confusion as an argument against the scaling hypothesis",
    "year": 2022,
    "category": "policy_development",
    "description": "Research publication: Causal confusion as an argument against the scaling hypothesis",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FZL4ftXvcuKmmobmj/causal-confusion-as-an-argument-against-the-scaling"
    ],
    "tags": [
      "ai",
      "ai risk",
      "causality",
      "language models",
      "scaling laws"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cdc985f5c71b2747",
    "title": "Getting from an unaligned AGI to an aligned AGI?",
    "year": 2022,
    "category": "public_awareness",
    "description": "|  |\n| --- |\n| **Summary / Preamble**In [AGI Ruin: A List of Lethalities](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities), Eliezer writes *\"A cognitive system with sufficiently high cognitive powers, **given any medium-bandwidth channel of causal influence**, will not find it difficult to bootstrap to overpowering capabilities independent of human infrastructure.\"*I have larger error-bars than Eliezer on some AI-safety-related beliefs, but I share many of his concerns (thanks in large part to being influenced by his writings).In this series I will try to explore if we might:* Start out with a superintelligent AGI that may be unaligned (but seems superficially aligned)\n* Only use the AGI in ways where it's channels of causal influence are minimized (and where great steps are taken to make it hard for the AGI to hack itself out of the \"box\" it's in)\n* Work quickly but step-by-step towards a AGI-system that probably is aligned, enabling us to use it in...",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ZmZBataeY58anJRBb/getting-from-an-unaligned-agi-to-an-aligned-agi"
    ],
    "tags": [
      "ai",
      "ai boxing (containment)",
      "ai success models",
      "ai-assisted/ai automated alignment",
      "outer alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d485ded980288fcf",
    "title": "Reflection Mechanisms as an Alignment target: A survey",
    "year": 2022,
    "category": "policy_development",
    "description": "This is a product of the [2022 AI Safety Camp](https://aisafety.camp/). The project has been done by [Marius Hobbhahn](https://www.mariushobbhahn.com/aboutme/) and [Eric Landgrebe](https://www.linkedin.com/in/eric-landgrebe-959698169/) under the supervision of [Beth Barnes](https://www.barnes.page/). We would like to thank Jacy Reese Anthis and Tyna Eloundou for detailed feedback.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/XyBWkoaqfnuEyNWXi/reflection-mechanisms-as-an-alignment-target-a-survey-1"
    ],
    "tags": [
      "ai",
      "ai safety camp",
      "ethics & morality",
      "human values",
      "metaethics"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e9d21da9d603dc26",
    "title": "Updated Deference is not a strong argument against the utility uncertainty approach to alignment",
    "year": 2022,
    "category": "public_awareness",
    "description": "**Thesis:** The [problem of fully updated deference](https://arbital.com/p/updated_deference/) is not a strong argument against the viability of the assistance games / utility uncertainty approach to AI (outer) alignment.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ikYKHkffKNJvBygXG/updated-deference-is-not-a-strong-argument-against-the"
    ],
    "tags": [
      "ai",
      "moral uncertainty",
      "value learning"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_32a3573e4e729b83",
    "title": "AI-Written Critiques Help Humans Notice Flaws",
    "year": 2022,
    "category": "public_awareness",
    "description": "This is a linkpost for a recent paper from the OpenAI alignment team (disclaimer: I used to work with this team). They summarize their results as:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/AHBejZBsaTR6dkRHs/ai-written-critiques-help-humans-notice-flaws"
    ],
    "tags": [
      "ai",
      "debate (ai safety technique)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5bec6d10fc525529",
    "title": "Conditioning Generative Models",
    "year": 2022,
    "category": "technical_research_breakthrough",
    "description": "*This post was written in response to Evan Hubinger's shortform prompt below, and benefited from discussions with him.*",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/nXeLPcT9uhfG3TMPS/conditioning-generative-models"
    ],
    "tags": [
      "ai",
      "deception",
      "language models",
      "simulator theory"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_64a8aad2adacee2f",
    "title": "Training Trace Priors and Speed Priors",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Thanks to Evan Hubinger for suggesting this idea.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/9o2mjdo7eb7677EJS/training-trace-priors-and-speed-priors"
    ],
    "tags": [
      "ai",
      "deception"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1003d180561f5005",
    "title": "Announcing Epoch: A research organization investigating the road to Transformative AI",
    "year": 2022,
    "category": "policy_development",
    "description": "Research publication: Announcing Epoch: A research organization investigating the road to Transformative AI",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/AJ6GHm5n6fBRJbMhq/announcing-epoch-a-research-organization-investigating-the"
    ],
    "tags": [
      "ai",
      "ai governance",
      "ai takeoff",
      "ai timelines",
      "compute",
      "forecasting & prediction",
      "organization updates",
      "technological forecasting",
      "transformative ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9774390262000655",
    "title": "Announcing the Inverse Scaling Prize ($250k Prize Pool)",
    "year": 2022,
    "category": "public_awareness",
    "description": "*TL;DR*: We're launching the [Inverse Scaling Prize](https://twitter.com/EthanJPerez/status/1541454949397041154): a contest with $250k in prizes for finding zero/few-shot text tasks where larger language models show increasingly undesirable behavior (\"inverse scaling\"). We hypothesize that inverse scaling is often a sign of an alignment failure and that more examples of alignment failures would benefit empirical alignment research. We believe that this contest is an unusually concrete, tractable, and safety-relevant problem for engaging alignment newcomers and the broader ML community. This post will focus on the relevance of the contest and the inverse scaling framework to longer-term AGI alignment concerns. See our [GitHub repo](https://github.com/inverse-scaling/prize) for contest details, prizes we'll award, and task evaluation criteria.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/eqxqgFxymP8hXDTt5/announcing-the-inverse-scaling-prize-usd250k-prize-pool"
    ],
    "tags": [
      "ai",
      "bounties & prizes (active)",
      "community",
      "inner alignment",
      "language models",
      "outer alignment",
      "scaling laws"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a543b42f2036017c",
    "title": "Deliberation, Reactions, and Control: Tentative Definitions and a Restatement of Instrumental Convergence",
    "year": 2022,
    "category": "policy_development",
    "description": "*This analysis is speculative. The framing has been refined in conversation and private reflection and research. To some extent it feels vacuous, but at least valuable for further research and communication.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/u4BaLRK6mJJcvycEk/deliberation-reactions-and-control-tentative-definitions-and"
    ],
    "tags": [
      "abstraction",
      "ai",
      "embedded agency",
      "goal-directedness",
      "instrumental convergence",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_29df865ef2e08513",
    "title": "Deliberation Everywhere: Simple Examples",
    "year": 2022,
    "category": "policy_development",
    "description": "*The analysis and definitions used here are tentative. My familiarity with the concrete systems discussed ranges from rough understanding (markets and parliaments), through abiding amateur interest (biology), to meaningful professional expertise (AI/ML things). The abstractions and terminology have been refined in conversation and private reflection, and the following examples are both generators and products of this conceptual framework.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/5GzqD7fgtxjepPERp/deliberation-everywhere-simple-examples"
    ],
    "tags": [
      "adaptation executors",
      "ai",
      "evolution",
      "goal-directedness",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f139bc6f71c559aa",
    "title": "Exploring Mild Behaviour in Embedded Agents",
    "year": 2022,
    "category": "policy_development",
    "description": "*Thanks to Tristan Cook, Emery Cooper, Nicolas Mace and Will Payne for discussions, advice, and comments. All views are my own.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/vNS4vGKGD6ygyz5ok/exploring-mild-behaviour-in-embedded-agents"
    ],
    "tags": [
      "ai",
      "bounded rationality",
      "embedded agency",
      "mild optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_09103d818ed13f90",
    "title": "Some alternative AI safety research projects",
    "year": 2022,
    "category": "public_awareness",
    "description": "These are some \"alternative\" (in the sense of non-mainstream) research projects or questions related to AI safety that seem both relevant and underexplored. If instead you think they aren't, let me know in the comments, and feel free to use the ideas as you want if you find them interesting.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/DypLJKRcQKt9hcpBP/some-alternative-ai-safety-research-projects"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7430aa69585622cf",
    "title": "What success looks like",
    "year": 2022,
    "category": "policy_development",
    "description": "**TL;DR:** We wrote a post on possible success stories of a transition to TAI to better understand which factors causally reduce the risk of AI risk. Furthermore, we separately explain these catalysts for success in more detail and this post can thus be thought of as a high-level overview of different AI governance strategies.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/aKpqwtZN6ifAhqJYK/what-success-looks-like"
    ],
    "tags": [
      "ai",
      "ai governance",
      "ai risk concrete stories",
      "ai success models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_354499b8f8e716b6",
    "title": "Will Capabilities Generalise More?",
    "year": 2022,
    "category": "public_awareness",
    "description": "[Nate](https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization) and [Eliezer](https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities) (Lethality 21) claim that capabilities generalise further than alignment once capabilities start generalising far at all. However, they have not articulated particularly detailed arguments for why this is the case. In this post I collect the arguments for and against the position I have been able to find or generate, and develop them (with a few hours' effort). I invite you to join me in better understanding this claim and its veracity by contributing your own arguments and improving mine.  \n  \n*Thanks to these people for their help with writing and/or contributing arguments: Vikrant Varma, Vika Krakovna, Mary Phuong, Rory Grieg, Tim Genewein, Rohin Shah.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/cq5x4XDnLcBrYbb66/will-capabilities-generalise-more"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_726feabf032b80ca",
    "title": "Latent Adversarial Training",
    "year": 2022,
    "category": "public_awareness",
    "description": "The Problem\n===========",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training"
    ],
    "tags": [
      "adversarial training",
      "ai",
      "deception"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4279a39ba234fbfb",
    "title": "Gradient hacking: definitions and examples",
    "year": 2022,
    "category": "policy_development",
    "description": "Gradient hacking is a hypothesized phenomenon where:",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/EeAgytDZbDjRznPMA/gradient-hacking-definitions-and-examples"
    ],
    "tags": [
      "ai",
      "gradient hacking"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e891d8a2e92bd0c8",
    "title": "Formal Philosophy and Alignment Possible Projects",
    "year": 2022,
    "category": "public_awareness",
    "description": "Research publication: Formal Philosophy and Alignment Possible Projects",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/fgAyy4gdDrbHFHjge/formal-philosophy-and-alignment-possible-projects"
    ],
    "tags": [
      "ai",
      "pibbss"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_73568075d0536cf3",
    "title": "Safetywashing",
    "year": 2022,
    "category": "policy_development",
    "description": "In southern California there's a two-acre butterfly preserve owned by the oil company Chevron. They spend little to maintain it, but many millions on television advertisements featuring it as evidence of their environmental stewardship.[[1]](#fn9m6ozmbv1h)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/xhD6SHAAE9ghKZ9HS/safetywashing"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_408f10c4603133ef",
    "title": "What Is The True Name of Modularity?",
    "year": 2022,
    "category": "public_awareness",
    "description": "Research publication: What Is The True Name of Modularity?",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/TTTHwLpcewGjQHWzh/what-is-the-true-name-of-modularity"
    ],
    "tags": [
      "ai",
      "causality",
      "information theory",
      "modularity",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9e2df1433f6b6796",
    "title": "AXRP Episode 16 - Preparing for Debate AI with Geoffrey Irving",
    "year": 2022,
    "category": "public_awareness",
    "description": "[Google Podcasts link](https://podcasts.google.com/feed/aHR0cHM6Ly9heHJwb2RjYXN0LmxpYnN5bi5jb20vcnNz/episode/NjBiYTRkNTAtM2Q3Yy00NjdlLWI0MmUtZGM4ZTRkZDk5MDE3)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/bLr68nrLSwgzqLpzu/axrp-episode-16-preparing-for-debate-ai-with-geoffrey-irving"
    ],
    "tags": [
      "ai",
      "audio",
      "axrp",
      "debate (ai safety technique)",
      "interviews"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d06a3b1bbda695af",
    "title": "[Linkpost] Existential Risk Analysis in Empirical Research Papers",
    "year": 2022,
    "category": "public_awareness",
    "description": "I've noted before that it can be difficult to separate progress in safety from progress in capabilities. However, doing so is important, as we want to ensure that we are making differential progress on safety, rather than just advancing safety as a consequence of advancing capabilities. In particular, I think that research should rigorously evaluate trade-offs between improving safety and advancing capabilities.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/5rNCGP8deEBjedCmH/linkpost-existential-risk-analysis-in-empirical-research"
    ],
    "tags": [
      "ai",
      "ai risk",
      "existential risk"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_554a81242acc887c",
    "title": "Remaking EfficientZero (as best I can)",
    "year": 2022,
    "category": "policy_development",
    "description": "Introduction\n------------",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/bPa6AzRgGZGmxbq6n/remaking-efficientzero-as-best-i-can"
    ],
    "tags": [
      "ai",
      "efficientzero",
      "machine learning  (ml)",
      "reinforcement learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_efa7e7df9ebf421b",
    "title": "Benchmark for successful concept extrapolation/avoiding goal misgeneralization",
    "year": 2022,
    "category": "public_awareness",
    "description": "If an AI has been trained on data about adults, what should it do when it encounters a child? When an AI encounters a situation that its training data hasn't covered, there's a risk that it will incorrectly generalize from what it has been trained for and do the wrong thing. [Aligned AI](https://buildaligned.ai/news/aligned-ai-releases-new-disambiguation-benchmark/) has released a new [benchmark](https://github.com/alignedai/HappyFaces) designed to measure how well image-classifying algorithms avoid goal misgeneralization. This post explains what goal misgeneralization is, what the new benchmark measures, and how the benchmark is related to goal misgeneralization and concept extrapolation.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/DiEWbwrChuzuhJhGr/benchmark-for-successful-concept-extrapolation-avoiding-goal"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_41bb5f86d43dd6f0",
    "title": "[AN #172] Sorry for the long hiatus!",
    "year": 2022,
    "category": "public_awareness",
    "description": "Listen to this newsletter on [The Alignment Newsletter Podcast](http://alignment-newsletter.libsyn.com/).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/rxsdSgnZrgYWc2XAp/an-172-sorry-for-the-long-hiatus"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7f8a5b019e4a4427",
    "title": "Introducing the Fund for Alignment Research (We're Hiring!)",
    "year": 2022,
    "category": "policy_development",
    "description": "*Cross-posted to the* [*EA Forum*](https://forum.effectivealtruism.org/posts/gNHjEmLeKM47FDdqM/introducing-the-fund-for-alignment-research-we-re-hiring-1)",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/hGE3Pcc7qmK75bjhc/introducing-the-fund-for-alignment-research-we-re-hiring"
    ],
    "tags": [
      "ai",
      "community"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4ed4247a616da0fa",
    "title": "Outer vs inner misalignment: three framings",
    "year": 2022,
    "category": "policy_development",
    "description": "A core concept in the field of AI alignment is a distinction between two types of misalignment: outer misalignment and inner misalignment. Roughly speaking, the outer alignment problem is the problem of specifying an reward function which captures human preferences; and the inner alignment problem is the problem of ensuring that a policy trained on that reward function actually tries to act in accordance with human preferences. (In other words, it's the distinction between aligning the \"outer\" training signal versus aligning the \"inner\" policy.) However, the distinction can be difficult to pin down precisely. In this post I'll give three and a half definitions, which each come progressively closer to capturing my current conception of it. I think Framing 1 is a solid starting point; Framings 1.5 and 2 seem like useful refinements, although less concrete; and Framing 3 is fairly speculative. For those who don't already have a solid grasp on the inner-outer misalignment distinction, I...",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/poyshiMEhJsAuifKt/outer-vs-inner-misalignment-three-framings-1"
    ],
    "tags": [
      "ai",
      "inner alignment",
      "outer alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_6429f0c2ec742359",
    "title": "Principles for Alignment/Agency Projects",
    "year": 2022,
    "category": "public_awareness",
    "description": "\"John, what do you think of this idea for an alignment research project?\"",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/A7GeRNLzuFnhvGGgb/principles-for-alignment-agency-projects"
    ],
    "tags": [
      "ai",
      "ai alignment fieldbuilding",
      "rationality"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_bfb844ec09db99ec",
    "title": "Race Along Rashomon Ridge",
    "year": 2022,
    "category": "public_awareness",
    "description": "### *Produced As Part Of The SERI ML Alignment Theory Scholars Program 2022 Research Sprint Under* [*John Wentworth*](https://www.lesswrong.com/users/johnswentworth)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Hna2P8gcTyRgNDYBY/race-along-rashomon-ridge"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)",
      "machine learning  (ml)",
      "seri mats"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c3610209b323df2f",
    "title": "Human values & biases are inaccessible to the genome",
    "year": 2022,
    "category": "policy_development",
    "description": "*Related to Steve Byrnes'*[*Social instincts are tricky because of the \"symbol grounding problem.\"*](https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/5F5Tz3u6kJbTNMqsb#13_2_2_Claim_2__Social_instincts_are_tricky_because_of_the__symbol_grounding_problem_) *I wouldn't have had this insight without several great discussions with Quintin Pope.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome"
    ],
    "tags": [
      "evolution",
      "heuristics & biases",
      "human values",
      "shard theory",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e4934023d857eae0",
    "title": "Safety considerations for online generative modeling",
    "year": 2022,
    "category": "policy_development",
    "description": "**Summary:** the [online decision transformer](https://arxiv.org/pdf/2202.05607.pdf) is a recent approach to creating agents in which a decision transformer is pre-trained offline (as usual) before producing its own trajectories which are fed back into the model in an online finetuning phase.  I argue that agents made with generative modeling have safety advantages - but capabilities *dis*advantages - over agents made with other RL approaches, and agents made with *online* generative modeling (like the online decision transformer) may maintain these safety advantages while being closer to parity in capabilities. I propose experiments to test all this. (There is also an appendix discussing the connections between some of these ideas and KL-regularized RL.)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/BMfNu82iunjqKyQA9/safety-considerations-for-online-generative-modeling"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3f47582818ed9dbd",
    "title": "Making it harder for an AGI to \"trick\" us, with STVs",
    "year": 2022,
    "category": "public_awareness",
    "description": "|  |\n| --- |\n| Summary / PreambleAI Alignment has various sub-areas. The area I focus on here is ways we might use a superintelligent AGI-system to help with creating an aligned AGI-system, even if the AGI we start out with isn't fully aligned.Imagine a superintelligence that \"pretends\" to be aligned. Such an AI may give output that *seems* to us like what we want. But for some types of requests, it's very hard to give output that *seems* to us like what we want without it *actually* being what we want (even for a superintelligence). Can we obtain new capabilities by making such requests, in such a way that the scope of things we can ask for in a safe way (without being \"tricked\" or manipulated) is increased? And if so, is it possible to eventually end up with an aligned AGI-system?One reason for exploring such strategies is contingency planning (what if we haven't solved alignment by the time the first superintelligent AGI-system arrives?). Another reason is that additional layers ...",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/xERh9dkBkHLHp7Lg6/making-it-harder-for-an-agi-to-trick-us-with-stvs"
    ],
    "tags": [
      "ai",
      "ai boxing (containment)",
      "ai success models",
      "ai-assisted/ai automated alignment",
      "outer alignment",
      "verification"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e2aa6228fce4fa51",
    "title": "Visualizing Neural networks, how to blame the bias",
    "year": 2022,
    "category": "public_awareness",
    "description": "Background\n==========",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Bb33LG2YC3oTpBoGj/visualizing-neural-networks-how-to-blame-the-bias"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_01d83b55c905bb1f",
    "title": "Grouped Loss may disfavor discontinuous capabilities",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Thanks to Evan Hubinger and Beth Barnes for comments on these ideas.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/PmhTzHHFEem5hX79R/grouped-loss-may-disfavor-discontinuous-capabilities"
    ],
    "tags": [
      "ai",
      "machine learning  (ml)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1a30968bcd96b4eb",
    "title": "On how various plans miss the hard bits of the alignment challenge",
    "year": 2022,
    "category": "policy_development",
    "description": "*This post has been recorded as part of the LessWrong Curated Podcast, and can be listened to on* [*Spotify*](https://open.spotify.com/episode/6cDwctrmpWW6UNEKTT3Vvf?si=7T7SaHNCQ9axzL4D-crPQw)*,* [*Apple Podcasts*](https://podcasts.apple.com/us/podcast/on-how-various-plans-miss-the-hard-bits-of/id1630783021?i=1000570244508)*, and* [*Libsyn*](https://sites.libsyn.com/421877/on-how-various-plans-miss-the-hard-bits-of-the-alignment-challenge-by-nate-soares)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment"
    ],
    "tags": [
      "ai",
      "ai risk",
      "research agendas",
      "threat models"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a7b0098393d41835",
    "title": "Mosaic and Palimpsests: Two Shapes of Research",
    "year": 2022,
    "category": "public_awareness",
    "description": "*(Minor update to change Steve's labelling following* [*this comment*](https://www.alignmentforum.org/posts/4BpeHPXMjRzopgAZd/mosaic-and-palimpsests-two-shapes-of-research?commentId=poz8BXhgR9HJojs3Q)*, and also because I realized that I never added the footnotes...)*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/4BpeHPXMjRzopgAZd/mosaic-and-palimpsests-two-shapes-of-research"
    ],
    "tags": [
      "conjecture (org)",
      "practice & philosophy of science",
      "rationality"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b5e393063142fdeb",
    "title": "Response to Blake Richards: AGI, generality, alignment, & loss functions",
    "year": 2022,
    "category": "policy_development",
    "description": "Blake Richards is a neuroscientist / AI researcher with appointments at [McGill](https://www.mcgill.ca/neuro/blake-richards-phd) & [MiLA](https://mila.quebec/en/person/blake-richards/). Much of his recent work has involved making connections between machine learning algorithms and the operating principles of the cortex and hippocampus, including theorizing about how the neocortex might accomplish something functionally similar to backprop. (Backprop itself is not biologically plausible.) I have read lots of his papers; they're always very interesting!",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/rgPxEKFBLpLqJpMBM/response-to-blake-richards-agi-generality-alignment-and-loss"
    ],
    "tags": [
      "ai",
      "ai safety public materials",
      "general intelligence"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4e34206dda1ddf6c",
    "title": "Acceptability Verification: A Research Agenda",
    "year": 2022,
    "category": "public_awareness",
    "description": "[This Google doc^](https://docs.google.com/document/d/199Lkh78UA2uI9ljLEy_aWR8RBetQLRO6Kqo3_Omi1e4/view) is a halted, formerly work-in-progress writeup of [Evan Hubinger's](https://www.lesswrong.com/users/evhub) AI alignment research agenda, authored by Evan. It dates back to around 2020, and so Evan's views on alignment have shifted since then.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/GeabLEXYP7oBMivmF/acceptability-verification-a-research-agenda"
    ],
    "tags": [
      "ai",
      "ai risk",
      "ai success models",
      "inner alignment",
      "myopia",
      "research agendas"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_193ac8385174f4ef",
    "title": "Artificial Sandwiching: When can we test scalable alignment protocols without humans?",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Epistemic status: Not a fleshed-out proposal. Brainstorming/eliciting ideas.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/nekLYqbCEBDEfbLzF/artificial-sandwiching-when-can-we-test-scalable-alignment"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f75de3e805867d92",
    "title": "Deep learning curriculum for large language model alignment",
    "year": 2022,
    "category": "public_awareness",
    "description": "This is a deep learning curriculum with a focus on topics relevant to large language model alignment. It is centered around papers and exercises, and is biased towards my own tastes.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/5uNfgjaDAwkhcLJca/deep-learning-curriculum-for-large-language-model-alignment"
    ],
    "tags": [
      "ai",
      "exercises / problem-sets",
      "language models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1904c9580bbdae92",
    "title": "Humans provide an untapped wealth of evidence about alignment",
    "year": 2022,
    "category": "public_awareness",
    "description": "*This post has been recorded as part of the LessWrong Curated Podcast, and can be listened to on* [*Spotify*](https://open.spotify.com/episode/0jpI7LLNzKsn6lwrsoDCc9)*,* [*Apple Podcasts*](https://podcasts.apple.com/us/podcast/humans-provide-an-untapped-wealth-of-evidence/id1630783021?i=1000575990542)*, and* [*Libsyn*](https://five.libsyn.com/episodes/view/23991000)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/CjFZeDD6iCnNubDoS/humans-provide-an-untapped-wealth-of-evidence-about"
    ],
    "tags": [
      "ai",
      "human values",
      "ontology",
      "shard theory"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0d46a6817a520f14",
    "title": "Circumventing interpretability: How to defeat mind-readers",
    "year": 2022,
    "category": "policy_development",
    "description": "*(Post now available as a pdf:* [*https://arxiv.org/abs/2212.11415*](https://arxiv.org/abs/2212.11415) *)*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/EhAbh2pQoAXkm9yor/circumventing-interpretability-how-to-defeat-mind-readers"
    ],
    "tags": [
      "ai",
      "conjecture (org)",
      "instrumental convergence",
      "interpretability (ml & ai)",
      "security mindset"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7803c66d7c96999c",
    "title": "A note about differential technological development",
    "year": 2022,
    "category": "public_awareness",
    "description": "Quick note: I occasionally run into arguments of the form \"my research advances capabilities, but it advances alignment more than it advances capabilities, so it's good on net\". I do not buy this argument, and think that in most such cases, this sort of research does more harm than good. (Cf. [differential technological development](https://forum.effectivealtruism.org/posts/g6549FAQpQ5xobihj/differential-technological-development).)",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/vQNJrJqebXEWjJfnz/a-note-about-differential-technological-development"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_256b2dbdb9bd6a9b",
    "title": "Notes on Learning the Prior",
    "year": 2022,
    "category": "policy_development",
    "description": "*This post was written to fulfil requirements of the SERI MATS Training Program.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ukidKsEio8hfB9uHT/notes-on-learning-the-prior"
    ],
    "tags": [
      "ai",
      "seri mats"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8a9b57c0a263ab22",
    "title": "Safety Implications of LeCun's path to machine intelligence",
    "year": 2022,
    "category": "policy_development",
    "description": "Yann LeCun recently posted [A Path Towards Autonomous Machine Intelligence](https://openreview.net/forum?id=BZ5a1r-kVsf), a high-level description of the architecture he considers most promising to advance AI capabilities.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/GrbeyZzp6NwzSWpds/safety-implications-of-lecun-s-path-to-machine-intelligence"
    ],
    "tags": [
      "ai",
      "machine learning  (ml)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0a292693e9c6c6e2",
    "title": "Why you might expect homogeneous take-off: evidence from ML research",
    "year": 2022,
    "category": "public_awareness",
    "description": "*This write-up was produced as part of the SERI MATS programme under Evan Hubinger's mentorship. It is also my first post on LW, so feedback is very welcome!*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/RQn45KzN5cojLLb3L/why-you-might-expect-homogeneous-take-off-evidence-from-ml"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "seri mats"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8672eb88a13d1108",
    "title": "How Interpretability can be Impactful",
    "year": 2022,
    "category": "policy_development",
    "description": "*This post was written as part of the* [*Stanford Existential Risks Initiative ML Alignment Theory Scholars (MATS) program*](https://www.alignmentforum.org/posts/FpokmCnbP3CEZ5h4t/ml-alignment-theory-program-under-evan-hubinger). *thanks to Evan Hubinger for insightful discussion.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Cj4hWE2xBf7t8nKkk/how-interpretability-can-be-impactful"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)",
      "seri mats"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_715d6ce7af8cccc2",
    "title": "Deception?! I ain't got time for that!",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Or ... How penalizing computation used during training disfavors deception*.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/C8XTFtiA5xtje6957/deception-i-ain-t-got-time-for-that"
    ],
    "tags": [
      "ai",
      "seri mats"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_372ff158811c3362",
    "title": "A distillation of Evan Hubinger's training stories (for SERI MATS)",
    "year": 2022,
    "category": "public_awareness",
    "description": "*This post is a* [*distillation*](https://www.lesswrong.com/posts/zo9zKcz47JxDErFzQ/call-for-distillers) *of Evan Hubinger's post \"*[*how do we become confident in the safety of a machine learning system?*](https://www.alignmentforum.org/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine)*\", made as part of the summer 2022* [*SERI MATS program*](https://www.lesswrong.com/posts/8vLvpxzpc6ntfBWNo/seri-ml-alignment-theory-scholars-program-2022)*. While I have attempted to understand and extrapolate Evan's opinions, this post has not been vetted. Likewise, I use* training stories *(and* contribution stories*) to describe the methodology of proposals for safe advanced AI without the endorsement of those proposals' authors and based on a relatively shallow understanding of those proposals (due to my inexperience and time constraints). The opinions presented in this post are my own unless otherwise noted.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/wPLeBqsLgJyFyuTr7/a-distillation-of-evan-hubinger-s-training-stories-for-seri"
    ],
    "tags": [
      "ai",
      "distillation & pedagogy",
      "seri mats"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_667c05311ee2eb7f",
    "title": "Training goals for large language models",
    "year": 2022,
    "category": "policy_development",
    "description": "*This post was written under Evan Hubinger's mentorship, as part of the*[*Stanford Existential Risks Initiative ML Alignment Theory Scholars (SERI MATS) program*](https://www.lesswrong.com/posts/8vLvpxzpc6ntfBWNo/seri-ml-alignment-theory-scholars-program-2022)*. Many of the ideas in this post, including the main idea behind the training goal, are due to Kyle McDonell and Laria Reynolds. In addition, I am grateful for comments and feedback from Arun Jose (who wrote a related post on*[*conditioning generative models for alignment*](https://www.alignmentforum.org/posts/JqnkeqaPseTgxLgEL/conditioning-generative-models-for-alignment)*) and Caspar Oesterheld, and for a helpful discussion with James Lucassen.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/dWJNFHnC4bkdbovug/training-goals-for-large-language-models"
    ],
    "tags": [
      "ai",
      "decision theory",
      "language models",
      "oracle ai",
      "self fulfilling/refuting prophecies",
      "seri mats"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a470fc91b2334e10",
    "title": "Conditioning Generative Models for Alignment",
    "year": 2022,
    "category": "public_awareness",
    "description": "*This post was written under Evan Hubinger's direct guidance and mentorship, as a part of the*[*Stanford Existential Risks Institute ML Alignment Theory Scholars (MATS) program*](https://www.lesswrong.com/posts/8vLvpxzpc6ntfBWNo/seri-ml-alignment-theory-scholars-program-2022)*. It builds on work done on simulator theory by Janus, who came up with the strategy this post aims to analyze; I'm also grateful to them for their comments and their mentorship during the AI Safety Camp, to Johannes Treutlein for his feedback and helpful discussion, and to Paul Colognese for his thoughts.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/JqnkeqaPseTgxLgEL/conditioning-generative-models-for-alignment"
    ],
    "tags": [
      "ai",
      "ai risk",
      "ai success models",
      "ai-assisted/ai automated alignment",
      "inner alignment",
      "language models",
      "oracle ai",
      "outer alignment",
      "research agendas",
      "self fulfilling/refuting prophecies"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e44e289bb50d71e5",
    "title": "Quantilizers and Generative Models",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Thanks to Evan Hubinger for discussions about quantilizers, and to James Lucassen for discussions about conditioned generative models. Many of these ideas are discussed in Jessica Taylor's* [Quantilizers: A Safer Alternative to Maximizers for Limited Optimization](https://intelligence.org/files/QuantilizersSaferAlternative.pdf)*: this post just expands on a particular thread of ideas in that paper. Throughout I'll refer to sections of the paper. I have some remaining confusion about the \"targeted impact\" section, and would appreciate clarifications/corrections!*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/tz3hoCs2efHjzNYm5/quantilizers-and-generative-models"
    ],
    "tags": [
      "ai",
      "quantilization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d612ba92f2b6e514",
    "title": "Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover",
    "year": 2022,
    "category": "policy_development",
    "description": "I think that [in the coming 15-30 years](https://www.cold-takes.com/where-ai-forecasting-stands-today/), the world could plausibly develop \"transformative AI\": AI powerful enough to bring us into a new, qualitatively different future, via [an explosion in science and technology R&D](https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#explosive-scientific-and-technological-advancement). This sort of AI [could](https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#impacts-of-pasta) be sufficient to make this the [most important century of all time for humanity](https://www.cold-takes.com/roadmap-for-the-most-important-century-series/).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to"
    ],
    "tags": [
      "ai",
      "situational awareness",
      "threat models",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_65f19a82ba9b9306",
    "title": "Help ARC evaluate capabilities of current language models (still need people)",
    "year": 2022,
    "category": "public_awareness",
    "description": "\\*\\*Still looking for people as of Sep 9th 2022\\*\\*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/e3j7h4mPHvkRynbco/help-arc-evaluate-capabilities-of-current-language-models"
    ],
    "tags": [
      "ai",
      "language models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8fb85070db1c237b",
    "title": "Bounded complexity of solving ELK and its implications",
    "year": 2022,
    "category": "public_awareness",
    "description": "This post was written for the SERI MATS program. I thank Evan Hubinger and Leo Gao for their mentorship in the program. Further thanks go to Simon Marshall and Leo Gao (again) for specific comments regarding the content of this post.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/8cr9fJnay97GEYPt3/bounded-complexity-of-solving-elk-and-its-implications"
    ],
    "tags": [
      "ai",
      "eliciting latent knowledge (elk)",
      "seri mats"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e2b42b57d6ac5034",
    "title": "Abram Demski's ELK thoughts and proposal - distillation",
    "year": 2022,
    "category": "public_awareness",
    "description": "This post was written for the SERI MATS program. I thank Evan Hubinger and Leo Gao for their mentorship in the program. Further thanks go to Evan Hubinger (again), Simon Marshall, and Johannes Treutlein for specific comments regarding the content of this post.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/kF74mHH6SujRoEEFA/abram-demski-s-elk-thoughts-and-proposal-distillation"
    ],
    "tags": [
      "ai",
      "distillation & pedagogy",
      "eliciting latent knowledge (elk)",
      "seri mats"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9bf0ea3e8a1f76ee",
    "title": "How to Diversify Conceptual Alignment: the Model Behind Refine",
    "year": 2022,
    "category": "public_awareness",
    "description": "*This post is part of the work done at* [*Conjecture*](https://conjecture.dev)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/5uiQkyKdejX3aEHLM/how-to-diversify-conceptual-alignment-the-model-behind"
    ],
    "tags": [
      "ai",
      "ai alignment fieldbuilding",
      "conjecture (org)",
      "intellectual progress (society-level)",
      "refine"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_30bc71315f9641ec",
    "title": "[AN #173] Recent language model results from DeepMind",
    "year": 2022,
    "category": "policy_development",
    "description": "Research publication: [AN #173] Recent language model results from DeepMind",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/HXDkCtk9tae5wFmjG/an-173-recent-language-model-results-from-deepmind"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_df10f8fc78d3a619",
    "title": "Conditioning Generative Models with Restrictions",
    "year": 2022,
    "category": "policy_development",
    "description": "*This is a followup to* [*Conditioning Generative Models*](https://www.alignmentforum.org/posts/nXeLPcT9uhfG3TMPS/conditioning-generative-models) *based on further discussions with Evan Hubinger, Nicholas Schiefer, Abram Demski, Curtis Huebner, Hoagy Cunningham, Derek Shiller, and James Lucassen, as well as broader conversations with many different people at the recent ARC/ELK retreat. For more background on this general direction see Johannes Treutlein's \"*[*Training goals for large language models*](https://www.alignmentforum.org/posts/dWJNFHnC4bkdbovug/training-goals-for-large-language-models)*\".*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/adiszfnFgPEnRsGSr/conditioning-generative-models-with-restrictions"
    ],
    "tags": [
      "ai",
      "language models",
      "outer alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7ac8f37eb1a31a9d",
    "title": "Robustness to Scaling Down: More Important Than I Thought",
    "year": 2022,
    "category": "public_awareness",
    "description": "*(Edit: I added text between \"...was really about reductions.\" and \"To use the mental move of robustness...\", because comments showed me I hadn't made my meaning clear enough.)*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pA3F9oejzvGg6Kf3a/robustness-to-scaling-down-more-important-than-i-thought"
    ],
    "tags": [
      "ai",
      "ai risk",
      "ai robustness",
      "conjecture (org)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cf09b90783f8a0e6",
    "title": "Brainstorm of things that could force an AI team to burn their lead",
    "year": 2022,
    "category": "policy_development",
    "description": "|  |\n| --- |\n| **Comments:** The following is a list (very lightly edited with help from Rob Bensinger) I wrote in July 2017, at Nick Beckstead's request, as part of a conversation we were having at the time. From my current vantage point, it strikes me as narrow and obviously generated by one person, listing the first things that came to mind on a particular day.I worry that it's easy to read the list below as saying that this narrow slice, all clustered in one portion of the neighborhood, is a very big slice of the space of possible ways an AGI group may have to burn down its lead.This is one of my models for how people wind up with really weird pictures of MIRI beliefs. I generate three examples that are clustered together because I'm bad at generating varied examples on the fly, while hoping that people can generalize to see the broader space these are sampled from; then people think I've got a fetish for the particular corner of the space spanned by the first few ideas that pop...",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/p3s8RvkcyTwzu27ps/brainstorm-of-things-that-could-force-an-ai-team-to-burn"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_72b8504e286d87e0",
    "title": "Reward is not the optimization target",
    "year": 2022,
    "category": "policy_development",
    "description": "*This insight was made possible by many conversations with Quintin Pope, where he challenged my implicit assumptions about alignment. I'm not sure who came up with this particular idea.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target"
    ],
    "tags": [
      "ai",
      "deconfusion",
      "inner alignment",
      "outer alignment",
      "reinforcement learning",
      "reward functions",
      "shard theory",
      "wireheading"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_fc0952b20fa2ea3d",
    "title": "NeurIPS ML Safety Workshop 2022",
    "year": 2022,
    "category": "policy_development",
    "description": "We're excited to announce the NeurIPS [ML Safety workshop](https://neurips2022.mlsafety.org/)! To our knowledge it is the first workshop at a top ML conference to emphasize and explicitly discuss x-risks.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pY4J2qNaHgKp2nbEd/neurips-ml-safety-workshop-2022"
    ],
    "tags": [
      "academic papers",
      "ai",
      "bounties (closed)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_dc26c4e249c20b7d",
    "title": "Active Inference as a formalisation of instrumental convergence",
    "year": 2022,
    "category": "policy_development",
    "description": "The goal of this post is mainly to increase the exposure of the AI alignment community to Active Inference theory, which seems to be highly relevant to the problem but is seldom mentioned on the forum.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ostLZyhnBPndno2zP/active-inference-as-a-formalisation-of-instrumental"
    ],
    "tags": [
      "ai",
      "free energy principle",
      "instrumental convergence"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_872d51370d1e0dd0",
    "title": "?Boundaries?, Part 1: a key missing concept from utility theory",
    "year": 2022,
    "category": "public_awareness",
    "description": "*This post has been recorded as part of the LessWrong Curated Podcast, and can be listened to on* [*Spotify*](https://open.spotify.com/episode/73rKuCaxCaAbLqNIvRVi99)*,* [*Apple Podcasts*](https://podcasts.apple.com/us/podcast/boundaries-part-1-a-key-missing-concept-from/id1630783021?i=1000571435503)*, and* [*Libsyn*](https://sites.libsyn.com/421877/boundaries-part-1-a-key-missing-concept-from-utility-theory-by-andrew-critch)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/8oMF8Lv5jiGaQSFvo/boundaries-part-1-a-key-missing-concept-from-utility-theory"
    ],
    "tags": [
      "boundaries / membranes [technical]",
      "game theory",
      "group rationality",
      "rationality",
      "world optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e09dca3b14edd4d0",
    "title": "AGI ruin scenarios are likely (and disjunctive)",
    "year": 2022,
    "category": "policy_development",
    "description": "*Note: As usual, Rob Bensinger helped me with editing. I recently discussed this model with Alex Lintz, who might soon post his own take on it (edit:* [*here*](https://forum.effectivealtruism.org/posts/eggdG27y75ot8dNn7/three-pillars-for-avoiding-agi-catastrophe-technical)*).*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ervaGwJ2ZcwqfCcLx/agi-ruin-scenarios-are-likely-and-disjunctive"
    ],
    "tags": [
      "ai",
      "ai risk"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0f8fd87e323fca6f",
    "title": "Levels of Pluralism",
    "year": 2022,
    "category": "public_awareness",
    "description": "*This post is part of the work done at* [*Conjecture*](https://conjecture.dev)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/wi3upQibefMcFs5to/levels-of-pluralism"
    ],
    "tags": [
      "conjecture (org)",
      "epistemology",
      "intellectual progress (society-level)",
      "practice & philosophy of science"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2b5d3c22c5d7eadf",
    "title": "Moral strategies at different capability levels",
    "year": 2022,
    "category": "policy_development",
    "description": "Let's consider three ways you can be altruistic towards another agent:",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/jDQm7YJxLnMnSNHFu/moral-strategies-at-different-capability-levels"
    ],
    "tags": [
      "ai",
      "decision theory",
      "ethics & morality",
      "world optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_974697b818bd90ea",
    "title": "Principles of Privacy for Alignment Research",
    "year": 2022,
    "category": "policy_development",
    "description": "The hard/useful parts of alignment research are largely about understanding agency/intelligence/etc. That sort of understanding naturally yields capabilities-relevant insights. So, alignment researchers naturally run into decisions about how private to keep their work.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/SsuqYoyBnheSj7jLw/principles-of-privacy-for-alignment-research"
    ],
    "tags": [
      "ai",
      "ai alignment fieldbuilding",
      "ai capabilities",
      "information hazards",
      "privacy"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_430bb12e03092b3b",
    "title": "Abstracting The Hardness of Alignment: Unbounded Atomic Optimization",
    "year": 2022,
    "category": "policy_development",
    "description": "*This post is part of the work done at* [*Conjecture*](https://conjecture.dev)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/uhxpJyGYQ5FQRvdjY/abstracting-the-hardness-of-alignment-unbounded-atomic"
    ],
    "tags": [
      "ai",
      "conjecture (org)",
      "optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_675957d55a602497",
    "title": "Conjecture: Internal Infohazard Policy",
    "year": 2022,
    "category": "policy_development",
    "description": "*This post benefited from feedback and comments from the whole Conjecture team, as well as others including Steve Byrnes, Paul Christiano, Leo Gao, Evan Hubinger, Daniel Kokotajlo, Vanessa Kosoy, John Wentworth, Eliezer Yudkowsky. Many others also kindly shared their feedback and thoughts on it formally or informally, and we are thankful for everyone's help on this work.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Gs29k3beHiqWFZqnn/conjecture-internal-infohazard-policy"
    ],
    "tags": [
      "ai",
      "conjecture (org)",
      "information hazards",
      "security mindset",
      "world optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a001fa2a76ad5ae3",
    "title": "Comparing Four Approaches to Inner Alignment",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Early work on this was supported by*[*CEEALAR*](https://ceealar.org/) *and was finished during an internship at*[*Conjecture*](https://www.conjecture.dev/) *under the mentorship of Adam Shimi.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/KWmrz9WbGntMGMb73/comparing-four-approaches-to-inner-alignment"
    ],
    "tags": [
      "ai",
      "ai risk",
      "inner alignment",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d8cf65c793dccf36",
    "title": "How transparency changed over time",
    "year": 2022,
    "category": "public_awareness",
    "description": "*This post is written as part of the SERI MATS program.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ngwNHAy5TjStZnJzQ/how-transparency-changed-over-time"
    ],
    "tags": [
      "ai",
      "seri mats"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5ae4e4da295b5da2",
    "title": "Two-year update on my personal AI timelines",
    "year": 2022,
    "category": "policy_development",
    "description": "I worked on my [draft report on biological anchors for forecasting AI timelines](https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines) mainly between ~May 2019 (three months after the release of GPT-2) and ~Jul 2020 (a month after the release of GPT-3), and posted it on LessWrong in Sep 2020 after an internal review process. At the time, my bottom line estimates from the bio anchors modeling exercise [were](https://docs.google.com/document/d/1cCJjzZaJ7ATbq8N2fvhmsDOUWdm7t3uSSXv6bD0E_GM/edit#heading=h.jhjg6byruuun):[[1]](#fn-LnaAQkuHYCr3b3oQ7-1)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines"
    ],
    "tags": [
      "ai",
      "ai timelines"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a0a1d2eac4f31cb9",
    "title": "Externalized reasoning oversight: a research direction for language model alignment",
    "year": 2022,
    "category": "technical_research_breakthrough",
    "description": "Research publication: Externalized reasoning oversight: a research direction for language model alignment",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for"
    ],
    "tags": [
      "ai",
      "chain-of-thought alignment",
      "inner alignment",
      "language models",
      "outer alignment",
      "seri mats"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_132a6c3dd6878461",
    "title": "Precursor checking for deceptive alignment",
    "year": 2022,
    "category": "public_awareness",
    "description": "This post is primarily an excerpt from \"[Acceptability Verification: a Research Agenda](https://www.alignmentforum.org/posts/GeabLEXYP7oBMivmF/acceptability-verification-a-research-agenda)\" that I think is useful enough on its own such that I've spun it off into its own post.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pRt4E3nmPBtWZiT4A/precursor-checking-for-deceptive-alignment"
    ],
    "tags": [
      "ai",
      "deception",
      "deceptive alignment",
      "interpretability (ml & ai)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_492e7f29b90b5f2c",
    "title": "Convergence Towards World-Models: A Gears-Level Model",
    "year": 2022,
    "category": "policy_development",
    "description": "1. Intuitions\n-------------",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/HzSdYWvdrdQqG9tqW/convergence-towards-world-models-a-gears-level-model"
    ],
    "tags": [
      "ai",
      "ai risk",
      "goal-directedness",
      "inner alignment",
      "mesa-optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_78614622e1874887",
    "title": "$20K In Bounties for AI Safety Public Materials",
    "year": 2022,
    "category": "policy_development",
    "description": "Research publication: $20K In Bounties for AI Safety Public Materials",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/gWM8cgZgZ9GQAYTqF/usd20k-in-bounties-for-ai-safety-public-materials"
    ],
    "tags": [
      "ai",
      "ai safety public materials",
      "bounties & prizes (active)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cf78071cf4fe8397",
    "title": "Bridging Expected Utility Maximization and Optimization",
    "year": 2022,
    "category": "public_awareness",
    "description": "Background\n==========",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/rQDYQrDjPGqjrf8Mk/bridging-expected-utility-maximization-and-optimization"
    ],
    "tags": [
      "agent foundations",
      "ai",
      "decision theory",
      "optimization",
      "philosophy",
      "pibbss",
      "utility functions"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_96a9d9c64f1a5953",
    "title": "Rant on Problem Factorization for Alignment",
    "year": 2022,
    "category": "policy_development",
    "description": "This post is the second in what is likely to become a series of uncharitable rants about alignment proposals (previously: [Godzilla Strategies](https://www.lesswrong.com/posts/DwqgLXn5qYC7GqExF/godzilla-strategies)). In general, these posts are intended to convey my underlying intuitions. They are *not* intended to convey my all-things-considered, reflectively-endorsed opinions. In particular, my all-things-considered reflectively-endorsed opinions are usually more kind. But I think it is valuable to make the underlying, not-particularly-kind intuitions publicly-visible, so people can debate underlying generators directly. I apologize in advance to all the people I insult in the process.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/tmuFmHuyb4eWmPXz8/rant-on-problem-factorization-for-alignment"
    ],
    "tags": [
      "ai",
      "debate (ai safety technique)",
      "factored cognition",
      "humans consulting hch",
      "ought"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d49273ef3611064e",
    "title": "Announcing the Introduction to ML Safety course",
    "year": 2022,
    "category": "public_awareness",
    "description": "Research publication: Announcing the Introduction to ML Safety course",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/4F8Bg8Z5cePTBofzo/announcing-the-introduction-to-ml-safety-course"
    ],
    "tags": [
      "ai",
      "community"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4304ff78db6aa96b",
    "title": "Steganography in Chain of Thought Reasoning",
    "year": 2022,
    "category": "public_awareness",
    "description": "Here I give a possible phenomenon of steganography in chain of thought reasoning, where a system doing multi-stage reasoning with natural language encodes hidden information in its outputs that is not observable by humans, but can be used to boost its performance on some task. I think this could happen as a result of optimization pressure and natural language null space. At the end is a sketch of a research idea to study this phenomenon empirically.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/yDcMDJeSck7SuBs24/steganography-in-chain-of-thought-reasoning"
    ],
    "tags": [
      "ai",
      "chain-of-thought alignment",
      "information theory",
      "machine learning  (ml)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a170d4f6c5c782e3",
    "title": "Interpretability/Tool-ness/Alignment/Corrigibility are not Composable",
    "year": 2022,
    "category": "policy_development",
    "description": "Interpretability\n----------------",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qXtbBAxmFkAQLQEJE/interpretability-tool-ness-alignment-corrigibility-are-not"
    ],
    "tags": [
      "ai",
      "corrigibility",
      "interpretability (ml & ai)",
      "tool ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_23835fdf8bf29952",
    "title": "Encultured AI Pre-planning, Part 1:  Enabling New Benchmarks",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Also available on the* [*EA Forum*](https://forum.effectivealtruism.org/posts/yczkGfcfWoRN6zfrf/encultured-ai-part-1-enabling-new-benchmarks)*.*  \n*Followed by:* [*Encultured AI, Part 2*](https://www.lesswrong.com/posts/2vxoTfuScspraSJeC/encultured-ai-part-2-providing-a-service) *(forthcoming)*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/AR6mfydDJiGksj6Co/encultured-ai-pre-planning-part-1-enabling-new-benchmarks"
    ],
    "tags": [
      "ai",
      "boundaries / membranes [technical]",
      "community",
      "encultured ai (org)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a386dd90266db2c4",
    "title": "Encultured AI, Part 1 Appendix: Relevant Research Examples",
    "year": 2022,
    "category": "policy_development",
    "description": "*Also available on the EA Forum.*  \n*Appendix to:* [*Encultured AI, Part 1: Enabling New Benchmarks*](https://www.lesswrong.com/posts/AR6mfydDJiGksj6Co/encultured-ai-part-1-enabling-new-benchmarks)  \n*Followed by: Encultured AI, Part 2: Providing a Service*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/PvuuBN39pmjw6wRpj/encultured-ai-part-1-appendix-relevant-research-examples"
    ],
    "tags": [
      "ai",
      "community",
      "encultured ai (org)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_09f4cbd11ffdcc11",
    "title": "General alignment properties",
    "year": 2022,
    "category": "policy_development",
    "description": "[AIXI](https://en.wikipedia.org/wiki/AIXI) and the genome are both ways of specifying intelligent agents.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FMdGt9S9irgxeD9Xz/general-alignment-properties"
    ],
    "tags": [
      "ai",
      "complexity of value",
      "embedded agency",
      "general alignment properties",
      "ontology",
      "shard theory"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_798e08ce6075f124",
    "title": "Announcing: Mechanism Design for AI Safety - Reading Group",
    "year": 2022,
    "category": "policy_development",
    "description": "We're starting a new reading group for people interested in applying mechanism design tools to technical AI alignment. If you're interested in joining, you can [apply here](https://docs.google.com/forms/d/1nLMqCfS99dvLWEmGhWUNRFWLN4u4VbWJbneLBL2uJjg/edit) by August 22nd (applying takes less than five minutes). If you have recommendations for papers to discuss, please mention them in the comments.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FhqZZFydyQG9WTSKR/announcing-mechanism-design-for-ai-safety-reading-group"
    ],
    "tags": [
      "mechanism design",
      "reading group",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f5c955d11623a9fe",
    "title": "How To Go From Interpretability To Alignment: Just Retarget The Search",
    "year": 2022,
    "category": "public_awareness",
    "description": "When people talk about [prosaic alignment proposals](https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai), there's a common pattern: they'll be outlining some overcomplicated scheme, and then they'll say \"oh, and assume we have great interpretability tools, this whole thing just works way better the better the interpretability tools are\", and then they'll go back to the overcomplicated scheme. (Credit to [Evan](https://www.lesswrong.com/users/evhub) for pointing out this pattern to me.) And then usually there's a whole discussion about the specific problems with the overcomplicated scheme.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget"
    ],
    "tags": [
      "ai",
      "ai risk",
      "inner alignment",
      "interpretability (ml & ai)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c4e264f15b16ec33",
    "title": "How Do We Align an AGI Without Getting Socially Engineered?  (Hint: Box It)",
    "year": 2022,
    "category": "policy_development",
    "description": "### *Produced during the Stanford Existential Risk Initiative (SERI) ML Alignment Theory Scholars (MATS) Program of 2022, under John Wentworth*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/p62bkNAciLsv6WFnR/how-do-we-align-an-agi-without-getting-socially-engineered"
    ],
    "tags": [
      "ai",
      "ai boxing (containment)",
      "seri mats"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2940f276fbf9aa7a",
    "title": "How much alignment data will we need in the long run?",
    "year": 2022,
    "category": "policy_development",
    "description": "This question stands out to me because:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qoz2ryN4GDqEWPBnQ/how-much-alignment-data-will-we-need-in-the-long-run-1"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_41047b8d88f14f1d",
    "title": "The alignment problem from a deep learning perspective",
    "year": 2022,
    "category": "policy_development",
    "description": "*This report (*[*now available on arxiv*](https://arxiv.org/abs/2209.00626)*) is intended as a concise introduction to the alignment problem for people familiar with machine learning. It translates previous arguments about misalignment into the context of deep learning by walking through an illustrative AGI training process (a framing drawn from* [*an earlier report by Ajeya Cotra*](https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to)*), and outlines possible research directions for addressing different facets of the problem.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/KbyRPCAsWv5GtfrbG/the-alignment-problem-from-a-deep-learning-perspective"
    ],
    "tags": [
      "ai",
      "ai risk"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b6383501d3e862e1",
    "title": "Shard Theory: An Overview",
    "year": 2022,
    "category": "policy_development",
    "description": "*Generated as part of SERI MATS, Team Shard's research, under John Wentworth.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview"
    ],
    "tags": [
      "ai",
      "complexity of value",
      "human values",
      "outer alignment",
      "psychology",
      "reinforcement learning",
      "research agendas",
      "seri mats",
      "shard theory",
      "subagents"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9b9b18bb45b552d2",
    "title": "Language models seem to be much better than humans at next-token prediction",
    "year": 2022,
    "category": "public_awareness",
    "description": "[Thanks to a variety of people for comments and assistance (especially Paul Christiano, Nostalgebraist, and Rafe Kennedy), and to various people for playing the game. Buck wrote the top-1 prediction web app; Fabien wrote the code for the perplexity experiment and did most of the analysis and wrote up the math here, Lawrence did the research on previous measurements. Epistemic status: we're pretty confident of our work here, but haven't engaged in a super thorough review process of all of it--this was more like a side-project than a core research project.]",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/htrZrxduciZ5QaCjw/language-models-seem-to-be-much-better-than-humans-at-next"
    ],
    "tags": [
      "ai",
      "language models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d1f6f3f6ad3562d4",
    "title": "Encultured AI Pre-planning, Part 2:  Providing a Service",
    "year": 2022,
    "category": "public_awareness",
    "description": "*Also available on the* [*EA Forum*](https://forum.effectivealtruism.org/posts/MWWZQ8C655iT9zzRd/encultured-ai-part-2-providing-a-service)*.*  \n*Preceded by:* [*Encultured AI Pre-planning, Part 1: Enabling New Benchmarks*](https://www.lesswrong.com/posts/AR6mfydDJiGksj6Co/encultured-ai-part-1-enabling-new-benchmarks)  \n*Followed by:* [*Announcing Encultured AI*](https://www.lesswrong.com/posts/ALkH4o53ofm862vxc/announcing-encultured-ai-building-a-video-game)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/2vxoTfuScspraSJeC/encultured-ai-pre-planning-part-2-providing-a-service"
    ],
    "tags": [
      "encultured ai (org)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_35e7c56f9cbff307",
    "title": "Seriously, what goes wrong with \"reward the agent when it makes you smile\"?",
    "year": 2022,
    "category": "public_awareness",
    "description": "Suppose you're training a huge neural network with some awesome future RL algorithm with clever exploration bonuses and a self-supervised pretrained multimodal initialization and a recurrent state. This NN implements an embodied agent which takes actions in reality (and also in some sim environments). You watch the agent remotely using a webcam (initially unbeknownst to the agent). When the AI's activities make you smile, you press the antecedent-computation-reinforcer button (known to some as the \"reward\" button). The agent is given some appropriate curriculum, like population-based self-play, so as to provide a steady skill requirement against which its intelligence is sharpened over training. Supposing the curriculum trains these agents out until they're generally intelligent--what comes next?",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/22xf8GmwqGzHbiuLg/seriously-what-goes-wrong-with-reward-the-agent-when-it"
    ],
    "tags": [
      "ai",
      "reward functions"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3203ef6a2ab36f39",
    "title": "Refining the Sharp Left Turn threat model, part 1: claims and mechanisms",
    "year": 2022,
    "category": "public_awareness",
    "description": "This is our current distillation of the [sharp left turn](https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization) threat model and an attempt to make it more concrete. We will discuss our understanding of the claims made in this threat model, and propose some mechanisms for how a sharp left turn could happen. This is a work in progress, and we welcome feedback and corrections.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/usKXS5jGDzjwqv3FJ/refining-the-sharp-left-turn-threat-model-part-1-claims-and"
    ],
    "tags": [
      "ai",
      "sharp left turn",
      "threat models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7503544570ac96d1",
    "title": "DeepMind alignment team opinions on AGI ruin arguments",
    "year": 2022,
    "category": "policy_development",
    "description": "We had some discussions of the [AGI ruin arguments](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities) within the DeepMind alignment team to clarify for ourselves which of these arguments we are most concerned about and what the implications are for our work. This post summarizes the opinions of a subset of the alignment team on these arguments. **Disclaimer**: these are our own opinions that do not represent the views of DeepMind as a whole or its broader community of safety researchers.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qJgz2YapqpFEDTLKn/deepmind-alignment-team-opinions-on-agi-ruin-arguments"
    ],
    "tags": [
      "ai",
      "ai risk",
      "deepmind"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a114e8892606b591",
    "title": "Gradient descent doesn't select for inner search",
    "year": 2022,
    "category": "policy_development",
    "description": "**TL;DR:** Gradient descent won't select for inner search processes because they're not compute & memory efficient.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/TdesHi8kkyokQdDoQ/gradient-descent-doesn-t-select-for-inner-search"
    ],
    "tags": [
      "ai",
      "inner alignment",
      "mesa-optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_381de5682ac7eceb",
    "title": "the Insulated Goal-Program idea",
    "year": 2022,
    "category": "public_awareness",
    "description": "*(this post has been written for the first* [*Refine*](https://www.lesswrong.com/posts/D7epkkJb3CqDTYgX9/refine-an-incubator-for-conceptual-alignment-research-bets) *blog post day, at the end of the week of readings, discussions, and exercises about epistemology for doing good conceptual research)*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/oTn2PPZLY7a2xJmqh/the-insulated-goal-program-idea"
    ],
    "tags": [
      "ai",
      "refine",
      "utility functions"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_fe6769330476f547",
    "title": "Steelmining via Analogy",
    "year": 2022,
    "category": "public_awareness",
    "description": "*This post has been written for the first* [*Refine*](https://www.lesswrong.com/posts/D7epkkJb3CqDTYgX9/refine-an-incubator-for-conceptual-alignment-research-bets) *blog post day, at the end of a week of readings, discussions, and exercises about epistemology for doing good conceptual research. Thanks Adam Shimi, Linda Linsefors, Dan Clothiaux for comments.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/MfCDfuBHXL5ijJFco/steelmining-via-analogy"
    ],
    "tags": [
      "ai",
      "analogy",
      "refine"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1e3e4a215724f78f",
    "title": "How I think about alignment",
    "year": 2022,
    "category": "public_awareness",
    "description": "*This was written as part of the first*[*Refine*](https://www.lesswrong.com/posts/5uiQkyKdejX3aEHLM/how-to-diversify-conceptual-alignment-the-model-behind) *blog post day. Thanks for comments by Chin Ze Shen, Tamsin Leake, Paul Bricman, Adam Shimi.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/9bpACZn6kG2Ec6CPu/how-i-think-about-alignment"
    ],
    "tags": [
      "ai",
      "goal-directedness",
      "refine",
      "value learning",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2f9ce8bcac0f88b6",
    "title": "Shapes of Mind and Pluralism in Alignment",
    "year": 2022,
    "category": "public_awareness",
    "description": "*This post is part of the work done at* [*Conjecture*](https://conjecture.dev)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/RrirwtP7cNmHtJRxE/shapes-of-mind-and-pluralism-in-alignment"
    ],
    "tags": [
      "ai",
      "ai risk",
      "conjecture (org)",
      "epistemology",
      "practice & philosophy of science"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  }
]