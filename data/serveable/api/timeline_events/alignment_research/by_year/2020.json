[
  {
    "id": "alignmentforum_555121ab60501803",
    "title": "[AN #79]: Recursive reward modeling as an alignment technique integrated with deep RL",
    "year": 2020,
    "category": "policy_development",
    "description": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/EoY6P6mpz7ZozhAxm/an-79-recursive-reward-modeling-as-an-alignment-technique"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_fcbad849b33212ac",
    "title": "[AN #80]: Why AI risk might be solved without additional intervention from longtermists",
    "year": 2020,
    "category": "public_awareness",
    "description": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/QknPz9JQTQpGdaWDp/an-80-why-ai-risk-might-be-solved-without-additional"
    ],
    "tags": [
      "ai",
      "ai risk",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_daea13bad1d31b1c",
    "title": "Exploring safe exploration",
    "year": 2020,
    "category": "public_awareness",
    "description": "*This post is an attempt at reformulating some of the points I wanted to make in \"[Safe exploration and corrigibility](https://www.alignmentforum.org/posts/87Y7w73phjBxnPyPD/safe-exploration-and-corrigibility)\" in a clearer way. This post is standalone and does not assume that post as background.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/NBffcjqm2P4dNbjrE/exploring-safe-exploration"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2ade106204f7ec23",
    "title": "(Double-)Inverse Embedded Agency Problem",
    "year": 2020,
    "category": "public_awareness",
    "description": "MIRI [has said a lot](https://intelligence.org/embedded-agency/) about the issue of embedded agency over the last year. However, I am yet to see them trying to make progress in what I see as the most promising areas.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/itGmH2AknmjWyAwj8/double-inverse-embedded-agency-problem"
    ],
    "tags": [
      "embedded agency"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_303d09e63768206d",
    "title": "[AN #81]: Universality as a potential solution to conceptual difficulties in intent alignment",
    "year": 2020,
    "category": "policy_development",
    "description": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/3kzFPA5uuaGZWg4PS/an-81-universality-as-a-potential-solution-to-conceptual"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_088f3d3eb0aa6beb",
    "title": "Outer alignment and imitative amplification",
    "year": 2020,
    "category": "public_awareness",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/33EKjmAdKFn3pbKPJ/outer-alignment-and-imitative-amplification"
    ],
    "tags": [
      "ai",
      "goodhart's law",
      "outer alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3dcc44b42816621c",
    "title": "Of arguments and wagers",
    "year": 2020,
    "category": "public_awareness",
    "description": "*Automatically crossposted from* [*ai-alignment.com*](https://ai-alignment.com/)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/aPsdGPCpcyPqkatgc/of-arguments-and-wagers"
    ],
    "tags": [
      "betting"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3b943f80b0c1f994",
    "title": "Malign generalization without internal search",
    "year": 2020,
    "category": "public_awareness",
    "description": "In [my last post](https://www.alignmentforum.org/posts/nFDXq7HTv9Xugcqaw/is-the-term-mesa-optimizer-too-narrow), I challenged the idea that inner alignment failures should be explained by appealing to agents which perform explicit internal search. By doing so, I argued that we should instead appeal to the more general concept of *malign generalization*, and treat mesa-misalignment as a special case.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ynt9TD6PrYw6iT49m/malign-generalization-without-internal-search"
    ],
    "tags": [
      "ai",
      "inner alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e62549b14685db33",
    "title": "Update on Ought's experiments on factored evaluation of arguments",
    "year": 2020,
    "category": "public_awareness",
    "description": "[Ought](https://ought.org/) has written a detailed update and analysis of recent experiments on factored cognition. These are experiments with human participants and don't involve any machine learning. The goal is to learn about the viability of [IDA](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd?_ga=2.44167719.2055190071.1578702594-1142780176.1552454685), [Debate](https://openai.com/blog/debate/), and related [approaches](https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84) to AI alignment. For background, here are some prior LW posts on Ought: [Ought: Why it Matters and How to Help](https://www.lesswrong.com/posts/cpewqG3MjnKJpCr7E/ought-why-it-matters-and-ways-to-help)), [Factored Cognition presentation](https://www.lesswrong.com/posts/DFkGStzvj3jgXibFG/factored-cognition).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pH3eKEAEupx8c2ep9/update-on-ought-s-experiments-on-factored-evaluation-of"
    ],
    "tags": [
      "factored cognition",
      "ought"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ae5c0c8af2c74f2a",
    "title": "[AN #82]: How OpenAI Five distributed their training computation",
    "year": 2020,
    "category": "policy_development",
    "description": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/6tikKda9LBzrkLfBJ/an-82-how-openai-five-distributed-their-training-computation"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_af9e73f2774260ef",
    "title": "Inner alignment requires making assumptions about human values",
    "year": 2020,
    "category": "public_awareness",
    "description": "Many approaches to AI alignment require making assumptions about what humans want. On a first pass, it might appear that inner alignment is a sub-component of AI alignment that doesn't require making these assumptions. This is because if we define the problem of inner alignment to be the problem of how to train an AI to be aligned with arbitrary reward functions, then a solution would presumably have no dependence on any particular reward function. We could imagine an alien civilization solving the same problem, despite using very different reward functions to train their AIs.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/6m5qqkeBTrqQsegGi/inner-alignment-requires-making-assumptions-about-human"
    ],
    "tags": [
      "inner alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f29f96117ba41a26",
    "title": "[AN #83]: Sample-efficient deep learning with ReMixMatch",
    "year": 2020,
    "category": "policy_development",
    "description": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ZrCsaCXrMTgrX9GzK/an-83-sample-efficient-deep-learning-with-remixmatch"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_95c18bf6745da9a2",
    "title": "New paper: The Incentives that Shape Behaviour",
    "year": 2020,
    "category": "public_awareness",
    "description": "Abstract: Which variables does an agent have an incentive to control with its decision, and which variables does it have an incentive to respond to? We formalise these incentives, and demonstrate unique graphical criteria for detecting them in any single decision causal influence diagram. To this end, we introduce structural causal influence models, a hybrid of the influence diagram and structural causal model frameworks. Finally, we illustrate how these incentives predict agent incentives in both fairness and AI safety applications.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/TgPCet7m9DnkuxyKP/new-paper-the-incentives-that-shape-behaviour"
    ],
    "tags": [
      "academic papers",
      "causality",
      "machine learning  (ml)",
      "mechanism design"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_aff57fe8a743c3d7",
    "title": "The two-layer model of human values, and problems with synthesizing preferences",
    "year": 2020,
    "category": "public_awareness",
    "description": "I have been thinking about Stuart Armstrong's [preference synthesis research agenda](https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into), and have long had the feeling that there's something off about the way that it is currently framed. In the post I try to describe why. I start by describing my current model of human values, how I interpret Stuart's implicit assumptions to conflict with it, and then talk about my confusion with regard to reconciling the two views.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/2yLn8iTrvHoEgqXcJ/the-two-layer-model-of-human-values-and-problems-with"
    ],
    "tags": [
      "complexity of value",
      "motivations",
      "value learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f7d37210ea313f84",
    "title": "AI Alignment 2018-19 Review",
    "year": 2020,
    "category": "policy_development",
    "description": "Research publication: AI Alignment 2018-19 Review",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/dKxX76SCfCvceJXHv/ai-alignment-2018-19-review"
    ],
    "tags": [
      "ai",
      "ai risk",
      "ai takeoff",
      "ai timelines",
      "corrigibility",
      "impact regularization",
      "inner alignment",
      "outer alignment",
      "utility functions",
      "value learning"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2ad928d2fdd15e1e",
    "title": "Using vector fields to visualise preferences and make them consistent",
    "year": 2020,
    "category": "public_awareness",
    "description": "*This post was written for [Convergence Analysis](https://www.convergenceanalysis.org/) by Michael Aird, based on ideas from Justin Shovelain and with ongoing guidance from him. Throughout the post, \"I\" will refer to Michael, while \"we\" will refer to Michael and Justin or to Convergence as an organisation.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ky988ePJvCRhmCwGo/using-vector-fields-to-visualise-preferences-and-make-them"
    ],
    "tags": [
      "ai",
      "convergence (org)",
      "value learning"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e012e1002b27139b",
    "title": "[AN #84] Reviewing AI alignment work in 2018-19",
    "year": 2020,
    "category": "policy_development",
    "description": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/6Rv9kLGmXrkqRrcK9/an-84-reviewing-ai-alignment-work-in-2018-19"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_78b78f3de87f4edb",
    "title": "Towards deconfusing values",
    "year": 2020,
    "category": "public_awareness",
    "description": "*NB: Kaj recently said [some similar and related things](https://www.lesswrong.com/posts/2yLn8iTrvHoEgqXcJ/the-two-layer-model-of-human-values-and-problems-with) while I was on hiatus from finishing this post. I recommend reading it for a different take on what I view as a line of thinking generated by similar insights.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/WAqG5BQMzAs34mpc2/towards-deconfusing-values"
    ],
    "tags": [
      "motivations",
      "value learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c43c6fc8c96d7fc2",
    "title": "Pessimism About Unknown Unknowns Inspires Conservatism",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": "[This](https://mkcohen-hosted-files.s3-us-west-1.amazonaws.com/Pessimism_alignmentforum.pdf) [EDIT: [final version](http://proceedings.mlr.press/v125/cohen20a/cohen20a.pdf), [presentation](https://www.learningtheory.org/colt2020/virtual/papers/paper_221.html)] is a design for a conservative agent that I worked on with Marcus Hutter. Conservative agents are reluctant to make unprecedented things happen. The agent also approaches at least human-level reward acquisition.",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/RzAmPDNciirWKdtc7/pessimism-about-unknown-unknowns-inspires-conservatism"
    ],
    "tags": [
      "ai",
      "conservatism (ai)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_62da1fe9fbe5e197",
    "title": "[AN #85]: The normative questions we should be asking for AI alignment, and a surprisingly good chatbot",
    "year": 2020,
    "category": "public_awareness",
    "description": "[View this email in your browser](https://mailchi.mp/84b4235cfa34/an-85-the-normative-questions-we-should-be-asking-for-ai-alignment-and-a-surprisingly-good-chatbot?e=[UNIQID])",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Mj259G5n5BxXXrZ7C/an-85-the-normative-questions-we-should-be-asking-for-ai"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1613a80307de005b",
    "title": "Writeup: Progress on AI Safety via Debate",
    "year": 2020,
    "category": "policy_development",
    "description": "This is a writeup of the research done by the \"Reflection-Humans\" team at OpenAI in Q3 and Q4 of 2019. During that period we investigated mechanisms that would allow evaluators to get correct and helpful answers from experts, without the evaluators themselves being expert in the domain of the questions. This follows from the original work on [AI Safety via Debate](https://arxiv.org/abs/1805.00899) and the [call for research on human aspects of AI safety](https://distill.pub/2019/safety-needs-social-scientists/ ), and is also closely related to work on [Iterated Amplification](https://openai.com/blog/amplifying-ai-training/).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1"
    ],
    "tags": [
      "ai",
      "debate (ai safety technique)",
      "factored cognition",
      "iterated amplification"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ce725f008fd91120",
    "title": "Synthesizing amplification and debate",
    "year": 2020,
    "category": "policy_development",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/dJSD5RK6Qoidb3QY5/synthesizing-amplification-and-debate"
    ],
    "tags": [
      "ai",
      "debate (ai safety technique)",
      "iterated amplification"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_aed9942034889121",
    "title": "Plausibly, almost every powerful algorithm would be manipulative",
    "year": 2020,
    "category": "public_awareness",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Ez4zZQKWgC6fE3h9G/plausibly-almost-every-powerful-algorithm-would-be"
    ],
    "tags": [
      "ai risk",
      "deception",
      "instrumental convergence"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c00e9fa6708717e1",
    "title": "On the falsifiability of hypercomputation",
    "year": 2020,
    "category": "public_awareness",
    "description": "*[ED NOTE: see Vanessa Kosoy's comment [here](https://www.alignmentforum.org/posts/yjC5LmjSRD2hR9Pfa/on-the-falsifiability-of-hypercomputation?commentId=C9sw4eBRxG3aetAM5); this post assumes a setting in which the oracle may be assumed to return a standard natural.]*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/yjC5LmjSRD2hR9Pfa/on-the-falsifiability-of-hypercomputation"
    ],
    "tags": [
      "falsifiability"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_df18fa2670ce4bf1",
    "title": "What can the principal-agent literature tell us about AI risk?",
    "year": 2020,
    "category": "public_awareness",
    "description": "*This work was done collaboratively with Tom Davidson.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Z5ZBPEgufmDsm7LAv/what-can-the-principal-agent-literature-tell-us-about-ai"
    ],
    "tags": [
      "ai risk",
      "principal-agent problems",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0c8ec3f6ce6e6815",
    "title": "[AN #86]: Improving debate and factored cognition through human experiments",
    "year": 2020,
    "category": "policy_development",
    "description": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/cZqPGDxbJcbShGwDn/an-86-improving-debate-and-factored-cognition-through-human"
    ],
    "tags": [
      "ai",
      "newsletters",
      "ought"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7c0efd06f2d96808",
    "title": "Distinguishing definitions of takeoff",
    "year": 2020,
    "category": "public_awareness",
    "description": "I find discussions about AI takeoff to be very confusing. Often, people will argue for \"slow takeoff\" or \"fast takeoff\" and then when I ask them to operationalize what those terms mean, they end up saying something quite different than what *I* thought those terms meant.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff"
    ],
    "tags": [
      "ai",
      "ai takeoff"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5c28a52bdafb53e9",
    "title": "The Reasonable Effectiveness of Mathematics or: AI vs sandwiches",
    "year": 2020,
    "category": "public_awareness",
    "description": "**TLDR:** I try to find the root causes of why math is useful.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qpbYwTqKQG8G7mdFK/the-reasonable-effectiveness-of-mathematics-or-ai-vs"
    ],
    "tags": [
      "logic & mathematics"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1c1a206f970454a8",
    "title": "The Catastrophic Convergence Conjecture",
    "year": 2020,
    "category": "policy_development",
    "description": "![](https://i.imgur.com/Rgc4aOs.png)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/w6BtMqKRLxG9bNLMr/the-catastrophic-convergence-conjecture"
    ],
    "tags": [
      "ai",
      "impact regularization",
      "instrumental convergence"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c51f5ca5b1d9e7c9",
    "title": "Bayesian Evolving-to-Extinction",
    "year": 2020,
    "category": "public_awareness",
    "description": "*The present discussion owes a lot to Scott Garrabrant and Evan Hubinger.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/u9Azdu6Z7zFAhd4rK/bayesian-evolving-to-extinction"
    ],
    "tags": [
      "ai",
      "bayes' theorem",
      "myopia"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e20e1fa5572ac783",
    "title": "Does iterated amplification tackle the inner alignment problem?",
    "year": 2020,
    "category": "public_awareness",
    "description": "When iterated distillation and amplification (IDA) was published, some people described it described as \"the first comprehensive proposal for training safe AI\". Having read a bit more about it, it seems that IDA is mainly a proposal for outer alignment and doesn't deal with the inner alignment problem at all. Am I missing something?",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/RxutizkDNKzYCcNRv/does-iterated-amplification-tackle-the-inner-alignment"
    ],
    "tags": [
      "inner alignment",
      "iterated amplification"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_72acecc68cc06340",
    "title": "Reference Post: Trivial Decision Theory Problem",
    "year": 2020,
    "category": "public_awareness",
    "description": "A trivial decision problem is one where there is only a single option that the agent can take. In that case, the most natural answer to the answer to the question, \"What action should we take?\" would be \"The only action that we can take!\". We will call this the *Triviality Perspective*.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/XAeWHqQTWjJmzB4k6/reference-post-trivial-decision-theory-problem"
    ],
    "tags": [
      "decision theory"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_fa3ffe0f4ae0f4ee",
    "title": "On the falsifiability of hypercomputation, part 2: finite input streams",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": "In [part 1](https://unstableontology.com/2020/02/07/on-the-falsifiability-of-hypercomputation/), I discussed the falsifiability of hypercomputation in a *typed* setting where putative oracles may be assumed to return natural numbers. In this setting, there are very powerful forms of hypercomputation (at least as powerful as each level in the [Arithmetic hierarchy](https://en.wikipedia.org/wiki/Arithmetical_hierarchy)) that are falsifiable.",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/PtaN3oMFPfAAuBNtw/on-the-falsifiability-of-hypercomputation-part-2-finite"
    ],
    "tags": [
      "falsifiability"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8f1f2796d06d9082",
    "title": "Wireheading and discontinuity",
    "year": 2020,
    "category": "policy_development",
    "description": "**Outline**: After a short discussion on the relationship between wireheading and reward hacking, I show why checking the continuity of a sensor function could be useful to detect wireheading in the context of continuous RL. Then, I give an example that adopts the presented formalism. I conclude with some observations.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/KLNDgqQLfpFXbhQak/wireheading-and-discontinuity"
    ],
    "tags": [
      "ai",
      "wireheading"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_357f5cf4c652cbd0",
    "title": "[AN #87]: What might happen as deep learning scales even further?",
    "year": 2020,
    "category": "policy_development",
    "description": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/69XPfonos795hD57o/an-87-what-might-happen-as-deep-learning-scales-even-further"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cf228da698105ab8",
    "title": "On unfixably unsafe AGI architectures",
    "year": 2020,
    "category": "policy_development",
    "description": "There's loads of discussion on ways that things can go wrong as we enter the post-AGI world. I think an especially important one for guiding current research is:",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qvyv72fCiC46sxfPt/on-unfixably-unsafe-agi-architectures"
    ],
    "tags": [
      "ai",
      "ai risk"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5b2d7f9140de1627",
    "title": "Tessellating Hills: a toy model for demons in imperfect search",
    "year": 2020,
    "category": "public_awareness",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/X7S3u5E4KktLp7gHz/tessellating-hills-a-toy-model-for-demons-in-imperfect"
    ],
    "tags": [
      "inner alignment",
      "optimization",
      "programming"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f35fcc7b111a920f",
    "title": "Goal-directed = Model-based RL?",
    "year": 2020,
    "category": "policy_development",
    "description": "**Epistemic Status**: quick write-up, in reaction to a serendipitous encounter with an idea. I see the main value of this post as decently presenting a potentially interesting take on a concept in AI safety to the community.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Tux9WH4daKcxjEetQ/goal-directed-model-based-rl"
    ],
    "tags": [
      "goal-directedness"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8fdd0974c5f0ac8e",
    "title": "Will AI undergo discontinuous progress?",
    "year": 2020,
    "category": "public_awareness",
    "description": "This post grew out of conversations with several people, including [Daniel Kokotajlo](https://www.lesswrong.com/users/daniel-kokotajlo), [grue\\_slinky](https://www.lesswrong.com/users/grue_slinky) and [Linda Lisefors](https://www.lesswrong.com/users/linda-linsefors), and is based in large part on a collection of scattered comments and blog-posts across lesswrong, along with some podcast interviews - e.g. [here](https://www.lesswrong.com/posts/CjW4axQDqLd2oDCGG/misconceptions-about-continuous-takeoff?commentId=YPodaAtRhN4qJefxb). The in-text links near quotes will take you to my sources.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/5WECpYABCT62TJrhY/will-ai-undergo-discontinuous-progress"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1e6bf1ec08c06fe0",
    "title": "Attainable Utility Preservation: Empirical Results",
    "year": 2020,
    "category": "policy_development",
    "description": "*Reframing Impact* has focused on supplying the right intuitions and framing. Now we can see how these intuitions about power and the AU landscape both predict and explain AUP's empirical success thus far.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/4J4TA2ZF3wmSxhxuc/attainable-utility-preservation-empirical-results"
    ],
    "tags": [
      "ai",
      "impact regularization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c9329ce90c9551da",
    "title": "How Low Should Fruit Hang Before We Pick It?",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": "*Even if we can measure how impactful an agent's actions are, how impactful do we let the agent be? This post uncovers a surprising fact: armed with just four numbers, we can set the impact level so that the agent chooses a reasonable, non-catastrophic plan on the first try. This understanding increases the competitiveness of impact-limited agents and helps us judge impact measures. Furthermore, the results help us better understand diminishing returns and cost-benefit tradeoffs.*",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/LfGzAduBWzY5gq6FE/how-low-should-fruit-hang-before-we-pick-it"
    ],
    "tags": [
      "ai",
      "impact regularization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_fe22308d33e2b05d",
    "title": "If I were a well-intentioned AI... I: Image classifier",
    "year": 2020,
    "category": "public_awareness",
    "description": "Introduction: If I were a well-intentioned AI...\n================================================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/gzWb5kWwzhdaqmyTt/if-i-were-a-well-intentioned-ai-i-image-classifier"
    ],
    "tags": [
      "adversarial examples",
      "ai",
      "machine learning  (ml)",
      "outer alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1dc23697da05da4a",
    "title": "[AN #88]: How the principal-agent literature relates to AI risk",
    "year": 2020,
    "category": "policy_development",
    "description": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/y9JeNZ2WAkR6MbBZH/an-88-how-the-principal-agent-literature-relates-to-ai-risk"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_6320bde3a6b60c7e",
    "title": "If I were a well-intentioned AI... II: Acting in a world",
    "year": 2020,
    "category": "policy_development",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ZKzAjKSeNRtiaeJns/if-i-were-a-well-intentioned-ai-ii-acting-in-a-world"
    ],
    "tags": [
      "ai",
      "outer alignment"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3ed02eb5d609ffd9",
    "title": "Reasons for Excitement about Impact of Impact Measure Research",
    "year": 2020,
    "category": "policy_development",
    "description": "Can we get impact measurement *right*? Does there exist One Equation To Rule Them All?",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/wAAvP8RG6EwzCvHJy/reasons-for-excitement-about-impact-of-impact-measure"
    ],
    "tags": [
      "ai",
      "impact regularization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d32b2cc700b53f60",
    "title": "Conclusion to 'Reframing Impact'",
    "year": 2020,
    "category": "public_awareness",
    "description": "![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/pbmk8ndyip6nyu4ntf6z.png)![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/icddpmwoxx5ftcysxo8k.png)![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/mxhzcdashtl5euloeolx.png)![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/d1mqg6p4ghuweu4sth5u.png)![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/veypvrfwfr1xwwz4zx8m.png)![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/qanem2tu332ayspkhutk.png)![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/lza8s3ncwyioba7gn5kc.png)![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnij...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/sHpiiZS2gPgoPnijX/conclusion-to-reframing-impact"
    ],
    "tags": [
      "ai",
      "impact regularization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0e12ffc390229a85",
    "title": "Trace: Goals and Principles",
    "year": 2020,
    "category": "public_awareness",
    "description": "In terms of research, I decided to devote the month of February mainly to foundations and tools. One project was to come up with a notation/language/framework which matches the way I've been thinking about computation - i.e. [DAGs with symmetry](https://www.lesswrong.com/posts/mZy6AMgCw9CPjNCoK/computational-model-causal-diagrams-with-symmetry) and [\"clouds\" representing DAG-structure-as-data](https://www.lesswrong.com/posts/G25RBnBk5BNpv3KyF/a-greater-than-b-greater-than-a-in-causal-dags). The tool I've been building - a Python library tentatively called Trace - isn't stable enough that I want to show it off yet, but I do think I've nailed down the core goals and principles, so it's time to write them up.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/rt5X74Az3mXwTubRA/trace-goals-and-principles"
    ],
    "tags": [
      "abstraction"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_31792fa82be34200",
    "title": "Cortes, Pizarro, and Afonso as Precedents for Takeover",
    "year": 2020,
    "category": "public_awareness",
    "description": "*Crossposted from [AI Impacts](https://aiimpacts.org/cortes-pizarro-and-afonso-as-precedents-for-ai-takeover/).*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ivpKSjM4D6FbqF4pZ/cortes-pizarro-and-afonso-as-precedents-for-takeover"
    ],
    "tags": [
      "ai",
      "history"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1d02913034284118",
    "title": "An Analytic Perspective on AI Alignment",
    "year": 2020,
    "category": "public_awareness",
    "description": "This is a perspective I have on how to do useful AI alignment research. Most perspectives I'm aware of are constructive: they have some blueprint for how to build an aligned AI system, and propose making it more concrete, making the concretisations more capable, and showing that it does in fact produce an aligned AI system. I do not have a constructive perspective - I'm not sure how to build an aligned AI system, and don't really have a favourite approach. Instead, I have an analytic perspective. I would like to understand AI systems that are built. I also want other people to understand them. I think that this understanding will hopefully act as a 'filter' that means that dangerous AI systems are not deployed. The following dot points lay out the perspective.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/8GdPargak863xaebm/an-analytic-perspective-on-ai-alignment"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_19852a880661c825",
    "title": "If I were a well-intentioned AI... IV: Mesa-optimising",
    "year": 2020,
    "category": "public_awareness",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/aqhMLqaoHb7uob7fr/if-i-were-a-well-intentioned-ai-iv-mesa-optimising"
    ],
    "tags": [
      "ai",
      "inner alignment",
      "mesa-optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_6c8452562691e8a3",
    "title": "Anthropics over-simplified: it's about priors, not updates",
    "year": 2020,
    "category": "public_awareness",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Hpam4RrJKfufXrmAi/anthropics-over-simplified-it-s-about-priors-not-updates"
    ],
    "tags": [
      "anthropics"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_280794cc600aa563",
    "title": "[AN #89]: A unifying formalism for preference learning algorithms",
    "year": 2020,
    "category": "policy_development",
    "description": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/7bNXqdDPYpnfCNQhA/an-89-a-unifying-formalism-for-preference-learning"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_964b28072f467b84",
    "title": "Zoom In: An Introduction to Circuits",
    "year": 2020,
    "category": "public_awareness",
    "description": "Chris Olah and the rest of the rest of the OpenAI Clarity team just published \"[Zoom In: An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/),\" a Distill article about some of the transparency research they've been doing which I think is very much worth taking a look at. I'll try to go over some of my particular highlights here, but I highly recommend reading the full article.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/MG4ZjWQDrdpgeu8wG/zoom-in-an-introduction-to-circuits"
    ],
    "tags": [
      "ai",
      "logic & mathematics",
      "openai",
      "programming"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_42bc43e92c830b86",
    "title": "[AN #90]: How search landscapes can contain self-reinforcing feedback loops",
    "year": 2020,
    "category": "policy_development",
    "description": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/7d2PsdHXrJnbofrvF/an-90-how-search-landscapes-can-contain-self-reinforcing"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8175e731f73e0e6d",
    "title": "What are some exercises for building/generating intuitions about key disagreements in AI alignment?",
    "year": 2020,
    "category": "public_awareness",
    "description": "I am interested in having my own opinion about more of the [key disagreements](https://www.lesswrong.com/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment) within the AI alignment field, such as whether there is a basin of attraction for corrigibility, whether there is [a theory of rationality that is sufficiently precise to build hierarchies of abstraction](https://www.greaterwrong.com/posts/suxvE2ddnYMPJN9HD/realism-about-rationality/comment/YMNwHcPNPd4pDK7MR), and to what extent there will be a [competence gap](https://agentfoundations.org/item?id=64).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/bDwQddhqaTiMhbpPF/what-are-some-exercises-for-building-generating-intuitions"
    ],
    "tags": [
      "intuition"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_11fe38781ba058c1",
    "title": "AI Alignment Podcast: On Lethal Autonomous Weapons with Paul Scharre",
    "year": 2020,
    "category": "policy_development",
    "description": "Most relevant to AI alignment, and a pertinent question to focus on for interested readers/listeners is: if we are are unable to establish a governance mechanism as a global community on the concept that we should not let AI make the decision to kill humans, then what effects will this have on and can we still deal with more subtle short-term alignment considerations and long-term AI x-risk?",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/xEzudcydk7APZbnai/ai-alignment-podcast-on-lethal-autonomous-weapons-with-paul"
    ],
    "tags": [
      "autonomous weapons",
      "interviews",
      "transcripts"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_feb0ab58247cedb8",
    "title": "What is Interpretability?",
    "year": 2020,
    "category": "policy_development",
    "description": "In this post we lay out some ideas around framing interpretability research which we have found quite useful. Our framing is goal-oriented, which we believe is important for making sure interpretability research is meaningful. We also go over a variety of dimensions which we think are useful to consider when thinking about interpretability research. We wanted to have a shared vocabulary when talking about this kind of research, and found that these ideas helped us communicate effectively.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/rSMbGFfsLMB3GWZtX/what-is-interpretability"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5b1612aa19d40c73",
    "title": "[AN #91]: Concepts, implementations, problems, and a benchmark for impact measurement",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/dJanptWZnZx5omwBz/an-91-concepts-implementations-problems-and-a-benchmark-for"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_fed24f043ccbd4d5",
    "title": "Alignment as Translation",
    "year": 2020,
    "category": "public_awareness",
    "description": "[Technology Changes Constraints](https://www.lesswrong.com/s/xEFeCwk3pdYdeG2rL/p/JJv8jmLYzYzdYkS3c) argues that economic constraints are usually modular with respect to technology changes - so for reasoning about technology changes, it's useful to cast them in terms of economic constraints. Two constraints we'll talk about here:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/42YykiTqtGMyJAjDM/alignment-as-translation"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f61bfa282f241f7d",
    "title": "Thinking About Filtered Evidence Is (Very!) Hard",
    "year": 2020,
    "category": "public_awareness",
    "description": "*The content of this post would not exist if not for conversations with Zack Davis, and owes something to conversations with Sam Eisenstat.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/fhJkQo34cYw6KqpH3/thinking-about-filtered-evidence-is-very-hard"
    ],
    "tags": [
      "epistemic hygiene",
      "filtered evidence",
      "rationality"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_00afed09943bf665",
    "title": "[Meta] Do you want AIS Webinars?",
    "year": 2020,
    "category": "public_awareness",
    "description": "Ever now and then I talk to someone who tell me that they can not get good feedback on their research (e.g. the they don't get much responses on their alignment forum post), and been thinking about how to solve this? Also, right now is a good time to try out various online solutions.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/BbrsgHPJmGxeg7nXG/meta-do-you-want-ais-webinars"
    ],
    "tags": [
      "ai",
      "community"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_28c9f38b43807c65",
    "title": "Deconfusing Human Values Research Agenda v1",
    "year": 2020,
    "category": "public_awareness",
    "description": "On Friday I attended the [2020 Foresight AGI Strategy Meeting](https://foresight.org/event/2020-foresight-agi-strategy-meeting/). Eventually a report will come out summarizing some of what was talked about, but for now I want to focus on what I talked about in my session on deconfusing human values. For that session I wrote up some notes summarizing what I've been working on and thinking about. None of it is new, but it is newly condensed in one place and in convenient list form, and it provides a decent summary of the current state of my research agenda for building beneficial superintelligent AI; a version 1 of my agenda, if you will. Thus, I hope this will be helpful in making it a bit clearer what it is I'm working on, why I'm working on it, and what direction my thinking is moving in. As always, if you're interesting in collaborating on things, whether that be discussing ideas or something more, please reach out.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/k8F8TBzuZtLheJt47/deconfusing-human-values-research-agenda-v1"
    ],
    "tags": [
      "ai",
      "metaethics",
      "perceptual control theory",
      "research agendas",
      "value learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e695f16ba4a31805",
    "title": "[AN #92]: Learning good representations with contrastive predictive coding",
    "year": 2020,
    "category": "public_awareness",
    "description": "Research publication: [AN #92]: Learning good representations with contrastive predictive coding",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/XE6LD2c9NtB7gMdEm/an-92-learning-good-representations-with-contrastive"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_86e4991caf1e071a",
    "title": "How important are MDPs for AGI (Safety)?",
    "year": 2020,
    "category": "policy_development",
    "description": "I don't think finite-state MDPs are a particularly powerful conceptual tool for designing strong RL algorithms. I'll consider the case of no function approximation first.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/6gL83HMF6tvPHKQxW/how-important-are-mdps-for-agi-safety"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9fbfa00d56e32f5f",
    "title": "What are the most plausible \"AI Safety warning shot\" scenarios?",
    "year": 2020,
    "category": "public_awareness",
    "description": "A \"AI safety warning shot\" is some event that causes a substantial fraction of the relevant human actors (governments, AI researchers, etc.) to become substantially more supportive of AI research and worried about existential risks posed by AI.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/hLKKH9CM6NDiJBabC/what-are-the-most-plausible-ai-safety-warning-shot-scenarios"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_37c4653c6f89b70d",
    "title": "My current framework for thinking about AGI timelines",
    "year": 2020,
    "category": "public_awareness",
    "description": "At the beginning of 2017, someone I deeply trusted said they thought AGI would come in 10 years, with 50% probability.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/w4jjwDPa853m9P4ag/my-current-framework-for-thinking-about-agi-timelines"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "ai timelines",
      "crucial considerations"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9843ed3236fb5278",
    "title": "Three Kinds of Competitiveness",
    "year": 2020,
    "category": "public_awareness",
    "description": "*Crossposted from [AI Impacts](https://aiimpacts.org/three-kinds-of-competitiveness/)*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/sD6KuprcS3PFym2eM/three-kinds-of-competitiveness"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_98bb204c7519335b",
    "title": "How special are human brains among animal brains?",
    "year": 2020,
    "category": "public_awareness",
    "description": "Humans are capable of feats of cognition that appear qualitatively more sophisticated than those of any other animals. Is this appearance of a qualitative difference indicative of human brains being essentially more complex than the brains of any other animal? Or is this \"qualitative difference\" illusory, with the vast majority of human cognitive feats explainable as nothing more than a scaled-up version of the cognitive feats of lower animals?",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/d2jgBurQygbXzhPxc/how-special-are-human-brains-among-animal-brains"
    ],
    "tags": [
      "ai timelines",
      "biology",
      "consciousness",
      "general intelligence",
      "neuroscience"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_80a73329506f5614",
    "title": "[AN #93]: The Precipice we're standing at, and how we can back away from it",
    "year": 2020,
    "category": "public_awareness",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/rPC9Y9b5vkTqakywC/an-93-the-precipice-we-re-standing-at-and-how-we-can-back"
    ],
    "tags": [
      "ai",
      "existential risk",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f1aec2e3cc5a4b07",
    "title": "Equilibrium and prior selection problems in multipolar deployment",
    "year": 2020,
    "category": "policy_development",
    "description": "To [avoid catastrophic conflict in multipolar AI scenarios](https://www.alignmentforum.org/posts/DbuCdEbkh4wL5cjJ5/preface-to-eaf-s-research-agenda-on-cooperation-conflict-and), we would like to design AI systems such that AI-enabled actors will tend to cooperate. This post is about some problems facing this effort and some possible solutions. To explain these problems, I'll take the view that the agents deployed by AI developers (the ''principals'') in a multipolar scenario are *moves in a game*. The payoffs to a principal in this game depend on how the agents behave over time. We can talk about the equilibria of this game, and so on. Ideally, we would be able to make guarantees like this:",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Tdu3tGT4i24qcLESh/equilibrium-and-prior-selection-problems-in-multipolar-1"
    ],
    "tags": [
      "ai",
      "ai governance",
      "multipolar scenarios"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_488aa02e04bf12da",
    "title": "Announcing Web-TAISU, May 13-17",
    "year": 2020,
    "category": "public_awareness",
    "description": "**I am excited to announce Web-TAISU!**  \n**May 13-17, 2020**",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/CMnMaTxNAhXfcEtgm/announcing-web-taisu-may-13-17"
    ],
    "tags": [
      "community page"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_69f2bdac8e378f73",
    "title": "Resources for AI Alignment Cartography",
    "year": 2020,
    "category": "policy_development",
    "description": "**I want to make an *actionable* map of AI alignment.**",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/4az2cFrJp3ya4y6Wx/resources-for-ai-alignment-cartography"
    ],
    "tags": [
      "ai",
      "research agendas"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_611d65026a0d4b07",
    "title": "An Orthodox Case Against Utility Functions",
    "year": 2020,
    "category": "public_awareness",
    "description": "*This post has benefitted from discussion with Sam Eisenstat, Scott Garrabrant, Tsvi Benson-Tilsen, Daniel Demski, Daniel Kokotajlo, and Stuart Armstrong. It started out as a thought about [Stuart Armstrong's research agenda](https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into-1).*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/A8iGaZ3uHNNGgJeaD/an-orthodox-case-against-utility-functions"
    ],
    "tags": [
      "ai",
      "decision theory",
      "indexical information",
      "rationality",
      "utility functions"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2c39d75c7c6ecbd5",
    "title": "[AN #94]: AI alignment as translation between humans and machines",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/gor57NZtxG4bq5eej/an-94-ai-alignment-as-translation-between-humans-and"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_874c8d8911638768",
    "title": "Asymptotically Unambitious AGI",
    "year": 2020,
    "category": "public_awareness",
    "description": "Research publication: Asymptotically Unambitious AGI",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pZhDWxDmwzuSwLjou/asymptotically-unambitious-agi"
    ],
    "tags": [
      "ai",
      "bounties (closed)",
      "impact regularization",
      "instrumental convergence"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_eb1ad3f15a8260a9",
    "title": "\"How conservative\" should the partial maximisers be?",
    "year": 2020,
    "category": "policy_development",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/jRHLCyRKsQv5u2Lph/how-conservative-should-the-partial-maximisers-be"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1b0f1c1c51b7f0cd",
    "title": "[AN #95]: A framework for thinking about how to make AI go well",
    "year": 2020,
    "category": "public_awareness",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/9et86yPRk6RinJNt3/an-95-a-framework-for-thinking-about-how-to-make-ai-go-well"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f470eb8b93f5a70c",
    "title": "AI Alignment Podcast: An Overview of Technical AI Alignment in 2018 and 2019 with Buck Shlegeris and Rohin Shah",
    "year": 2020,
    "category": "policy_development",
    "description": "Just a year ago we released a two part episode titled [An Overview of Technical AI Alignment with Rohin Shah](https://futureoflife.org/2019/04/11/an-overview-of-technical-ai-alignment-with-rohin-shah-part-1/). That conversation provided details on the views of central AI alignment research organizations and many of the ongoing research efforts for designing safe and aligned systems. Much has happened in the past twelve months, so we've invited Rohin -- along with fellow researcher Buck Shlegeris -- back for a follow-up conversation. Today's episode focuses especially on the state of current research efforts for beneficial AI, as well as Buck's and Rohin's thoughts about the varying approaches and the difficulties we still face. This podcast thus serves as a non-exhaustive overview of how the field of AI alignment has updated and how thinking is progressing.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/6skeZgctugzBBEBw3/ai-alignment-podcast-an-overview-of-technical-ai-alignment"
    ],
    "tags": [
      "ai",
      "interviews",
      "moral uncertainty",
      "regulation and ai risk",
      "transcripts",
      "value learning"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_20c7c73eb778b27e",
    "title": "AI Services as a Research Paradigm",
    "year": 2020,
    "category": "policy_development",
    "description": "Introduction\n============",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/z2ofM2oZQwmcWFt8N/ai-services-as-a-research-paradigm"
    ],
    "tags": [
      "ai",
      "ai services (cais)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1833daf1121085b5",
    "title": "Inner alignment in the brain",
    "year": 2020,
    "category": "public_awareness",
    "description": "**Abstract:** We can think of the brain crudely as (1) a neocortex which runs an amazingly capable quasi-general-purpose learning-and-planning algorithm, and (2) subcortical structures (midbrain, etc.), one of whose functions is to calculate rewards that get sent to up the neocortex to direct it. But the relationship is actually more complicated than that. \"Reward\" is not the only informational signal sent up to the neocortex; meanwhile information is also flowing back down in the opposite direction. What's going on? How does all this work? Where do emotions fit in? Well, I'm still confused on many points, but I think I'm making progress. In this post I will describe my current picture of this system.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/DWFx2Cmsvd4uCKkZ4/inner-alignment-in-the-brain"
    ],
    "tags": [
      "ai",
      "inner alignment",
      "neocortex",
      "neuroscience",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9fdb067a24041dc6",
    "title": "[AN #96]: Buck and I discuss/argue about AI Alignment",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/YyKKMeCCxnzdohuxj/an-96-buck-and-i-discuss-argue-about-ai-alignment"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_222dfa0bcba6391e",
    "title": "Problem relaxation as a tactic",
    "year": 2020,
    "category": "public_awareness",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/JcpwEKbmNHdwhpq5n/problem-relaxation-as-a-tactic"
    ],
    "tags": [
      "ai",
      "rationality",
      "techniques"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b13455d69d0bb442",
    "title": "What makes counterfactuals comparable?",
    "year": 2020,
    "category": "public_awareness",
    "description": "l was attempting to write a reference post on the concept of comparability in decision theory problems, but I realised that I don't yet have a strong enough grasp on the various positions that one could adopt to write a post worthy of being a reference. I'll quote my draft quite liberally below:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/6E6D3qLPM3urXDPpK/what-makes-counterfactuals-comparable-1"
    ],
    "tags": [
      "ai",
      "counterfactuals",
      "decision theory"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b11a47358c570e11",
    "title": "[AN #97]: Are there historical examples of large, robust discontinuities?",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/WknLjywekGajwD2fp/an-97-are-there-historical-examples-of-large-robust"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_08cf4306f9a54736",
    "title": "What is the alternative to intent alignment called?",
    "year": 2020,
    "category": "public_awareness",
    "description": "Paul defines intent alignment of an AI A to a human H as the criterion that A is trying to do what H wants it to do. What term do people use for the definition of alignment in which A is trying to achieve H's goals (whether or not H intends for A to achieve H's goals)?",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/NuhsBLxxswinm2JKZ/what-is-the-alternative-to-intent-alignment-called"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7f4ecef8e113f9e4",
    "title": "Topological metaphysics: relating point-set topology and locale theory",
    "year": 2020,
    "category": "public_awareness",
    "description": "The following is an informal exposition of some mathematical concepts from *[Topology via Logic](https://www.amazon.com/Topology-Cambridge-Theoretical-Computer-Science/dp/0521576512)*, with special attention to philosophical implications. Those seeking more technical detail should simply read the book.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/yTvZFzcgt7rGYMxP5/topological-metaphysics-relating-point-set-topology-and"
    ],
    "tags": [
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b6dd472a6a052885",
    "title": "How does iterated amplification exceed human abilities?",
    "year": 2020,
    "category": "public_awareness",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ajQzejMYizfX4dMWK/how-does-iterated-amplification-exceed-human-abilities"
    ],
    "tags": [
      "ai",
      "iterated amplification"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_78004975cbfbb7e3",
    "title": "How uniform is the neocortex?",
    "year": 2020,
    "category": "public_awareness",
    "description": "How uniform is the neocortex?\n=============================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/WFopenhCXyHX3ukw3/how-uniform-is-the-neocortex"
    ],
    "tags": [
      "ai",
      "neocortex",
      "neuroscience",
      "predictive processing",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cef4b7a1811e1a93",
    "title": "Competitive safety via gradated curricula",
    "year": 2020,
    "category": "public_awareness",
    "description": "*Epistemic status: brainstorming some speculative research directions. Not trying to thoroughly justify the claims I'm making.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/vLepnCxCWW6YTw8eW/competitive-safety-via-gradated-curricula"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2a3545fc9f4358c0",
    "title": "[AN #98]: Understanding neural net training by seeing which gradients were helpful",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Sj9YurD9vwpfPErs2/an-98-understanding-neural-net-training-by-seeing-which"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_054a2bfba124eaa8",
    "title": "Specification gaming: the flip side of AI ingenuity",
    "year": 2020,
    "category": "policy_development",
    "description": "*(Originally posted to the* [*Deepmind Blog*](https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity)*)*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/7b2RJJQ76hjZwarnj/specification-gaming-the-flip-side-of-ai-ingenuity"
    ],
    "tags": [
      "ai",
      "goodhart's law"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3d6e175d6dc8a3f5",
    "title": "Corrigibility as outside view",
    "year": 2020,
    "category": "public_awareness",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/BMj6uMuyBidrdZkiD/corrigibility-as-outside-view"
    ],
    "tags": [
      "ai",
      "corrigibility",
      "inside/outside view"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_17c2cd4f5354b565",
    "title": "[AN #99]: Doubling times for the efficiency of AI algorithms",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/R6gPKJAq6dbuLNkwG/an-99-doubling-times-for-the-efficiency-of-ai-algorithms"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1e303dc03ac09398",
    "title": "Multi-agent safety",
    "year": 2020,
    "category": "public_awareness",
    "description": "*Note: this post is most explicitly about safety in multi-agent training regimes. However, many of the arguments I make are also more broadly applicable - for example, when training a single agent in a complex environment, challenges arising from the environment could play an analogous role to challenges arising from other agents. In particular, I expect that the diagram in the 'Developing General Intelligence' section will be applicable to most possible ways of training an AGI.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/BXMCgpktdiawT3K5v/multi-agent-safety"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_78d27b7ee103d2ce",
    "title": "The Mechanistic and Normative Structure of Agency",
    "year": 2020,
    "category": "public_awareness",
    "description": "Winning, Jason (2019). *The Mechanistic and Normative Structure of Agency*. Dissertation, University of California San Diego.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/QBHxfATzdASQcXwan/the-mechanistic-and-normative-structure-of-agency"
    ],
    "tags": [
      "ai",
      "rationality"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_37442e305ca52913",
    "title": "Probabilities, weights, sums: pretty much the same for reward functions",
    "year": 2020,
    "category": "policy_development",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pxWEKPHNBzXZWi2rB/probabilities-weights-sums-pretty-much-the-same-for-reward"
    ],
    "tags": [
      "ai",
      "reward functions"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7fa4aa440cfda677",
    "title": "[AN #100]: What might go wrong if you learn a reward function while acting",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": "**Newsletter #100 (!!)**",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/GYmDaFgePMchYj6P7/an-100-what-might-go-wrong-if-you-learn-a-reward-function"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5353b9246380daef",
    "title": "AGIs as collectives",
    "year": 2020,
    "category": "policy_development",
    "description": "*Note that I originally used the term* population AGI, *but changed it to* collective AGI *to match Bostrom's usage in* Superintelligence*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/HekjhtWesBWTQW5eF/agis-as-collectives"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b59381f21edb033c",
    "title": "How can Interpretability help Alignment?",
    "year": 2020,
    "category": "public_awareness",
    "description": "Introduction\n============",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/uRnprGSiLGXv35foX/how-can-interpretability-help-alignment"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)",
      "machine learning  (ml)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_65926377e9a903ef",
    "title": "AI Safety Discussion Days",
    "year": 2020,
    "category": "policy_development",
    "description": "AI Safety Discussion Days",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/32QD3tRfognNHN9xw/ai-safety-discussion-days"
    ],
    "tags": [
      "ai",
      "community"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ca77dbe5de5abefe",
    "title": "[AN #101]: Why we should rigorously measure and forecast AI progress",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/axzPYvcmWr2TwvnLi/an-101-why-we-should-rigorously-measure-and-forecast-ai"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_38810ad3c8e80a6e",
    "title": "An overview of 11 proposals for building safe advanced AI",
    "year": 2020,
    "category": "policy_development",
    "description": "*This is the blog post version of [the paper by the same name](https://arxiv.org/abs/2012.07532). Special thanks to Kate Woolverton, Paul Christiano, Rohin Shah, Alex Turner, William Saunders, Beth Barnes, Abram Demski, Scott Garrabrant, Sam Eisenstat, and Tsvi Benson-Tilsen for providing helpful comments and feedback on this post and the talk that preceded it.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai"
    ],
    "tags": [
      "ai",
      "ai risk",
      "ai success models",
      "debate (ai safety technique)",
      "inner alignment",
      "iterated amplification",
      "myopia",
      "outer alignment",
      "research agendas"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b0eaa3c757c8ad35",
    "title": "Possible takeaways from the coronavirus pandemic for slow AI takeoff",
    "year": 2020,
    "category": "policy_development",
    "description": "*(Cross-posted from [personal blog](https://vkrakovna.wordpress.com/2020/05/31/possible-takeaways-from-the-coronavirus-pandemic-for-slow-ai-takeoff/). Summarized in [Alignment Newsletter #104](https://mailchi.mp/ba4d1765368f/an-104-the-perils-of-inaccessible-information-and-what-we-can-learn-about-ai-alignment-from-covid). Thanks to Janos Kramar for his helpful feedback on this post.)*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/wTKjRFeSjKLDSWyww/possible-takeaways-from-the-coronavirus-pandemic-for-slow-ai"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "covid-19",
      "existential risk",
      "world optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_fdc33ef63f2ec976",
    "title": "Sparsity and interpretability?",
    "year": 2020,
    "category": "public_awareness",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/maBNBgopYxb9YZP8B/sparsity-and-interpretability-1"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_bd82c86a3ddc5d2c",
    "title": "Building brain-inspired AGI is infinitely easier than understanding the brain",
    "year": 2020,
    "category": "public_awareness",
    "description": "*Epistemic status: Trying to explain why I have certain intuitions. Not sure whether people will find this obvious vs controversial.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/PTkd8nazvH9HQpwP8/building-brain-inspired-agi-is-infinitely-easier-than"
    ],
    "tags": [
      "ai",
      "neuroscience"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_bf8cec9afcdccae7",
    "title": "Inaccessible information",
    "year": 2020,
    "category": "policy_development",
    "description": "Suppose that I have a great model for predicting \"what will Alice say next?\"",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ZyWyAJbedvEgRT2uF/inaccessible-information"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_377dc467f1f90736",
    "title": "[AN #102]: Meta learning by GPT-3, and a list of full proposals for AI alignment",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/D3hP47pZwXNPRByj8/an-102-meta-learning-by-gpt-3-and-a-list-of-full-proposals"
    ],
    "tags": [
      "ai",
      "gpt",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_64f18b0e78cea4e5",
    "title": "Focus: you are allowed to be bad at accomplishing your goals",
    "year": 2020,
    "category": "policy_development",
    "description": "When asked about what it means for a system to be [goal-directed](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/DfcywmqRSkBaCB6Ma), one common answer draws on some version of Dennett's [intentional stance](https://en.wikipedia.org/wiki/Intentional_stance): a goal-directed system is a system such that modeling it as having a goal provides accurate and efficient predictions about its behavior. I agree up to that point. But then, some people follow up by saying that the prediction is that the system will accomplish its goal. For example, it makes sense to model AlphaGo as goal-directed towards winning at Go, because it will eventually win. And taking the intentional stance allows me to predict that.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/X5WTgfX5Ly4ZNHWZD/focus-you-are-allowed-to-be-bad-at-accomplishing-your-goals"
    ],
    "tags": [
      "ai",
      "goal-directedness",
      "rationality"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9a640eb49f1ce8b8",
    "title": "Reply to Paul Christiano on Inaccessible Information",
    "year": 2020,
    "category": "policy_development",
    "description": "In [Inaccessible Information](https://www.lesswrong.com/posts/ZyWyAJbedvEgRT2uF/inaccessible-information), Paul Christiano lays out a fundamental challenge in training machine learning systems to give us insight into parts of the world that we cannot directly verify. The core problem he lays out is as follows.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/A9vvxguZMytsN3ze9/reply-to-paul-christiano-on-inaccessible-information"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_82a566fcda3ff489",
    "title": "Goal-directedness is behavioral, not structural",
    "year": 2020,
    "category": "public_awareness",
    "description": "Goal-directedness is the term used by the AI Safety community to point to a specific property: following a goal. It comes from Rohin Shah's [post](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/DfcywmqRSkBaCB6Ma) in his [sequence](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc), but the intuition pervades many safety issues and current AI approaches. Yet it lacks a formal definition, or even a decomposition into more or less formal subcomponents.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/9pxcekdNjE7oNwvcC/goal-directedness-is-behavioral-not-structural"
    ],
    "tags": [
      "ai",
      "goal-directedness",
      "rationality"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_539f0ed1ff9a51ea",
    "title": "More on disambiguating \"discontinuity\"",
    "year": 2020,
    "category": "public_awareness",
    "description": "There have already been [numerous](https://sideways-view.com/2018/02/24/takeoff-speeds/) [posts](https://s-risks.org/a-framework-for-thinking-about-ai-timescales/) [and](https://www.lesswrong.com/posts/AfGmsjGPXN97kNp57/arguments-about-fast-takeoff?commentId=phQ3sZj7RmCDTjfvn) [discussions](https://www.lesswrong.com/posts/AfGmsjGPXN97kNp57/arguments-about-fast-takeoff?commentId=JEkP5AmXmi4dHHpqo) [related](https://aiimpacts.org/discontinuous-progress-investigation/) [to](https://www.alignmentforum.org/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment) [disambiguating](https://www.alignmentforum.org/posts/CjW4axQDqLd2oDCGG/misconceptions-about-continuous-takeoff) [the](https://www.alignmentforum.org/posts/cxgtQXnH2uDGBJJGa/redefining-fast-takeoff) [term](https://www.alignmentforum.org/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff) [\"discontinuity\"](https://www.alignmentforum.org/posts/5WECpYABCT62TJrhY/will-ai-undergo-discontinuous-progress). Here...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/C9YMrPAyMXfB8cLPb/more-on-disambiguating-discontinuity"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_caa2320f0ecc6353",
    "title": "[AN #103]: ARCHES: an agenda for existential safety, and combining natural language with deep RL",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/gToGqwS9z2QFvwJ7b/an-103-arches-an-agenda-for-existential-safety-and-combining"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e0fd79d4a9a86f30",
    "title": "Preparing for \"The Talk\" with AI projects",
    "year": 2020,
    "category": "policy_development",
    "description": "*Epistemic status: Written for Blog Post Day III. I don't get to talk to people \"in the know\" much, so maybe this post is obsolete in some way.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/QSBgGv8byWMjmaGE5/preparing-for-the-talk-with-ai-projects"
    ],
    "tags": [
      "ai",
      "world optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_6d6258c34650b451",
    "title": "What are the high-level approaches to AI alignment?",
    "year": 2020,
    "category": "public_awareness",
    "description": "I'm writing a post comparing some high-level approaches to AI alignment in terms of their [false positive risk](https://www.lesswrong.com/posts/JYdGCrD55FhS4iHvY/robustness-to-fundamental-uncertainty-in-agi-alignment-1). Trouble is, there's no standard agreement on what various high-level approaches to AI alignment there are today, either in terms of what constitutes these high-level approaches or where to draw the line in categorizing various specific approaches.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/H9sxfAZGGAsx5BdYD/what-are-the-high-level-approaches-to-ai-alignment"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b5f7947171ec8bb3",
    "title": "Relating HCH and Logical Induction",
    "year": 2020,
    "category": "public_awareness",
    "description": "I'd like to communicate a simple model of the relationship between [logical induction](https://intelligence.org/2016/09/12/new-paper-logical-induction/) and [HCH](https://ai-alignment.com/humans-consulting-hch-f893f6051455) which I've known about for some time. This is more or less a combination of ideas from Sam, Tsvi, and Scott, but I don't know that any of them endorse the full analogy as I'll state it.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/R3HAvMGFNJGXstckQ/relating-hch-and-logical-induction"
    ],
    "tags": [
      "ai",
      "humans consulting hch",
      "logical induction",
      "logical uncertainty"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f86b8bcf435a7d0f",
    "title": "[AN #104]: The perils of inaccessible information, and what we can learn about AI alignment from COVID",
    "year": 2020,
    "category": "public_awareness",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/eE4QrWsdYQxNynbTM/an-104-the-perils-of-inaccessible-information-and-what-we"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1fba8f9379efcd3a",
    "title": "The ground of optimization",
    "year": 2020,
    "category": "public_awareness",
    "description": "*This work was supported by OAK, a monastic community in the Berkeley hills. This document could not have been written without the daily love of living in this beautiful community. The work involved in writing this cannot be separated from the sitting, chanting, cooking, cleaning, crying, correcting, fundraising, listening, laughing, and teaching of the whole community.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/znfkdCoHMANwqc2WE/the-ground-of-optimization-1"
    ],
    "tags": [
      "ai",
      "dynamical systems",
      "general intelligence",
      "optimization",
      "selection vs control",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_bd1df714a76db503",
    "title": "Relevant pre-AGI possibilities",
    "year": 2020,
    "category": "policy_development",
    "description": "*Epistemic status: I started this as an AI Impacts research project, but given that it's fundamentally a fun speculative brainstorm, it worked better as a blog post.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/zjhZpZi76kEBRnjiw/relevant-pre-agi-possibilities"
    ],
    "tags": [
      "ai",
      "computing overhang",
      "narrow ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9596d03efbeb1ca9",
    "title": "Plausible cases for HRAD work, and locating the crux in the \"realism about rationality\" debate",
    "year": 2020,
    "category": "policy_development",
    "description": "This post is my attempt to summarize and distill the major public debates about MIRI's [highly reliable agent designs](https://intelligence.org/files/TechnicalAgenda.pdf) (HRAD) work (which includes work on decision theory), including the discussions in [Realism about rationality](https://www.greaterwrong.com/posts/suxvE2ddnYMPJN9HD/realism-about-rationality) and Daniel Dewey's [My current thoughts on MIRI's \"highly reliable agent design\" work](https://forum.effectivealtruism.org/posts/SEL9PW8jozrvLnkb4/my-current-thoughts-on-miri-s-highly-reliable-agent-design). Part of the difficulty with discussing the value of HRAD work is that it's not even clear what the disagreement is about, so my summary takes the form of multiple possible \"worlds\" we might be in; each world consists of a positive case for doing HRAD work, along with the potential objections to that case, which results in one or more cruxes.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/BGxTpdBGbwCWrGiCL/plausible-cases-for-hrad-work-and-locating-the-crux-in-the"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_243c980b03ba06f0",
    "title": "Locality of goals",
    "year": 2020,
    "category": "public_awareness",
    "description": "Introduction\n============",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/HkWB5KCJQ2aLsMzjt/locality-of-goals"
    ],
    "tags": [
      "ai",
      "goal-directedness",
      "rationality"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9401ac7353242872",
    "title": "Modelling Continuous Progress",
    "year": 2020,
    "category": "public_awareness",
    "description": "I [have previously argued](https://www.alignmentforum.org/posts/5WECpYABCT62TJrhY/will-ai-undergo-discontinuous-progress) for two claims about AI takeoff speeds. First, almost everyone agrees that if we had AGI, progress would be very fast. Second, the major disagreement is between those who think progress will be discontinuous and sudden (such as Eliezer Yudkowsky, MIRI) and those who think progress will be very fast by normal historical standards but continuous (Paul Chrisiano, Robin Hanson).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/66FKFkWAugS8diydF/modelling-continuous-progress"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9ec0d4ce85f41c0d",
    "title": "[AN #105]: The economic trajectory of humanity, and what we might mean by optimization",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/gWRJDwqHnmJhurXgo/an-105-the-economic-trajectory-of-humanity-and-what-we-might"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_94b4f8e448015aa0",
    "title": "AI safety via market making",
    "year": 2020,
    "category": "public_awareness",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/YWwzccGbcHMJMpT45/ai-safety-via-market-making"
    ],
    "tags": [
      "ai",
      "market making (ai safety technique)",
      "myopia"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_86f5dac406d33bfa",
    "title": "AI Benefits Post 2: How AI Benefits Differs from AI Alignment & AI for Good",
    "year": 2020,
    "category": "policy_development",
    "description": "This is a post in a series on \"AI Benefits.\" It is cross-posted from my [personal blog](https://cullenokeefe.com/blog/ai-benefits-2). For other entries in this series, navigate to the [AI Benefits Blog Series Index page](https://www.cullenokeefe.com/ai-benefits-index).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Z5XXdQDxhpgiXASQW/ai-benefits-post-2-how-ai-benefits-differs-from-ai-alignment"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_edbb18a4efb9aecc",
    "title": "Web AI discussion Groups",
    "year": 2020,
    "category": "public_awareness",
    "description": "After the success of Web Taisu, I have decided to organize a similar event.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/omj76gXR67jsG4hxs/web-ai-discussion-groups"
    ],
    "tags": [
      "ai",
      "community"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_31136813627eb1d9",
    "title": "Comparing AI Alignment Approaches to Minimize False Positive Risk",
    "year": 2020,
    "category": "public_awareness",
    "description": "Introduction\n============",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/eXNy48LxxfgETdtYB/comparing-ai-alignment-approaches-to-minimize-false-positive"
    ],
    "tags": [
      "ai",
      "debate (ai safety technique)",
      "existential risk",
      "value learning"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_30f02f903c57567b",
    "title": "[AN #106]: Evaluating generalization ability of learned reward models",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/dEqjwwvYtg9NEmZoq/an-106-evaluating-generalization-ability-of-learned-reward"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7ec2e6e63be53f18",
    "title": "Evan Hubinger on Inner Alignment, Outer Alignment, and Proposals for Building Safe Advanced AI",
    "year": 2020,
    "category": "policy_development",
    "description": "It's well-established in the AI alignment literature what happens when an AI system learns or is given an objective that doesn't fully capture what we want.  Human preferences and values are inevitably left out and the AI, likely being a powerful optimizer, will take advantage of the dimensions of freedom afforded by the misspecified objective and set them to extreme values. This may allow for better optimization on the goals in the objective function, but can have catastrophic consequences for human preferences and values the system fails to consider. Is it possible for misalignment to also occur between the model being trained and the objective function used for training? The answer looks like yes. Evan Hubinger from the Machine Intelligence Research Institute joins us on this episode of the AI Alignment Podcast to discuss how to ensure alignment between a model being trained and the objective function used to train it, as well as to evaluate three proposals for building safe adva...",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qZGoHkRgANQpGHWnu/evan-hubinger-on-inner-alignment-outer-alignment-and"
    ],
    "tags": [
      "ai",
      "inner alignment",
      "interviews",
      "outer alignment",
      "transcripts"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b955993f8d4043d4",
    "title": "The \"AI Debate\" Debate",
    "year": 2020,
    "category": "public_awareness",
    "description": "As far as I can tell, I have a disjoint set of concerns to many of the concerns I've heard expressed in conversations about [AI Safety via Debate](https://www.lesswrong.com/posts/wo6NsBtn3WJDCeWsx/ai-safety-via-debate).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/L3QDs6of4Rb2TgpRD/the-ai-debate-debate"
    ],
    "tags": [
      "ai",
      "debate (ai safety technique)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ef5dc0ed98201db1",
    "title": "Goals and short descriptions",
    "year": 2020,
    "category": "policy_development",
    "description": "Research publication: Goals and short descriptions",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/d4NgfKY3cq9yiBLSM/goals-and-short-descriptions"
    ],
    "tags": [
      "ai",
      "goal-directedness"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ebec4adfb87b9a34",
    "title": "AI Unsafety via Non-Zero-Sum Debate",
    "year": 2020,
    "category": "policy_development",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/BRiMQELD5WYyvncTE/ai-unsafety-via-non-zero-sum-debate"
    ],
    "tags": [
      "ai",
      "debate (ai safety technique)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0a108e3716c05b10",
    "title": "Tradeoff between desirable properties for baseline choices in impact measures",
    "year": 2020,
    "category": "policy_development",
    "description": "*(Cross-posted to* [*personal blog*](https://vkrakovna.wordpress.com/2020/07/05/tradeoff-between-desirable-properties-for-baseline-choices-in-impact-measures/)*. Summarized in* [*Alignment Newsletter #108*](https://mailchi.mp/05518aad6baf/an-108why-we-should-scrutinize-arguments-for-ai-risk)*. Thanks to Carroll Wainwright, Stuart Armstrong, Rohin Shah and Alex Turner for helpful feedback on this post.)*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/nLhfRpDutEdgr6PKe/tradeoff-between-desirable-properties-for-baseline-choices"
    ],
    "tags": [
      "ai",
      "impact regularization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1f311769994542a7",
    "title": "Learning the prior",
    "year": 2020,
    "category": "public_awareness",
    "description": "Suppose that I have a dataset D of observed (*x*, *y*) pairs, and I'm interested in predicting the label *y*\\* for each point *x*\\* in some new set D\\*. Perhaps D is a set of forecasts from the last few years, and D\\* is a set of questions about the coming years that are important for planning.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/SL9mKhgdmDKXmxwE4/learning-the-prior"
    ],
    "tags": [
      "ai",
      "priors",
      "rationality"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_bb8470d58ae94012",
    "title": "Better priors as a safety problem",
    "year": 2020,
    "category": "public_awareness",
    "description": "(*Related:* [*Inaccessible Information*](https://ai-alignment.com/inaccessible-information-c749c6a88ce)*,* [*What does the universal prior actually look like?*](https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/)*,* [*Learning the prior*](https://ai-alignment.com/learning-the-prior-48f61b445c04))",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/roA83jDvq7F2epnHK/better-priors-as-a-safety-problem"
    ],
    "tags": [
      "ai",
      "rationality"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_274cab2273406030",
    "title": "What does it mean to apply decision theory?",
    "year": 2020,
    "category": "policy_development",
    "description": "*Based on discussions with Stuart Armstrong and Daniel Kokotajlo.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/wgdfBtLmByaKYovYe/what-does-it-mean-to-apply-decision-theory"
    ],
    "tags": [
      "ai",
      "bounded rationality",
      "decision theory",
      "law-thinking",
      "rationality"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_6e4ffaa1bfba7a4d",
    "title": "AI Research Considerations for Human Existential Safety (ARCHES)",
    "year": 2020,
    "category": "public_awareness",
    "description": "Andrew Critch's ([Academian](https://www.lessestwrong.com/users/academian)) and David Krueger's review of 29 AI (existential) safety research directions, each with an illustrative analogy, examples of current work and potential synergies between research directions, and discussion of ways the research approach might lower (or raise) existential risk.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/QmfjZMr9HxLwHcDQB/ai-research-considerations-for-human-existential-safety"
    ],
    "tags": [
      "ai",
      "world optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_73349bffd4c48a64",
    "title": "Arguments against myopic training",
    "year": 2020,
    "category": "policy_development",
    "description": "*Note that this post has been edited to clarify the difference between explicitly assigning a reward to an action based on its later consequences, versus implicitly reinforcing an action by assigning high reward during later timesteps when its consequences are observed. I'd previously conflated these in a confusing way; thanks to Rohin for highlighting this issue.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/GqxuDtZvfgL2bEQ5v/arguments-against-myopic-training"
    ],
    "tags": [
      "ai",
      "myopia"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d256e899047dd41c",
    "title": "Mesa-Optimizers vs \"Steered Optimizers\"",
    "year": 2020,
    "category": "public_awareness",
    "description": "Research publication: Mesa-Optimizers vs \"Steered Optimizers\"",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/SJXujr5a2NcoFebr4/mesa-optimizers-vs-steered-optimizers"
    ],
    "tags": [
      "ai",
      "inner alignment",
      "mesa-optimization",
      "optimization",
      "outer alignment",
      "selection vs control"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f0df039e92dca764",
    "title": "A space of proposals for building safe advanced AI",
    "year": 2020,
    "category": "public_awareness",
    "description": "I liked [Evan's post on 11 proposals for safe AGI](https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai). However, I was a little confused about why he chose these specific proposals; it feels like we could generate many more by stitching together the different components he identifies, such as different types of amplification and different types of robustness tools. So I'm going to take a shot at describing a set of dimensions of variation which capture the key differences between these proposals, and thereby describe an underlying space of possible approaches to safety.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/S9GxuAEeQomnLkeNt/a-space-of-proposals-for-building-safe-advanced-ai"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b1e1a87b35a3e91c",
    "title": "Talk: Key Issues In Near-Term AI Safety Research",
    "year": 2020,
    "category": "public_awareness",
    "description": "I gave a [talk](https://www.youtube.com/watch?v=LHEE_iqzv-8) for the Foresight Institute yesterday, followed by a talk from Dan Elton (NIH) on explainable AI and a panel discussion that included Robert Kirk and Richard Mallah.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/yijG7ptfqFBR8w885/talk-key-issues-in-near-term-ai-safety-research"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cfc3c1348365437b",
    "title": "New paper: AGI Agent Safety by Iteratively Improving the Utility Function",
    "year": 2020,
    "category": "policy_development",
    "description": "This post is to announce my new paper [AGI Agent Safety by Iteratively Improving the Utility Function](https://arxiv.org/abs/2007.05411). I am also using this post to add some extra background information that is not on the paper. Questions and comments are welcome below.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/HWRR8YzuM63yZyTPG/new-paper-agi-agent-safety-by-iteratively-improving-the"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4d40ca113ad94978",
    "title": "How should AI debate be judged?",
    "year": 2020,
    "category": "policy_development",
    "description": "[Epistemic status: thinking out loud. I haven't thought that much about AI debate, and may be missing basic things.]",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/m7oGxvouzzeQKiGJH/how-should-ai-debate-be-judged"
    ],
    "tags": [
      "ai",
      "debate (ai safety technique)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cab116a8a15ba8f5",
    "title": "Alignment proposals and complexity classes",
    "year": 2020,
    "category": "public_awareness",
    "description": "In the original \"[AI safety via debate](https://arxiv.org/pdf/1805.00899.pdf)\" paper, Geoffrey Irving et al. introduced the concept of analyzing different alignment proposals from the perspective of what complexity class they are able to access under optimal play. I think this is a pretty neat way to analyze different alignment proposals--in particular, I think it can help us gain some real insights into how far into the superhuman different systems are able to go. Thus, the goal of this post is to try to catalog different alignment proposals based on the metric of what complexity class they have so far been proven to access.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/N64THGX7XNCqRtvPG/alignment-proposals-and-complexity-classes"
    ],
    "tags": [
      "ai",
      "formal proof",
      "research agendas"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_00671cab97bcd7dc",
    "title": "[AN #108]: Why we should scrutinize arguments for AI risk",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/T5awG3XQKJtprABsy/an-108-why-we-should-scrutinize-arguments-for-ai-risk"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_eda7d8f54688ad21",
    "title": "[AN #107]: The convergent instrumental subgoals of goal-directed agents",
    "year": 2020,
    "category": "policy_development",
    "description": "Research publication: [AN #107]: The convergent instrumental subgoals of goal-directed agents",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pjTF49Rnc878jZSAZ/an-107-the-convergent-instrumental-subgoals-of-goal-directed"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_6ad942c687ee89f4",
    "title": "Environments as a bottleneck in AGI development",
    "year": 2020,
    "category": "policy_development",
    "description": "Given a training environment or dataset, a training algorithm, an optimiser, and a model class capable of implementing an AGI (with the right parameters), there are two interesting questions we might ask about how conducive that environment is for training an AGI. The first is: how much do AGIs from that model class outperform non-AGIs? The second is: how straightforward is the path to reaching an AGI? We can visualise these questions in terms of the loss landscape of those models when evaluated on the training environment. The first asks how low the set of AGIs is, compared with the rest of the landscape. The second asks how favourable the paths through that loss landscape to get to AGIs are - that is, do the local gradients usually point in the right direction, and how deep are the local minima?",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/vqpEC3MPioHX7bv4t/environments-as-a-bottleneck-in-agi-development"
    ],
    "tags": [
      "ai",
      "ai timelines"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a88f6548ef36b264",
    "title": "Why is pseudo-alignment \"worse\" than other ways ML can fail to generalize?",
    "year": 2020,
    "category": "public_awareness",
    "description": "I have only just read the [mesa optimizers paper](https://arxiv.org/pdf/1906.01820.pdf), and I don't understand what it adds to the pre-existing picture that \"ML can fail to generalize outside the train distribution and this is bad.\"",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/TSmgTGaLyhL965jX6/why-is-pseudo-alignment-worse-than-other-ways-ml-can-fail-to"
    ],
    "tags": [
      "academic papers",
      "ai",
      "inner alignment",
      "mesa-optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ea4ecdc23d987ddc",
    "title": "To what extent is GPT-3 capable of reasoning?",
    "year": 2020,
    "category": "public_awareness",
    "description": "ETA 8/19/20: This interview was conducted with AIDungeon's Dragon model in Custom mode. At the time of writing, this mode's first reply was sampled from GPT-2.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/L5JSMZQvkBAx9MD5A/to-what-extent-is-gpt-3-capable-of-reasoning"
    ],
    "tags": [
      "ai",
      "ai timelines",
      "gpt"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_243d1bb3a061ff6f",
    "title": "Parallels Between AI Safety by Debate and Evidence Law",
    "year": 2020,
    "category": "public_awareness",
    "description": "In this post, I highlight some parallels between [AI Safety by Debate](https://openai.com/blog/debate/) (\"Debate\") and [evidence law](https://www.law.cornell.edu/rules/fre).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/SrsH2MyyH8MqH9QSr/parallels-between-ai-safety-by-debate-and-evidence-law"
    ],
    "tags": [
      "ai",
      "debate (ai safety technique)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5b14f2cbab716045",
    "title": "$1000 bounty for OpenAI to show whether GPT3 was \"deliberately\" pretending to be stupider than it is",
    "year": 2020,
    "category": "public_awareness",
    "description": "Twitter thread by Eliezer Yudkowsky, with the bounty in bold:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/H9knnv8BWGKj6dZim/usd1000-bounty-for-openai-to-show-whether-gpt3-was"
    ],
    "tags": [
      "ai",
      "bounties & prizes (active)",
      "gpt"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_998a27a2d7825463",
    "title": "Alignment As A Bottleneck To Usefulness Of GPT-3",
    "year": 2020,
    "category": "public_awareness",
    "description": "So there's this thing where GPT-3 is able to do addition, it has the internal model to do addition, but it takes a little poking and prodding to actually get it to do addition. \"Few-shot learning\", as [the paper](https://arxiv.org/abs/2005.14165) calls it. Rather than prompting the model with",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/BnDF5kejzQLqd5cjH/alignment-as-a-bottleneck-to-usefulness-of-gpt-3"
    ],
    "tags": [
      "ai",
      "gpt",
      "outer alignment"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a95d38424b96a575",
    "title": "Competition: Amplify Rohin's Prediction on AGI researchers & Safety Concerns",
    "year": 2020,
    "category": "public_awareness",
    "description": "*EDIT: The competition is now closed, thanks to everyone who participated! Rohin's posterior distribution is [here](https://elicit.ought.org/builder/rBxYYzM-f), and winners are in [this comment](https://www.lesswrong.com/posts/Azqmzp5JoXJihMcr4/competition-amplify-rohin-s-prediction-on-agi-researchers?commentId=sSDa4D75mhndJjy7t).*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Azqmzp5JoXJihMcr4/competition-amplify-rohin-s-prediction-on-agi-researchers"
    ],
    "tags": [
      "ai",
      "bounties (closed)",
      "forecasting & prediction",
      "rationality"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_6b1fabd1edc4debc",
    "title": "[Preprint] The Computational Limits of Deep Learning",
    "year": 2020,
    "category": "public_awareness",
    "description": "*\"The Computational Limits of Deep Learning\" by Neil C. Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F. Manso*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/buhaT2pxsfLrknzxT/preprint-the-computational-limits-of-deep-learning"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_14d03ad74b243aea",
    "title": "[AN #109]: Teaching neural nets to generalize the way humans would",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/TWdnCi4kPjTapYjh6/an-109-teaching-neural-nets-to-generalize-the-way-humans"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_89430ad40b04f2d1",
    "title": "Weak HCH accesses EXP",
    "year": 2020,
    "category": "public_awareness",
    "description": "*This post is a follow-up to my \"[Alignment proposals and complexity classes](https://www.alignmentforum.org/posts/N64THGX7XNCqRtvPG/alignment-proposals-and-complexity-classes)\" post. Thanks to Sam Eisenstat for helping with part of the proof here.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/CtGH3yEoo4mY2taxe/weak-hch-accesses-exp"
    ],
    "tags": [
      "ai",
      "formal proof",
      "humans consulting hch"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_15744a6a4b64c91e",
    "title": "Can you get AGI from a Transformer?",
    "year": 2020,
    "category": "policy_development",
    "description": "***UPDATE IN 2023: I wrote this a long time ago and you should NOT assume that I still agree with all or even most of what I wrote here. I'm keeping it posted as-is for historical interest.***",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/SkcM4hwgH3AP6iqjs/can-you-get-agi-from-a-transformer"
    ],
    "tags": [
      "ai",
      "ai timelines",
      "gpt"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7399d1d32f2b81ac",
    "title": "Optimizing arbitrary expressions with a linear number of queries to a Logical Induction Oracle (Cartoon Guide)",
    "year": 2020,
    "category": "public_awareness",
    "description": "This is Logical Induction, or LI for short.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/H32NbFcqjTxy2pvaq/optimizing-arbitrary-expressions-with-a-linear-number-of"
    ],
    "tags": [
      "ai",
      "logical induction",
      "logical uncertainty",
      "oracle ai",
      "rationality"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_75e2d1f5cfddf9f3",
    "title": "Developmental Stages of GPTs",
    "year": 2020,
    "category": "public_awareness",
    "description": "***Epistemic Status:** I only know as much as anyone else in my reference class (I build ML models, I can grok the GPT papers, and I don't work for OpenAI or a similar lab). But I think my thesis is original.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/3nDR23ksSQJ98WNDm/developmental-stages-of-gpts"
    ],
    "tags": [
      "ai",
      "ai risk",
      "ai timelines",
      "existential risk",
      "gpt",
      "openai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3039c46ab771d769",
    "title": "What specific dangers arise when asking GPT-N to write an Alignment Forum post?",
    "year": 2020,
    "category": "public_awareness",
    "description": "Last year Stuart Armstrong [announced a contest](https://www.alignmentforum.org/posts/cSzaxcmeYW6z7cgtc/contest-usd1-000-for-good-questions-to-ask-to-an-oracle-ai) to come up with the best questions to ask an Oracle AI. Wei Dai [wrote](https://www.alignmentforum.org/posts/cSzaxcmeYW6z7cgtc/contest-usd1-000-for-good-questions-to-ask-to-an-oracle-ai?commentId=JMABP4HCXFvAX8JXw),",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Et2pWrj4nWfdNAawh/what-specific-dangers-arise-when-asking-gpt-n-to-write-an"
    ],
    "tags": [
      "ai",
      "community",
      "gpt",
      "mesa-optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_195572815c62bd17",
    "title": "[AN #110]: Learning features from human feedback to enable reward learning",
    "year": 2020,
    "category": "policy_development",
    "description": "Research publication: [AN #110]: Learning features from human feedback to enable reward learning",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/P6eWEMCrSjbuWwESk/an-110-learning-features-from-human-feedback-to-enable"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d3a08f4d7c83da61",
    "title": "What Failure Looks Like: Distilling the Discussion",
    "year": 2020,
    "category": "policy_development",
    "description": "The comments under a post often contains valuable insights and additions. They are also often very long and involved, and harder to cite than posts themselves. Given this, I was motivated to try to [distill](https://distill.pub/2017/research-debt/) some comment sections on LessWrong, in part to start exploring whether we can build some norms and some features to help facilitate this kind of intellectual work more regularly. So this is my attempt to summarise the post and discussion around [What Failure Looks Like](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like) by Paul Christiano.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/6jkGf5WEKMpMFXZp2/what-failure-looks-like-distilling-the-discussion"
    ],
    "tags": [
      "ai",
      "ai risk",
      "multipolar scenarios",
      "site meta",
      "threat models"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a2becb6f07357a2f",
    "title": "Learning the prior and generalization",
    "year": 2020,
    "category": "policy_development",
    "description": "*This post is a response to Paul Christiano's post \"[Learning the prior](https://www.alignmentforum.org/posts/SL9mKhgdmDKXmxwE4/learning-the-prior).\"*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/YhQr36yGkhe6x8Fyn/learning-the-prior-and-generalization"
    ],
    "tags": [
      "ai",
      "priors"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5ec205c8c57c5642",
    "title": "What if memes are common in highly capable minds?",
    "year": 2020,
    "category": "public_awareness",
    "description": "The meme-theoretic view of humans says: Memes are to humans as sailors are to ships in the age of sail.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/6iedrXht3GpKTQWRF/what-if-memes-are-common-in-highly-capable-minds"
    ],
    "tags": [
      "ai",
      "memetics",
      "robust agents"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ebe3f4e540c8faae",
    "title": "\"Go west, young man!\" - Preferences in (imperfect) maps",
    "year": 2020,
    "category": "public_awareness",
    "description": "Many people are very nationalistic, putting their country above all others. Such people can be hazy about what \"above all others\" can mean, outside of a few clear examples - eg winning a total war totally. They're also very hazy on what is meant by \"their country\" - geography is certainly involved, as is proclaimed or legal nationality, maybe some ethnic groups or a language, or even just giving deference to certain ideals.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pfmFe5fgEn2weJuer/go-west-young-man-preferences-in-imperfect-maps"
    ],
    "tags": [
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_408c69497f6bcc99",
    "title": "Power as Easily Exploitable Opportunities",
    "year": 2020,
    "category": "policy_development",
    "description": "![](https://lh3.googleusercontent.com/jw_CDro-KPlyanfkSN9N7re7sw-FDQevSrgd1qUOKIQLX6d1iCoQIc4wl-tqkRQo3Yt1z1Mj7X0WtLbJXdvcl91nuDbjlFmx52tCO_OQO7uuW_oBF4K2EXl_nQRHzO8YRbK3gu88)*(Talk given at* [*an event on Sunday 28th of June*](https://www.lesswrong.com/posts/iZ3AisoiB6qKY6Ctz/sunday-jun-28-more-online-talks-by-curated-authors)*. TurnTrout is responsible for the talk, Jacob Lagerros and David Lambert edited the transcript.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/eqov4SEYEbeFMXegR/power-as-easily-exploitable-opportunities"
    ],
    "tags": [
      "ai",
      "instrumental convergence",
      "lesswrong event transcripts"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ab1714c054b6bcc1",
    "title": "Inner Alignment: Explain like I'm 12 Edition",
    "year": 2020,
    "category": "public_awareness",
    "description": "*(This is an unofficial explanation of Inner Alignment based on the Miri paper [Risks from Learned Optimization in Advanced Machine Learning Systems](https://arxiv.org/abs/1906.01820) (which is almost identical to the [LW sequence](https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB)) and the [Future of Life podcast](https://futureoflife.org/2020/07/01/evan-hubinger-on-inner-alignment-outer-alignment-and-proposals-for-building-safe-advanced-ai/) with Evan Hubinger ([Miri](https://intelligence.org/team/)/[LW](https://www.lesswrong.com/users/evhub)). It's meant for anyone who found the sequence too long/challenging/technical to read.)*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/AHhCrJ2KpTjsCSwbt/inner-alignment-explain-like-i-m-12-edition"
    ],
    "tags": [
      "ai",
      "inner alignment",
      "mesa-optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9556c2cd2b8d6085",
    "title": "Three mental images from thinking about AGI debate & corrigibility",
    "year": 2020,
    "category": "public_awareness",
    "description": "Here are three mental images I've used when sporadically struggling to understand the ideas and prospects for [AI safety via debate](https://arxiv.org/abs/1805.00899), [IDA, and related proposals](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd). I have not been closely following the discussion, and may well be missing things, and I don't know whether these mental images are helpful or misleading.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/WjY9y7r52vaNZ2WmH/three-mental-images-from-thinking-about-agi-debate-and"
    ],
    "tags": [
      "ai",
      "corrigibility",
      "debate (ai safety technique)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_911ea940d7222e40",
    "title": "Interpretability in ML: A Broad Overview",
    "year": 2020,
    "category": "public_awareness",
    "description": "(Reposting because I think a GreaterWrong bug on submission made this post invisible for a while last week so I'm trying again on LW.)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/57fTWCpsAyjeAimTp/interpretability-in-ml-a-broad-overview-2"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)",
      "machine learning  (ml)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d748b44740bf5984",
    "title": "Infinite Data/Compute Arguments in Alignment",
    "year": 2020,
    "category": "public_awareness",
    "description": "*This is a reference post. It explains a fairly standard class of arguments, and is intended to be the opposite of novel; I just want a standard explanation to link to when invoking these arguments.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/7CJBiHYxebTmMfGs3/infinite-data-compute-arguments-in-alignment"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1a6910f4f9def8bc",
    "title": "[AN #111]: The Circuits hypotheses for deep learning",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/5CApLZiHGkt37nRQ2/an-111-the-circuits-hypotheses-for-deep-learning"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_aedea795b548c83e",
    "title": "The Fusion Power Generator Scenario",
    "year": 2020,
    "category": "public_awareness",
    "description": "Suppose, a few years from now, I prompt GPT-N to design a cheap, simple fusion power generator - something I could build in my garage and use to power my house. GPT-N succeeds. I build the fusion power generator, find that it works exactly as advertised, share the plans online, and soon the world has easy access to cheap, clean power.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/2NaAhMPGub8F2Pbr7/the-fusion-power-generator-scenario"
    ],
    "tags": [
      "ai",
      "ai risk",
      "information hazards",
      "tool ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d4ddf68fa8630c06",
    "title": "Book review: Architects of Intelligence by Martin Ford (2018)",
    "year": 2020,
    "category": "policy_development",
    "description": "*Cross-posted from the [EA Forum](https://forum.effectivealtruism.org/posts/R5Drd54mRmSujdNyz/book-review-architects-of-intelligence-by-martin-ford-2018).*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/iZS3am4acMh8g4Ycb/book-review-architects-of-intelligence-by-martin-ford-2018"
    ],
    "tags": [
      "ai",
      "ai governance",
      "ai risk",
      "book reviews / media reviews"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_eded43cb436b92fe",
    "title": "Will OpenAI's work unintentionally increase existential risks related to AI?",
    "year": 2020,
    "category": "public_awareness",
    "description": "[The original question was \"Is OpenAI increasing the existential risks related to AI?\" I changed it to the current one following a [discussion](https://www.alignmentforum.org/posts/CD8gcugDu5z2Eeq7k/is-openai-increasing-the-existential-risks-related-to-ai?commentId=o5jsD7s5BfKWJYxqN) with Rohin in the comments. It clarifies that my question asks about the consequences of OpenAI's work will assuming positive and aligned intentions.]",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/CD8gcugDu5z2Eeq7k/will-openai-s-work-unintentionally-increase-existential"
    ],
    "tags": [
      "ai",
      "ai risk",
      "gpt",
      "openai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_235c3f54728bcf88",
    "title": "Matt Botvinick on the spontaneous emergence of learning algorithms",
    "year": 2020,
    "category": "policy_development",
    "description": "Matt Botvinick is Director of Neuroscience Research at DeepMind. In [this interview](https://youtu.be/3t06ajvBtl0?t=3647), he discusses results from a [2018 paper](https://sci-hub.tw/https://www.nature.com/articles/s41593-018-0147-8) which describe conditions under which reinforcement learning algorithms will spontaneously give rise to separate full-fledged reinforcement learning algorithms that differ from the original. Here are some notes I gathered from the interview and paper:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Wnqua6eQkewL3bqsF/matt-botvinick-on-the-spontaneous-emergence-of-learning"
    ],
    "tags": [
      "ai",
      "emergent behavior ( emergence )",
      "inner alignment",
      "machine learning  (ml)",
      "mesa-optimization",
      "neocortex",
      "neuroscience"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4b0cd1fcc2e00bac",
    "title": "Alignment By Default",
    "year": 2020,
    "category": "public_awareness",
    "description": "Suppose AI continues on its current trajectory: deep learning continues to get better as we throw more data and compute at it, researchers keep trying random architectures and using whatever seems to work well in practice. Do we end up with aligned AI \"by default\"?",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Nwgdq6kHke5LY692J/alignment-by-default"
    ],
    "tags": [
      "abstraction",
      "ai",
      "natural abstraction"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_547ae7c5b784a3c7",
    "title": "Blog post: A tale of two research communities",
    "year": 2020,
    "category": "policy_development",
    "description": "This is a copy of a [blog post](https://www.york.ac.uk/assuring-autonomy/news/blog/ai-safety-research-communities/) from Francis Rhys Ward, an incoming doctoral student in Safe and Trusted AI at Imperial College London. I just discovered this on the website of the [Assuring Autonomy International Programme](https://www.york.ac.uk/assuring-autonomy/) and thought it was worth cross-posting here.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FzF4Xok63ZCZNjmGY/blog-post-a-tale-of-two-research-communities"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_531b319d81d697c2",
    "title": "[AN #112]: Engineering a Safer World",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/bP6KA2JJQMke8H4Au/an-112-engineering-a-safer-world"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_73d81a2cfd48b26a",
    "title": "Mapping Out Alignment",
    "year": 2020,
    "category": "policy_development",
    "description": "This week, the [key alignment group](https://www.lesswrong.com/posts/EnnCnnjB52MDGtiYF/solving-key-alignment-problems-group), we answered two questions, 5-minute timer style:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/jeiz7WfCnGQWoShkT/mapping-out-alignment"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3ea19d8d03e8fb9c",
    "title": "My Understanding of Paul Christiano's Iterated Amplification AI Safety Research Agenda",
    "year": 2020,
    "category": "policy_development",
    "description": "*Crossposted from the [EA forum](https://forum.effectivealtruism.org/posts/2ZeHrfJr9uHHJ2e8J/my-understanding-of-paul-christiano-s-iterated-amplification)*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/PT8vSxsusqWuN7JXp/my-understanding-of-paul-christiano-s-iterated-amplification"
    ],
    "tags": [
      "ai",
      "iterated amplification"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4df3f0b3f13dc5cf",
    "title": "Search versus design",
    "year": 2020,
    "category": "public_awareness",
    "description": "*This work was supported by OAK, a monastic community in the Berkeley hills. It could not have been written without the daily love of living in this beautiful community. The work involved in writing this cannot be separated from the sitting, chanting, cooking, cleaning, crying, correcting, fundraising, listening, laughing, and teaching of the whole community.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/r3NHPD3dLFNk9QE2Y/search-versus-design-1"
    ],
    "tags": [
      "ai",
      "distinctions",
      "interpretability (ml & ai)",
      "machine learning  (ml)",
      "optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2e36f187f3768db3",
    "title": "Goal-Directedness: What Success Looks Like",
    "year": 2020,
    "category": "public_awareness",
    "description": "This sequence already contains a couple of blog posts, exploring different aspects of goal-directedness. But one question has never been fully addressed: what constraints should a good formalization of goal-directedness satisfy? An answer is useful both for people like me which study this topic, and for people trying to assess the value of this research. The following is my personal view, as always informed with discussion with Michele Campolo, Joe Collman and Sabrina Tang.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/jP4cx3TCweDngSLS6/goal-directedness-what-success-looks-like"
    ],
    "tags": [
      "ai",
      "goal-directedness"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e71224864e3fa919",
    "title": "Mesa-Search vs Mesa-Control",
    "year": 2020,
    "category": "policy_development",
    "description": "~~I currently see the~~ [~~spontaneous emergence of learning algorithms~~](https://www.lesswrong.com/posts/Wnqua6eQkewL3bqsF/matt-botvinick-on-the-spontaneous-emergence-of-learning) ~~as significant evidence for the commonality of~~ [~~mesa-optimization~~](https://www.lesswrong.com/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction) ~~in existing ML, and suggestive evidence for the commonality of inner alignment problems in near-term ML.~~",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/WmBukJkEFM72Xr397/mesa-search-vs-mesa-control"
    ],
    "tags": [
      "ai",
      "inner alignment",
      "mesa-optimization",
      "selection vs control"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3e912ec166c473e1",
    "title": "Radical Probabilism",
    "year": 2020,
    "category": "public_awareness",
    "description": "*This is an expanded version of* [*my talk*](https://www.lesswrong.com/posts/ZM63n353vh2ag7z4p/radical-probabilism-transcript)*. I assume a high degree of familiarity with Bayesian probability theory.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/xJyY5QkQvNJpZLJRo/radical-probabilism-1"
    ],
    "tags": [
      "bayes' theorem",
      "conservation of expected evidence",
      "epistemology",
      "logical induction",
      "probabilistic reasoning",
      "probability & statistics",
      "problem of old evidence",
      "radical probabilism",
      "rationality"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1b9f4e416a1fb174",
    "title": "Looking for adversarial collaborators to test our Debate protocol",
    "year": 2020,
    "category": "public_awareness",
    "description": "EDIT: We're also looking for people to become trained Honest debaters, which requires a greater time commitment (ideally >=5 hours per week for >= 2 months) but for which we're offering $30/hr. If you're interested in doing that, please fill out this form: <https://forms.gle/2bv1Z8eCYPfyqxRF9>",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/w7mS6syTderWihHPM/looking-for-adversarial-collaborators-to-test-our-debate"
    ],
    "tags": [
      "adversarial collaboration",
      "ai",
      "debate (ai safety technique)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e9bab64f635d60c2",
    "title": "AI safety as featherless bipeds *with broad flat nails*",
    "year": 2020,
    "category": "public_awareness",
    "description": "There's a [famous story about Diogenes and Plato](https://en.wikipedia.org/wiki/Diogenes#In_Athens):",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/gWxMZisqE2j2kHCd2/ai-safety-as-featherless-bipeds-with-broad-flat-nails"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cc13c491df98d2f7",
    "title": "[AN #113]: Checking the ethical intuitions of large language models",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/tDDDZ2nZdvyziwSvv/an-113-checking-the-ethical-intuitions-of-large-language"
    ],
    "tags": [
      "ai",
      "language models",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_bdf2997a9edf9a6e",
    "title": "Universality Unwrapped",
    "year": 2020,
    "category": "public_awareness",
    "description": "**Introduction**\n================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/farherQcqFQXqRcvv/universality-unwrapped"
    ],
    "tags": [
      "ai",
      "deception",
      "distillation & pedagogy",
      "humans consulting hch"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e405b0b0b5a30bdc",
    "title": "What's a Decomposable Alignment Topic?",
    "year": 2020,
    "category": "public_awareness",
    "description": "What's an alignment topic where, if someone decomposed the overall task, a small group of smart people (like here on Lesswrong) could make conceptual progress? By \"smart\", assume they can [notice confusion](https://www.lesswrong.com/s/zpCiuR4T343j9WkcK), google, and program.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/4bAd9mFBLAFxR3MSk/what-s-a-decomposable-alignment-topic"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0b1b2bbe014c110d",
    "title": "[AN #114]: Theory-inspired safety solutions for powerful Bayesian RL agents",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/kxPiL4zNSPR249wsC/an-114-theory-inspired-safety-solutions-for-powerful"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_fbc330c9e00692fe",
    "title": "Introduction To The Infra-Bayesianism Sequence",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/zB4f7QqKhBHa5b37a/introduction-to-the-infra-bayesianism-sequence"
    ],
    "tags": [
      "ai",
      "bayes' theorem",
      "decision theory",
      "infra-bayesianism"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_38bdc87ccf314b39",
    "title": "Proofs Section 2.3 (Updates, Decision Theory)",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/9ekP8FojvLa8Pr6P7/proofs-section-2-3-updates-decision-theory"
    ],
    "tags": [
      "ai",
      "decision theory",
      "formal proof",
      "infra-bayesianism",
      "logic & mathematics"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c07536d81bf22e02",
    "title": "Proofs Section 2.2 (Isomorphism to Expectations)",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/8tLPYYQJM8SwL2xn9/proofs-section-2-2-isomorphism-to-expectations"
    ],
    "tags": [
      "ai",
      "decision theory",
      "formal proof",
      "infra-bayesianism",
      "logic & mathematics"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1bbd93ee3efe91f6",
    "title": "Proofs Section 2.1 (Theorem 1, Lemmas)",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": "Fair upfront warning: This is not a particularly readable proof section. There's a bunch of dense notation, logical leaps due to illusion of transparency since I've spent months getting fluent with these concepts, and a relative lack of editing since it's long. If you really want to read this, I'd suggest PM-ing me to get a link to MIRIxDiscord, where I'd be able to guide you through it and answer questions. This post will be recapping the notions and building up an arsenal of lemmas, the [next one](https://www.alignmentforum.org/posts/8tLPYYQJM8SwL2xn9/proofs-section-2-2-isomorphism-to-expectations) will show the isomorphism theorem, translation theorems, and behavior of mixing, and the [last one](https://www.alignmentforum.org/posts/9ekP8FojvLa8Pr6P7/proofs-section-2-3-updates-decision-theory) is about updates and the decision-theory results. It's advised to have them open in different tabs and go between them as needed. .mjx-chtml {display: inline-block; line-height: 0; text-inde...",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/xQYF3LR64NYn8vkoy/proofs-section-2-1-theorem-1-lemmas"
    ],
    "tags": [
      "ai",
      "decision theory",
      "formal proof",
      "infra-bayesianism",
      "logic & mathematics"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0cd8e518ee1e021b",
    "title": "Proofs Section 1.2 (Mixtures, Updates, Pushforwards)",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/b9jubzqz866CModHB/proofs-section-1-2-mixtures-updates-pushforwards"
    ],
    "tags": [
      "ai",
      "formal proof",
      "infra-bayesianism",
      "logic & mathematics"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_bbf116d56f31343a",
    "title": "Proofs Section 1.1 (Initial results to LF-duality)",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/PTcktJADsAmpYEjoP/proofs-section-1-1-initial-results-to-lf-duality"
    ],
    "tags": [
      "ai",
      "formal proof",
      "infra-bayesianism",
      "logic & mathematics"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1bd4645765b4f159",
    "title": "Belief Functions And Decision Theory",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/e8qFDMzs2u9xf5ie6/belief-functions-and-decision-theory"
    ],
    "tags": [
      "ai",
      "decision theory",
      "infra-bayesianism",
      "logic & mathematics"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_deb31b3e66f91751",
    "title": "Basic Inframeasure Theory",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/YAa4qcMyoucRS2Ykr/basic-inframeasure-theory"
    ],
    "tags": [
      "ai",
      "decision theory",
      "infra-bayesianism",
      "logic & mathematics"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_61ecb19b4c26b6c1",
    "title": "Model splintering: moving from one imperfect model to another",
    "year": 2020,
    "category": "public_awareness",
    "description": "1. The big problem\n==================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1"
    ],
    "tags": [
      "ai",
      "iterated amplification",
      "machine learning  (ml)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_12181b452dd5dda0",
    "title": "Updates and additions to \"Embedded Agency\"",
    "year": 2020,
    "category": "public_awareness",
    "description": "Abram Demski and Scott Garrabrant's \"[Embedded Agency](https://intelligence.org/embedded-agency/)\" has been updated with quite a bit of new content from Abram. All the changes are live today, and can be found at any of these links:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/9vYg8MyLL4cMMaPQJ/updates-and-additions-to-embedded-agency"
    ],
    "tags": [
      "ai",
      "embedded agency",
      "site meta"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8d3e422a605509ac",
    "title": "Safe Scrambling?",
    "year": 2020,
    "category": "public_awareness",
    "description": "Status: half-formed thought on a potential piece of an alignment strategy that I've not heard discussed but probably exists somewhere, might just be missing a concept name.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/jHzb5SmviScXdtT2m/safe-scrambling"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d4d490e8363e47ea",
    "title": "interpreting GPT: the logit lens",
    "year": 2020,
    "category": "public_awareness",
    "description": "This post relates an observation I've made in my work with GPT-2, which I have not seen made elsewhere.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens"
    ],
    "tags": [
      "ai",
      "gears-level",
      "gpt",
      "interpretability (ml & ai)",
      "machine learning  (ml)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ae9c97ec80809e1b",
    "title": "[AN #115]: AI safety research problems in the AI-GA framework",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/bevquxoYwkMx3NK6L/an-115-ai-safety-research-problems-in-the-ai-ga-framework"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_99c56a863b9ad14b",
    "title": "Using GPT-N to Solve Interpretability of Neural Networks: A Research Agenda",
    "year": 2020,
    "category": "public_awareness",
    "description": "Tl;dr We are attempting to make neural networks (NN) modular, have GPT-N interpret each module for us, in order to catch mesa-alignment and inner-alignment failures.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/zXfqftW8y69YzoXLj/using-gpt-n-to-solve-interpretability-of-neural-networks-a"
    ],
    "tags": [
      "ai",
      "gpt",
      "interpretability (ml & ai)",
      "machine learning  (ml)",
      "research agendas"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_120607fb810a0bf6",
    "title": "[AN #116]: How to make explanations of neurons compositional",
    "year": 2020,
    "category": "policy_development",
    "description": "HIGHLIGHTS\n==========",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/jCZhy3nqH2MoethZQ/an-116-how-to-make-explanations-of-neurons-compositional"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_33c52573ac6821ec",
    "title": "Safer sandboxing via collective separation",
    "year": 2020,
    "category": "public_awareness",
    "description": "*Epistemic status: speculative brainstorming. Follow-up to [this post on AGIs as collectives](https://www.alignmentforum.org/posts/HekjhtWesBWTQW5eF/agis-as-populations). Note that I've changed the term* population AGI *to* collective AGI *for consistency with Bostrom's use in* Superintelligence*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Fji2nHBaB6SjdSscr/safer-sandboxing-via-collective-separation"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_56c393054dfcec52",
    "title": "Safety via selection for obedience",
    "year": 2020,
    "category": "policy_development",
    "description": "[In a previous post](https://www.alignmentforum.org/s/boLPsyNwd6teK5key/p/BXMCgpktdiawT3K5v), I argued that it's plausible that \"the most interesting and intelligent behaviour [of AGIs] won't be directly incentivised by their reward functions\" - instead, \"many of the selection pressures exerted upon them will come from *emergent* interaction dynamics\". If I'm right, and the easiest way to build AGI is using [open-ended](https://arxiv.org/abs/2006.07495) environments and reward functions, then we should be less optimistic about using scalable oversight techniques for the purposes of safety - since capabilities researchers won't need good oversight techniques to get to AGI, and most training will occur in environments in which good and bad behaviour aren't well-defined anyway. In this scenario, the best approach to improving safety might involve structural modifications to training environments to change the emergent incentives of agents, as I'll explain in this post.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/7jNveWML34EsjCD4c/safety-via-selection-for-obedience"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9c7af1fb8611ec2b",
    "title": "Do mesa-optimizer risk arguments rely on the train-test paradigm?",
    "year": 2020,
    "category": "public_awareness",
    "description": "Going by the [Risks from Learned Optimization sequence](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB), it's not clear if mesa-optimization is a big threat if the model continues to be updated throughout deployment. I suspect this has been discussed before (links welcome), but I didn't find anything with a quick search.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/j5foHZhZ7RBhwRL7Z/do-mesa-optimizer-risk-arguments-rely-on-the-train-test"
    ],
    "tags": [
      "ai",
      "mesa-optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_eeedd8f780c03709",
    "title": "Decision Theory is multifaceted",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": "Related: [Conceptual Problems with UDT and Policy Selection](https://www.alignmentforum.org/posts/9sYzoRnmqmxZm4Whf/conceptual-problems-with-udt-and-policy-selection), [Formalising decision theory is hard](https://www.alignmentforum.org/posts/S3W4Xrmp6AL7nxRHd/formalising-decision-theory-is-hard)",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/vrJBQZJpvswXFFkcd/decision-theory-is-multifaceted"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1621cb4e50ec4e73",
    "title": "My computational framework for the brain",
    "year": 2020,
    "category": "public_awareness",
    "description": "*(See comment* [*here*](https://www.lesswrong.com/posts/diruo47z32eprenTg/my-computational-framework-for-the-brain?commentId=c6qzzKWAEfTQyoK7F#c6qzzKWAEfTQyoK7F) *for some updates and corrections and retractions. --Steve, 2022)*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/diruo47z32eprenTg/my-computational-framework-for-the-brain"
    ],
    "tags": [
      "ai",
      "free energy principle",
      "neocortex",
      "neuroscience"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f585815cc42a9b4a",
    "title": "Comparing Utilities",
    "year": 2020,
    "category": "policy_development",
    "description": "*(This is a basic point about utility theory which many will already be familiar with. I draw some non-obvious conclusions which may be of interest to you even if you think you know this from the title -- but the main point is to communicate the basics. I'm posting it to the alignment forum because I've heard misunderstandings of this from some in the AI alignment research community.)*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/cYsGrWEzjb324Zpjx/comparing-utilities"
    ],
    "tags": [
      "population ethics",
      "utilitarianism",
      "utility functions"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0e82ad20b0f91516",
    "title": "[AN #117]: How neural nets would fare under the TEVV framework",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/8H5JbowLTJoNHzLuH/an-117-how-neural-nets-would-fare-under-the-tevv-framework"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_96fa84c21cf9e228",
    "title": "The \"Backchaining to Local Search\" Technique in AI Alignment",
    "year": 2020,
    "category": "public_awareness",
    "description": "In the spirit of this [post](https://www.alignmentforum.org/posts/7CJBiHYxebTmMfGs3/infinite-data-compute-arguments-in-alignment) by John S. Wentworth, this is a reference for a technique I learned from Evan Hubinger. He's probably not the first to use it, but he introduced it to me, so he gets the credit.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qEjh8rpxjG4qGtfuK/the-backchaining-to-local-search-technique-in-ai-alignment"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_816e3ccc4edb6175",
    "title": "Why GPT wants to mesa-optimize & how we might change this",
    "year": 2020,
    "category": "public_awareness",
    "description": "This post was inspired by orthonormal's post [Developmental Stages of GPTs](https://www.lesswrong.com/posts/3nDR23ksSQJ98WNDm/developmental-stages-of-gpts) and the discussion that followed, so only part of it is original.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this"
    ],
    "tags": [
      "ai",
      "gpt",
      "mesa-optimization",
      "myopia"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_84a0970145fc4e9f",
    "title": "Clarifying \"What failure looks like\"",
    "year": 2020,
    "category": "policy_development",
    "description": "*Thanks to Jess Whittlestone, Daniel Eth, Shahar Avin, Rose Hadshar, Eliana Lorch, Alexis Carlier, Flo Dorner, Kwan Yee Ng, Lewis Hammond, Phil Trammell and Jenny Xiao for valuable conversations, feedback and other support. I am especially grateful to Jess Whittlestone for long conversations and detailed feedback on drafts, and her guidance on which threads to pursue and how to frame this post. All errors are my own.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/v6Q7T335KCMxujhZu/clarifying-what-failure-looks-like"
    ],
    "tags": [
      "ai",
      "ai risk",
      "ai risk concrete stories",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8754c8c4b0c4e0c9",
    "title": "Needed: AI infohazard policy",
    "year": 2020,
    "category": "policy_development",
    "description": "The premise of AI risk is that AI is a danger, and therefore research into AI might be dangerous. In the AI alignment community, we're trying to do research which makes AI safer, but occasionally we might come up with results that have significant implications for AI capability as well. Therefore, it seems prudent to come up with a set of guidelines that address:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/3D3DsX5rMbk3jEZ5h/needed-ai-infohazard-policy"
    ],
    "tags": [
      "ai",
      "information hazards"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e89387e438793c89",
    "title": "[AN #118]: Risks, solutions, and prioritization in a world with many AI systems",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/8eX8DJctsACtR2sfX/an-118-risks-solutions-and-prioritization-in-a-world-with"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3f04bdb019d6d470",
    "title": "What to do with imitation humans, other than asking them what the right thing to do is?",
    "year": 2020,
    "category": "public_awareness",
    "description": "This question is about whether you have clever ideas about how to use AI imitations of humans for AI safety. The two main ideas I'm familiar with only seem to interface with these imitations as if they're humans.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/LAR2ajpFDueNg45Mk/what-to-do-with-imitation-humans-other-than-asking-them-what"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4035e6cb5eae10b8",
    "title": "AGI safety from first principles: Introduction",
    "year": 2020,
    "category": "public_awareness",
    "description": "*This is the first part of a six-part report called* AGI safety from first principles, *in which I've attempted to put together the most complete and compelling case I can for why the development of AGI might pose an existential threat. The report stems from my dissatisfaction with existing arguments about the potential risks from AGI. Early work tends to be less relevant in the context of modern machine learning; more recent work is scattered and brief. I originally intended to just summarise other people's arguments, but as this report has grown, it's become more representative of my own views and less representative of anyone else's. So while it covers the standard ideas, I also think that it provides a new perspective on how to think about AGI - one which doesn't take any previous claims for granted, but attempts to work them out from first principles.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/8xRSjC76HasLnMGSf/agi-safety-from-first-principles-introduction"
    ],
    "tags": [
      "ai",
      "ai safety public materials"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c1e75b4916e38413",
    "title": "AGI safety from first principles: Superintelligence",
    "year": 2020,
    "category": "policy_development",
    "description": "In order to understand superintelligence, we should first characterise what we mean by intelligence. We can start with Legg's well-known definition, which identifies intelligence as [the ability to do well on a broad range of cognitive tasks](https://arxiv.org/abs/0712.3329).[[1]](#fn-fY9S4v85NDnW9simo-1) The key distinction I'll draw in this section is between agents that understand how to do well at many tasks because they have been specifically optimised for each task (which I'll call the task-based approach to AI), versus agents which can understand new tasks with little or no task-specific training, by generalising from previous experience (the generalisation-based approach).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/eG3WhHS8CLNxuH6rT/agi-safety-from-first-principles-superintelligence"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_586ad76471402121",
    "title": "AGI safety from first principles: Goals and Agency",
    "year": 2020,
    "category": "policy_development",
    "description": "The fundamental concern motivating the second species argument is that AIs will gain too much power over humans, and then use that power in ways we don't endorse. Why might they end up with that power? I'll distinguish three possibilities:",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/bz5GdmCWj8o48726N/agi-safety-from-first-principles-goals-and-agency"
    ],
    "tags": [
      "agency",
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ad9b15522e32e47f",
    "title": "\"Unsupervised\" translation as an (intent) alignment problem",
    "year": 2020,
    "category": "public_awareness",
    "description": "Suppose that we want to translate between English and an alien language (Klingon). We have plenty of Klingon text, and separately we have plenty of English text, but it's not matched up and there are no bilingual speakers.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/saRRRdMnMPXXtQBNi/unsupervised-translation-as-an-intent-alignment-problem"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8e603cbb72cfb5b1",
    "title": "[AN #119]: AI safety when agents are shaped by environments, not rewards",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Kx7nv8dHtFig9ud7C/an-119-ai-safety-when-agents-are-shaped-by-environments-not"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9403709932346259",
    "title": "AGI safety from first principles: Alignment",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": "*Parts of this section were rewritten in mid-October.*",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/PvA2gFMAaHCHfMXrw/agi-safety-from-first-principles-alignment"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c1dc9d644bd3109c",
    "title": "Hiring engineers and researchers to help align GPT-3",
    "year": 2020,
    "category": "public_awareness",
    "description": "My team at OpenAI, which works on aligning GPT-3, is hiring ML engineers and researchers. **Apply** [**here**](https://jobs.lever.co/openai/98599d5b-2d1d-4127-b9b5-708343c8730b) **for the ML engineer role and** [**here**](https://jobs.lever.co/openai/24bd9cf4-fe95-4fb9-b4b2-8daa8fd8480c) **for the ML researcher role.**",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/dJQo7xPn4TyGnKgeC/hiring-engineers-and-researchers-to-help-align-gpt-3"
    ],
    "tags": [
      "ai",
      "community",
      "gpt",
      "openai",
      "practical"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_755992d5694791e0",
    "title": "AGI safety from first principles: Control",
    "year": 2020,
    "category": "policy_development",
    "description": "It's important to note that my previous arguments by themselves do not imply that AGIs will end up in control of the world instead of us. As an analogy, scientific knowledge allows us to be much more capable than stone-age humans. Yet if dropped back in that time with just our current knowledge, I very much doubt that one modern human could take over the stone-age world. Rather, this last step of the argument relies on additional predictions about the dynamics of the transition from humans being the smartest agents on Earth to AGIs taking over that role. These will depend on technological, economic and political factors, as I'll discuss in this section. One recurring theme will be the importance of our expectation that AGIs will be deployed as software that can be run on many different computers, rather than being tied to a specific piece of hardware as humans are.[[1]](#fn-dsp2JcwB2QZAcKg2a-1)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/eGihD5jnD6LFzgDZA/agi-safety-from-first-principles-control"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1f1b838aec949ee4",
    "title": "AGI safety from first principles: Conclusion",
    "year": 2020,
    "category": "policy_development",
    "description": "Let's recap the second species argument as originally laid out, along with the additional conclusions and clarifications from the rest of the report.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Ni8ocGupB2kGG2fA7/agi-safety-from-first-principles-conclusion"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_68d43711c83b2572",
    "title": "The Alignment Problem: Machine Learning and Human Values",
    "year": 2020,
    "category": "public_awareness",
    "description": "[*The Alignment Problem: Machine Learning and Human Values*](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/153669519X), by Brian Christian, was just released. This is an extended summary + opinion, a version without the quotes from the book will go out in the next Alignment Newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/gYfgWSxCpFdk2cZfE/the-alignment-problem-machine-learning-and-human-values"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ef4eaa981e7a5201",
    "title": "[AN #120]: Tracing the intellectual roots of AI and AI alignment",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/b9b4y2azGjthGBEFb/an-120-tracing-the-intellectual-roots-of-ai-and-ai-alignment"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1d5f3a2f13120789",
    "title": "Toy Problem: Detective Story Alignment",
    "year": 2020,
    "category": "public_awareness",
    "description": "Suppose I train some simple unsupervised topic model (e.g. [LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)) on a bunch of books. I look through the topics it learns, and find one corresponding to detective stories. The problem: I would like to use the identified detective-story cluster to generate detective stories from GPT.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/4kYkYSKSALH4JaQ99/toy-problem-detective-story-alignment"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_da165e67a0885e27",
    "title": "The Solomonoff Prior is Malign",
    "year": 2020,
    "category": "policy_development",
    "description": "This argument came to my attention from [this post](https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/) by Paul Christiano. I also found [this clarification](https://www.lesswrong.com/posts/jP3vRbtvDtBtgvkeb/clarifying-consequentialists-in-the-solomonoff-prior) helpful. I found [these counter-arguments](https://www.lesswrong.com/posts/Ecxevhvx85Y4eyFcu/weak-arguments-against-the-universal-prior-being-malign) stimulating and have included some discussion of them.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Tr7tAyt5zZpdTwTQK/the-solomonoff-prior-is-malign"
    ],
    "tags": [
      "ai",
      "distillation & pedagogy",
      "inner alignment",
      "priors",
      "solomonoff induction"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_59fc00ba17bc6f7f",
    "title": "[AN #121]: Forecasting transformative AI timelines using biological anchors",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/cxQtz3RP4qsqTkEwL/an-121-forecasting-transformative-ai-timelines-using"
    ],
    "tags": [
      "ai",
      "forecasting & prediction",
      "transformative ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0e33754ca220361a",
    "title": "Box inversion hypothesis",
    "year": 2020,
    "category": "public_awareness",
    "description": "*This text originated from a retreat in late 2018, where researchers from FHI, MIRI and CFAR did an extended double-crux on AI safety paradigms, with Eric Drexler and Scott Garrabrant in the core.  In the past two years I tried to improve it in terms of understandability multiple times, but empirically it seems quite inadequate. As it seems unlikely I will have time to invest further work into improving it, I'm publishing it as it is, with the hope that someone else will maybe understand the ideas even at this form, and describe them more clearly.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/TQwXPHfyyQwr22NMh/box-inversion-hypothesis"
    ],
    "tags": [
      "ai",
      "ai services (cais)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1064f4c92924f57c",
    "title": "[AN #122]: Arguing for AGI-driven existential risk from first principles",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/MxHiYZJjYm53ATxhb/an-122-arguing-for-agi-driven-existential-risk-from-first"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f9ea65c11456eb5c",
    "title": "The date of AI Takeover is not the day the AI takes over",
    "year": 2020,
    "category": "public_awareness",
    "description": "Instead, it's the point of no return--the day we AI risk reducers lose the ability to significantly reduce AI risk. This might happen years before classic milestones like \"World GWP doubles in four years\" and \"Superhuman AGI is deployed.\"",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/JPan54R525D68NoEt/the-date-of-ai-takeover-is-not-the-day-the-ai-takes-over"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "ai timelines"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7cb2522efada9b89",
    "title": "Introduction to Cartesian Frames",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": "This is the first post in a sequence on **Cartesian frames**, a new way of modeling agency that has recently shaped my thinking a lot.",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/BSpdshJWGAW6TuNzZ/introduction-to-cartesian-frames"
    ],
    "tags": [
      "ai",
      "embedded agency"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1e94de601c5cf7f1",
    "title": "Reply to Jebari and Lundborg on Artificial Superintelligence",
    "year": 2020,
    "category": "policy_development",
    "description": "Jebari and Lundborg have recently published an article entitled [*Artificial superintelligence and its limits: why AlphaZero cannot become a general agent*](https://link.springer.com/article/10.1007/s00146-020-01070-3). It focuses on the thorny issue of agency in superintelligent AIs. I'm glad to see more work on this crucial topic; however, I have significant disagreements with their terminology and argumentation, as I outline in this reply. Note that it was written rather quickly, and so might lack clarity in some places, or fail to convey some nuances of the original article. I welcome comments and further responses.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/rokpjK3jcy5aKKwiT/reply-to-jebari-and-lundborg-on-artificial-superintelligence-1"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f62880c2461cb785",
    "title": "Supervised learning of outputs in the brain",
    "year": 2020,
    "category": "public_awareness",
    "description": "*Follow-up to:* [*My computational framework for the brain*](https://www.lesswrong.com/posts/diruo47z32eprenTg/my-computational-framework-for-the-brain).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/jNrDzyc8PJ9HXtGFm/supervised-learning-of-outputs-in-the-brain"
    ],
    "tags": [
      "ai",
      "machine learning  (ml)",
      "neuroscience",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_fdde67d0d9ffcd07",
    "title": "A Correspondence Theorem",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": "I've been [thinking lately](https://www.lesswrong.com/posts/74crqQnH8v9JtJcda/egan-s-theorem) about formalizations of the [Correspondence Principle](https://en.wikipedia.org/wiki/Correspondence_principle) - the idea that new theories should reproduce old theories, at least in the places where the old theories work. Special relativity reduces to Galilean relativity at low speed/energy, general relativity reduces to Newtonian gravity when the fields are weak, quantum mechanics should reproduce classical mechanics at large scale, etc. More conceptually, it's the idea that flowers are \"real\": any model which does a sufficiently-good job of predicting the world around me should have some kind of structure in it corresponding to my notion of a flower (though it may not be ontologically basic).",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FWuByzM9T5qq2PF2n/a-correspondence-theorem"
    ],
    "tags": [
      "logic & mathematics",
      "practice & philosophy of science",
      "rationality"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_92741a9531f7cfe0",
    "title": "Security Mindset and Takeoff Speeds",
    "year": 2020,
    "category": "public_awareness",
    "description": "About this post\n---------------",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Lfk2FXBwrpoM6Jm7p/security-mindset-and-takeoff-speeds"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "ai timelines",
      "dialogue (format)",
      "security mindset"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_79bad74b007f523f",
    "title": "Dutch-Booking CDT: Revised Argument",
    "year": 2020,
    "category": "policy_development",
    "description": "*This post has benefited greatly from discussion with Sam Eisenstat, Caspar Oesterheld, and Daniel Kokotajlo.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/X7k23zk9aBjjpgLd3/dutch-booking-cdt-revised-argument"
    ],
    "tags": [
      "ai",
      "decision theory"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3eb10d91e041fbf6",
    "title": "Draft papers for REALab and Decoupled Approval on tampering",
    "year": 2020,
    "category": "public_awareness",
    "description": "Hi everyone, we (Ramana Kumar, Jonathan Uesato, Victoria Krakovna, Tom Everitt, and Richard Ngo) have been working on a strand of work researching tampering problems, and we've written up our progress in two papers. We're sharing drafts in advance here because we'd like to get feedback from everyone here.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/X23q6T4CDifHykqi4/draft-papers-for-realab-and-decoupled-approval-on-tampering"
    ],
    "tags": [
      "ai",
      "embedded agency",
      "reinforcement learning",
      "reward functions",
      "wireheading"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2dad9d2285cd61ec",
    "title": "[AN #123]: Inferring what is valuable in order to align recommender systems",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/HbtRFDiyTDpPfRLqm/an-123-inferring-what-is-valuable-in-order-to-align"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_6679bd407056f3c3",
    "title": "AI risk hub in Singapore?",
    "year": 2020,
    "category": "policy_development",
    "description": "I tentatively guess that if Singapore were to become a thriving hub for AI risk reduction, this would reduce AI risk by 16%. Moreover I think making this happen is fairly tractable and extremely neglected. In this post I sketch my reasons. I'm interested to hear what the community thinks.  \n  \nMy experience (and what I've been told) is that everyone generally agrees that it would be good for AI risk awareness to be raised in Asia, but conventional wisdom is that it's the job of people like [Brian Tse](https://www.fhi.ox.ac.uk/team/brian-tse/) to do that and most other people would only make things worse by trying to help. I think this is mostly right; my only disagreement is that I think the rest of us should look harder for ways to help, and be willing to sacrifice more if need be. For example, I suggested to MIRI that they move to Singapore, not because they could or should try to influence the government or anything like that, but because their presence in Singapore would make it...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/QTL5tRz7Q54bpcwdE/ai-risk-hub-in-singapore-1"
    ],
    "tags": [
      "ai",
      "ai risk",
      "community",
      "the sf bay area",
      "world optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_19b1f4bc45b179a8",
    "title": "\"Inner Alignment Failures\" Which Are Actually Outer Alignment Failures",
    "year": 2020,
    "category": "policy_development",
    "description": "*If you don't know what \"inner\" and \"outer\" optimization are, or why birth control or masturbation might be examples, then* [*check out one of the posts here*](https://www.lesswrong.com/tag/inner-alignment) *before reading this one. Thanks to Evan, Scott, and Richard for discussions around these ideas - though I doubt all their objections are settled yet.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/HYERofGZE6j9Tuigi/inner-alignment-failures-which-are-actually-outer-alignment"
    ],
    "tags": [
      "ai",
      "evolution",
      "inner alignment",
      "outer alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2ddc7951b87609ec",
    "title": "Confucianism in AI Alignment",
    "year": 2020,
    "category": "public_awareness",
    "description": "*I hear there's a thing where people write a lot in November, so I'm going to try writing a blog post every day. Disclaimer: this post is less polished than my median. And my median post isn't very polished to begin with.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/3aDeaJzxinoGNWNpC/confucianism-in-ai-alignment"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9cf8d4234b24dbf2",
    "title": "Subagents of Cartesian Frames",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": "Here, we introduce and discuss the concept of a subagent in the [Cartesian Frames](https://www.lesswrong.com/posts/BSpdshJWGAW6TuNzZ/introduction-to-cartesian-frames) paradigm.",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/nwrkwTd6uKBesYYfx/subagents-of-cartesian-frames"
    ],
    "tags": [
      "ai",
      "embedded agency",
      "subagents"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5a81b7a59af37a5a",
    "title": "[AN #124]: Provably safe exploration through shielding",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/TJqfcEyDdLwkDxZZC/an-124-provably-safe-exploration-through-shielding"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0f20e9f11eb7da65",
    "title": "Defining capability and alignment in gradient descent",
    "year": 2020,
    "category": "policy_development",
    "description": "This is the first post in a series where I'll explore AI alignment in a simplified setting: a neural network that's being trained by gradient descent. I'm choosing this setting because it involves a well-defined optimization process that has enough complexity to be interesting, but that's still understandable enough to make crisp mathematical statements about. As a result, it serves as a good starting point for rigorous thinking about alignment.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Xg2YycEfCnLYrCcjy/defining-capability-and-alignment-in-gradient-descent"
    ],
    "tags": [
      "ai",
      "inner alignment",
      "mesa-optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b28743e69f3e0612",
    "title": "Does SGD Produce Deceptive Alignment?",
    "year": 2020,
    "category": "public_awareness",
    "description": "[Deceptive alignment](https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment) was first introduced in [Risks from Learned Optimization](https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB), which contained initial versions of the arguments discussed here. Additional arguments were discovered in [this episode](https://futureoflife.org/2020/07/01/evan-hubinger-on-inner-alignment-outer-alignment-and-proposals-for-building-safe-advanced-ai/) of the AI Alignment Podcast and in conversation with Evan Hubinger.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ocWqg2Pf2br4jMmKA/does-sgd-produce-deceptive-alignment"
    ],
    "tags": [
      "ai",
      "deceptive alignment",
      "distillation & pedagogy",
      "inner alignment",
      "machine learning  (ml)",
      "mesa-optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_dea29a00b6e182dd",
    "title": "When Hindsight Isn't 20/20: Incentive Design With Imperfect Credit Allocation",
    "year": 2020,
    "category": "public_awareness",
    "description": "A crew of pirates all keep their gold in one very secure chest, with labelled sections for each pirate. Unfortunately, one day a storm hits the ship, tossing everything about. After the storm clears, the gold in the chest is all mixed up. The pirates each know how much gold they had - indeed, they're rather obsessive about it - but they don't trust each other to give honest numbers. How can they figure out how much gold each pirate had in the chest?",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/XPRAY34Sutc2wWYZf/when-hindsight-isn-t-20-20-incentive-design-with-imperfect"
    ],
    "tags": [
      "game theory",
      "mechanism design",
      "world modeling",
      "world optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_baed50b975b7c4ed",
    "title": "Why You Should Care About Goal-Directedness",
    "year": 2020,
    "category": "public_awareness",
    "description": "**Introduction**\n================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/q9BmNh35xgXPRgJhm/why-you-should-care-about-goal-directedness"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7d1073e9b4902059",
    "title": "Clarifying inner alignment terminology",
    "year": 2020,
    "category": "policy_development",
    "description": "I have seen [a lot of](https://www.alignmentforum.org/posts/HYERofGZE6j9Tuigi/inner-alignment-failures-which-are-actually-outer-alignment) [confusion recently](https://www.alignmentforum.org/posts/Xg2YycEfCnLYrCcjy/defining-capability-and-alignment-in-gradient-descent) surrounding exactly how outer and inner alignment should be defined and I want to try and provide my attempt at a clarification.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/SzecSPYxqRa5GCaSF/clarifying-inner-alignment-terminology"
    ],
    "tags": [
      "ai",
      "deconfusion"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7275dabc5f94cc81",
    "title": "[AN #125]: Neural network scaling laws across multiple modalities",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/XPqMbtpbku8aN55wd/an-125-neural-network-scaling-laws-across-multiple"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_82ded0e6629c8309",
    "title": "Learning Normativity: A Research Agenda",
    "year": 2020,
    "category": "public_awareness",
    "description": "*(Related to* [*Inaccessible Information*](https://www.alignmentforum.org/posts/ZyWyAJbedvEgRT2uF/inaccessible-information)*,* [*Learning the Prior*](https://www.alignmentforum.org/posts/SL9mKhgdmDKXmxwE4/learning-the-prior)*, and* [*Better Priors as a Safety Problem*](https://www.alignmentforum.org/posts/roA83jDvq7F2epnHK/better-priors-as-a-safety-problem)*. Builds on several of my* [*alternate alignment ideas*](https://www.lesswrong.com/s/SBfqYgHf2zvxyKDtB)*.)*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/2JGu9yxiJkoGdQR4s/learning-normativity-a-research-agenda"
    ],
    "tags": [
      "ai",
      "rationality"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b0dbef7645330638",
    "title": "A Correspondence Theorem in the Maximum Entropy Framework",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": "Classical mechanics didn't work any less well once we discovered quantum, Galilean relativity and Newtonian gravity didn't work any less well once we discovered special and general relativity, etc. This is the [correspondence principle](https://plato.stanford.edu/entries/bohr-correspondence/#GenCorPri), aka [Egan's Law](https://www.lesswrong.com/tag/egans-law): in general, to the extent that old models match reality, new models must reproduce the old.",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/XMGWdfTC7XjgTz3X7/a-correspondence-theorem-in-the-maximum-entropy-framework"
    ],
    "tags": [
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_aaba06c101037ea7",
    "title": "Communication Prior as Alignment Strategy",
    "year": 2020,
    "category": "public_awareness",
    "description": "Alice has one of three objects:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/zAvhvGa6ToieNGuy2/communication-prior-as-alignment-strategy"
    ],
    "tags": [
      "ai",
      "value learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4eca850265e6b730",
    "title": "Misalignment and misuse: whose values are manifest?",
    "year": 2020,
    "category": "public_awareness",
    "description": "*Crossposted from [world spirit sock puppet](https://worldspiritsockpuppet.com/meteuphoric.html).*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/AomSXpFcqmgeDyWWo/misalignment-and-misuse-whose-values-are-manifest"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_850089265cec10da",
    "title": "Early Thoughts on Ontology/Grounding Problems",
    "year": 2020,
    "category": "public_awareness",
    "description": "These all seem to be pointing to different aspects of the same problem.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/an29DQgYaKbyQprns/early-thoughts-on-ontology-grounding-problems"
    ],
    "tags": [
      "rationality",
      "symbol grounding"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8e2c8ca81dbde692",
    "title": "A guide to Iterated Amplification & Debate",
    "year": 2020,
    "category": "public_awareness",
    "description": "This post is about two proposals for aligning AI systems in a scalable way:",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/vhfATmAoJcN8RqGg6/a-guide-to-iterated-amplification-and-debate"
    ],
    "tags": [
      "ai",
      "ai risk",
      "debate (ai safety technique)",
      "factored cognition",
      "humans consulting hch",
      "iterated amplification"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3b805aa388400ce9",
    "title": "Normativity",
    "year": 2020,
    "category": "public_awareness",
    "description": "Now that I've written [Learning Normativity](https://www.lesswrong.com/posts/2JGu9yxiJkoGdQR4s/learning-normativity-a-research-agenda), I have some more clarity around the concept of \"normativity\" I was trying to get at, and want to write about it more directly. Whereas that post was more oriented toward the machine learning side of things, this post is more oriented toward the philosophical side. However, it *is* still relevant to the research direction, and I'll mention some issues relevant to value learning and other alignment approaches.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/tCex9F9YptGMpk2sT/normativity"
    ],
    "tags": [
      "ai",
      "human values",
      "meta-philosophy",
      "moral uncertainty",
      "rationality",
      "value learning",
      "world optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e499ff1de7aded97",
    "title": "The Pointers Problem: Human Values Are A Function Of Humans' Latent Variables",
    "year": 2020,
    "category": "public_awareness",
    "description": "An AI actively trying to figure out what I want might show me snapshots of different possible worlds and ask me to rank them. Of course, I do not have the processing power to examine entire worlds; all I can really do is look at some pictures or video or descriptions. The AI might show me a bunch of pictures from one world in which a genocide is quietly taking place in some obscure third-world nation, and another in which no such genocide takes place. Unless the AI *already* considers that distinction important enough to draw my attention to it, I probably won't notice it from the pictures, and I'll rank those worlds similarly - even though I'd prefer the one without the genocide. Even if the AI does happen to show me some mass graves (probably secondhand, e.g. in pictures of news broadcasts), and I rank them low, it may just learn that I prefer my genocides under-the-radar.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans"
    ],
    "tags": [
      "ai",
      "rationality",
      "the pointers problem",
      "value learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4a829d76c2bb606c",
    "title": "Inner Alignment in Salt-Starved Rats",
    "year": 2020,
    "category": "public_awareness",
    "description": "*(See comment* [*here*](https://www.lesswrong.com/posts/wcNEXDHowiWkRxDNv/inner-alignment-in-salt-starved-rats?commentId=Lm5GwMqFnavJN2kHq#Lm5GwMqFnavJN2kHq) *for some corrections and retractions. --Steve, 2022)*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/wcNEXDHowiWkRxDNv/inner-alignment-in-salt-starved-rats"
    ],
    "tags": [
      "ai",
      "inner alignment",
      "interpretability (ml & ai)",
      "neuroscience"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a4cb5b1ce2fb3771",
    "title": "Some AI research areas and their relevance to existential safety",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": "***Followed by:*** [*What Multipolar Failure Looks Like, and Robust Agent-Agnostic Processes (RAAPs)*](https://www.alignmentforum.org/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic)*, which provides examples of multi-stakeholder/multi-agent interactions leading to extinction events.*",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1"
    ],
    "tags": [
      "academic papers",
      "agent foundations",
      "ai",
      "existential risk",
      "world optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e437b7f1e56542b0",
    "title": "Persuasion Tools: AI takeover without AGI or agency?",
    "year": 2020,
    "category": "policy_development",
    "description": "*[epistemic status: speculation]*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qKvn7rxP2mzJbKfcA/persuasion-tools-ai-takeover-without-agi-or-agency"
    ],
    "tags": [
      "ai",
      "ai persuasion",
      "threat models",
      "world optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c0e8954d26e2abdf",
    "title": "Non-Obstruction: A Simple Concept Motivating Corrigibility",
    "year": 2020,
    "category": "policy_development",
    "description": "*Thanks to Mathias Bonde, Tiffany Cai, Ryan Carey, Michael Cohen, Joe Collman, Andrew Critch, Abram Demski, Michael Dennis, Thomas Gilbert, Matthew Graves, Koen Holtman, Evan Hubinger, Victoria Krakovna, Amanda Ngo, Rohin Shah, Adam Shimi, Logan Smith, and Mark Xu for their thoughts.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Xts5wm3akbemk4pDa/non-obstruction-a-simple-concept-motivating-corrigibility"
    ],
    "tags": [
      "ai",
      "corrigibility"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0f5544f649dc97f8",
    "title": "Continuing the takeoffs debate",
    "year": 2020,
    "category": "public_awareness",
    "description": "[Here's an intuitively compelling argument](https://www.lesswrong.com/posts/tjH8XPxAnr6JRbh7k/hard-takeoff): only a few million years after diverging from chimpanzees, humans became much more capable, at a rate that was very rapid compared with previous progress. This supports the idea that AIs will, at some point, also start becoming more capable at a very rapid rate. Paul Christiano has made an [influential response](https://sideways-view.com/2018/02/24/takeoff-speeds/); the goal of this post is to evaluate and critique it. Note that the arguments discussed in this post are quite speculative and uncertain, and also cover only a small proportion of the factors which should influence our views on takeoff speeds - so in the process of writing it I've made only a small update towards very fast takeoff. Also, given that Paul's vision of a continuous takeoff occurs much faster than any mainstream view, I expect that even totally resolving this debate would have relatively few implicatio...",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Tpn2Fx9daLvj28kes/continuing-the-takeoffs-debate"
    ],
    "tags": [
      "ai",
      "ai takeoff"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_dd8421e6a2c91eb4",
    "title": "Commentary on AGI Safety from First Principles",
    "year": 2020,
    "category": "policy_development",
    "description": "My *AGI safety from first principles* report (which is [now online here](https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ)) was originally circulated as a google doc. Since there was a lot of good discussion in comments on the original document, I thought it would be worthwhile putting some of it online, and have copied out most of the substantive comment threads here. Many thanks to all of the contributors for their insightful points, and to Habryka for helping with formatting. Note that in some cases comments may refer to parts of the report that didn't make it into the public version.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/oiuZjPfknKsSc5waC/commentary-on-agi-safety-from-first-principles"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f267835a15e5ee0e",
    "title": "Critiques of the Agent Foundations agenda?",
    "year": 2020,
    "category": "public_awareness",
    "description": "What are some substantial critiques of the agent foundations research agenda?",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/3jqKmuG7zq2qQLSBT/critiques-of-the-agent-foundations-agenda"
    ],
    "tags": [
      "agent foundations",
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_82f462772ed16082",
    "title": "[AN #126]: Avoiding wireheading by decoupling action feedback from action effects",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ak8a6fbhXbdqH3FgD/an-126-avoiding-wireheading-by-decoupling-action-feedback"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_73798d888d505aa9",
    "title": "Idealized Factored Cognition",
    "year": 2020,
    "category": "public_awareness",
    "description": "*(This post is part of a sequence that's meant to be read in order; see the [preface](https://www.lesswrong.com/posts/fnrpxdnodQmanibmB/preface-to-the-sequence-on-factored-cognition).)*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/S5oWwZMJBvfChSquW/idealized-factored-cognition"
    ],
    "tags": [
      "ai",
      "debate (ai safety technique)",
      "factored cognition",
      "humans consulting hch",
      "rationality"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ff5b2df9e3d7ba91",
    "title": "In a multipolar scenario, how do people expect systems to be trained to interact with systems developed by other labs?",
    "year": 2020,
    "category": "public_awareness",
    "description": "I haven't seen much discussion of this, but it seems like an important factor in how well AI systems deployed by actors with different goals manage to avoid conflict (cf. my discussion of equilibrium and prior selection problems [here](https://www.alignmentforum.org/posts/Tdu3tGT4i24qcLESh/equilibrium-and-prior-selection-problems-in-multipolar-1)).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Pkthep47ukcrK3MNm/in-a-multipolar-scenario-how-do-people-expect-systems-to-be"
    ],
    "tags": [
      "ai",
      "multipolar scenarios"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3d8660b5c9a43615",
    "title": "Recursive Quantilizers II",
    "year": 2020,
    "category": "public_awareness",
    "description": "I originally introduced the recursive quantilizers idea [here](https://www.lesswrong.com/s/SBfqYgHf2zvxyKDtB/p/bEa4FuLS4r7hExoty), but didn't provide a formal model until my recent [Learning Normativity](https://www.lesswrong.com/posts/2JGu9yxiJkoGdQR4s/learning-normativity-a-research-agenda) post. That formal model had some problems. I'll correct some of those problems here. My new model is closer to HCH+IDA, and so, is even closer to Paul Christiano style systems than my previous.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/YNuJjRuxsWWzfvder/recursive-quantilizers-ii"
    ],
    "tags": [
      "ai",
      "meta-philosophy",
      "quantilization",
      "value learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4f60b180b5902547",
    "title": "[AN #127]: Rethinking agency: Cartesian frames as a formalization of ways to carve up the world into an agent and its environment",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ZZDHoqpHmChxEYMme/an-127-rethinking-agency-cartesian-frames-as-a-formalization"
    ],
    "tags": [
      "ai",
      "boundaries / membranes [technical]",
      "newsletters"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_630b2d68a3790718",
    "title": "AI Problems Shared by Non-AI Systems",
    "year": 2020,
    "category": "policy_development",
    "description": "*I am grateful to Cara Selvarajah for numerous discussions regarding this post.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/iGs2jHc6Mcm3jtefk/ai-problems-shared-by-non-ai-systems"
    ],
    "tags": [
      "ai",
      "world optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_edc6171827b32cf6",
    "title": "Conservatism in neocortex-like AGIs",
    "year": 2020,
    "category": "public_awareness",
    "description": "*(Related to* [*Stuart Armstrong's post last summer on Model Splintering*](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1)*.)*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/c92YC89tznC7579Ej/conservatism-in-neocortex-like-agis"
    ],
    "tags": [
      "ai",
      "conservatism (ai)",
      "neuroscience"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f56abd059a454b21",
    "title": "[AN #128]: Prioritizing research on AI existential safety based on its application to governance demands",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/esjMWREvj3WKZpBZd/an-128-prioritizing-research-on-ai-existential-safety-based"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_522cb2982c20ffb9",
    "title": "Avoiding Side Effects in Complex Environments",
    "year": 2020,
    "category": "policy_development",
    "description": "Previously:[*Attainable Utility Preservation: Empirical Results*](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW/p/4J4TA2ZF3wmSxhxuc)*; summarized in* [*AN #105*](https://www.lesswrong.com/posts/gWRJDwqHnmJhurXgo/an-105-the-economic-trajectory-of-humanity-and-what-we-might#PREVENTING_BAD_BEHAVIOR_)",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/5kurn5W62C5CpSWq6/avoiding-side-effects-in-complex-environments"
    ],
    "tags": [
      "ai",
      "cellular automata",
      "impact regularization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4efbd94663eff0d1",
    "title": "Clarifying Factored Cognition",
    "year": 2020,
    "category": "public_awareness",
    "description": "This post is sort of an intermediate between parts 1 and 2 of the sequence. It makes three points that I think people tend to get wrong.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/eCWkJrFff7oMLwjEp/clarifying-factored-cognition"
    ],
    "tags": [
      "ai",
      "debate (ai safety technique)",
      "factored cognition",
      "humans consulting hch"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_63bb8c02798d5c36",
    "title": "Risk Map of AI Systems",
    "year": 2020,
    "category": "public_awareness",
    "description": "*Joint work with Jan Kulveit. Many thanks to the various people who gave me feedback.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/QskBy5uDd2oeEGkBB/risk-map-of-ai-systems"
    ],
    "tags": [
      "ai",
      "ai risk",
      "carving / clustering reality",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_afa9224051a4e00e",
    "title": "Homogeneity vs. heterogeneity in AI takeoff scenarios",
    "year": 2020,
    "category": "policy_development",
    "description": "*Special thanks to Kate Woolverton for comments and feedback.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/mKBfa8v4S9pNKSyKK/homogeneity-vs-heterogeneity-in-ai-takeoff-scenarios"
    ],
    "tags": [
      "ai",
      "ai takeoff"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8099cfd43448ee15",
    "title": "Less Basic Inframeasure Theory",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/idP5E5XhJGh9T5Yq9/less-basic-inframeasure-theory"
    ],
    "tags": [
      "ai",
      "infra-bayesianism",
      "rationality"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1a468d34b2ceef34",
    "title": "[AN #129]: Explaining double descent by measuring bias and variance",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/r3AcHkAXPbjPwXFjc/an-129-explaining-double-descent-by-measuring-bias-and"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e93eef718fe8472c",
    "title": "Extrapolating GPT-N performance",
    "year": 2020,
    "category": "public_awareness",
    "description": "[Brown et al. (2020)](https://arxiv.org/pdf/2005.14165.pdf) (which describes the development of GPT-3) contains measurements of how 8 transformers of different sizes perform on several different benchmarks. In this post, I project how performance could improve for larger models, and give an overview of issues that may appear when scaling-up. Note that these benchmarks are for 'downstream tasks' that are different from the training task (which is to predict the next token); these extrapolations thus cannot be directly read off the scaling laws in OpenAI's Scaling Laws for Neural Language Models ([Kaplan et al., 2020](https://arxiv.org/pdf/2001.08361.pdf)) or Scaling Laws for Autoregressive Generative Modelling ([Henighan et al., 2020](https://arxiv.org/pdf/2010.14701.pdf)).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/k2SNji3jXaLGhBeYP/extrapolating-gpt-n-performance"
    ],
    "tags": [
      "ai",
      "ai timelines",
      "gpt",
      "language models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_95dc03f91373f125",
    "title": "Hierarchical planning: context agents",
    "year": 2020,
    "category": "policy_development",
    "description": "*If you end up applying this post, please do it in the name of safety research.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/6ayQbR5opoTN4AgFb/hierarchical-planning-context-agents"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f3515b42aa127138",
    "title": "2020 AI Alignment Literature Review and Charity Comparison",
    "year": 2020,
    "category": "policy_development",
    "description": "*cross-posted to the EA forum* [*here*](https://forum.effectivealtruism.org/posts/K7Z87me338BQT3Mcv/2020-ai-alignment-literature-review-and-charity-comparison)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pTYDdcag9pTzFQ7vw/2020-ai-alignment-literature-review-and-charity-comparison"
    ],
    "tags": [
      "ai",
      "literature reviews"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a433beae9c276004",
    "title": "TAI Safety Bibliographic Database",
    "year": 2020,
    "category": "policy_development",
    "description": "*Authors: Jess Riedel and Angelica Deibel*  \n[Cross-posted to EA Forum](https://forum.effectivealtruism.org/posts/S7x3ztfd9h8ux68wN/tai-safety-bibliographic-database)",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/4DegbDJJiMX2b3EKm/tai-safety-bibliographic-database"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1d3becb495f00b9f",
    "title": "Debate update: Obfuscated arguments problem",
    "year": 2020,
    "category": "technical_research_breakthrough",
    "description": "This is an update on the work on AI Safety via Debate that we previously wrote about [here](https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1).",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem"
    ],
    "tags": [
      "ai",
      "debate (ai safety technique)",
      "iterated amplification",
      "openai",
      "outer alignment"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_048616e5c600aaac",
    "title": "2019 Review Rewrite: Seeking Power is Often Robustly Instrumental in MDPs",
    "year": 2020,
    "category": "public_awareness",
    "description": "For the 2019 LessWrong review, I've completely rewritten my post [*Seeking Power is Often Robustly Instrumental in MDPs*](https://www.lesswrong.com/posts/6DuJxY8X45Sco4bS2/seeking-power-is-often-provably-instrumentally-convergent-in). The post explains the key insights of [my theorems on power-seeking and instrumental convergence / robust instrumentality](https://arxiv.org/abs/1912.01683v6). The new version is more substantial, more nuanced, and better motivated, without sacrificing the broad accessibility or the cute drawings of the original.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/mxXcPzpgGx4f8eK7v/2019-review-rewrite-seeking-power-is-often-robustly"
    ],
    "tags": [
      "ai",
      "instrumental convergence",
      "lesswrong review",
      "myopia"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5cf6dbe41151b29e",
    "title": "Announcing AXRP, the AI X-risk Research Podcast",
    "year": 2020,
    "category": "public_awareness",
    "description": "Happy holidays! Today, I'm launching AXRP (pronounced axe-urp), the AI X-risk Research Podcast. The episodes involve me interviewing a researcher about a paper they've written, talking about the ideas in the paper and why they matter. The first three guests are [Adam Gleave](https://www.gleave.me/), [Rohin Shah](https://rohinshah.com/), and [Andrew Critch](http://acritch.com/). You can listen on all major podcast distribution networks by searching \"AXRP\", or read transcripts online at [axrp.net](https://axrp.net/). If you have comments about the show, feel free to leave them here, or to email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/NWi8ztKCbguBEAwdG/announcing-axrp-the-ai-x-risk-research-podcast-1"
    ],
    "tags": [
      "ai",
      "axrp",
      "project announcement"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0c1ff263b04b9567",
    "title": "[AN #130]: A new AI x-risk podcast, and reviews of the field",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/h2ipMwfx4D3oenzu2/an-130-a-new-ai-x-risk-podcast-and-reviews-of-the-field"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_24c5d8b76ae96be8",
    "title": "Operationalizing compatibility with strategy-stealing",
    "year": 2020,
    "category": "policy_development",
    "description": "*Thanks to Noa Nabeshima and Kate Woolverton for helpful comments and feedback.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/WwJdaymwKq6qyJqBX/operationalizing-compatibility-with-strategy-stealing"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9c543c7b51c910c2",
    "title": "Defusing AGI Danger",
    "year": 2020,
    "category": "policy_development",
    "description": "This represents thinking about AGI safety done under mentorship by Evan Hubinger. Thanks also to Buck Shlegeris, Noa Nabeshima, Thomas Kwa, Sydney Von Arx and Jack Ryan for helpful discussion and comments.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/BSrfDWpHgFpzGRwJS/defusing-agi-danger"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_6d26d59814ae3682",
    "title": "Why Neural Networks Generalise, and Why They Are (Kind of) Bayesian",
    "year": 2020,
    "category": "public_awareness",
    "description": "Currently, we do not have a good theoretical understanding of how or why neural networks actually work. For example, we know that large neural networks are sufficiently expressive to compute almost any kind of function. Moreover, most functions that fit a given set of training data will not generalise well to new data. And yet, if we train a neural network we will usually obtain a function that gives good generalisation. What is the mechanism behind this phenomenon?",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/YSFJosoHYFyXjoYWa/why-neural-networks-generalise-and-why-they-are-kind-of"
    ],
    "tags": [
      "ai",
      "lottery ticket hypothesis"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3e78c1f93299be8c",
    "title": "Against GDP as a metric for timelines and takeoff speeds",
    "year": 2020,
    "category": "policy_development",
    "description": "Or: Why AI Takeover Might Happen Before GDP Accelerates, and Other Thoughts On What Matters for Timelines and Takeoff Speeds\n============================================================================================================================",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/aFaKhG86tTrKvtAnT/against-gdp-as-a-metric-for-timelines-and-takeoff-speeds"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "ai timelines",
      "center on long-term risk (clr)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_26d131087cd40fcb",
    "title": "AXRP Episode 1 - Adversarial Policies with Adam Gleave",
    "year": 2020,
    "category": "policy_development",
    "description": "[Google Podcasts link](https://podcasts.google.com/feed/aHR0cHM6Ly9heHJwb2RjYXN0LmxpYnN5bi5jb20vcnNz/episode/ZjgxZWU3YjYtMTdmMS00NmQ1LWExZjctZGY0NGQzZGJlNjBi)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/8MZ72PYa3kRe4yRDD/axrp-episode-1-adversarial-policies-with-adam-gleave"
    ],
    "tags": [
      "adversarial examples",
      "ai",
      "audio",
      "axrp",
      "interviews",
      "reinforcement learning"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_03aead3b05c0aec5",
    "title": "AXRP Episode 2 - Learning Human Biases with Rohin Shah",
    "year": 2020,
    "category": "policy_development",
    "description": "[Google Podcasts link](https://podcasts.google.com/feed/aHR0cHM6Ly9heHJwb2RjYXN0LmxpYnN5bi5jb20vcnNz/episode/OTY2ZDUzNmEtYjRjOC00ODZiLTliZGMtMWQwOTMyMzdhZWY1)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/BJAcnMBHGua3tFKu5/axrp-episode-2-learning-human-biases-with-rohin-shah"
    ],
    "tags": [
      "ai",
      "audio",
      "axrp",
      "heuristics & biases",
      "interviews",
      "inverse reinforcement learning"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0c3d18fc7091aa16",
    "title": "AXRP Episode 3 - Negotiable Reinforcement Learning with Andrew Critch",
    "year": 2020,
    "category": "policy_development",
    "description": "[Google Podcasts link](https://podcasts.google.com/feed/aHR0cHM6Ly9heHJwb2RjYXN0LmxpYnN5bi5jb20vcnNz/episode/ZmFmZmFkMTctZmJhZC00ODRhLThhZGUtNjk1NGU1ZWI2NDJi)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/u7o7HtChnZ5x8SqvA/axrp-episode-3-negotiable-reinforcement-learning-with-andrew"
    ],
    "tags": [
      "ai",
      "audio",
      "axrp",
      "disagreement",
      "interviews",
      "moral uncertainty",
      "reinforcement learning",
      "utilitarianism"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c2c316f306c154be",
    "title": "Debate Minus Factored Cognition",
    "year": 2020,
    "category": "public_awareness",
    "description": "[AI safety via debate](https://www.alignmentforum.org/posts/wo6NsBtn3WJDCeWsx/ai-safety-via-debate) has been, so far, associated with [Factored Cognition](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd). There are good reasons for this. For one thing, Factored Cognition gives us a potential *gold standard for amplification* -- what it means to give very, very good answers to questions. Namely, HCH. To the extent that we buy HCH as a gold standard, proving that debate approximates HCH in some sense would give us some assurances about what it is accomplishing.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/a2NZr87sGYpXhzsth/debate-minus-factored-cognition"
    ],
    "tags": [
      "ai",
      "debate (ai safety technique)",
      "factored cognition"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8b200ee0c90ea416",
    "title": "[AN #131]: Formalizing the argument of ignored attributes in a utility function",
    "year": 2020,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/k2sBrR4gJX9BNTuoa/an-131-formalizing-the-argument-of-ignored-attributes-in-a"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  }
]