[
  {
    "id": "alignmentforum_dcff3a5a68bf1029",
    "title": "AI Alignment, Philosophical Pluralism, and the Relevance of Non-Western Philosophy",
    "year": 2021,
    "category": "policy_development",
    "description": "*This is an extended transcript of* [*the talk I gave at EAGxAsiaPacific 2020*](https://www.youtube.com/watch?v=dbMp4pFVwnU)*. In the talk, I present a somewhat critical take on how AI alignment has grown as a field, and how, from my perspective, it deserves considerably more philosophical and disciplinary diversity than it has enjoyed so far. I'm sharing it here in the hopes of generating discussion about the disciplinary and philosophical paradigms that (I understand) the AI alignment community to be rooted in, and whether or how we should move beyond them. Some sections cover introductory material that most people here are likely to be familiar with, so feel free to skip them.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/jS2iiDPqMvZ2tnik2/ai-alignment-philosophical-pluralism-and-the-relevance-of"
    ],
    "tags": [
      "ai",
      "meta-philosophy",
      "metaethics",
      "philosophy",
      "suffering",
      "value learning",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_898da267695e9d47",
    "title": "Reflections on Larks' 2020 AI alignment literature review",
    "year": 2021,
    "category": "policy_development",
    "description": "*This work was supported by OAK, a monastic community in the Berkeley hills. It could not have been written without the daily love of living in this beautiful community.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/uEo4Xhp7ziTKhR6jq/reflections-on-larks-2020-ai-alignment-literature-review"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_095c961a75cdab45",
    "title": "Multi-dimensional rewards for AGI interpretability and control",
    "year": 2021,
    "category": "policy_development",
    "description": "*Update August 2021:* Re-reading this post, I continue to think this is a good and important idea, and I was very happy to learn after I wrote it that what I had in mind here is really a plausible, viable thing to do, even given the cost and performance requirements that people will demand of our future AGIs. I base that belief on the fact that (I now think) the brain does more-or-less exactly what I talk about here (see my post [A model of decision-making in the brain](https://www.lesswrong.com/posts/e5duEqhAhurT8tCyr/a-model-of-decision-making-in-the-brain-the-short-version)), and also on the fact that the machine learning literature also has things like this (see the comments section at the bottom).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Lj9QXcqkcuR4iHJ7Q/multi-dimensional-rewards-for-agi-interpretability-and"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)",
      "reinforcement learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9dcfdc08d29e5ae5",
    "title": "The Pointers Problem: Clarifications/Variations",
    "year": 2021,
    "category": "policy_development",
    "description": "I've recently had several conversations about John Wentworth's post [The Pointers Problem](https://www.lesswrong.com/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans). I think there is some confusion about this post, because there are several related issues, which different people may take as primary. All of these issues are important to \"the pointers problem\", but John's post articulates a specific problem in a way that's not quite articulated anywhere else.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/7Zn4BwgsiPFhdB6h8/the-pointers-problem-clarifications-variations"
    ],
    "tags": [
      "ai",
      "rationality",
      "the pointers problem"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4219581f5fa93723",
    "title": "[AN #132]: Complex and subtly incorrect arguments as an obstacle to debate",
    "year": 2021,
    "category": "policy_development",
    "description": "[AN #132]: Complex and subtly incorrect arguments as an obstacle to debate\n \n \n \n \n Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/huNvfttDpxCApC3xZ/an-132-complex-and-subtly-incorrect-arguments-as-an-obstacle"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e2c9b02677500772",
    "title": "Review of 'But exactly how complex and fragile?'",
    "year": 2021,
    "category": "policy_development",
    "description": "I've thought about (concepts related to) the fragility of value [quite](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW/p/w6BtMqKRLxG9bNLMr#Objective_vs_value_specific_catastrophes) [a bit](https://www.lesswrong.com/posts/Xts5wm3akbemk4pDa/non-obstruction-a-simple-concept-motivating-corrigibility) over the last year, and so I returned to Katja Grace's [*But exactly how complex and fragile?*](https://www.lesswrong.com/posts/xzFQp7bmkoKfnae9R/but-exactly-how-complex-and-fragile)with renewed appreciation (I'd previously commented only [a very brief microcosm](https://www.lesswrong.com/posts/xzFQp7bmkoKfnae9R/but-exactly-how-complex-and-fragile?commentId=GAxppfoKhiFRrWHgK) of this review). I'm glad that Katja wrote this post and I'm glad that everyone commented. I often see [private Google docs full of nuanced discussion which will never see the light of day](https://www.lesswrong.com/posts/hnvPCZ4Cx35miHkw3/why-is-so-much-discussion-happening-in-private-google-docs), and that makes me sa...",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/r6p5cqT6aWYGCYHJx/review-of-but-exactly-how-complex-and-fragile"
    ],
    "tags": [
      "ai",
      "complexity of value",
      "lesswrong review"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3aaa1a501a97ed4c",
    "title": "Eight claims about multi-agent AGI safety",
    "year": 2021,
    "category": "policy_development",
    "description": "There are quite a few arguments about how interactions between multiple AGIs affect risks from AGI development. I've identified at least eight distinct but closely-related claims which it seems worthwhile to disambiguate. I've split them up into four claims about the process of training AGIs, and four claims about the process of deploying AGIs; after listing them, I go on to explain each in more detail. Note that while I believe that all of these ideas are interesting enough to warrant further investigation, I don't currently believe that all of them are true as stated. In particular, I think that so far there's been little compelling explanation of why interactions between many aligned AIs might have castastrophic effects on the world (as is discussed in point 7).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/dSAJdi99XmqftqXXq/eight-claims-about-multi-agent-agi-safety"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_77b71b9037cd56f0",
    "title": "The Case for a Journal of AI Alignment",
    "year": 2021,
    "category": "policy_development",
    "description": "When you have some nice research in AI Alignment, where do you publish it? Maybe your research fits with a ML or AI conference. But some papers/research are hard sells to traditional venues: things like [Risks from Learned Optimization](https://arxiv.org/abs/1906.01820), [Logical Induction](https://arxiv.org/pdf/1609.03543.pdf), and a lot of research done on this Forum.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/hNNM6gP5yZcHffmpD/the-case-for-a-journal-of-ai-alignment"
    ],
    "tags": [
      "ai",
      "world optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_de4c0455a1d7f727",
    "title": "Imitative Generalisation (AKA 'Learning the Prior')",
    "year": 2021,
    "category": "public_awareness",
    "description": "Research publication: Imitative Generalisation (AKA 'Learning the Prior')",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/JKj5Krff5oKMb8TjT/imitative-generalisation-aka-learning-the-prior-1"
    ],
    "tags": [
      "ai",
      "debate (ai safety technique)",
      "distillation & pedagogy",
      "iterated amplification",
      "openai",
      "outer alignment"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d8cec8fbf59ec688",
    "title": "Review of Soft Takeoff Can Still Lead to DSA",
    "year": 2021,
    "category": "public_awareness",
    "description": "A few months after writing [this post](https://www.lesswrong.com/posts/PKy8NuNPknenkDY74/soft-takeoff-can-still-lead-to-decisive-strategic-advantage) I realized that one of the key arguments was importantly flawed. I therefore recommend against inclusion in the 2019 review. This post presents an improved version of the original argument, explains the flaw, and then updates my all-things-considered view accordingly.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/P448hmmAeGepQDREs/review-of-soft-takeoff-can-still-lead-to-dsa"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "lesswrong review"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c04f8dfe06338ebb",
    "title": "Prediction can be Outer Aligned at Optimum",
    "year": 2021,
    "category": "public_awareness",
    "description": "This post argues that many prediction tasks are *outer aligned at optimum*. In particular, I think that the malignity of the universal prior should be treated as an inner alignment problem rather than an outer alignment problem. The main argument is entirely in the first section; treat the rest as appendices.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/3D2MGF2fZhWSNb7aw/prediction-can-be-outer-aligned-at-optimum"
    ],
    "tags": [
      "ai",
      "outer alignment",
      "solomonoff induction"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8cf8b19f327c7deb",
    "title": "Transparency and AGI safety",
    "year": 2021,
    "category": "policy_development",
    "description": "Introduction\n============",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/QirLfXhDPYWCP8PK5/transparency-and-agi-safety"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4f7d67ccb3b9b492",
    "title": "Review of 'Debate on Instrumental Convergence between LeCun, Russell, Bengio, Zador, and More'",
    "year": 2021,
    "category": "public_awareness",
    "description": "*I* [*think*](https://www.lesswrong.com/posts/6DuJxY8X45Sco4bS2/seeking-power-is-often-robustly-instrumental-in-mdps#A_note_on_terminology) *that 'robust instrumentality' is a more apt name for 'instrumental convergence.' That said, for backwards compatibility, this post often uses the latter.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ZPEGLoWMN242Dob6g/review-of-debate-on-instrumental-convergence-between-lecun"
    ],
    "tags": [
      "ai",
      "instrumental convergence",
      "lesswrong review"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_918349ea8f7c8dcb",
    "title": "[AN #133]: Building machines that can cooperate (with humans, institutions, or other machines)",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/S8khsrXnHEwYbhd8X/an-133-building-machines-that-can-cooperate-with-humans"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_edfe4377ebf54d73",
    "title": "Some recent survey papers on (mostly near-term) AI safety, security, and assurance",
    "year": 2021,
    "category": "policy_development",
    "description": "As part of a project I am doing at work, I took a look around to find recent overview / survey / literature review papers on several topics related to AI safety, security, and assurance. The focus was primarily on nearer-term issues, but with an eye towards longer-term risks as well. Several of the papers I found will be familiar to many people here, but others might not be, so I am sharing this list in case it is helpful to anybody else.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/GTcWrenvDMsThTQ26/some-recent-survey-papers-on-mostly-near-term-ai-safety"
    ],
    "tags": [
      "ai",
      "literature reviews"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_fadeb3cc71fcc0ad",
    "title": "Thoughts on Iason Gabriel's Artificial Intelligence, Values, and Alignment",
    "year": 2021,
    "category": "public_awareness",
    "description": "Iason Gabriel's 2020 article [Artificial Intelligence, Values, and Alignment](https://arxiv.org/pdf/2001.09768.pdf) is a philosophical perspective on what the goal of alignment actually is, and how we might accomplish it. In the best spirit of modern philosophy, it provides a helpful framework for organizing what has already been written about levels at which we might align AI systems, and also provides a neat set of connections between concepts in AI alignment and concepts in modern philosophy.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Z2rkdEAJ9MvYPBeYW/thoughts-on-iason-gabriel-s-artificial-intelligence-values"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_aaa896cd597e2544",
    "title": "Why I'm excited about Debate",
    "year": 2021,
    "category": "public_awareness",
    "description": "I think [Debate](https://openai.com/blog/debate/) is probably the most exciting existing safety research direction. This is a pretty significant shift from my opinions when I first read about it, so it seems worth outlining what's changed. I'll group my points into three categories. Points 1-3 are strategic points about deployment of useful AGIs. Points 4-6 are technical points about Debate. Points 7-9 are meta-level points about how to evaluate safety techniques, in particular responding to [Beth Barnes' recent post on obfuscated arguments in Debate](https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/LDsSqXf9Dpu3J3gHD/why-i-m-excited-about-debate"
    ],
    "tags": [
      "ai",
      "debate (ai safety technique)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b75e30a367f20454",
    "title": "Literature Review on Goal-Directedness",
    "year": 2021,
    "category": "policy_development",
    "description": "**Introduction: Questioning Goals**\n===================================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/cfXwr6NC9AqZ9kr8g/literature-review-on-goal-directedness"
    ],
    "tags": [
      "ai",
      "goal-directedness",
      "literature reviews"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_773d5a27b20bb1ba",
    "title": "Birds, Brains, Planes, and AI: Against Appeals to the Complexity/Mysteriousness/Efficiency of the Brain",
    "year": 2021,
    "category": "public_awareness",
    "description": "*[Epistemic status: Strong opinions lightly held, this time with a cool graph.]*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/HhWhaSzQr6xmBki8F/birds-brains-planes-and-ai-against-appeals-to-the-complexity"
    ],
    "tags": [
      "ai",
      "ai timelines",
      "center on long-term risk (clr)",
      "history",
      "technological forecasting"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7ad3af07b34991fe",
    "title": "Some thoughts on risks from narrow, non-agentic AI",
    "year": 2021,
    "category": "policy_development",
    "description": "Here are some concerns which have been raised about the development of advanced AI:",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/AWbtbmC6rAg6dh75b/some-thoughts-on-risks-from-narrow-non-agentic-ai"
    ],
    "tags": [
      "ai",
      "narrow ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a08784881780a40e",
    "title": "Against the Backward Approach to Goal-Directedness",
    "year": 2021,
    "category": "public_awareness",
    "description": "Introduction: Forward and Backward Approaches\n=============================================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/adKSWktLbxfihDANM/against-the-backward-approach-to-goal-directedness"
    ],
    "tags": [
      "ai",
      "goal-directedness"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3910ea66302bf20e",
    "title": "Infra-Bayesianism Unwrapped",
    "year": 2021,
    "category": "policy_development",
    "description": "**Introduction**\n================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Zi7nmuSmBFbQWgFBa/infra-bayesianism-unwrapped"
    ],
    "tags": [
      "ai",
      "decision theory",
      "distillation & pedagogy",
      "infra-bayesianism",
      "logical uncertainty"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f0298abb1a865cce",
    "title": "[AN #134]: Underspecification as a cause of fragility to distribution shift",
    "year": 2021,
    "category": "public_awareness",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/nM99oLhRzrmLWozoM/an-134-underspecification-as-a-cause-of-fragility-to"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3b88d36a7069cfcc",
    "title": "Poll: Which variables are most strategically relevant?",
    "year": 2021,
    "category": "public_awareness",
    "description": "Which variables are most important for predicting and influencing how AI goes?",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/yhb5BNksWcESezp7p/poll-which-variables-are-most-strategically-relevant"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "ai timelines",
      "world optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d9de55330127f581",
    "title": "Optimal play in human-judged Debate usually won't answer your question",
    "year": 2021,
    "category": "policy_development",
    "description": "Epistemic status: highly confident (99%+) this is an issue for *optimal* play with human consequentialist judges. Thoughts on practical implications are more speculative, and involve much hand-waving (70% sure I'm not overlooking a trivial fix, and that this can't be safely ignored).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/35748mXjzwxDrX7yQ/optimal-play-in-human-judged-debate-usually-won-t-answer"
    ],
    "tags": [
      "ai",
      "debate (ai safety technique)",
      "rationality"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_dd273c6968479590",
    "title": "[AN #135]: Five properties of goal-directed systems",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/wJYitLpqujQqwX7ke/an-135-five-properties-of-goal-directed-systems"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_92b41738908e808c",
    "title": "Extracting Money from Causal Decision Theorists",
    "year": 2021,
    "category": "public_awareness",
    "description": "My paper with my Ph.D. advisor Vince Conitzer titled \"Extracting Money from Causal Decision Theorists\" has been [formally published](https://doi.org/10.1093/pq/pqaa086) (Open Access) in *The Philosophical Quarterly*. Probably many of you have seen either earlier drafts of this paper or similar arguments that others have independently given on this forum (e.g., Stuart Armstrong [posted](https://www.alignmentforum.org/posts/Kr76XzME7TFkN937z/predictors-exist-cdt-going-bonkers-forever) about an almost identical scenario; Abram Demski's post on [Dutch-Booking CDT](https://www.alignmentforum.org/posts/X7k23zk9aBjjpgLd3/dutch-booking-cdt-revised-argument) also has some similar ideas) and elsewhere (e.g., [Spencer (forthcoming)](http://www.jackspencer.org/uploads/1/4/0/3/14038590/anaa037.pdf)  and [Ahmed (unpublished)](https://www.academia.edu/36270656/Sequential_Choice_and_the_Agents_Perspective?email_work_card=title) both make arguments that resemble some points from our paper).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/xPeWJaAzp2LeDdP4Z/extracting-money-from-causal-decision-theorists"
    ],
    "tags": [
      "rationality"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_52b1ce720a153bf6",
    "title": "AMA on EA Forum: Ajeya Cotra, researcher at Open Phil",
    "year": 2021,
    "category": "public_awareness",
    "description": "Hi all, I'm [Ajeya](https://www.openphilanthropy.org/about/team/ajeya-cotra), and I'll be doing an AMA on the EA Forum (this is a linkpost for my announcement there). I would love to get questions from LessWrong and Alignment Forum users as well -- please head on over if you have any questions for me!",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/F2C6KKRXGeZ424mi7/ama-on-ea-forum-ajeya-cotra-researcher-at-open-phil"
    ],
    "tags": [
      "ai",
      "ama",
      "community",
      "world optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_03a23a21e2f486fc",
    "title": "A Critique of Non-Obstruction",
    "year": 2021,
    "category": "policy_development",
    "description": "Epistemic status: either I'm confused, or non-obstruction isn't what I want.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ZqfT5xTuNf6okrepY/a-critique-of-non-obstruction"
    ],
    "tags": [
      "ai",
      "corrigibility",
      "impact regularization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e71f390234a0c088",
    "title": "Distinguishing claims about training vs deployment",
    "year": 2021,
    "category": "policy_development",
    "description": "Given the rapid progress in machine learning over the last decade in particular, I think that the core arguments about why AGI might be dangerous should be formulated primarily in terms of concepts from machine learning. One important way to do this is to distinguish between claims about training processes which produce AGIs, versus claims about AGIs themselves, which I'll call *deployment* claims. I think many foundational concepts in AI safety are clarified by this distinction. In this post I outline some of them, and state new versions of the orthogonality and instrumental convergence theses which take this distinction into account.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/L9HcyaiWBLYe7vXid/distinguishing-claims-about-training-vs-deployment"
    ],
    "tags": [
      "ai",
      "distinctions",
      "orthogonality thesis"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_10196edbeb76c1a8",
    "title": "Counterfactual Planning in AGI Systems",
    "year": 2021,
    "category": "policy_development",
    "description": "Counterfactual planning is a design approach for creating a range of\nsafety mechanisms that can be applied in hypothetical future AI\nsystems which have Artificial General Intelligence.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/7EnZgaepSBwaZXA5y/counterfactual-planning-in-agi-systems"
    ],
    "tags": [
      "ai",
      "corrigibility",
      "counterfactuals",
      "decision theory",
      "embedded agency",
      "intelligence explosion"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3b7d0886fee5a880",
    "title": "[AN #136]: How well will GPT-N perform on downstream tasks?",
    "year": 2021,
    "category": "public_awareness",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/HJMQg8MksHq5ipDpN/an-136-how-well-will-gpt-n-perform-on-downstream-tasks"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_951337533c534cb8",
    "title": "Creating AGI Safety Interlocks",
    "year": 2021,
    "category": "policy_development",
    "description": "*In the third post in this sequence, I will define a counterfactual\nplanning agent which has three safety interlocks.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/BZKLf629NDNfEkZzJ/creating-agi-safety-interlocks"
    ],
    "tags": [
      "ai",
      "corrigibility",
      "counterfactuals",
      "intelligence explosion",
      "world optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_68aa02077de9904f",
    "title": "Timeline of AI safety",
    "year": 2021,
    "category": "policy_development",
    "description": "[Here](https://timelines.issarice.com/wiki/Timeline_of_AI_safety) is a timeline of AI safety that I originally wrote in 2017. The timeline has been updated several times since then, mostly by Vipul Naik.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/SEfjw57Qw8mCzy36n/timeline-of-ai-safety"
    ],
    "tags": [
      "ai",
      "history of rationality"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_bba1de057ab5280d",
    "title": "Epistemology of HCH",
    "year": 2021,
    "category": "public_awareness",
    "description": "Introduction\n============",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/CDSXoC54CjbXQNLGr/epistemology-of-hch"
    ],
    "tags": [
      "ai",
      "epistemology",
      "humans consulting hch",
      "practice & philosophy of science"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_258290fac8bbc041",
    "title": "[AN #137]: Quantifying the benefits of pretraining on downstream task performance",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/85HgXZvNdTdfRJhar/an-137-quantifying-the-benefits-of-pretraining-on-downstream"
    ],
    "tags": [],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_920ed48a2e6919d0",
    "title": "Institute for Assured Autonomy (IAA) newsletter",
    "year": 2021,
    "category": "public_awareness",
    "description": "Just a very brief link to a resource that people may not know about:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/GtEpGu93zsLuZSSZS/institute-for-assured-autonomy-iaa-newsletter"
    ],
    "tags": [],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e4e5428853e4741d",
    "title": "Mapping the Conceptual Territory in AI Existential Safety and Alignment",
    "year": 2021,
    "category": "policy_development",
    "description": "*[(Crossposted from my blog)](https://jbkjr.com/posts/2020/12/mapping_conceptual_territory_AI_safety_alignment/)*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/HEZgGBZTpT4Bov7nH/mapping-the-conceptual-territory-in-ai-existential-safety"
    ],
    "tags": [
      "ai",
      "corrigibility",
      "debate (ai safety technique)",
      "delegation",
      "humans consulting hch",
      "inner alignment",
      "iterated amplification",
      "mesa-optimization",
      "outer alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0c5cf17d3fbc78e2",
    "title": "Tournesol, YouTube and AI Risk",
    "year": 2021,
    "category": "public_awareness",
    "description": "Introduction\n============",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/8q2ySr7yxx7MSR35i/tournesol-youtube-and-ai-risk"
    ],
    "tags": [
      "ai",
      "social media"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_111792882d00a45d",
    "title": "Suggestions of posts on the AF to review",
    "year": 2021,
    "category": "public_awareness",
    "description": "How does one write a good and useful review of a technical post on the Alignment Forum?",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/6hdxTTPWF2iAbXjAb/suggestions-of-posts-on-the-af-to-review"
    ],
    "tags": [
      "ai",
      "ai risk",
      "community",
      "intellectual progress (society-level)",
      "intellectual progress via lesswrong"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_62bf952412bf08df",
    "title": "Disentangling Corrigibility: 2015-2021",
    "year": 2021,
    "category": "policy_development",
    "description": "Since the term *corrigibility*\n[was introduced in\n2015](https://intelligence.org/files/Corrigibility.pdf),\nthere has been a lot of discussion about corrigibility, [on this\nforum](https://www.lesswrong.com/tag/corrigibility) and elsewhere.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/MiYkTp6QYKXdJbchu/disentangling-corrigibility-2015-2021"
    ],
    "tags": [
      "ai",
      "corrigibility",
      "wireheading"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7fb8e12312317cb2",
    "title": "Graphical World Models, Counterfactuals, and Machine Learning Agents",
    "year": 2021,
    "category": "policy_development",
    "description": "*This is the second post in a sequence. For the introduction post, see\n[here](https://www.alignmentforum.org/posts/7EnZgaepSBwaZXA5y/counterfactual-planning-in-agi-systems)*.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/q4j7qbEZRaTAA9Kxf/graphical-world-models-counterfactuals-and-machine-learning"
    ],
    "tags": [
      "ai",
      "counterfactuals",
      "decision theory",
      "myopia"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a0592fdfc8b88536",
    "title": "Safely controlling the AGI agent reward function",
    "year": 2021,
    "category": "policy_development",
    "description": "*In this fifth post in the sequence, I show\nthe construction a counterfactual planning agent with an\ninput terminal that can be used to iteratively improve the agent's\nreward function while it runs.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/o3smzgcH8MR9RcMgZ/safely-controlling-the-agi-agent-reward-function"
    ],
    "tags": [
      "ai",
      "corrigibility",
      "counterfactuals",
      "wireheading"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1f49ed0a6981faa1",
    "title": "[AN #138]: Why AI governance should find problems rather than just solving them",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/XJqtRWnNRLaqJ8RCx/an-138-why-ai-governance-should-find-problems-rather-than"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_131fe90e3960212a",
    "title": "AXRP Episode 4 - Risks from Learned Optimization with Evan Hubinger",
    "year": 2021,
    "category": "policy_development",
    "description": "[Google Podcasts link](https://podcasts.google.com/feed/aHR0cHM6Ly9heHJwb2RjYXN0LmxpYnN5bi5jb20vcnNz/episode/NmY0Y2NjODMtNjc2NS00OWZmLWFmNWUtNWIzOWQxMjZmN2Vm)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/EszCTbovFfpJd5C8N/axrp-episode-4-risks-from-learned-optimization-with-evan"
    ],
    "tags": [
      "ai",
      "audio",
      "axrp",
      "community",
      "inner alignment",
      "interviews",
      "mesa-optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a0e3291c8fd88f1e",
    "title": "Formal Solution to the Inner Alignment Problem",
    "year": 2021,
    "category": "public_awareness",
    "description": "We've written a [paper](https://arxiv.org/abs/2102.08686) on online imitation learning, and our construction allows us to bound the extent to which mesa-optimizers could accomplish anything. This is not to say it will definitely be easy to eliminate mesa-optimizers in practice, but investigations into how to do so could look here as a starting point. The way to avoid outputting predictions that may have been corrupted by a mesa-optimizer is to ask for help when plausible stochastic models disagree about probabilities.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/CnruhwFGQBThvgJiX/formal-solution-to-the-inner-alignment-problem"
    ],
    "tags": [
      "academic papers",
      "ai",
      "ai risk",
      "conservatism (ai)",
      "inner alignment",
      "mesa-optimization",
      "world modeling techniques"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1e7bd578296c3005",
    "title": "Utility Maximization = Description Length Minimization",
    "year": 2021,
    "category": "public_awareness",
    "description": "There's a useful intuitive notion of \"optimization\" as pushing the world into a small set of states, starting from any of a large number of states. Visually:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/voLHQgNncnjjgAPH7/utility-maximization-description-length-minimization"
    ],
    "tags": [
      "ai",
      "information theory",
      "optimization",
      "rationality",
      "utility functions"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_62d2534e6665ff3f",
    "title": "[AN #139]: How the simplicity of reality explains the success of neural nets",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/6kgBAJBGp5Yum8oGj/an-139-how-the-simplicity-of-reality-explains-the-success-of"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_19f8650f4f80be3d",
    "title": "Bootstrapped Alignment",
    "year": 2021,
    "category": "public_awareness",
    "description": "*NB: I doubt any of this is very original. In fact, it's probably right there in the original Friendly AI writings and I've just forgotten where. Nonetheless, I think this is something worth exploring lest we lose sight of it.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/teCsd4Aqg9KDxkaC9/bootstrapped-alignment"
    ],
    "tags": [
      "ai",
      "goodhart's law"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b89d416ccc7e1194",
    "title": "Full-time AGI Safety!",
    "year": 2021,
    "category": "public_awareness",
    "description": "Research publication: Full-time AGI Safety!",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/tnEQMnpyBFK5QBRz3/full-time-agi-safety"
    ],
    "tags": [
      "community"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8327908b83017a0f",
    "title": "Fun with +12 OOMs of Compute",
    "year": 2021,
    "category": "policy_development",
    "description": "*Or: Big Timelines Crux Operationalized*\n========================================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/rzqACeBGycZtqCfaX/fun-with-12-ooms-of-compute"
    ],
    "tags": [
      "ai",
      "ai timelines",
      "world optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f412b1d9fedb9698",
    "title": "How does bee learning compare with machine learning?",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This is a write-up of work I did as an Open Philanthropy intern. However, the conclusions don't necessarily reflect Open Phil's institutional view.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/yW3Tct2iyBMzYhTw7/how-does-bee-learning-compare-with-machine-learning"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_59c9838e4ea8cb15",
    "title": "Book review: \"A Thousand Brains\" by Jeff Hawkins",
    "year": 2021,
    "category": "public_awareness",
    "description": "Jeff Hawkins gets full credit for getting me first interested in the idea that neuroscience might lead to artificial general intelligence--an idea which gradually turned into an all-consuming hobby, and more recently a new job. I'm not alone in finding him inspiring. Andrew Ng claimed [here](https://www.forbes.com/sites/roberthof/2014/08/28/interview-inside-google-brain-founder-andrew-ngs-plans-to-transform-baidu/?sh=102a616640a4) that Hawkins helped convince him, as a young professor, that a simple scaled-up learning algorithm could reach Artificial General Intelligence (AGI). (Ironically, Hawkins scoffs at the deep neural nets built by Ng and others--Hawkins would say: \"Yes yes, a simple scaled-up learning algorithm can reach AGI, but not *that* learning algorithm!!\")",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ixZLTmFfnKRbaStA5/book-review-a-thousand-brains-by-jeff-hawkins"
    ],
    "tags": [
      "ai",
      "inner alignment",
      "neuromorphic ai",
      "neuroscience",
      "outer alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1b10ea9fa5fdb2cd",
    "title": "[AN #140]: Theoretical models that predict scaling laws",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter [**resources here**](http://rohinshah.com/alignment-newsletter/). In particular, you can look through [**this spreadsheet**](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Yt5wAXMc7D2zLpQqx/an-140-theoretical-models-that-predict-scaling-laws"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cbcf1c1ce382a061",
    "title": "The case for aligning narrowly superhuman models",
    "year": 2021,
    "category": "policy_development",
    "description": "*I wrote this post to get people's takes on a type of work that seems exciting to me personally; I'm not speaking for Open Phil as a whole. Institutionally, we are very uncertain whether to prioritize this (and if we do where it should be housed and how our giving should be structured). **We are not seeking grant applications on this topic right now.***",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models"
    ],
    "tags": [
      "ai",
      "gpt",
      "language models",
      "machine learning  (ml)",
      "outer alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a3f8aade404e5b40",
    "title": "MIRI comments on Cotra's \"Case for Aligning Narrowly Superhuman Models\"",
    "year": 2021,
    "category": "public_awareness",
    "description": "Below, I've copied comments left by MIRI researchers Eliezer Yudkowsky and Evan Hubinger on March 1-3 on a draft of Ajeya Cotra's \"[Case for Aligning Narrowly Superhuman Models](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models).\" I've included back-and-forths with Cotra, and interjections by me and Rohin Shah.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/AyfDnnAdjG7HHeD3d/miri-comments-on-cotra-s-case-for-aligning-narrowly"
    ],
    "tags": [
      "ai",
      "gpt",
      "interpretability (ml & ai)",
      "outer alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_302e91321def0cc3",
    "title": "Epistemological Framing for AI Alignment Research",
    "year": 2021,
    "category": "public_awareness",
    "description": "Introduction\n============",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Y4YHTBziAscS5WPN7/epistemological-framing-for-ai-alignment-research"
    ],
    "tags": [
      "ai",
      "ai risk",
      "epistemology",
      "intellectual progress (society-level)",
      "practice & philosophy of science"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2608810fd28e1eb3",
    "title": "CLR's recent work on multi-agent systems",
    "year": 2021,
    "category": "policy_development",
    "description": "Introduction\n============",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/EzoCZjTdWTMgacKGS/clr-s-recent-work-on-multi-agent-systems"
    ],
    "tags": [
      "ai",
      "center on long-term risk (clr)",
      "coordination / cooperation",
      "risks of astronomical suffering (s-risks)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ffcdfe927f0c36bd",
    "title": "Towards a Mechanistic Understanding of Goal-Directedness",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This post is part of the research I have done at MIRI with mentorship and guidance from Evan Hubinger.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/nTiAyxFybZ7jgtWvn/towards-a-mechanistic-understanding-of-goal-directedness"
    ],
    "tags": [
      "ai",
      "goal-directedness"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3bd6328c19085207",
    "title": "AXRP Episode 5 - Infra-Bayesianism with Vanessa Kosoy",
    "year": 2021,
    "category": "policy_development",
    "description": "[Google Podcasts link](https://podcasts.google.com/feed/aHR0cHM6Ly9heHJwb2RjYXN0LmxpYnN5bi5jb20vcnNz/episode/YjExOTA0NmItMDBmZC00Yzc5LTgwMGYtOTRkNDkyMzcwZDk3)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FkMPXiomjGBjMfosg/axrp-episode-5-infra-bayesianism-with-vanessa-kosoy"
    ],
    "tags": [
      "ai",
      "audio",
      "axrp",
      "counterfactual mugging",
      "epistemology",
      "functional decision theory",
      "infra-bayesianism",
      "interviews",
      "newcomb's problem"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4d4fa40bfcd32fda",
    "title": "Extended Picture Theory or Models inside Models inside Models",
    "year": 2021,
    "category": "public_awareness",
    "description": "This post offers a model of meaning (and hence truth) based on extending Wittgenstein's [Picture Theory](https://www.wikiwand.com/en/Picture_theory_of_language). I believe that this model is valuable enough to be worth presenting on its own. I've left justification for another post, but all I'll say for now is that I believe this model to be useful even if it isn't ultimately true. I'll hint at the applications to AI safety, but these won't be fully developed either.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/nvLNjY7aoh2i7JxbB/extended-picture-theory-or-models-inside-models-inside"
    ],
    "tags": [
      "epistemology",
      "philosophy of language"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_fe3009a48eabd41b",
    "title": "[AN #141]: The case for practicing alignment work on GPT-3 and other large models",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/29QmG4bQDFtAzSmpv/an-141-the-case-for-practicing-alignment-work-on-gpt-3-and"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_de2612a2b9c3f6f0",
    "title": "Open Problems with Myopia",
    "year": 2021,
    "category": "policy_development",
    "description": "Thanks to Noa Nabeshima for helpful discussion and comments.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia"
    ],
    "tags": [
      "ai",
      "decision theory",
      "myopia",
      "open problems"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_285797e63a56ead0",
    "title": "TASP Ep 3 - Optimal Policies Tend to Seek Power",
    "year": 2021,
    "category": "public_awareness",
    "description": "Welcome to the Technical AI Safety Podcast, the show where I interview computer scientists about their papers. This month I covered [Optimal Policies Tend to Seek Power](https://arxiv.org/abs/1912.01683v6), which is closely related to [Seeking Power is Often Robustly Instrumental in MDPs](https://www.lesswrong.com/posts/6DuJxY8X45Sco4bS2/seeking-power-is-often-robustly-instrumental-in-mdps) which is a part of the [Reframing Impact](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW) sequence and was recently a part of the [2019 review](https://www.lesswrong.com/posts/kdGSTBj3NA2Go3XaE/2019-review-voting-results).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/eM6SgkXDbFXav4kD4/tasp-ep-3-optimal-policies-tend-to-seek-power"
    ],
    "tags": [
      "ai",
      "instrumental convergence"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_447a3ceee355ae5f",
    "title": "Behavioral Sufficient Statistics for Goal-Directedness",
    "year": 2021,
    "category": "policy_development",
    "description": "*Note: this is a new version -- with a new title -- of my recent post \"A Behavioral Definition of Goal-Directedness\". Most of the formulas are the same, except for the triviality one that deals better with what I wanted; the point of this rewrite is to present the ideas in a perspective that makes sense. I'm not proposing a definition of goal-directedness, but just sufficient statistics on the complete behavior that make a behavioral study of goal-directedness more human-legible.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/jkRFZNAZmWskTdCSt/behavioral-sufficient-statistics-for-goal-directedness"
    ],
    "tags": [
      "ai",
      "ai risk",
      "goal-directedness",
      "kolmogorov complexity"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_be30ea7fcf6abc60",
    "title": "Four Motivations for Learning Normativity",
    "year": 2021,
    "category": "public_awareness",
    "description": "I have been pretty satisfied with my [desiderata for learning normativity](https://www.lesswrong.com/s/Gmc7vtnpyKZRHWdt5/p/2JGu9yxiJkoGdQR4s#Summary_of_Desiderata), but I *haven't* been very satisfied with my explanation of why exactly these desiderata are important. I have a sense that it's not just a grab-bag of cool stuff; something about trying to do all those things at once points at something important.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/oqghwKKifztYWLsea/four-motivations-for-learning-normativity"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_719de0af72a38816",
    "title": "AI x-risk reduction: why I chose academia over industry",
    "year": 2021,
    "category": "policy_development",
    "description": "I've been leaning towards a career in academia for >3 years, and recently got a tenure track role at Cambridge.  This post sketches out my reasoning for preferring academia over industry.  \n  \n**Thoughts on Industry Positions:**  \nA lot of people working on AI x-risk seem to think it's better to be in industry.  I think the main arguments for that side of things are:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/4jFnquoHuoaTqdphu/ai-x-risk-reduction-why-i-chose-academia-over-industry"
    ],
    "tags": [
      "ai",
      "mentorship [topic of]",
      "world optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a2dcc2c90c41e2a6",
    "title": "Comments on \"The Singularity is Nowhere Near\"",
    "year": 2021,
    "category": "public_awareness",
    "description": "I followed a link on Twitter to a fun and informative 2015 blog post by Tim Dettmers:",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/P7P2iG4zvBNANvQFK/comments-on-the-singularity-is-nowhere-near"
    ],
    "tags": [
      "ai",
      "ai timelines",
      "neuroscience"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c67157d535548bff",
    "title": "Intermittent Distillations #1",
    "year": 2021,
    "category": "policy_development",
    "description": "This is my low-budget version of Rohin's [Alignment Newsletter](https://rohinshah.com/alignment-newsletter/).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pqkdsqd6s6w2HtT9g/intermittent-distillations-1"
    ],
    "tags": [
      "ai",
      "rationality"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1eb97fca8419d495",
    "title": "HCH Speculation Post #2A",
    "year": 2021,
    "category": "public_awareness",
    "description": "The first draft of this post started with a point that was clear, cohesive, and wrong. So instead, you get this bunch of rambling that I think should be interesting.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/MnCMkh7hirX8YwT2t/hch-speculation-post-2a"
    ],
    "tags": [
      "ai",
      "humans consulting hch"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_64237bed49ba067a",
    "title": "[AN #142]: The quest to understand a network well enough to reimplement it by hand",
    "year": 2021,
    "category": "public_awareness",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/JGByt8TrxREo4twaw/an-142-the-quest-to-understand-a-network-well-enough-to"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_21c5d85faf4d21a9",
    "title": "Generalizing POWER to multi-agent games",
    "year": 2021,
    "category": "public_awareness",
    "description": "### Acknowledgements:",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/MJc9AqyMWpG3BqfyK/generalizing-power-to-multi-agent-games"
    ],
    "tags": [
      "ai",
      "instrumental convergence",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a3fac625899e3e29",
    "title": "My research methodology",
    "year": 2021,
    "category": "public_awareness",
    "description": "(*Thanks to Ajeya Cotra, Nick Beckstead, and Jared Kaplan for helpful comments on a draft of this post*.)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/EF5M6CmKRd6qZk27Z/my-research-methodology"
    ],
    "tags": [
      "ai",
      "research taste",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b03e1414e703b567",
    "title": "Against evolution as an analogy for how humans will create AGI",
    "year": 2021,
    "category": "policy_development",
    "description": "Background\n==========",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pz7Mxyr7Ac43tWMaC/against-evolution-as-an-analogy-for-how-humans-will-create"
    ],
    "tags": [
      "ai",
      "evolution",
      "inner alignment",
      "neuroscience"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cd5b297a034f448f",
    "title": "[AN #143]: How to make embedded agents that reason probabilistically about their environments",
    "year": 2021,
    "category": "public_awareness",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter [resources here](http://rohinshah.com/alignment-newsletter/). In particular, you can look through [this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/beLgLr6edbZw4koh2/an-143-how-to-make-embedded-agents-that-reason"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_378e12e8266c53e3",
    "title": "My AGI Threat Model: Misaligned Model-Based RL Agent",
    "year": 2021,
    "category": "policy_development",
    "description": "[Rohin Shah advocates](https://www.youtube.com/watch?v=VC_J_skJNMs) a vigorous discussion of \"Threat Models\", i.e. stories for how AGI is developed, what the AGI then looks like, and then what might go catastrophically wrong.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent"
    ],
    "tags": [
      "ai",
      "inner alignment",
      "outer alignment",
      "reinforcement learning",
      "threat models"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7d72316b2ff1cc1e",
    "title": "Inframeasures and Domain Theory",
    "year": 2021,
    "category": "technical_research_breakthrough",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/vrbidMiczaoHBhZGp/inframeasures-and-domain-theory"
    ],
    "tags": [
      "ai",
      "domain theory",
      "infra-bayesianism",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_17f92a83396cdd65",
    "title": "Review of \"Fun with +12 OOMs of Compute\"",
    "year": 2021,
    "category": "public_awareness",
    "description": "Introduction\n============",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/bAAtiG8og7CxH3cXG/review-of-fun-with-12-ooms-of-compute"
    ],
    "tags": [
      "ai",
      "ai risk",
      "ai timelines",
      "intellectual progress (society-level)",
      "intellectual progress via lesswrong"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_bb7a4e5316661111",
    "title": "Transparency Trichotomy",
    "year": 2021,
    "category": "public_awareness",
    "description": "Introduction\n============",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/cgJ447adbMAeoKTSt/transparency-trichotomy"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0cfbe881362f3882",
    "title": "How do we prepare for final crunch time?",
    "year": 2021,
    "category": "technical_research_breakthrough",
    "description": "*[Crossposted from* [*Musings and Rough Drafts*](https://musingsandroughdrafts.wordpress.com/2021/03/12/how-do-we-prepare-for-final-crunch-time-some-initial-thoughts/)*.]*",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/wyYubb3eC5FS365nk/how-do-we-prepare-for-final-crunch-time"
    ],
    "tags": [
      "ai",
      "world optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_70145e3dc6995938",
    "title": "What Multipolar Failure Looks Like, and Robust Agent-Agnostic Processes (RAAPs)",
    "year": 2021,
    "category": "policy_development",
    "description": "***With:** Thomas Krendl Gilbert, who provided comments, interdisciplinary feedback, and input on the RAAP concept.  Thanks also for comments from Ramana Kumar.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic"
    ],
    "tags": [
      "ai",
      "ai risk concrete stories",
      "coordination / cooperation",
      "existential risk",
      "high reliability organizations",
      "moloch",
      "multipolar scenarios",
      "threat models"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2669ff0cde3645b5",
    "title": "[AN #144]: How language models can also be finetuned for non-language tasks",
    "year": 2021,
    "category": "public_awareness",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/by5NkEoSC4gvo9bQ2/an-144-how-language-models-can-also-be-finetuned-for-non"
    ],
    "tags": [
      "ai",
      "language models"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_63ed746ba31f25a5",
    "title": "My take on Michael Littman on \"The HCI of HAI\"",
    "year": 2021,
    "category": "policy_development",
    "description": "*This is independent research. To make it possible for me to continue writing posts like this, please consider* [*supporting me*](https://www.alexflint.io/donate.html)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/wydAtj6FkPDHkdtzS/my-take-on-michael-littman-on-the-hci-of-hai"
    ],
    "tags": [
      "ai",
      "inverse reinforcement learning",
      "reinforcement learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_19273ff27b979f49",
    "title": "The Many Faces of Infra-Beliefs",
    "year": 2021,
    "category": "policy_development",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/GS5P7LLLbSSExb3Sk/the-many-faces-of-infra-beliefs"
    ],
    "tags": [
      "counterfactuals",
      "decision theory",
      "infra-bayesianism"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2f58baff05742d1a",
    "title": "Testing The Natural Abstraction Hypothesis: Project Intro",
    "year": 2021,
    "category": "public_awareness",
    "description": "The [natural abstraction hypothesis](https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default#Unsupervised__Natural_Abstractions) says that",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro"
    ],
    "tags": [
      "natural abstraction",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a1523a7bf7a6b299",
    "title": "Alignment Newsletter Three Year Retrospective",
    "year": 2021,
    "category": "public_awareness",
    "description": "It has now been just shy of three years since the first Alignment Newsletter was published. I figure it's time for an update to the [one-year retrospective](https://www.alignmentforum.org/posts/3onCb5ph3ywLQZMX2/alignment-newsletter-one-year-retrospective), and another very short [survey](https://docs.google.com/forms/d/e/1FAIpQLScRyiBrZoR_AxcHUlbGbp62dZUx36UuF_zAxO_ky948d-MwSw/viewform?usp=sf_link). **Please take the** [**survey**](https://docs.google.com/forms/d/e/1FAIpQLScRyiBrZoR_AxcHUlbGbp62dZUx36UuF_zAxO_ky948d-MwSw/viewform?usp=sf_link)**!** The mandatory questions take **just 2 minutes**!",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/L7yHdqRiHKd3FhQ7B/alignment-newsletter-three-year-retrospective"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cd0296d707ea9c44",
    "title": "Which counterfactuals should an AI follow?",
    "year": 2021,
    "category": "public_awareness",
    "description": "*Thanks to Rebecca Gorman for her help with this post.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/j7kyt6sHEjukRND8B/which-counterfactuals-should-an-ai-follow"
    ],
    "tags": [
      "ai",
      "rationality"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d651603cae5fc034",
    "title": "Another (outer) alignment failure story",
    "year": 2021,
    "category": "policy_development",
    "description": "Research publication: Another (outer) alignment failure story",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story"
    ],
    "tags": [
      "ai",
      "ai risk",
      "outer alignment",
      "threat models"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_01b45b5d40a83c3a",
    "title": "Solving the whole AGI control problem, version 0.0001",
    "year": 2021,
    "category": "policy_development",
    "description": "![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d154e5cdd701b9d73a8784d3456611535472f5191ee3a023.jpg)The bridge to AGI control. Not *quiiiiite* ready for rush-hour traffic... Mind the gaps!! [(image source)](https://www.tendersontime.com/blogdetails/construction-aarapathai-bridge-6656/)(Update: Note that most of the things I wrote in this post are superseded (or at least explained better) in my later [\"Intro to Brain-Like-AGI Safety\" post series](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8).)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Gfw7JMdKirxeSPiAk/solving-the-whole-agi-control-problem-version-0-0001"
    ],
    "tags": [
      "ai",
      "ai success models",
      "conservatism (ai)",
      "corrigibility",
      "interpretability (ml & ai)",
      "tool ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_037e009a9ffa4811",
    "title": "If you don't design for extrapolation, you'll extrapolate poorly - possibly fatally",
    "year": 2021,
    "category": "policy_development",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/4wa9XGnJHB3apPqoq/if-you-don-t-design-for-extrapolation-you-ll-extrapolate"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c38225b47b19c451",
    "title": "AXRP Episode 6 - Debate and Imitative Generalization with Beth Barnes",
    "year": 2021,
    "category": "policy_development",
    "description": "[Google Podcasts link](https://podcasts.google.com/feed/aHR0cHM6Ly9heHJwb2RjYXN0LmxpYnN5bi5jb20vcnNz/episode/NGY3ZTBiMzgtYWM5Ny00NDZmLTgxZmQtOTc1M2VhMGVkMzQz)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/behyPgMWFhXpKi73P/axrp-episode-6-debate-and-imitative-generalization-with-beth"
    ],
    "tags": [
      "ai",
      "audio",
      "axrp",
      "debate (ai safety technique)",
      "interviews"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_12b0c58fb7d78405",
    "title": "[AN #145]: Our three year anniversary!",
    "year": 2021,
    "category": "public_awareness",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/bER8yqrmHatrES9nR/an-145-our-three-year-anniversary"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ad6fa9b8bcc96bf5",
    "title": "My Current Take on Counterfactuals",
    "year": 2021,
    "category": "policy_development",
    "description": "*[Epistemic status: somewhat lower confidence based on the fact that I haven't worked out a detailed theory along the lines I've suggested, yet.]*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/yXfka98pZXAmXiyDp/my-current-take-on-counterfactuals"
    ],
    "tags": [
      "ai",
      "counterfactuals",
      "decision theory",
      "rationality"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8ffd1db048dfffd9",
    "title": "Opinions on Interpretable Machine Learning and 70 Summaries of Recent Papers",
    "year": 2021,
    "category": "technical_research_breakthrough",
    "description": "**[Peter Hase](https://peterbhase.github.io/)**",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/GEPX7jgLMB8vR2qaK/opinions-on-interpretable-machine-learning-and-70-summaries"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)",
      "machine learning  (ml)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_05c78390d701e28c",
    "title": "Intermittent Distillations #2",
    "year": 2021,
    "category": "policy_development",
    "description": "Servant of Many Masters: Shifting priorities in Pareto-optimal sequential decision-making (Andrew Critch and Stuart Russell)\n============================================================================================================================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Rjrq6xPoavgC4JznB/intermittent-distillations-2"
    ],
    "tags": [
      "ai",
      "rationality"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5a74ff3b7db0e1b5",
    "title": "[AN #146]: Plausible stories of how we might fail to avert an existential catastrophe",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/AwxBGFy59DYDk4ooe/an-146-plausible-stories-of-how-we-might-fail-to-avert-an"
    ],
    "tags": [
      "autonomous weapons"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cb8adee160ea31d4",
    "title": "Computing Natural Abstractions: Linear Approximation",
    "year": 2021,
    "category": "public_awareness",
    "description": "*Background:* [*Testing The Natural Abstraction Hypothesis*](https://www.lesswrong.com/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro)",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/f6oWbqxEwktfPrKJw/computing-natural-abstractions-linear-approximation"
    ],
    "tags": [
      "natural abstraction",
      "rationality"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e3d979d3f6f27eb1",
    "title": "Gradations of Inner Alignment Obstacles",
    "year": 2021,
    "category": "policy_development",
    "description": "The existing definitions of deception, inner optimizer, and some other terms tend to strike me as \"stronger than necessary\" depending on the context. If weaker definitions are similarly problematic, this means we need stronger methods to prevent them! I illustrate this and make some related (probably contentious) claims.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/wpbpvjZCK3JhzpR2D/gradations-of-inner-alignment-obstacles"
    ],
    "tags": [
      "ai",
      "inner alignment",
      "lottery ticket hypothesis",
      "mesa-optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_997a47b6b3516d3f",
    "title": "[AN #147]: An overview of the interpretability landscape",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/zFwie6AoPyqGmMSsc/an-147-an-overview-of-the-interpretability-landscape"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a86d3a08178c9e4d",
    "title": "Probability theory and logical induction as lenses",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This is independent research. To make it possible for me to continue writing posts like this, please consider* [*supporting me*](https://www.alexflint.io/donate.html)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Zd5Bsra7ar2pa3bwS/probability-theory-and-logical-induction-as-lenses"
    ],
    "tags": [
      "ai",
      "rationality"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a3f748da0f448227",
    "title": "Naturalism and AI alignment",
    "year": 2021,
    "category": "public_awareness",
    "description": "Edit 1: Allergic to [naturalism](https://en.wikipedia.org/wiki/Ethical_naturalism) and other realist positions? You can still benefit from reading this, by considering [ideal observer theory](https://en.wikipedia.org/wiki/Ideal_observer_theory) instead. I am claiming that something like an ideal-observer AI can be built, and that there is a non-trivial chance such an agent becomes aligned (after being given enough knowledge about the physical world).  \nEdit 2: The best objection I've received so far is \"I am sceptical of this approach\"; [others](https://casparoesterheld.com/2018/08/06/moral-realism-and-ai-alignment/) find it at least plausible. If you know why this approach does not work, please leave a comment. If you think the orthogonality thesis renders all the realism-inspired approaches to alignment useless, please explain why/how.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Jo2LWuuGEGHHfGZCM/naturalism-and-ai-alignment"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5d819f28ba700267",
    "title": "FAQ: Advice for AI Alignment Researchers",
    "year": 2021,
    "category": "public_awareness",
    "description": "To quote Andrew Critch:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/kdwk5aHNjM53PZFKL/faq-advice-for-ai-alignment-researchers"
    ],
    "tags": [
      "ai",
      "careers"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_338edf175eb52d4c",
    "title": "Announcing the Alignment Research Center",
    "year": 2021,
    "category": "public_awareness",
    "description": "(Cross-post from [ai-alignment.com](https://ai-alignment.com/announcing-the-alignment-research-center-a9b07f77431b))",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/3ejHFgQihLG4L6WQf/announcing-the-alignment-research-center"
    ],
    "tags": [
      "ai",
      "organization updates"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5f0a5b688c51f625",
    "title": "Agents Over Cartesian World Models",
    "year": 2021,
    "category": "public_awareness",
    "description": "Thanks to Adam Shimi, Alex Turner, Noa Nabeshima, Neel Nanda, Sydney Von Arx, Jack Ryan, and Sidney Hough for helpful discussion and comments.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/LBNjeGaJZw7QdybMw/agents-over-cartesian-world-models"
    ],
    "tags": [
      "agency",
      "ai",
      "boundaries / membranes [technical]"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_75f27ca66fd73e17",
    "title": "[AN #148]: Analyzing generalization across more axes than just accuracy or loss",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/H79dxa7XXMBhwqZLm/an-148-analyzing-generalization-across-more-axes-than-just"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_6910748a761d904a",
    "title": "AMA: Paul Christiano, alignment researcher",
    "year": 2021,
    "category": "public_awareness",
    "description": "I'll be running an Ask Me Anything on this post from Friday (April 30) to Saturday (May 1).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/7qhtuQLCCvmwCPfXK/ama-paul-christiano-alignment-researcher"
    ],
    "tags": [
      "ai",
      "ama"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_91fa65f19202e5c6",
    "title": "Draft report on existential risk from power-seeking AI",
    "year": 2021,
    "category": "public_awareness",
    "description": "I've written a draft report evaluating a version of the overall case for existential risk from misaligned AI, and taking an initial stab at quantifying the risk from this version of the threat. I've made the draft viewable as a public google doc [here](https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit#) (Edit: arXiv version [here](https://arxiv.org/abs/2206.13353), video presentation [here](https://harvard.zoom.us/rec/play/kZc6rR2Ynx6z_J4rmmJUaq12-AcVDZLlY9eniU5ZKioodJm_yJORyy9UiLI8zFRuQvfLDAYO3j5TPciD.WSBvvCvXIto0mF4w?continueMode=true&_x_zm_rtaid=4hUT6sI8R4GPtHZS-tZBXg.1651957013327.8c9781863a967470f7270aa15ef4c1f4&_x_zm_rhtaid=917), human-narrated audio version [here](https://joecarlsmithaudio.buzzsprout.com/2034731/12113681-is-power-seeking-ai-an-existential-risk)). Feedback would be welcome.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/HduCjmXTBD4xYTegv/draft-report-on-existential-risk-from-power-seeking-ai"
    ],
    "tags": [
      "ai",
      "instrumental convergence"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f827f0c2658b5ad8",
    "title": "Low-stakes alignment",
    "year": 2021,
    "category": "public_awareness",
    "description": "Right now I'm working on finding a good objective to optimize with ML, rather than trying to make sure our models are robustly optimizing that objective. (This is roughly \"[outer alignment](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/pL56xPoniLvtMDQ4J).\")",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/TPan9sQFuPP6jgEJo/low-stakes-alignment"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e50220558ade0cb8",
    "title": "[Weekly Event] Alignment Researcher Coffee Time (in Walled Garden)",
    "year": 2021,
    "category": "public_awareness",
    "description": "I'm organizing a weekly one hour coffee time for alignment researchers to talk about their research and what they're interested about, every Monday starting tomorrow. The time (9pm CEST, see [here](https://everytimezone.com/s/0d39d311) for your timezone) was decided after I polled a number of people, to allow timezones from PT to IDT (sorry for people in Asia and Australia).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/2Ps9easGbqdMP6win/weekly-event-alignment-researcher-coffee-time-in-walled"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_64835f7e77795dd8",
    "title": "Parsing Abram on Gradations of Inner Alignment Obstacles",
    "year": 2021,
    "category": "policy_development",
    "description": "*This is independent research. To make further posts like this possible, please consider* [*supporting me*](https://www.alexflint.io/donate.html)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pTm6aEvmepJEA5cuK/parsing-abram-on-gradations-of-inner-alignment-obstacles"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e533c5574b1c46a1",
    "title": "Mundane solutions to exotic problems",
    "year": 2021,
    "category": "policy_development",
    "description": "I'm looking for alignment techniques that are [indefinitely scalable](https://ai-alignment.com/directions-and-desiderata-for-ai-control-b60fca0da8f4) and that [work in any situation we can dream up](https://ai-alignment.com/my-research-methodology-b94f2751cb2c). That means I spend time thinking about \"exotic\" problems -- like AI systems reasoning about their own training process or about humanity's far future.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/d5m3G3ov5phZu7FX3/mundane-solutions-to-exotic-problems"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ca624badb16a9112",
    "title": "[AN #149]: The newsletter's editorial policy",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Yj9hW27sMJ4Hx4Bd4/an-149-the-newsletter-s-editorial-policy"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_50acd1416eb21320",
    "title": "Parsing Chris Mingard on Neural Networks",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This is independent research. To make further posts like this possible, please consider* [*supporting me*](https://www.alexflint.io/donate.html)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/5p4ynEJQ8nXxp2sxC/parsing-chris-mingard-on-neural-networks"
    ],
    "tags": [
      "ai",
      "kolmogorov complexity"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_67626b44cd21b4f2",
    "title": "Less Realistic Tales of Doom",
    "year": 2021,
    "category": "policy_development",
    "description": "Realistic tales of doom must weave together many political, technical, and economic considerations into a single story. Such tales provide concrete projections but omit discussion of less probable paths to doom. To rectify this, here are some concrete, less realistic tales of doom; consider them fables, not stories.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ZRTr6rEcpYtfMTDBs/less-realistic-tales-of-doom"
    ],
    "tags": [
      "ai",
      "ai risk",
      "threat models"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_49cb28f225610477",
    "title": "Pre-Training + Fine-Tuning Favors Deception",
    "year": 2021,
    "category": "public_awareness",
    "description": "Thanks to Evan Hubinger for helpful comments and discussion.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/rZTjsKy4Jvu6krWJt/pre-training-fine-tuning-favors-deception"
    ],
    "tags": [
      "ai",
      "inner alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4aeabf0a7c1e5d40",
    "title": "[Event] Weekly Alignment Research Coffee Time (05/10)",
    "year": 2021,
    "category": "public_awareness",
    "description": "Just like every Monday now, researchers in AI Alignment are invited for a coffee time, to talk about their research and what they're into.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ErXseAhtiymqRdCq9/event-weekly-alignment-research-coffee-time-05-10"
    ],
    "tags": [
      "ai",
      "community"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8248c5d43a00dc7d",
    "title": "Yampolskiy on AI Risk Skepticism",
    "year": 2021,
    "category": "policy_development",
    "description": "Roman Yampolskiy posted a preprint for \"AI Risk Skepticism\". Here's the abstract:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/D3PnBxkj5jkKPm6jr/yampolskiy-on-ai-risk-skepticism"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_266a0756fe09b5ca",
    "title": "[AN #150]: The subtypes of Cooperative AI research",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/3zmKzbMPjPvEcZfkn/an-150-the-subtypes-of-cooperative-ai-research"
    ],
    "tags": [
      "ai",
      "coordination / cooperation"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_01ccfc4f6c67010b",
    "title": "Formal Inner Alignment, Prospectus",
    "year": 2021,
    "category": "policy_development",
    "description": "Most of the work on inner alignment so far has been informal or semi-formal (with the notable exception of a little work on minimal circuits). I feel this has resulted in some misconceptions about the problem. I want to write up a large document clearly defining the formal problem and detailing some formal directions for research. Here, I outline my intentions, inviting the reader to provide feedback and ***point me to any formal work or areas of potential formal work*** which should be covered in such a document. (Feel free to do that last one without reading further, if you are time-constrained!)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/a7jnbtoKFyvu5qfkd/formal-inner-alignment-prospectus"
    ],
    "tags": [
      "ai",
      "inner alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8a00a82fce89e224",
    "title": "Understanding the Lottery Ticket Hypothesis",
    "year": 2021,
    "category": "public_awareness",
    "description": "*Financial status: This is independent research. I welcome* [*financial support*](https://www.alexflint.io/donate.html) *to make further posts like this possible.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/dpzLqQQSs7XRacEfK/understanding-the-lottery-ticket-hypothesis"
    ],
    "tags": [
      "ai",
      "lottery ticket hypothesis"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b106920c069ddac9",
    "title": "AXRP Episode 7 - Side Effects with Victoria Krakovna",
    "year": 2021,
    "category": "policy_development",
    "description": "[Google Podcasts link](https://podcasts.google.com/feed/aHR0cHM6Ly9heHJwb2RjYXN0LmxpYnN5bi5jb20vcnNz/episode/ZTQ1NjdlMWEtMmQzNC00Y2FlLThlZDMtYzgwZWJmNzRjMWFj)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/C9vj5ZX3KsgFfwXAN/axrp-episode-7-side-effects-with-victoria-krakovna"
    ],
    "tags": [
      "ai",
      "audio",
      "axrp",
      "impact regularization",
      "interviews"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_bc99107b3afbf2e9",
    "title": "Intermittent Distillations #3",
    "year": 2021,
    "category": "public_awareness",
    "description": "Mundane solutions to exotic problems (Paul Christiano)\n======================================================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/jnHxfXgyQj3ALsD5a/intermittent-distillations-3"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9930ffc1998a31dc",
    "title": "[Event] Weekly Alignment Research Coffee Time (05/17)",
    "year": 2021,
    "category": "public_awareness",
    "description": "Just like every Monday now, researchers in AI Alignment are invited for a coffee time, to talk about their research and what they're into.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/gLRphsnSHefpcqZoF/event-weekly-alignment-research-coffee-time-05-17"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c9fd1fee4ae3197b",
    "title": "Knowledge Neurons in Pretrained Transformers",
    "year": 2021,
    "category": "public_awareness",
    "description": "This is a link post for the Dai et al. paper \"[Knowledge Neurons in Pretrained Transformers](https://arxiv.org/abs/2104.08696)\" that was published on the arXiv last month. I think this paper is probably the most exciting machine learning paper I've read so far this year and I'd highly recommend others check it out as well. *Edit: Maybe not; I think [Paul's skeptical take here is quite reasonable](https://www.lesswrong.com/posts/LdoKzGom7gPLqEZyQ/knowledge-neurons-in-pretrained-transformers?commentId=tqgQ9eyqWvk28nBG5#comments).*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/LdoKzGom7gPLqEZyQ/knowledge-neurons-in-pretrained-transformers"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e5cddb9bd54f2a4c",
    "title": "[AN #151]: How sparsity in the final layer makes a neural net debuggable",
    "year": 2021,
    "category": "public_awareness",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/t2yeWvpGvzQ9sFrWc/an-151-how-sparsity-in-the-final-layer-makes-a-neural-net"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1e24b9f6f5106c07",
    "title": "AI Safety Research Project Ideas",
    "year": 2021,
    "category": "public_awareness",
    "description": "This post contains project ideas in AI Safety from Owain Evans and Stuart Armstrong (researchers at [FHI](https://www.fhi.ox.ac.uk/research/research-areas/#aisafety_tab)). Projects are aimed at students, postdocs or summer research fellows who would be interested in collaborating on them for 2-6 months with Owain or Stuart. We are happy to explore possible funding options for the duration of the project. If you are interested in discussing mentorship or academic collaborations please get in touch -- details are at the bottom of this post. The deadline is EOD 20th of June but we encourage people to apply early.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/f69LK7CndhSNA7oPn/ai-safety-research-project-ideas"
    ],
    "tags": [
      "ai",
      "ai risk",
      "practical"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2fe3a6d4a7961971",
    "title": "[Event] Weekly Alignment Research Coffee Time (05/24)",
    "year": 2021,
    "category": "public_awareness",
    "description": "Just like every Monday now, researchers in AI Alignment are invited for a coffee time, to talk about their research and what they're into.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/KGJC6HLG5hcFR7pM4/event-weekly-alignment-research-coffee-time-05-24"
    ],
    "tags": [
      "community"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5f6dd2d01a1576dc",
    "title": "Finite Factored Sets",
    "year": 2021,
    "category": "public_awareness",
    "description": "This is the edited transcript of a talk introducing finite factored sets. For most readers, it will probably be the best starting point for learning about factored sets.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/N5Jm6Nj4HkNKySA5Z/finite-factored-sets"
    ],
    "tags": [
      "abstraction",
      "causality",
      "finite factored sets"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_eba03feafa513e2b",
    "title": "Problems facing a correspondence theory of knowledge",
    "year": 2021,
    "category": "policy_development",
    "description": "*Financial status: This is independent research. I welcome* [*financial support*](https://www.alexflint.io/donate.html) *to make further posts like this possible.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/YdxG2D3bvG5YsuHpG/problems-facing-a-correspondence-theory-of-knowledge"
    ],
    "tags": [
      "rationality",
      "truth, semantics, & meaning"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e4ac3117db9b3bd1",
    "title": "Decoupling deliberation from competition",
    "year": 2021,
    "category": "public_awareness",
    "description": "I view [intent alignment](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6) as one step towards a broader goal of decoupling deliberation from competition*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/7jSvfeyh8ogu8GcE6/decoupling-deliberation-from-competition"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cc128d58b3675fff",
    "title": "MDP models are determined by the agent architecture and the environmental dynamics",
    "year": 2021,
    "category": "policy_development",
    "description": "[*Seeking Power is Often Robustly Instrumental in MDPs*](https://www.lesswrong.com/posts/6DuJxY8X45Sco4bS2/seeking-power-is-often-robustly-instrumental-in-mdps) relates the structure of the agent's environment (the 'Markov decision process (MDP) model') to the tendencies of optimal policies for different reward functions in that environment ('instrumental convergence'). The results tell us what optimal decision-making 'tends to look like' in a given environment structure, formalizing reasoning that says e.g. that most agents stay alive because that helps them achieve their goals.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/XkXL96H6GknCbT5QH/mdp-models-are-determined-by-the-agent-architecture-and-the"
    ],
    "tags": [
      "ai",
      "instrumental convergence"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_218909fa95bcc10d",
    "title": "List of good AI safety project ideas?",
    "year": 2021,
    "category": "public_awareness",
    "description": "Can we compile a list of good project ideas related to AI safety that people can work on? There are occasions at work when I have the opportunity to propose interesting project ideas for potential funding, and it would be really useful if there was somewhere I could look for projects that people here would really like someone to work on, even if they themselves don't have the time or resources to do so. I also keep meeting people who are searching for useful alignment-related projects they can work on for school, work, or as personal projects, and I think a list of project ideas might be helpful for them as well.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/stdfRDMF3sFpSsGeG/list-of-good-ai-safety-project-ideas"
    ],
    "tags": [
      "ai",
      "collections and resources"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e2ea474691a9f413",
    "title": "AXRP Episode 7.5 - Forecasting Transformative AI from Biological Anchors with Ajeya Cotra",
    "year": 2021,
    "category": "public_awareness",
    "description": "*Audio unavailable for this episode.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/CuDYhLLXq6FuHvGZc/axrp-episode-7-5-forecasting-transformative-ai-from"
    ],
    "tags": [
      "ai",
      "axrp",
      "forecasting & prediction",
      "inside/outside view",
      "interviews"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_52c4926dd7df7b88",
    "title": "Teaching ML to answer questions honestly instead of predicting human answers",
    "year": 2021,
    "category": "policy_development",
    "description": "(*Note: very much work in progress, unless you want to follow along with my research you'll probably want to wait for an improved/simplified/clarified algorithm*.)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/QqwZ7cwEA2cxFEAun/teaching-ml-to-answer-questions-honestly-instead-of"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cdc88eaf2b5b3f56",
    "title": "[Event] Weekly Alignment Research Coffee Time",
    "year": 2021,
    "category": "public_awareness",
    "description": "Just like every Monday now, researchers in AI Alignment are invited for a coffee time, to talk about their research and what they're into.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/cysgh8zpmvt56f6Qw/event-weekly-alignment-research-coffee-time"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3a4b57ffc4de8239",
    "title": "\"Existential risk from AI\" survey results",
    "year": 2021,
    "category": "policy_development",
    "description": "I sent a two-question survey to ~117 people working on long-term AI risk, asking about the level of existential risk from \"humanity not doing enough technical AI safety research\" and from \"AI systems not doing/optimizing what the people deploying them wanted/intended\".",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results"
    ],
    "tags": [
      "ai",
      "surveys"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_6fdded65b2c16fd3",
    "title": "Thoughts on the Alignment Implications of Scaling Language Models",
    "year": 2021,
    "category": "public_awareness",
    "description": "[Epistemic status: slightly rambly, mostly personal intuition and opinion that will probably be experimentally proven wrong within a year considering how fast stuff moves in this field]",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/EmxfgPGvaKqhttPM8/thoughts-on-the-alignment-implications-of-scaling-language"
    ],
    "tags": [
      "ai",
      "gpt",
      "language models",
      "machine learning  (ml)",
      "outer alignment",
      "scaling laws",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_07ece067b966ff73",
    "title": "Rogue AGI Embodies Valuable Intellectual Property",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This post was written by Mark Xu based on interviews with Carl Shulman. It was paid for by Open Philanthropy but is not representative of their views.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FM49gHBrs5GTx7wFf/rogue-agi-embodies-valuable-intellectual-property"
    ],
    "tags": [
      "ai",
      "ai risk",
      "economic consequences of agi",
      "threat models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a78aa6d477b83b38",
    "title": "Review of \"Learning Normativity: A Research Agenda\"",
    "year": 2021,
    "category": "policy_development",
    "description": "Introduction\n============",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ykvw6sMQD7JXK5cdJ/review-of-learning-normativity-a-research-agenda"
    ],
    "tags": [
      "ai",
      "intellectual progress via lesswrong"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2665774613c80b6d",
    "title": "Some AI Governance Research Ideas",
    "year": 2021,
    "category": "policy_development",
    "description": "*Compiled by Markus Anderljung and Alexis Carlier*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/RBsTG5F2LqsMaqdzP/some-ai-governance-research-ideas"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a284274dcb72d0c5",
    "title": "Speculations against GPT-n writing alignment papers",
    "year": 2021,
    "category": "public_awareness",
    "description": "Some alignment proposals I have heard discussed involve getting GPT-n to write alignment papers, and using GPT-n as a transparency tool on itself. This is my speculations on ways it could go wrong.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/wkhfytDQvfx3Jeie9/speculations-against-gpt-n-writing-alignment-papers"
    ],
    "tags": [
      "ai",
      "gpt",
      "interpretability (ml & ai)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_537cc19486d5a64f",
    "title": "Game-theoretic Alignment in terms of Attainable Utility",
    "year": 2021,
    "category": "technical_research_breakthrough",
    "description": "### Acknowledgements:",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/buaGz3aiqCotzjKie/game-theoretic-alignment-in-terms-of-attainable-utility"
    ],
    "tags": [
      "ai",
      "game theory"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_41f0d57f568dad73",
    "title": "Big picture of phasic dopamine",
    "year": 2021,
    "category": "policy_development",
    "description": "*(**Update Jan. 2023:** This article has important errors. I am leaving it as-is in case people want to see the historical trail of me gradually making progress. Most of the content here is revised and better-explained in my later post series* [*Intro to Brain-Like AGI Safety*](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8)*, see especially Posts* [*5*](https://www.alignmentforum.org/posts/F759WQ8iKjqBncDki/intro-to-brain-like-agi-safety-5-the-long-term-predictor-and) *and* [*6*](https://www.alignmentforum.org/posts/qNZSBqLEh4qLRqgWW/intro-to-brain-like-agi-safety-6-big-picture-of-motivation)*. Anyway, I still think that this post has lots of big kernels of truth, and that my updates since writing it have mostly been centered around how that big picture is implemented in neuroanatomy.)*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/jrewt3rLFiKWrKuyZ/big-picture-of-phasic-dopamine"
    ],
    "tags": [
      "ai",
      "neuroscience",
      "reinforcement learning",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_827fce9b938f21e7",
    "title": "Supplement to \"Big picture of phasic dopamine\"",
    "year": 2021,
    "category": "public_awareness",
    "description": "This is \"Supplementary Information\" to my post [\"Big Picture of Phasic Dopamine\"](https://www.lesswrong.com/posts/jrewt3rLFiKWrKuyZ/big-picture-of-phasic-dopamine).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/BwaxYiJ3ZmXHLoZJ6/supplement-to-big-picture-of-phasic-dopamine"
    ],
    "tags": [
      "ai",
      "neuroscience",
      "reinforcement learning",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_43105989664e6df3",
    "title": "Survey on AI existential risk scenarios",
    "year": 2021,
    "category": "policy_development",
    "description": "*Cross-posted to the [EA forum](https://forum.effectivealtruism.org/posts/2tumunFmjBuXdfF2F/survey-on-ai-existential-risk-scenarios-1)*.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/WiXePTj7KeEycbiwK/survey-on-ai-existential-risk-scenarios"
    ],
    "tags": [
      "ai",
      "ai risk",
      "ai safety camp",
      "surveys",
      "threat models"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7a5f09a04b10d72a",
    "title": "Evan Hubinger on Homogeneity in Takeoff Speeds, Learned Optimization and Interpretability",
    "year": 2021,
    "category": "policy_development",
    "description": "Below is the transcript of my chat with Evan Hubinger, interviewed in the context of [the inside view](https://anchor.fm/inside-view) a podcast about AI Alignment. The links below will redirect you to corresponding timestamps in the [youtube video](https://www.youtube.com/channel/UCb9F9_uV24PGj6x63PhXEVw).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/NFfZsWrzALPdw54NL/evan-hubinger-on-homogeneity-in-takeoff-speeds-learned"
    ],
    "tags": [
      "ai",
      "myopia"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8aa8c62ff4f837ba",
    "title": "AXRP Episode 8 - Assistance Games with Dylan Hadfield-Menell",
    "year": 2021,
    "category": "policy_development",
    "description": "[Google Podcasts link](https://podcasts.google.com/feed/aHR0cHM6Ly9heHJwb2RjYXN0LmxpYnN5bi5jb20vcnNz/episode/NTU4OTk4MTctNGY4NS00OWRkLThkYjMtMzdlMmVmNGJmZjZi)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/fzFyCJ6gB9kBL9RqW/axrp-episode-8-assistance-games-with-dylan-hadfield-menell"
    ],
    "tags": [
      "ai",
      "audio",
      "axrp",
      "center for human-compatible ai (chai)",
      "corrigibility",
      "interviews",
      "inverse reinforcement learning"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_062b7782e10c89cd",
    "title": "A naive alignment strategy and optimism about generalization",
    "year": 2021,
    "category": "policy_development",
    "description": "(*Context: my* [*last post*](https://ai-alignment.com/a-problem-and-three-ideas-800b42a14f66) *was trying to patch a certain naive strategy for AI alignment, but I didn't articulate clearly what the naive strategy is. I think it's worth explaining the naive strategy in its own post, even though it's not a novel idea*.)",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/QvtHSsZLFCAHmzes7/a-naive-alignment-strategy-and-optimism-about-generalization"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c09f4ed72470ae64",
    "title": "Answering questions honestly given world-model mismatches",
    "year": 2021,
    "category": "policy_development",
    "description": "(*This post is superseded by our writeup on* [*Eliciting Latent Knowledge*](https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge)*.)*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/SRJ5J9Tnyq7bySxbt/answering-questions-honestly-given-world-model-mismatches"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4a95f2597dbc790f",
    "title": "Avoiding the instrumental policy by hiding information about humans",
    "year": 2021,
    "category": "policy_development",
    "description": "I've been thinking about situations where alignment fails because \"predict what a human would say\" (or more generally \"game the loss function,\" what I call the instrumental policy) is easier to learn than \"answer questions honestly\" ([overview](https://www.alignmentforum.org/posts/QvtHSsZLFCAHmzes7/a-naive-alignment-strategy-and-optimism-about-generalization)).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/roZvoF6tRH6xYtHMF/avoiding-the-instrumental-policy-by-hiding-information-about"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_29e7ef59d181e810",
    "title": "Looking Deeper at Deconfusion",
    "year": 2021,
    "category": "public_awareness",
    "description": "Introduction\n============",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/5Nz4PJgvLCpJd6YTA/looking-deeper-at-deconfusion"
    ],
    "tags": [
      "ai",
      "deconfusion",
      "epistemology",
      "intellectual progress (society-level)",
      "rationality"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3be33ac925d8661c",
    "title": "Open problem: how can we quantify player alignment in 2x2 normal-form games?",
    "year": 2021,
    "category": "public_awareness",
    "description": "In my experience, [constant-sum games](http://www.cs.umd.edu/~hajiagha/474GT13/Lecture09102013.pdf) are considered to provide \"maximally unaligned\" incentives, and [common-payoff games](http://www.cs.umd.edu/~hajiagha/474GT13/Lecture09102013.pdf) are considered to provide \"maximally aligned\" incentives. How do we quantitatively interpolate between these two extremes? That is, given an arbitrary 2?2.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align...",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ghyw76DfRyiiMxo3t/open-problem-how-can-we-quantify-player-alignment-in-2x2"
    ],
    "tags": [
      "ai",
      "deconfusion",
      "game theory",
      "open problems"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e3c4ed5bcc26db37",
    "title": "Reward Is Not Enough",
    "year": 2021,
    "category": "public_awareness",
    "description": "Three case studies\n==================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/frApEhpyKQAcFvbXJ/reward-is-not-enough"
    ],
    "tags": [
      "ai",
      "corrigibility",
      "neuroscience",
      "reinforcement learning",
      "subagents"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3236e524a3170acd",
    "title": "Insufficient Values",
    "year": 2021,
    "category": "public_awareness",
    "description": "I'm [Jose](https://www.lesswrong.com/posts/hKNJSiyzB5jDKFytn/?commentId=uTZqYft4LJpCE36je).  I realized recently I wasn't taking existential risk seriously enough, and in April, a year after I first applied, I started running a MIRIx group in my college.  I'll write summaries of the sessions that I thought were worth sharing.  Most of the members are very new to FAI, so this will partly be an incentive to push upward and partly my own review process.  Hopefully some of this will be helpful to others.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Pd53Mip7Aa3TsdA7E/insufficient-values"
    ],
    "tags": [
      "ai",
      "coherent extrapolated volition",
      "inner alignment",
      "outer alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a6c5dc4af96ba213",
    "title": "[AN #152]: How we've overestimated few-shot learning capabilities",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/iCzGrppxQAJhRXhmD/an-152-how-we-ve-overestimated-few-shot-learning"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8b64ed70b950fddf",
    "title": "Pros and cons of working on near-term technical AI safety and assurance",
    "year": 2021,
    "category": "policy_development",
    "description": "*Cross-posted from the EA Forum:* [*https://forum.effectivealtruism.org/posts/Ry4C4CKZvuRG7ztxY/pros-and-cons-of-working-on-near-term-technical-ai-safety*](https://forum.effectivealtruism.org/posts/Ry4C4CKZvuRG7ztxY/pros-and-cons-of-working-on-near-term-technical-ai-safety)",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/gBLs3GefMdtWe6iSk/pros-and-cons-of-working-on-near-term-technical-ai-safety"
    ],
    "tags": [
      "ai",
      "world optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_91cb9a8ca0a4b414",
    "title": "Knowledge is not just precipitation of action",
    "year": 2021,
    "category": "public_awareness",
    "description": "Knowledge is not just precipitation of action\n=============================================",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/JMpERTz9TcnMfEapF/knowledge-is-not-just-precipitation-of-action"
    ],
    "tags": [
      "ai",
      "rationality",
      "truth, semantics, & meaning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e2b3dca6c45dace9",
    "title": "Parameter counts in Machine Learning",
    "year": 2021,
    "category": "public_awareness",
    "description": "**In short:** we have compiled information about the date of development and trainable parameter counts of n=139 machine learning systems between 1952 and 2021. This is, as far as we know, the biggest public dataset of its kind. You can access our dataset [here](https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/edit#gid=0), and the code to produce an interactive visualization is available [here](https://colab.research.google.com/drive/11m0AfSQnLiDijtE1fsIPqF-ipbTQcsFp?usp=sharing).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/GzoWcYibWYwJva8aL/parameter-counts-in-machine-learning"
    ],
    "tags": [
      "ai",
      "machine learning  (ml)",
      "scaling laws"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3994bd514d834695",
    "title": "Environmental Structure Can Cause Instrumental Convergence",
    "year": 2021,
    "category": "technical_research_breakthrough",
    "description": "**Edit, 5/16/23: I think this post is beautiful, correct in its narrow technical claims, and practically irrelevant to alignment. This post presents a cripplingly unrealistic picture of the role of reward functions in reinforcement learning. Reward functions are not \"goals\", real-world policies are not \"optimal\", and the mechanistic function of reward is (usually) to provide policy gradients to update the policy network.**",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/b6jJddSvWMdZHJHh3/environmental-structure-can-cause-instrumental-convergence"
    ],
    "tags": [
      "ai",
      "ai risk",
      "instrumental convergence"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ebd148bbe5b8f24f",
    "title": "Frequent arguments about alignment",
    "year": 2021,
    "category": "public_awareness",
    "description": "Here, I'll review some arguments that frequently come up in discussions about alignment research, involving one person skeptical of the endeavor (called Skeptic) and one person advocating to do more of it (called Advocate). I mostly endorse the views of the Advocate, but the Skeptic isn't a strawman and makes some decent points. The dialog is mostly based on conversations I've had with people who work on machine learning but don't specialize in safety and alignment.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/6ccG9i5cTncebmhsH/frequent-arguments-about-alignment"
    ],
    "tags": [
      "ai",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4584d4c3632b39e7",
    "title": "Alex Turner's Research, Comprehensive Information Gathering",
    "year": 2021,
    "category": "technical_research_breakthrough",
    "description": "Introduction\n============",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/rxsg2sTyHGnMTYbeH/alex-turner-s-research-comprehensive-information-gathering"
    ],
    "tags": [
      "ai",
      "ai risk",
      "deconfusion",
      "impact regularization",
      "instrumental convergence"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_20bfe7f746257f57",
    "title": "Empirical Observations of Objective Robustness Failures",
    "year": 2021,
    "category": "policy_development",
    "description": "[Inner alignment](https://arxiv.org/abs/1906.01820) and [objective](https://www.alignmentforum.org/posts/2mhFMgtAjFJesaSYR/2-d-robustness) [robustness](https://www.alignmentforum.org/posts/SzecSPYxqRa5GCaSF/clarifying-inner-alignment-terminology) have been frequently discussed in the alignment community since the publication of \"[Risks from Learned Optimization](https://arxiv.org/abs/1906.01820)\" (RFLO). These concepts identify a problem beyond [outer alignment](https://www.alignmentforum.org/tag/outer-alignment)/reward specification: even if the reward or objective function is perfectly specified, there is a risk of a model pursuing a different objective than the one it was trained on when deployed out-of-distribution (OOD). They also point to a different type of robustness problem than the kind usually discussed in the OOD robustness literature; typically, when a model is deployed OOD, it either performs well or simply fails to take useful actions (a *capability robustness* failur...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/iJDmL7HJtN5CYKReM/empirical-observations-of-objective-robustness-failures"
    ],
    "tags": [
      "agency",
      "ai",
      "ai safety camp",
      "goal-directedness",
      "inner alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b63c174e94f4db7f",
    "title": "Discussion: Objective Robustness and Inner Alignment Terminology",
    "year": 2021,
    "category": "policy_development",
    "description": "In the alignment community, there seem to be two main ways to frame and define objective robustness and inner alignment. They are quite similar, mainly differing in the manner in which they focus on the same basic underlying problem. We'll call these the objective-focused approach and the generalization-focused approach. We don't delve into these issues of framing the problem in [Empirical Observations of Objective Robustness Failures](https://www.lesswrong.com/posts/iJDmL7HJtN5CYKReM/empirical-observations-of-objective-robustness-failures), where we present empirical observations of objective robustness failures. Instead, we think it is worth having a separate discussion of the matter. These issues have been mentioned only infrequently in a few comments on the Alignment Forum, so it seemed worthwhile to write a post describing the framings and their differences in an effort to promote further discussion in the community.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pDaxobbB9FG5Dvqyv/discussion-objective-robustness-and-inner-alignment"
    ],
    "tags": [
      "agency",
      "ai",
      "ai safety camp",
      "goal-directedness",
      "inner alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5c17a1c86bb188f6",
    "title": "AXRP Episode 9 - Finite Factored Sets with Scott Garrabrant",
    "year": 2021,
    "category": "public_awareness",
    "description": "[Google Podcasts link](https://podcasts.google.com/feed/aHR0cHM6Ly9heHJwb2RjYXN0LmxpYnN5bi5jb20vcnNz/episode/NjhiNGU4ZGQtODRlZi00NmZmLWI1ZDMtN2ZiNDJlMzRjZmY2)",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/s4FNjvrJG6zmYdBuG/axrp-episode-9-finite-factored-sets-with-scott-garrabrant"
    ],
    "tags": [
      "abstraction",
      "ai",
      "audio",
      "axrp",
      "causality",
      "embedded agency",
      "finite factored sets",
      "interviews"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_01b3267572e210ae",
    "title": "[AN #153]: Experiments that demonstrate failures of objective robustness",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/u9CqcufkAJBwXdbx7/an-153-experiments-that-demonstrate-failures-of-objective"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_935b21ab38f5ea60",
    "title": "Finite Factored Sets: LW transcript with running commentary",
    "year": 2021,
    "category": "public_awareness",
    "description": "This is an edited transcript of Scott Garrabrant's May 23 LessWrong talk on finite factored sets. Video:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/6t9F5cS3JjtSspbAZ/finite-factored-sets-lw-transcript-with-running-commentary"
    ],
    "tags": [
      "ai",
      "finite factored sets"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_800f9b0fc10ba702",
    "title": "Brute force searching for alignment",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This is a speculative idea I came up with when writing* [*clankers*](https://docs.google.com/document/d/1lJFaONgHExizl5WRYO_TRrB9LohM7sWJkjWfbe2bXlk/edit?usp=sharing)*, which was one of the short stories in the* [*AI vignettes workshop*](https://docs.google.com/document/d/1y5CE--Sn0wtszcbtSp3MW49anJdVV_pWDExzQZX3BhI/edit?usp=sharing)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/MRFXpedeKJRa324dL/brute-force-searching-for-alignment"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_396cd34f032b41ad",
    "title": "Progress on Causal Influence Diagrams",
    "year": 2021,
    "category": "policy_development",
    "description": "*By Tom Everitt, Ryan Carey, Lewis Hammond, James Fox, Eric Langlois, and Shane Legg*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Cd7Hw492RqooYgQAS/progress-on-causal-influence-diagrams"
    ],
    "tags": [
      "ai",
      "causality",
      "game theory",
      "incentives"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0aa53ca5de496ee9",
    "title": "[AN #154]: What economic growth theory has to say about transformative AI",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/9AcEy2zThvceT9kve/an-154-what-economic-growth-theory-has-to-say-about"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9b0ad9b3fd662b39",
    "title": "Musings on general systems alignment",
    "year": 2021,
    "category": "policy_development",
    "description": "Epistemic status: Musings from the past month. Still far too vague for satisfaction.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/hKMgCaAYS4hnanxBL/musings-on-general-systems-alignment"
    ],
    "tags": [
      "ai",
      "deconfusion"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d636c1d844f686c8",
    "title": "Thoughts on safety in predictive learning",
    "year": 2021,
    "category": "public_awareness",
    "description": "*(Many thanks to Abram Demski for an extensive discussion that helped clarify my thoughts. Thanks Abram &* [*John Maxwell*](https://www.lesswrong.com/users/john_maxwell) *for comments and criticisms on a draft.)*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ey7jACdF4j6GrQLrG/thoughts-on-safety-in-predictive-learning"
    ],
    "tags": [
      "ai",
      "mesa-optimization",
      "self fulfilling/refuting prophecies"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9ceabacf94486103",
    "title": "Experimentally evaluating whether honesty generalizes",
    "year": 2021,
    "category": "policy_development",
    "description": "If we train our ML systems to answer questions honestly in cases where humans can check the answer, will they generalize to behave honestly on questions where we can't check?",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/BxersHYN2qcFoonwg/experimentally-evaluating-whether-honesty-generalizes"
    ],
    "tags": [
      "ai",
      "alignment research center"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1a940b344c2b555c",
    "title": "Confusions re: Higher-Level Game Theory",
    "year": 2021,
    "category": "policy_development",
    "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FPML8k4QtjJxk3Y4M/confusions-re-higher-level-game-theory"
    ],
    "tags": [
      "game theory",
      "rationality",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ab50d3b8a4182075",
    "title": "A simple example of conditional orthogonality in finite factored sets",
    "year": 2021,
    "category": "public_awareness",
    "description": "Recently, MIRI researcher Scott Garrabrant has [publicized his work on finite factored sets](https://www.lesswrong.com/s/kxs3eeEti9ouwWFzr/p/N5Jm6Nj4HkNKySA5Z). It allegedly offers a way to understand agency and causality in a set-up like the [causal graphs championed by Judea Pearl](https://en.wikipedia.org/wiki/Causal_graph). Unfortunately, the [definition of conditional orthogonality](https://www.lesswrong.com/s/kxs3eeEti9ouwWFzr/p/N5Jm6Nj4HkNKySA5Z#2b__Conditional_Orthogonality) is very confusing. I'm not aware of any public examples of people demonstrating that they understand it, but I didn't really understand it until an hour ago, and I've heard others say that it went above their heads. So, I'd like to give an example of it here.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qGjCt4Xq83MBaygPx/a-simple-example-of-conditional-orthogonality-in-finite"
    ],
    "tags": [
      "ai",
      "finite factored sets",
      "rationality"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_fdfba6e353ad4890",
    "title": "A second example of conditional orthogonality in finite factored sets",
    "year": 2021,
    "category": "public_awareness",
    "description": "Yesterday, I wrote a [post](https://danielfilan.com/2021/07/05/simple_example_conditional_orthogonality_ffs.html) that gave an example of conditional non-orthogonality in [finite factored sets](https://www.alignmentforum.org/s/kxs3eeEti9ouwWFzr/p/N5Jm6Nj4HkNKySA5Z). I encourage you to read that post first. However, I'm kind of dissatisfied with it because it doesn't show any interesting cases of conditional orthogonality (despite the title seeming to promise that). So I'd like to show you one today.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/GFGNwCwkffBevyXR2/a-second-example-of-conditional-orthogonality-in-finite"
    ],
    "tags": [
      "ai",
      "finite factored sets"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8859993dd89ebf35",
    "title": "A world in which the alignment problem seems lower-stakes",
    "year": 2021,
    "category": "public_awareness",
    "description": "The danger from power-seeking is not *intrinsic* to the alignment problem. This danger also depends on [the structure of the agent's environment](https://www.lesswrong.com/posts/b6jJddSvWMdZHJHh3/environmental-structure-can-cause-instrumental-convergence).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/sunXMY5WyDcrHsNRr/a-world-in-which-the-alignment-problem-seems-lower-stakes"
    ],
    "tags": [
      "ai",
      "instrumental convergence"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_783db9c442a30be1",
    "title": "[AN #155]: A Minecraft benchmark for algorithms that learn without reward functions",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/a7YgzDYx4FhdB3TmR/an-155-a-minecraft-benchmark-for-algorithms-that-learn"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c253c83142d5d354",
    "title": "BASALT: A Benchmark for Learning from Human Feedback",
    "year": 2021,
    "category": "policy_development",
    "description": "Copying the abstract of the [paper](https://arxiv.org/abs/2107.01969):",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/RyH8LtgMbRAJ9Dv6R/basalt-a-benchmark-for-learning-from-human-feedback"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e27c4686b3e051bd",
    "title": "Intermittent Distillations #4: Semiconductors, Economics, Intelligence, and Technological Progress.",
    "year": 2021,
    "category": "policy_development",
    "description": "The Semiconductor Supply Chain: Assessing National Competitiveness (Saif M. Khan, Alexander Mann, Dahlia Peterson)\n==================================================================================================================",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/rQGW2GqHAFprupYkf/intermittent-distillations-4-semiconductors-economics"
    ],
    "tags": [
      "ai",
      "automation",
      "economic consequences of agi",
      "practice & philosophy of science",
      "superintelligence",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_07dfd5bb52faebf5",
    "title": "The accumulation of knowledge: literature review",
    "year": 2021,
    "category": "public_awareness",
    "description": "*Financial status: This is independent research, now supported by a grant. I welcome* [*financial support*](https://www.alexflint.io/donate.html) *.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/dkruhqAEhXnbAk7iJ/the-accumulation-of-knowledge-literature-review"
    ],
    "tags": [
      "ai",
      "rationality",
      "truth, semantics, & meaning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_938a86739b9d126f",
    "title": "The More Power At Stake, The Stronger Instrumental Convergence Gets For Optimal Policies",
    "year": 2021,
    "category": "policy_development",
    "description": "**Edit, 5/16/23: I think this post is beautiful, correct in its narrow technical claims, and practically irrelevant to alignment. This post presents a cripplingly unrealistic picture of the role of reward functions in reinforcement learning. Reward functions are not \"goals\", real-world policies are not \"optimal\", and the mechanistic function of reward is (usually) to provide policy gradients to update the policy network.**",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Yc5QSSZCQ9qdyxZF6/the-more-power-at-stake-the-stronger-instrumental"
    ],
    "tags": [
      "ai",
      "instrumental convergence"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c6f3d77362632396",
    "title": "Answering questions honestly instead of predicting human answers: lots of problems and some solutions",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This post is the result of work I did with Paul Christiano on the ideas in his \"[Teaching ML to answer questions honestly instead of predicting human answers](https://www.alignmentforum.org/posts/QqwZ7cwEA2cxFEAun/teaching-ml-to-answer-questions-honestly-instead-of)\" post. In addition to expanding upon what is in that post in terms of identifying numerous problems with the proposal there and identifying ways in which some of those problems can be patched, I think that this post also provides a useful window into what Paul-style research looks like from a non-Paul perspective.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/gEw8ig38mCGjia7dj/answering-questions-honestly-instead-of-predicting-human"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_afafca6d2fd98a62",
    "title": "Model-based RL, Desires, Brains, Wireheading",
    "year": 2021,
    "category": "public_awareness",
    "description": "Research publication: Model-based RL, Desires, Brains, Wireheading",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/K5ikTdaNymfWXQHFb/model-based-rl-desires-brains-wireheading"
    ],
    "tags": [
      "ai",
      "corrigibility",
      "inner alignment",
      "wireheading"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_89ad27783eb0b3e9",
    "title": "Fractional progress estimates for AI timelines and implied resource requirements",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This post was written by Mark Xu based on interviews with Carl Shulman. It was paid for by Open Philanthropy but is not representative of their views. A draft was sent to Robin Hanson for review but received no response.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/h3ejmEeNniDNFXTgp/fractional-progress-estimates-for-ai-timelines-and-implied"
    ],
    "tags": [
      "ai",
      "ai timelines",
      "surveys"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8ce3a1eaefd78d8b",
    "title": "[AN #156]: The scaling hypothesis: a plan for building AGI",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/XusDPpXr6FYJqWkxh/an-156-the-scaling-hypothesis-a-plan-for-building-agi"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c6af21acc42e5aa6",
    "title": "Bayesianism versus conservatism versus Goodhart",
    "year": 2021,
    "category": "public_awareness",
    "description": "Key argument: if we use a non-Bayesian conservative approach, such as a minimum over different utility functions, then we better have a good reason as to why that would work. But if we have that reason, we can use it to make the whole thing into a Bayesian mix, which can also allow us to trade off that advantage against other possible gains.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/EFZ64igiNNwiLHaYk/bayesianism-versus-conservatism-versus-goodhart"
    ],
    "tags": [
      "ai",
      "goodhart's law"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_37981ef8e5154a85",
    "title": "A model of decision-making in the brain (the short version)",
    "year": 2021,
    "category": "public_awareness",
    "description": "(UPDATE: For a revised-and-improved version of this post, see [this later post I wrote (especially Section 6.2.2)](https://www.alignmentforum.org/posts/qNZSBqLEh4qLRqgWW/intro-to-brain-like-agi-safety-6-big-picture-of-motivation#6_2_2_Quick_run_through).)",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/e5duEqhAhurT8tCyr/a-model-of-decision-making-in-the-brain-the-short-version"
    ],
    "tags": [
      "ai",
      "neuroscience",
      "reinforcement learning",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9f1508627eebe76d",
    "title": "Re-Define Intent Alignment?",
    "year": 2021,
    "category": "policy_development",
    "description": "I think Evan's [Clarifying Inner Alignment Terminology](https://www.lesswrong.com/posts/SzecSPYxqRa5GCaSF/clarifying-inner-alignment-terminology) is quite clever; more well-optimized than it may at first appear. However, do think there are a couple of things which don't work as well as they could:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/7fkaJLzRiEr2hmSDi/re-define-intent-alignment"
    ],
    "tags": [
      "ai",
      "inner alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d8aca17ba2c71b1e",
    "title": "[AN #157]: Measuring misalignment in the technology underlying Copilot",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/cyTP4ZMnN6RFu9L62/an-157-measuring-misalignment-in-the-technology-underlying"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_de684c73d6f2901b",
    "title": "AXRP Episode 10 - AI's Future and Impacts with Katja Grace",
    "year": 2021,
    "category": "public_awareness",
    "description": "[Google Podcasts link](https://podcasts.google.com/feed/aHR0cHM6Ly9heHJwb2RjYXN0LmxpYnN5bi5jb20vcnNz/episode/MDQ5OGFmYzgtMWUzNi00NTVkLWI3NjAtYWQ4ZmEyMDRkNTNh)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/xbABZRxoSTAnsf8os/axrp-episode-10-ai-s-future-and-impacts-with-katja-grace"
    ],
    "tags": [
      "ai",
      "audio",
      "axrp",
      "interviews",
      "technological forecasting"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_795b758ec4f9185f",
    "title": "Refactoring Alignment (attempt #2)",
    "year": 2021,
    "category": "policy_development",
    "description": "[I've been](https://www.lesswrong.com/posts/7fkaJLzRiEr2hmSDi/re-define-intent-alignment) poking at Evan's [Clarifying Inner Alignment Terminology.](https://www.lesswrong.com/posts/SzecSPYxqRa5GCaSF/clarifying-inner-alignment-terminology) His post gives two separate pictures (the objective-focused approach, which he focuses on, and the generalization-focused approach, which he mentions at the end). We can consolidate those pictures into one [and-or graph](https://en.m.wikipedia.org/wiki/And%E2%80%93or_tree) as follows:",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/vayxfTSQEDtwhPGpW/refactoring-alignment-attempt-2"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ee9f2ddda47cac12",
    "title": "How much compute was used to train DeepMind's generally capable agents?",
    "year": 2021,
    "category": "public_awareness",
    "description": "I'm talking about [these agents](https://www.lesswrong.com/out?url=https%3A%2F%2Fdeepmind.com%2Fblog%2Farticle%2Fgenerally-capable-agents-emerge-from-open-ended-play) ([LW thread here](https://www.lesswrong.com/posts/mTGrrX8SZJ2tQDuqz/deepmind-generally-capable-agents-emerge-from-open-ended))",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/KaPaTdpLggdMqzdyo/how-much-compute-was-used-to-train-deepmind-s-generally"
    ],
    "tags": [
      "ai",
      "deepmind"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_18c56c6c148660e1",
    "title": "[AN #158]: Should we be optimistic about generalization?",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/79qCdyfGxWNKbH8zk/an-158-should-we-be-optimistic-about-generalization"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c37b15190bbd3ee9",
    "title": "LCDT, A Myopic Decision Theory",
    "year": 2021,
    "category": "policy_development",
    "description": "The looming shadow of deception\n===============================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Y76durQHrfqwgwM5o/lcdt-a-myopic-decision-theory"
    ],
    "tags": [
      "ai",
      "causal decision theory",
      "deception",
      "decision theory",
      "myopia"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cf3f0ada24030252",
    "title": "Garrabrant and Shah on human modeling in AGI",
    "year": 2021,
    "category": "policy_development",
    "description": "This is an edited transcript of a conversation between Scott Garrabrant (MIRI) and Rohin Shah (DeepMind) about whether researchers should focus more on approaches to AI alignment that don't require highly capable AI systems to do much human modeling. CFAR's Eli Tyre facilitated the conversation.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Wap8sSDoiigrJibHA/garrabrant-and-shah-on-human-modeling-in-agi"
    ],
    "tags": [
      "ai",
      "factored cognition",
      "humans consulting hch",
      "interpretability (ml & ai)",
      "iterated amplification",
      "mesa-optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e9e39c1b3c00e788",
    "title": "[AN #159]: Building agents that know how to experiment, by training on procedurally generated games",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/zvWqPmQasssaAWkrj/an-159-building-agents-that-know-how-to-experiment-by"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_bf93c510fba6fa36",
    "title": "Value loading in the human brain: a worked example",
    "year": 2021,
    "category": "public_awareness",
    "description": "**UPDATE MARCH 2022: I later revised and improved this post--see** [**[Intro to brain-like-AGI safety] 7. From hardcoded drives to foresighted plans: A worked example**](https://www.alignmentforum.org/posts/zXibERtEWpKuG5XAC/intro-to-brain-like-agi-safety-7-from-hardcoded-drives-to)**. I'm leaving this old version here, but I strongly suggest you click over to the later version instead. Goodbye!**",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/iMM6dvHzco6jBMFMX/value-loading-in-the-human-brain-a-worked-example"
    ],
    "tags": [
      "ai",
      "goal-directedness",
      "neuroscience",
      "planning & decision-making"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ddd30df957ebd43f",
    "title": "Traps of Formalization in Deconfusion",
    "year": 2021,
    "category": "public_awareness",
    "description": "Introduction\n============",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pEB3LrNxvMKFLGBSG/traps-of-formalization-in-deconfusion"
    ],
    "tags": [
      "deconfusion",
      "rationality",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b99c70c3b6326804",
    "title": "What 2026 looks like",
    "year": 2021,
    "category": "policy_development",
    "description": "This was written for the [Vignettes Workshop](https://www.lesswrong.com/posts/jusSrXEAsiqehBsmh/vignettes-workshop-ai-impacts).[[1]](https://www.lesswrong.com/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like-daniel-s-median-future?commentId=wL7FSxbsJs5EEZZEj) The goal is to write out a **detailed** future history (\"trajectory\") that is as realistic (to me) as I can currently manage, i.e. I'm not aware of any alternative trajectory that is similarly detailed and clearly **more** plausible to me. The methodology is roughly: Write a future history of 2022. Condition on it, and write a future history of 2023. Repeat for 2024, 2025, etc. (I'm posting 2022-2026 now so I can get feedback that will help me write 2027+. I intend to keep writing until the story reaches singularity/extinction/utopia/etc.)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like"
    ],
    "tags": [
      "ai",
      "ai persuasion",
      "ai takeoff",
      "ai timelines",
      "forecasting & prediction",
      "forecasts (specific predictions)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0526ce96e90d96d7",
    "title": "Research agenda update",
    "year": 2021,
    "category": "public_awareness",
    "description": "*\"Our greatest fear should not be of failure, but of succeeding at something that doesn't really matter.\"  -DL Moody (allegedly)*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/DkfGaZTgwsE7XZq9k/research-agenda-update"
    ],
    "tags": [
      "ai",
      "neuroscience",
      "research agendas"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_829a5b1fd0cf995a",
    "title": "Seeking Power is Convergently Instrumental in a Broad Class of Environments",
    "year": 2021,
    "category": "policy_development",
    "description": "Edit, 5/16/23: I think this post is beautiful, correct in its narrow technical claims, and practically irrelevant to alignment. This post presents an unrealistic picture of the role of reward functions in reinforcement learning, conflating \"utility\" with \"reward\" in a type-incorrect fashion. Reward functions are not \"goals\", real-world policies are not \"optimal\", and the mechanistic function of reward is (usually) to provide policy gradients to update the policy network.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/hzeLSQ9nwDkPc4KNt/seeking-power-is-convergently-instrumental-in-a-broad-class"
    ],
    "tags": [
      "ai",
      "instrumental convergence"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4e7ed11a11c2ac85",
    "title": "Applications for Deconfusing Goal-Directedness",
    "year": 2021,
    "category": "policy_development",
    "description": "Atonement for my sins towards deconfusion\n=========================================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ECPmgwwWBikTtdqXo/applications-for-deconfusing-goal-directedness"
    ],
    "tags": [
      "ai",
      "deconfusion",
      "goal-directedness",
      "inner alignment",
      "instrumental convergence",
      "optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_662b064b1bbfafd6",
    "title": "Goal-Directedness and Behavior, Redux",
    "year": 2021,
    "category": "public_awareness",
    "description": "Beyond past confusions\n======================",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/YApiu7x3oTTzDgFFN/goal-directedness-and-behavior-redux"
    ],
    "tags": [
      "ai",
      "deconfusion",
      "goal-directedness"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7154aca101dbeb10",
    "title": "Automating Auditing: An ambitious concrete technical research proposal",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This post was originally written as a research proposal for the new AI alignment research organization Redwood Research, detailing an ambitious, concrete technical alignment proposal that I'm excited about work being done on, in a similar vein to Ajeya Cotra's \"[The case for aligning narrowly superhuman models](https://www.alignmentforum.org/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models).\" Regardless of whether Redwood actually ends up working on this proposal, which they may or may not, I think there's still a lot of low-hanging fruit here and I'd be excited about anybody giving just the auditing game, or the full automating auditing proposal, a try. If you're interested in working on something like this, feel free to reach out to me at [evanjhub@gmail.com](mailto:evanjhub@gmail.com).*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/cQwT8asti3kyA62zc/automating-auditing-an-ambitious-concrete-technical-research"
    ],
    "tags": [
      "ai",
      "auditing games"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_da77213025a90beb",
    "title": "Some criteria for sandwiching projects",
    "year": 2021,
    "category": "public_awareness",
    "description": "I liked [Ajeya's\npost](https://www.alignmentforum.org/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models)\na lot, and I think the alignment community should try to do sandwiching projects along\nthe lines she describes. Here I wanted to flesh out some potential criteria\nfor a good sandwiching project; there's not too much original thinking\nhere but I found it helpful to write out. Most of the criteria are\nactually about the chosen domain, not the plan for attacking it.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Gfbf7RsE2fvxGXKC5/some-criteria-for-sandwiching-projects"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ce8cbd1d3e99b2fb",
    "title": "Power-seeking for successive choices",
    "year": 2021,
    "category": "policy_development",
    "description": "From single choice to successive choices\n========================================",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/5Hc4R6rj5yJ3xBhiX/power-seeking-for-successive-choices"
    ],
    "tags": [
      "ai",
      "deconfusion",
      "instrumental convergence",
      "power seeking (ai)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b0316162b299d0ae",
    "title": "A review of \"Agents and Devices\"",
    "year": 2021,
    "category": "policy_development",
    "description": "In the small group of people who care about deconfusing goal-directedness and like Daniel Dennett's intentional stance, there's one paper everyone mentions: [Agents and Devices](https://arxiv.org/abs/1805.12387), by Orseau, McGill and Legg. It's also pretty much the only paper ever mentioned on that topic.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/WrsQfBRqAPKiyGygT/a-review-of-agents-and-devices"
    ],
    "tags": [
      "agency",
      "ai",
      "deconfusion",
      "goal-directedness",
      "rationality",
      "stances"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a59a81b1715930bf",
    "title": "[AN #160]: Building AIs that learn and think like people",
    "year": 2021,
    "category": "public_awareness",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/dkeDMktXtSjfoWnan/an-160-building-ais-that-learn-and-think-like-people"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e8f312deeddd71e7",
    "title": "Approaches to gradient hacking",
    "year": 2021,
    "category": "public_awareness",
    "description": "[Gradient hacking](https://www.alignmentforum.org/posts/uXH4r6MmKPedk8rMA/gradient-hacking) is a fascinating type of deception, because of its anchoring in current ML. Even if it applies to more general settings, gradient hacking is considered foremost in the context of gradient descent, the most common training method at the moment.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/S2jsBsZvqjBZa3pKT/approaches-to-gradient-hacking"
    ],
    "tags": [
      "ai",
      "ai risk",
      "deconfusion",
      "gradient hacking",
      "inner alignment",
      "mesa-optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_87618eaab9f0d94c",
    "title": "Modelling Transformative AI Risks (MTAIR) Project: Introduction",
    "year": 2021,
    "category": "policy_development",
    "description": "Numerous books, articles, and blog posts have laid out reasons to think that AI might pose catastrophic or existential risks for the future of humanity. However, these reasons often differ from each other both in details and in main conceptual arguments, and other researchers have questioned or disputed many of the key assumptions and arguments.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qnA6paRwMky3Q6ktk/modelling-transformative-ai-risks-mtair-project-introduction"
    ],
    "tags": [
      "ai",
      "deconfusion",
      "distinctions",
      "world modeling",
      "world modeling techniques"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_212832d6f1505689",
    "title": "[AN #161]: Creating generalizable reward functions for multiple tasks by learning a model of functional similarity",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/wMCbo7HX3cFbtHZcM/an-161-creating-generalizable-reward-functions-for-multiple"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2ea165ca7f95b105",
    "title": "Provide feedback on Open Philanthropy's AI alignment RFP",
    "year": 2021,
    "category": "public_awareness",
    "description": "Open Philanthropy is planning a request for proposals (RFP) for AI alignment projects working with deep learning systems, and we're looking for feedback on the RFP and on the research directions we're looking for proposals within. We'd be really interested in feedback from people on the Alignment Forum on the current (incomplete) draft of the RFP.  \n  \nThe main RFP text can be viewed [**here**](https://docs.google.com/document/d/1NQYS_4Yf3GRtT4tz41HCNYfdCTQQtuxoouLn1Lj4XSs/edit?usp=sharing). It links to several documents describing two of the research directions we're interested in:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/TmhrC93mj2Pgsox9t/provide-feedback-on-open-philanthropy-s-ai-alignment-rfp"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_dd10a71460d11001",
    "title": "Analogies and General Priors on Intelligence",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This post is part 2 in our* [*sequence on Modeling Transformative AI Risk*](https://www.alignmentforum.org/s/aERZoriyHfCqvWkzg)*. We are building a model to understand debates around existential risks from advanced AI. The model is made with* [*Analytica*](https://en.wikipedia.org/wiki/Analytica_(software)) *software, and consists of nodes (representing key hypotheses and cruxes) and edges (representing the relationships between these cruxes), with final output corresponding to the likelihood of various potential failure scenarios. You can read more about the motivation for our project and how the model works in the* [***Introduction post***](https://www.alignmentforum.org/posts/qnA6paRwMky3Q6ktk/modelling-transformative-ai-risks-mtair-project-introduction)*. Future posts will explain how different considerations, such as AI takeoff or mesa-optimization, are incorporated into the model.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/yFQkFNCszoJPZTnK6/analogies-and-general-priors-on-intelligence"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "evolution",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7fec40c6f9d02f6f",
    "title": "AI Safety Papers: An App for the TAI Safety Database",
    "year": 2021,
    "category": "public_awareness",
    "description": "[**AI Safety Papers**](https://ai-safety-papers.quantifieduncertainty.org) is a website to quickly explore papers around AI Safety. The code is hosted on Github [here](https://github.com/QURIresearch/ai-safety-papers).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/GgusnG2tiPEa4aYFS/ai-safety-papers-an-app-for-the-tai-safety-database"
    ],
    "tags": [
      "ai",
      "ai alignment fieldbuilding",
      "quri",
      "software tools"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9e014bca6c84d142",
    "title": "AI Risk for Epistemic Minimalists",
    "year": 2021,
    "category": "policy_development",
    "description": "*Financial status: This is independent research, now supported by a grant. I welcome further* [*financial support*](https://www.alexflint.io/donate.html)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/8fpzBHt7e6n7Qjoo9/ai-risk-for-epistemic-minimalists"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_440711917a129370",
    "title": "Extraction of human preferences  ???",
    "year": 2021,
    "category": "public_awareness",
    "description": "**Introduction**\n================",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/PZYD5kBpeHWgE5jX4/extraction-of-human-preferences"
    ],
    "tags": [
      "ai",
      "ai safety camp",
      "reinforcement learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7c03eb8446614461",
    "title": "Welcome & FAQ!",
    "year": 2021,
    "category": "public_awareness",
    "description": "*The AI Alignment Forum was launched in 2018. Since then, several hundred researchers have contributed approximately two thousand posts and nine thousand comments. Nearing the third birthday of the Forum, we are publishing this updated and clarified FAQ.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Yp2vYb4zHXEeoTkJc/welcome-and-faq"
    ],
    "tags": [
      "site meta"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_455a70b8e1b18a9f",
    "title": "(apologies for Alignment Forum server outage last night)",
    "year": 2021,
    "category": "public_awareness",
    "description": "The Alignment Forum was down between approx 1:30AM and 7:00AM PDT last night due to what seems to be a memory leak issue (postmortem ongoing). We're setting up some additional monitoring to ensure this doesn't happen again.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/rxMmhJbNbuywFqcBk/apologies-for-alignment-forum-server-outage-last-night"
    ],
    "tags": [
      "site meta"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_77a4447207f1cb9c",
    "title": "MIRI/OP exchange about decision theory",
    "year": 2021,
    "category": "policy_development",
    "description": "Open Philanthropy's Joe Carlsmith and Nick Beckstead had a short conversation about [decision theory](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh/p/zcPLNNw4wgBX5k8kQ) a few weeks ago with MIRI's Abram Demski and Scott Garrabrant (and me) and LW's Ben Pace. I'm copying it here because I thought others might find it useful.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FBbHEjkZzdupcjkna/miri-op-exchange-about-decision-theory-1"
    ],
    "tags": [
      "ai",
      "causal decision theory",
      "decision theory",
      "embedded agency",
      "evidential decision theory",
      "functional decision theory",
      "rationality"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_96a64fdb46a4da0e",
    "title": "Introduction to Reducing Goodhart",
    "year": 2021,
    "category": "public_awareness",
    "description": "(*This work was supported by CEEALAR and LTFF. Thanks to James Flaville, Jason Green-Lowe, Michele Campolo, Justis Mills, Peter Barnett, and Steve Byrnes for conversations.*)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/RozggPiqQxzzDaNYF/introduction-to-reducing-goodhart"
    ],
    "tags": [
      "ai",
      "goodhart's law",
      "value learning"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f141fca899fb3e36",
    "title": "[AN #162]: Foundation models: a paradigm shift within AI",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Haawpd5rZrzkzvYRC/an-162-foundation-models-a-paradigm-shift-within-ai"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3bac2fc72dbd34b1",
    "title": "Can you control the past?",
    "year": 2021,
    "category": "policy_development",
    "description": "(Cross-posted from [Hands and Cities](https://handsandcities.com/2021/08/27/can-you-control-the-past/). Lots of stuff familiar to LessWrong folks interested in decision theory.)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/PcfHSSAMNFMgdqFyB/can-you-control-the-past"
    ],
    "tags": [
      "decision theory",
      "rationality"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_fcacb4add6cd9c29",
    "title": "What are good alignment conference papers?",
    "year": 2021,
    "category": "public_awareness",
    "description": "I regularly debate with people whether pushing for more mainstream publications in ML/AI venues by alignment researchers is a good thing. So I want to find data: alignment papers published at NeurIPS and other top conferences (journals too, but there're less relevant in computer science) by researchers. I have already some ways of looking for papers like that (including the [AI Safety Papers website](https://ai-safety-papers.quantifieduncertainty.org/)), but I'm curious if people here have favorite that they think I should really know/really shouldn't miss.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/cSNaxb8wu564x9n6r/what-are-good-alignment-conference-papers"
    ],
    "tags": [
      "ai",
      "ai risk",
      "community",
      "literature reviews"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f48f894127cc2d9a",
    "title": "A short introduction to machine learning",
    "year": 2021,
    "category": "public_awareness",
    "description": "Despite the current popularity of machine learning, I haven't found any short introductions to it which quite match the way I prefer to introduce people to the field. So here's my own. Compared with other introductions, I've focused less on explaining each concept in detail, and more on explaining how they relate to other important concepts in AI, especially in diagram form. If you're new to machine learning, you shouldn't expect to fully understand most of the concepts explained here just after reading this post - the goal is instead to provide a broad framework which will contextualise more detailed explanations you'll receive from elsewhere.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qE73pqxAZmeACsAdF/a-short-introduction-to-machine-learning"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_269c9228c5550543",
    "title": "Alignment Research = Conceptual Alignment Research + Applied Alignment Research",
    "year": 2021,
    "category": "public_awareness",
    "description": "Instead of talking about alignment research, we should differentiate **conceptual** alignment research and **applied** alignment research.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/2Xfv3GQgo2kGER8vA/alignment-research-conceptual-alignment-research-applied"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c7dc9b4ffb674627",
    "title": "Grokking the Intentional Stance",
    "year": 2021,
    "category": "public_awareness",
    "description": "Considering how much I've been using \"[the intentional stance](https://mitpress.mit.edu/books/intentional-stance)\" in my thinking about the nature of agency and goals and discussions of the matter recently, I figured it would be a good idea to, y'know, *actually read* what Dan Dennett originally wrote about it. While doing so, I realized that he was already considering some nuances in the subject that [the Wikipedia summary](https://en.wikipedia.org/wiki/Intentional_stance) of the intentional stance leaves out but that are nonetheless relevant to the issues we face when attempting to e.g. formalize the approach, or think more clearly about the nature of agency in the context of alignment. I don't expect many LessWrongers will read the original book in full, but I do expect that some additional clarity on what exactly Dennett was claiming about the nature of agency and goals will be helpful in having less confused intuitions and discussions about the subject.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/jHSi6BwDKTLt5dmsG/grokking-the-intentional-stance"
    ],
    "tags": [
      "agency",
      "ai",
      "dissolving the question",
      "goal-directedness",
      "intentionality",
      "stances"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ec0f51143946123c",
    "title": "Reward splintering as reverse of interpretability",
    "year": 2021,
    "category": "public_awareness",
    "description": "There is a sense in which [reward splintering](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1) is the reverse of interpretability.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/JZEpqrLh2xHx2xfAd/reward-splintering-as-reverse-of-interpretability"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_514f722e8ce4e678",
    "title": "Call for research on evaluating alignment (funding + advice available)",
    "year": 2021,
    "category": "public_awareness",
    "description": "Research publication: Call for research on evaluating alignment (funding + advice available)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/7Rvctxk73BrKqEaqh/call-for-research-on-evaluating-alignment-funding-advice"
    ],
    "tags": [
      "ai",
      "community",
      "grants & fundraising opportunities",
      "inner alignment",
      "outer alignment"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b4706f691040de33",
    "title": "NIST AI Risk Management Framework request for information (RFI)",
    "year": 2021,
    "category": "policy_development",
    "description": "*(Cross-post from the* [*EA Forum*](https://forum.effectivealtruism.org/posts/sQz9zTw49twFTsqMg/nist-ai-risk-management-framework-request-for-information)*)*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/uFLCwj6jcvnvMtBk3/nist-ai-risk-management-framework-request-for-information"
    ],
    "tags": [
      "ai",
      "community"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_93a73a4ba8df148f",
    "title": "Thoughts on gradient hacking",
    "year": 2021,
    "category": "public_awareness",
    "description": "Gradient hacking is the hypothesised phenomenon of a machine learning model, during training, deliberate thinking in ways which guide gradient descent to update its parameters in the directions it desires. The key intuition here is that because the loss landscape of a model is based on the cognition it does, models can make decisions for the purpose of affecting their loss landscapes, thereby affecting the directions in which they are updated. [Evan writes](https://www.alignmentforum.org/posts/uXH4r6MmKPedk8rMA/gradient-hacking):",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/egzqHKkzhuZuivHZ4/thoughts-on-gradient-hacking"
    ],
    "tags": [
      "ai",
      "gradient hacking",
      "mesa-optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_6a4be448c457c9d7",
    "title": "Multi-Agent Inverse Reinforcement Learning: Suboptimal Demonstrations and  Alternative Solution Concepts",
    "year": 2021,
    "category": "public_awareness",
    "description": "This research was recently completed within the AI Safety division of the Stanford Existential Risk Initiative and concerns methods for reward learning in multi-agent systems.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pRD5u2omuDoMTuH39/multi-agent-inverse-reinforcement-learning-suboptimal"
    ],
    "tags": [
      "ai",
      "reinforcement learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8ab2295334fef8e3",
    "title": "Distinguishing AI takeover scenarios",
    "year": 2021,
    "category": "policy_development",
    "description": "*Epistemic status: lots of this involves interpreting/categorising other people's scenarios, and could be wrong. We'd really appreciate being corrected if so. [ETA: so far, no corrections.]*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios"
    ],
    "tags": [
      "ai",
      "ai risk",
      "outer alignment",
      "threat models",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e4095ce8389643e7",
    "title": "[AN #163]: Using finite factored sets for causal and temporal inference",
    "year": 2021,
    "category": "public_awareness",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/9Hxa6pxRrxkwjBKib/an-163-using-finite-factored-sets-for-causal-and-temporal"
    ],
    "tags": [
      "ai",
      "finite factored sets"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0d20bb442d0cd7ec",
    "title": "Countably Factored Spaces",
    "year": 2021,
    "category": "technical_research_breakthrough",
    "description": "A followup to Scott's \"Finite Factored Sets\", specifically the Applications part at the end where he talked about the [infinite case](https://www.alignmentforum.org/s/kxs3eeEti9ouwWFzr/p/yGFiw23pJ32obgLbw#7_2__Infinity).",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/QEfbg6vbjGgfFzJM4/countably-factored-spaces"
    ],
    "tags": [
      "finite factored sets"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_49136bf679edffe4",
    "title": "The alignment problem in different capability regimes",
    "year": 2021,
    "category": "public_awareness",
    "description": "I think the alignment problem looks different depending on the capability level of systems you're trying to align. And I think that different researchers often have different capability levels in mind when they talk about the alignment problem. I think this leads to confusion. I'm going to use the term \"regimes of the alignment problem\" to refer to the different perspectives on alignment you get from considering systems with different capability levels.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/HHunb8FPnhWaDAQci/the-alignment-problem-in-different-capability-regimes"
    ],
    "tags": [
      "ai",
      "ai capabilities",
      "ai risk"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2d0bb65aeab75c46",
    "title": "The Blackwell order as a formalization of knowledge",
    "year": 2021,
    "category": "public_awareness",
    "description": "*Financial status: This is independent research, now supported by a grant. I welcome further* [*financial support*](https://www.alexflint.io/donate.html)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/wEjozSY9rhkpAaABt/the-blackwell-order-as-a-formalization-of-knowledge"
    ],
    "tags": [
      "information theory"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9e3e9cded76242e3",
    "title": "Paths To High-Level Machine Intelligence",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This post is part 3 in our* [*sequence on Modeling Transformative AI Risk*](https://www.alignmentforum.org/s/aERZoriyHfCqvWkzg)*. We are building a model to understand debates around existential risks from advanced AI. The model is made with* [*Analytica*](https://en.wikipedia.org/wiki/Analytica_(software)) *software, and consists of nodes (representing key hypotheses and cruxes) and edges (representing the relationships between these cruxes), with final output corresponding to the likelihood of various potential failure scenarios. You can read more about the motivation for our project and how the model works in the* [*Introduction post*](https://www.alignmentforum.org/posts/qnA6paRwMky3Q6ktk/modelling-transformative-ai-risks-mtair-project-introduction)*. The previous post in the sequence,* [*Analogies and General Priors on Intelligence*](https://www.alignmentforum.org/s/aERZoriyHfCqvWkzg/p/yFQkFNCszoJPZTnK6)*, investigated the nature of intelligence as it pertains to advanced AI.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/amK9EqxALJXyd9Rb2/paths-to-high-level-machine-intelligence"
    ],
    "tags": [
      "ai",
      "ai timelines",
      "whole brain emulation",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_dc9b447881b009d1",
    "title": "Measurement, Optimization, and Take-off Speed",
    "year": 2021,
    "category": "policy_development",
    "description": "*Crossposted from my blog,* [*Bounded Regret*](https://bounded-regret.ghost.io/measurement-and-optimization/)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/u4mFjCPviXHAPjZK7/measurement-optimization-and-take-off-speed"
    ],
    "tags": [
      "ai",
      "optimization",
      "practice & philosophy of science"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5e5dfb41fb887955",
    "title": "[AN #164]: How well can language models write code?",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/p8xcZerxHWi4nLorx/an-164-how-well-can-language-models-write-code"
    ],
    "tags": [
      "ai",
      "language models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8b7dc6e9c81d18c9",
    "title": "How truthful is GPT-3? A benchmark for language models",
    "year": 2021,
    "category": "public_awareness",
    "description": "This is an edited excerpt of a new ML paper ([pdf](https://arxiv.org/abs/2109.07958), [code](https://github.com/sylinrl/TruthfulQA)) by Stephanie Lin (FHI Oxford), [Jacob Hilton](https://www.jacobh.co.uk/) (OpenAI) and [Owain Evans](https://owainevans.github.io/) (FHI Oxford). The paper is under review at NeurIPS.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/PF58wEdztZFX2dSue/how-truthful-is-gpt-3-a-benchmark-for-language-models"
    ],
    "tags": [
      "academic papers",
      "ai",
      "ai risk",
      "language models",
      "truth, semantics, & meaning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b8d55b1700020ca4",
    "title": "Economic AI Safety",
    "year": 2021,
    "category": "public_awareness",
    "description": "*Crossposted from my blog,* [*Bounded Regret*](https://bounded-regret.ghost.io/economic-ai-safety/)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/RLcEQtoc5EqTxdan8/economic-ai-safety"
    ],
    "tags": [
      "ai",
      "privacy"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3e03f255912fb117",
    "title": "Immobile AI makes a move: anti-wireheading, ontology change, and model splintering",
    "year": 2021,
    "category": "public_awareness",
    "description": "Research projects\n-----------------",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ZqSESJYcA8m2Hh7Qm/immobile-ai-makes-a-move-anti-wireheading-ontology-change"
    ],
    "tags": [
      "ai",
      "research agendas"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d609ad78d952d775",
    "title": "Goodhart Ethology",
    "year": 2021,
    "category": "policy_development",
    "description": "To answer your first question, ethology is the study of animal behavior in the wild.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/z2BPxcFfhKho89D8L/goodhart-ethology"
    ],
    "tags": [
      "ai",
      "goodhart's law",
      "value learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d02edebb7e42468f",
    "title": "Investigating AI Takeover Scenarios",
    "year": 2021,
    "category": "policy_development",
    "description": "*Epistemic status: lots of this involves interpreting/categorising other people's scenarios, and could be wrong. We'd really appreciate being corrected if so.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/zkF9PNSyDKusoyLkP/investigating-ai-takeover-scenarios"
    ],
    "tags": [
      "ai",
      "ai risk",
      "threat models"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7f3bd53639e08234",
    "title": "The theory-practice gap",
    "year": 2021,
    "category": "public_awareness",
    "description": "[Thanks to Richard Ngo, Damon Binder, Summer Yue, Nate Thomas, Ajeya Cotra, Alex Turner, and other Redwood Research people for helpful comments; thanks Ruby Bloom for formatting this for the Alignment Forum for me.]",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/xRyLxfytmLFZ6qz5s/the-theory-practice-gap"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8b2d2f34c58c5331",
    "title": "[Book Review] \"The Alignment Problem\" by Brian Christian",
    "year": 2021,
    "category": "policy_development",
    "description": "I came to this book with [an ax to grind](https://www.lesswrong.com/posts/ZYDkHWjShKazTywbg/review-the-alignment-problem-by-brian-christian?commentId=CwaL3brZi5QbeLrtt). I combed through page after page for factual errors, minor misrepresentations or even just contestable opinions. I spotted (what seemed like) omission after omission only to be frustrated just a few pages later when Brian Christian addressed them. In the *Chapter 5: Shaping* I thought I found a major mistake. Brian Christian addressed Skinnerian operant conditioning without addressing the real way we manages human groups: leading by example.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ZYDkHWjShKazTywbg/book-review-the-alignment-problem-by-brian-christian"
    ],
    "tags": [
      "ai",
      "ai risk",
      "book reviews / media reviews"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f06ecceae1b570cb",
    "title": "AI, learn to be conservative, then learn to be less so: reducing side-effects, learning preserved features, and going beyond conservatism",
    "year": 2021,
    "category": "public_awareness",
    "description": "Research projects\n-----------------",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/y2XyxomuEpMaRYDQw/ai-learn-to-be-conservative-then-learn-to-be-less-so"
    ],
    "tags": [
      "ai",
      "research agendas"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_56f632a542a87ca1",
    "title": "Announcing the Vitalik Buterin Fellowships in AI Existential Safety!",
    "year": 2021,
    "category": "public_awareness",
    "description": "*Epistemic status: describing fellowships that I am helping with the administration of.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/hSdgugekxgdyacXTu/announcing-the-vitalik-buterin-fellowships-in-ai-existential"
    ],
    "tags": [
      "ai",
      "existential risk",
      "project announcement"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_797ef53546284e42",
    "title": "David Wolpert on Knowledge",
    "year": 2021,
    "category": "policy_development",
    "description": "*Financial status: This is independent research, now supported by a grant. I welcome* [*financial support*](https://www.alexflint.io/donate.html)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/hPPGuiXf3zhKqgCMb/david-wolpert-on-knowledge"
    ],
    "tags": [
      "epistemology",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ea7f52cf74b05478",
    "title": "Redwood Research's current project",
    "year": 2021,
    "category": "policy_development",
    "description": "Here's a description of the project [Redwood Research](http://redwoodresearch.org) is working on at the moment. First I'll say roughly what we're doing, and then I'll try to explain why I think this is a reasonable applied alignment project, and then I'll talk a bit about the takeaways I've had from the project so far.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/k7oxdbNaGATZbtEg3/redwood-research-s-current-project"
    ],
    "tags": [
      "ai",
      "organization updates",
      "redwood research"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_22796d09bf2b33bd",
    "title": "[AN #165]: When large models are more likely to lie",
    "year": 2021,
    "category": "public_awareness",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/NR4rgfKu63TcqLxcH/an-165-when-large-models-are-more-likely-to-lie"
    ],
    "tags": [
      "ai",
      "treacherous turn"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e345d0a78beb4f2d",
    "title": "Pathways: Google's AGI",
    "year": 2021,
    "category": "public_awareness",
    "description": "A month ago, Jeff Dean, lead of Google AI, [announced at TED](https://qz.com/2042493/pathways-google-is-developing-a-superintelligent-multipurpose-ai/) that Google is developing a \"general purpose intelligence system\". To give a bit of context, last January, Google already published a paper on a [1.6-trillion parameter model](https://arxiv.org/abs/2101.03961) based on the architecture of *switch transformers*, which improves by on order of magnitude upon GPT-3. Yet, I've heard that Google usually only publishes a weaker version of the algorithms they actually develop and deploy at scale.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/bEKW5gBawZirJXREb/pathways-google-s-agi"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f2018605130d3dfc",
    "title": "Cognitive Biases in Large Language Models",
    "year": 2021,
    "category": "public_awareness",
    "description": "> Humans, one might say, are the cyanobacteria of AI: we constantly emit large amounts of structured data, which implicitly rely on logic, causality, object permanence, history--all of that good stuff. All of that is implicit and encoded into our writings and videos and 'data exhaust'. A model learning to predict must learn to understand all of that to get the best performance; as it predicts the easy things which are mere statistical pattern-matching, what's left are the hard things. [Gwern](https://www.gwern.net/Scaling-hypothesis#:~:text=Humans%2C%20one,hard%20things.)\n> \n>",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/fFF3G4W8FbXigS4gr/cognitive-biases-in-large-language-models"
    ],
    "tags": [
      "ai",
      "language models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0c07fca6f12fba6c",
    "title": "AXRP Episode 11 - Attainable Utility and Power with Alex Turner",
    "year": 2021,
    "category": "policy_development",
    "description": "[Google Podcasts link](https://podcasts.google.com/feed/aHR0cHM6Ly9heHJwb2RjYXN0LmxpYnN5bi5jb20vcnNz/episode/OGFiMTdiNjYtMjEzMS00YTBmLTg2YTgtYTQ1ZjdmZmM4Yjk0)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/vcrXS5DmvBuJaKucp/axrp-episode-11-attainable-utility-and-power-with-alex"
    ],
    "tags": [
      "ai",
      "audio",
      "axrp",
      "impact regularization",
      "instrumental convergence",
      "interviews"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_618f5df280838447",
    "title": "AI takeoff story: a continuation of progress by other means",
    "year": 2021,
    "category": "public_awareness",
    "description": "*Thanks to Vladimir Mikulik for suggesting that I write this, and to Rohin Shah and Daniel Kokotajlo for kindly providing feedback.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Fq8ybxtcFvKEsWmF8/ai-takeoff-story-a-continuation-of-progress-by-other-means"
    ],
    "tags": [
      "ai",
      "ai risk",
      "ai takeoff",
      "existential risk",
      "fiction"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8e42ba5d79f78afb",
    "title": "Selection Theorems: A Program For Understanding Agents",
    "year": 2021,
    "category": "public_awareness",
    "description": "What's the type signature of an agent?",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents"
    ],
    "tags": [
      "ai",
      "inner alignment",
      "outer alignment",
      "rationality",
      "research agendas",
      "selection theorems",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_37c472100e3cac8b",
    "title": "Collection of arguments to expect (outer and inner) alignment failure?",
    "year": 2021,
    "category": "public_awareness",
    "description": "Various arguments have been made for why advanced AI systems will plausibly not have the goals their operators intended them to have (due to either [outer](https://www.lesswrong.com/tag/outer-alignment) or [inner](https://www.lesswrong.com/tag/inner-alignment) alignment failure).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/m5or8yzrw9GLavz9b/collection-of-arguments-to-expect-outer-and-inner-alignment"
    ],
    "tags": [
      "ai",
      "inner alignment",
      "outer alignment"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_db3aaed0b092657c",
    "title": "Brain-inspired AGI and the \"lifetime anchor\"",
    "year": 2021,
    "category": "public_awareness",
    "description": "Last year Ajeya Cotra published a [draft report on AI timelines](https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines). (See also: [summary and commentary by Holden Karnofsky](https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/), [podcast interview with Ajeya](https://www.lesswrong.com/posts/CuDYhLLXq6FuHvGZc/axrp-episode-7-5-forecasting-transformative-ai-from).)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/W6wBmQheDiFmfJqZy/brain-inspired-agi-and-the-lifetime-anchor"
    ],
    "tags": [
      "ai",
      "ai timelines",
      "computing overhang",
      "neuromorphic ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0e12e5ee7bf7fe53",
    "title": "Unsolved ML Safety Problems",
    "year": 2021,
    "category": "public_awareness",
    "description": "*Cross-posted from the [BAIR Blog](https://bair.berkeley.edu/blog/2021/09/29/ml-safety/) and from my blog, [Bounded Regret](https://bounded-regret.ghost.io/unsolved-ml-safety-problems/).*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/AwMb7C72etphiRvah/unsolved-ml-safety-problems"
    ],
    "tags": [
      "ai",
      "machine learning  (ml)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_15aec086e51c4e6f",
    "title": "A brief review of the reasons multi-objective RL could be important in AI Safety Research",
    "year": 2021,
    "category": "public_awareness",
    "description": "*By Ben Smith,* [*Roland Pihlakas*](https://www.lesswrong.com/users/roland-pihlakas)*, and* [*Robert Klassert*](https://www.lesswrong.com/users/klaro)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/i5dLfi6m6FCexReK9/a-brief-review-of-the-reasons-multi-objective-rl-could-be"
    ],
    "tags": [
      "ai",
      "ai risk",
      "ai safety camp",
      "psychology",
      "reinforcement learning"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c7c03ddf9b59cc0c",
    "title": "AI learns betrayal and how to avoid it",
    "year": 2021,
    "category": "public_awareness",
    "description": "Research projects\n-----------------",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/oeCXS2ZCn4rPyq7LQ/ai-learns-betrayal-and-how-to-avoid-it"
    ],
    "tags": [
      "ai",
      "research agendas",
      "treacherous turn"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_6476b8d4ff6938a5",
    "title": "My take on Vanessa Kosoy's take on AGI safety",
    "year": 2021,
    "category": "policy_development",
    "description": "*Confidence level: Low*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/SzrmsbkqydpZyPuEh/my-take-on-vanessa-kosoy-s-take-on-agi-safety"
    ],
    "tags": [
      "ai",
      "infra-bayesianism",
      "reinforcement learning"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cc094f5fbfd7eb70",
    "title": "Takeoff Speeds and Discontinuities",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This post is part 4 in our* [*sequence on Modeling Transformative AI Risk*](https://www.alignmentforum.org/s/aERZoriyHfCqvWkzg)*. We are building a model to understand debates around existential risks from advanced AI. The model is made with* [*Analytica*](https://en.wikipedia.org/wiki/Analytica_(software)) *software, and consists of nodes (representing key hypotheses and cruxes) and edges (representing the relationships between these cruxes), with final outputs corresponding to the likelihood of various potential failure scenarios. You can read more about the motivation for our project and how the model works in the* [*Introduction post*](https://www.alignmentforum.org/posts/qnA6paRwMky3Q6ktk/modelling-transformative-ai-risks-mtair-project-introduction)*. The previous post in the sequence,* [*Paths to High-Level Machine Intelligence*](https://www.alignmentforum.org/posts/amK9EqxALJXyd9Rb2/paths-to-high-level-machine-intelligence)*, investigated how and when HLMI will be developed.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pGXR2ynhe5bBCCNqn/takeoff-speeds-and-discontinuities"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ba386f05916eb8ad",
    "title": "What Selection Theorems Do We Expect/Want?",
    "year": 2021,
    "category": "public_awareness",
    "description": "This post contains some of my current best guesses at aspects of agent type signatures for which I expect there are useful [Selection Theorems](https://www.lesswrong.com/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents), as well as properties of selection optima which I expect are key to proving these type signatures.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/RuDD3aQWLDSb4eTXP/what-selection-theorems-do-we-expect-want"
    ],
    "tags": [
      "selection theorems",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_342892949984eda9",
    "title": "Meta learning to gradient hack",
    "year": 2021,
    "category": "public_awareness",
    "description": "Research publication: Meta learning to gradient hack",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/jh6dkqN2wd7fCRfB5/meta-learning-to-gradient-hack"
    ],
    "tags": [
      "ai",
      "gradient hacking",
      "inner alignment",
      "mesa-optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8e1b6e70824f39f9",
    "title": "The Simulation Hypothesis Undercuts the SIA/Great Filter Doomsday Argument",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This post was written by Mark Xu based on interviews with Carl Shulman. It was paid for by Open Philanthropy but is not representative of their views.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/HF2vpnmgqmHyLGRrA/the-simulation-hypothesis-undercuts-the-sia-great-filter"
    ],
    "tags": [
      "anthropics",
      "simulation hypothesis",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_bec23481b0286619",
    "title": "Force neural nets to use models, then detect these",
    "year": 2021,
    "category": "public_awareness",
    "description": "Research projects\n-----------------",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pdJqEzbQrucTEF6DW/force-neural-nets-to-use-models-then-detect-these"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cdd39a64b5c2c53e",
    "title": "We're Redwood Research, we do applied alignment research, AMA",
    "year": 2021,
    "category": "public_awareness",
    "description": "Redwood Research is a longtermist organization working on AI alignment based in Berkeley, California. We're going to do an AMA this week; we'll answer questions mostly on Wednesday and Thursday this week (6th and 7th of October). Buck Shlegeris, Bill Zito, myself, and perhaps other people will be answering questions.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qHvHvBR8L6oycnMXe/we-re-redwood-research-we-do-applied-alignment-research-ama"
    ],
    "tags": [
      "ai",
      "ama",
      "q&a (format)",
      "redwood research"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b20f32dd2543639b",
    "title": "Preferences from (real and hypothetical) psychology papers",
    "year": 2021,
    "category": "public_awareness",
    "description": "Research projects\n-----------------",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FsxPNRJ5NQkrSKyDx/preferences-from-real-and-hypothetical-psychology-papers"
    ],
    "tags": [
      "ai",
      "machine learning  (ml)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c775394505157aac",
    "title": "Automated Fact Checking: A Look at the Field",
    "year": 2021,
    "category": "public_awareness",
    "description": "Intro / Motivation\n------------------",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/CWD8FxA3yJPmZE9o3/automated-fact-checking-a-look-at-the-field"
    ],
    "tags": [
      "ai",
      "machine learning  (ml)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2a20bf637d2a95f1",
    "title": "Safety-capabilities tradeoff dials are inevitable in AGI",
    "year": 2021,
    "category": "policy_development",
    "description": "A safety-capabilities tradeoff is when you have something like a dial on your AGI, and one end of the dial says \"more safe but less capable\", and the other end of the dial says \"less safe but more capable\".",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/tmyTb4bQQi7C47sde/safety-capabilities-tradeoff-dials-are-inevitable-in-agi"
    ],
    "tags": [
      "ai",
      "world optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_796cd4d44fbdcf6f",
    "title": "[AN #166]: Is it crazy to claim we're in the most important century?",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/YbAZ4nSA8itL2EkDb/an-166-is-it-crazy-to-claim-we-re-in-the-most-important"
    ],
    "tags": [
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_88afce05a21769ec",
    "title": "Intelligence or Evolution?",
    "year": 2021,
    "category": "policy_development",
    "description": "Here are two kinds of super high-level explanation for how things turned out in the future:",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/TATWqHvxKEpL34yKz/intelligence-or-evolution"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9479fe2caf053543",
    "title": "On Solving Problems Before They Appear: The Weird Epistemologies of Alignment",
    "year": 2021,
    "category": "public_awareness",
    "description": "[*Crossposted*](https://forum.effectivealtruism.org/posts/AjhTQAKJqLbJnbq9B/on-solving-problems-before-they-appear-the-weird) *to the EA Forum*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FQqcejhNWGG8vHDch/on-solving-problems-before-they-appear-the-weird"
    ],
    "tags": [
      "ai",
      "ai risk",
      "epistemology",
      "intellectual progress (society-level)",
      "practice & philosophy of science"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e21d0dcc064835a7",
    "title": "Modeling Risks From Learned Optimization",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This post, which deals with how risks from learned optimization and inner alignment can be understood, is part 5 in our* [*sequence on Modeling Transformative AI Risk*](https://www.alignmentforum.org/s/aERZoriyHfCqvWkzg)*. We are building a model to understand debates around existential risks from advanced AI. The model is made with* [*Analytica*](https://en.wikipedia.org/wiki/Analytica_(software)) *software, and consists of nodes (representing key hypotheses and cruxes) and edges (representing the relationships between these cruxes), with final output corresponding to the likelihood of various potential failure scenarios. You can read more about the motivation for our project and how the model works in the* [*Introduction post*](https://www.alignmentforum.org/posts/qnA6paRwMky3Q6ktk/modelling-transformative-ai-risks-mtair-project-introduction)*. The previous post in the sequence,* [*Takeoff Speeds and Discontinuities*](https://www.alignmentforum.org/posts/pGXR2ynhe5bBCCNqn/takeoff...",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/T9oFjteStcE2ijCJi/modeling-risks-from-learned-optimization"
    ],
    "tags": [
      "ai",
      "mesa-optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_99b5d7440ed67d3b",
    "title": "[Proposal] Method of locating useful subnets in large models",
    "year": 2021,
    "category": "public_awareness",
    "description": "I've seen it suggested (e.g, [here](https://www.alignmentforum.org/posts/SzrmsbkqydpZyPuEh/my-take-on-vanessa-kosoy-s-take-on-agi-safety)) that we could tackle the outer alignment problem by using interpretability tools to locate the learned \"human values\" subnet of powerful, unaligned models. Here I outline a general method of extracting such subnets from a large model.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Ji6hQbwH7tK7mejhk/proposal-method-of-locating-useful-subnets-in-large-models"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)",
      "machine learning  (ml)",
      "mesa-optimization",
      "outer alignment",
      "reinforcement learning"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ea9b0f92b3ec82df",
    "title": "Memetic hazards of AGI architecture posts",
    "year": 2021,
    "category": "public_awareness",
    "description": "I've been thinking recently and writing a post about potential AGI architecture that seems possible to make with current technology in 3 to 5 years, and even faster if significant effort will be put to that goal.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/XTGceK7xE8rxrRLcr/memetic-hazards-of-agi-architecture-posts-1"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0afa069b05fdde19",
    "title": "Optimization Concepts in the Game of Life",
    "year": 2021,
    "category": "public_awareness",
    "description": "**Abstract:** We define robustness and retargetability (two of Flint's measures of optimization) in Conway's Game of Life and apply the definitions to a few examples. The same approach likely works in most embedded settings, and provides a frame for conceptualizing and quantifying these aspects of agency. We speculate on the relationship between robustness and retargetability, and identify various directions for future work.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/mL8KdftNGBScmBcBg/optimization-concepts-in-the-game-of-life"
    ],
    "tags": [
      "cellular automata",
      "dynamical systems",
      "embedded agency",
      "goal-directedness",
      "optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9b8bb7d1bc4a06e3",
    "title": "Epistemic Strategies of Selection Theorems",
    "year": 2021,
    "category": "policy_development",
    "description": "Introduction: Epistemic Strategies Redux\n========================================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/LWmmfTvptiJp7wvFg/epistemic-strategies-of-selection-theorems"
    ],
    "tags": [
      "ai",
      "ai risk",
      "epistemic review",
      "epistemology",
      "intellectual progress (individual-level)",
      "selection effects",
      "selection theorems",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_db2bea21e8c3df7d",
    "title": "[MLSN #1]: ICLR Safety Paper Roundup",
    "year": 2021,
    "category": "public_awareness",
    "description": "As part of a larger community building effort, I am writing a monthly safety newsletter which is designed to cover empirical safety research and be palatable to the broader machine learning research community. You can [subscribe here](https://newsletter.mlsafety.org) or follow the newsletter on [twitter](https://twitter.com/ml_safety) here.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/8Gv5zSCnGeLxK5FAF/mlsn-1-iclr-safety-paper-roundup"
    ],
    "tags": [
      "ai",
      "machine learning  (ml)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_92bd30912c7ecf36",
    "title": "Truthful AI: Developing and governing AI that does not lie",
    "year": 2021,
    "category": "policy_development",
    "description": "This post contains the abstract and executive summary of a new 96-page [paper](https://arxiv.org/abs/2110.06674) from authors at the Future of Humanity Institute and OpenAI.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/aBixCPqSnTsPsTJBQ/truthful-ai-developing-and-governing-ai-that-does-not-lie"
    ],
    "tags": [
      "ai",
      "ai governance",
      "ai risk",
      "epistemology",
      "gpt",
      "honesty",
      "truth, semantics, & meaning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_10b177507847c386",
    "title": "On The Risks of Emergent Behavior in Foundation Models",
    "year": 2021,
    "category": "policy_development",
    "description": "*This post first appeared as a [commentary](https://crfm.stanford.edu/2021/10/18/commentaries.html) for the paper \"On The Opportunities and Risks of Foundation Models\".*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/QmdrkuArFHphqANRE/on-the-risks-of-emergent-behavior-in-foundation-models"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f9598242868ee165",
    "title": "Beyond the human training distribution: would the AI CEO create almost-illegal teddies?",
    "year": 2021,
    "category": "policy_development",
    "description": "**tl;dr**: *I showthat model splintering can be seen as going beyond the human training distribution (the distribution of real and imagined situations we have firm or vague preferences over), and argue why this is at the heart of AI alignment.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/tHChCJB9piCTD7HEx/beyond-the-human-training-distribution-would-the-ai-ceo"
    ],
    "tags": [
      "ai",
      "complexity of value"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_377144b6c908f057",
    "title": "[AN #167]: Concrete ML safety problems and their relevance to x-risk",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/suy5w8cWZJZsv2XES/an-167-concrete-ml-safety-problems-and-their-relevance-to-x"
    ],
    "tags": [
      "ai",
      "newsletters"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1fc6c6af4b07f158",
    "title": "AGI Safety Fundamentals curriculum and application",
    "year": 2021,
    "category": "policy_development",
    "description": "Over the last year EA Cambridge has been designing and running an online program aimed at effectively introducing the field of AGI safety; the most recent cohort included around 150 participants and 25 facilitators from around the world. Dewi Erwan runs the program; I designed the curriculum, the latest version of which appears in [the linked document](https://docs.google.com/document/d/1mTm_sT2YQx3mRXQD6J2xD2QJG1c3kHyvX8kQc_IQ0ns/edit?usp=sharing). We expect the program to be most useful to people with technical backgrounds (e.g. maths, CS, or ML), although the curriculum is intended to be accessible for those who aren't familiar with machine learning, and participants will be put in groups with others from similar backgrounds. **If you're interested in joining the next version of the course (taking place January - March 2022)** [**apply here to be a participant**](https://airtable.com/shr9R2syz8wc2ao7p) **or** [**here to be a facilitator**](https://airtable.com/shr0IO5TTZEY5FFxY)*...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Zmwkz2BMvuFFR8bi3/agi-safety-fundamentals-curriculum-and-application"
    ],
    "tags": [
      "ai",
      "community"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1005825f80a42092",
    "title": "Emergent modularity and safety",
    "year": 2021,
    "category": "public_awareness",
    "description": "Biological neural networks (i.e. brains) and artificial neural networks have sufficient commonalities that it's often reasonable to treat our knowledge about one as a good starting point for reasoning about the other. So one way to predict how the field of neural network interpretability will develop is by looking at how neuroscience interprets the workings of human brains. I think there are several interesting things to be learned from this, but the one I'll focus on in this post is the concept of *modularity*: the fact that different parts of the brain carry out different functions. Neuroscientists have mapped many different skills (such as language use, memory consolidation, and emotional responses) to specific brain regions. Note that this doesn't always give us much direct insight into how the skills themselves work - but it does make follow-up research into those skills much easier. I'll argue that, for the purposes of AGI safety, this type of understanding may also directly e...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/zvEbeZ6opjPJiQnFE/emergent-modularity-and-safety"
    ],
    "tags": [
      "ai",
      "modularity"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c692b2d5e0dcb73a",
    "title": "Epistemic Strategies of Safety-Capabilities Tradeoffs",
    "year": 2021,
    "category": "public_awareness",
    "description": "Introduction: Epistemic Strategies Redux\n========================================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/hWag6E7XPCbdfaoKZ/epistemic-strategies-of-safety-capabilities-tradeoffs"
    ],
    "tags": [
      "ai",
      "ai capabilities",
      "ai risk",
      "tradeoffs"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ec47715d3ff61b7d",
    "title": "General alignment plus human values, or alignment via human values?",
    "year": 2021,
    "category": "public_awareness",
    "description": "*Thanks to Rebecca Gorman for discussions that lead to these insights.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/3e6pmovj6EJ729M2i/general-alignment-plus-human-values-or-alignment-via-human"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c6d24ec4564cc076",
    "title": "Towards Deconfusing Gradient Hacking",
    "year": 2021,
    "category": "policy_development",
    "description": "*[Epistemic status: brainstorming, less confused than last time since it seems to provide for a nice taxonomy of things to look into rather than just a bunch of random loose threads.]*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/u3fP8vjGsDCT7X54H/towards-deconfusing-gradient-hacking"
    ],
    "tags": [
      "ai",
      "gradient hacking",
      "inner alignment",
      "mesa-optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1e1e9d0f48ab01ea",
    "title": "Selfishness, preference falsification, and AI alignment",
    "year": 2021,
    "category": "public_awareness",
    "description": "If aliens were to try to infer human values, there are a few information sources they could start looking at.  One would be individual humans, who would want things on an individual basis.  Another would be expressions of collective values, such as Internet protocols, legal codes of states, and religious laws.  A third would be values that are implied by the presence of functioning minds in the universe at all, such as a value for logical consistency.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ZddY8BZbvoXHEvDHf/selfishness-preference-falsification-and-ai-alignment"
    ],
    "tags": [
      "ai",
      "human values",
      "world optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_adea8ad1da45b846",
    "title": "[AN #168]: Four technical topics for which Open Phil is soliciting grant proposals",
    "year": 2021,
    "category": "policy_development",
    "description": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/AXj9KSvda6XwNwLrS/an-168-four-technical-topics-for-which-open-phil-is"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ca5317adfcf2a974",
    "title": "Request for proposals for projects in AI alignment that work with deep learning systems",
    "year": 2021,
    "category": "public_awareness",
    "description": "As part of our work on reducing [potential risks from advanced artificial intelligence](https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity), Open Philanthropy is seeking proposals for projects working with deep learning systems that could help us understand and make progress on [AI alignment](https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/the-open-phil-ai-fellowship#examples): the problem of creating AI systems more capable than their designers that robustly try to do what their designers intended. We are interested in proposals that fit within certain research directions, described below and given as posts in the rest of [this sequence](https://www.alignmentforum.org/s/Tp3ryR4AxY56ctGh2), that we think could contribute to reducing the risks we are most concerned about.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/H5iePjNKaaYQyZpgR/request-for-proposals-for-projects-in-ai-alignment-that-work"
    ],
    "tags": [
      "ai",
      "grants & fundraising opportunities"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_25a02867ad29d482",
    "title": "Measuring and forecasting risks",
    "year": 2021,
    "category": "policy_development",
    "description": "*By Jacob Steinhardt, with input from Beth Barnes*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/7DhwRLoKm4nMrFFsH/measuring-and-forecasting-risks"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_355d212a093ad987",
    "title": "Interpretability",
    "year": 2021,
    "category": "public_awareness",
    "description": "*Chris Olah wrote the following topic prompt for the Open Phil 2021 request for proposals on the alignment of AI systems. We (Asya Bergal and Nick Beckstead) are running the Open Phil RFP and are posting each section as a sequence on the Alignment Forum. Although Chris wrote this document, we didn't want to commit him to being responsible for responding to comments on it by posting it.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/CzZ6Fch4JSpwCpu6C/interpretability"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_efbc21c7607e2580",
    "title": "Truthful and honest AI",
    "year": 2021,
    "category": "policy_development",
    "description": "*By Owain Evans, with input from Jacob Hilton, Dan Hendrycks, Asya Bergal, Owen Cotton-Barratt, and Rohin Shah*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/sdxZdGFtAwHGFGKhg/truthful-and-honest-ai"
    ],
    "tags": [
      "ai",
      "ai risk",
      "language models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_329e98f1c1cbd69e",
    "title": "A very crude deception eval is already passed",
    "year": 2021,
    "category": "public_awareness",
    "description": "I was thinking about possible evals that would tell us when we're getting to models that are capable of deception. One not-very-good idea I had was just to measure zero-shot understanding of relevant deception scenarios in a language model. I don't think this tells us very much about whether the model is in question is actually trying to deceive us, but it's a tiny bit interesting. Anyway, it seems like large language models look like they can do decent enough deception + theory of mind in a story-like setting that this is mostly already passed.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/22GrdspteQc8EonMn/a-very-crude-deception-eval-is-already-passed"
    ],
    "tags": [
      "ai",
      "ai evaluations",
      "treacherous turn"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f341a70b550117a0",
    "title": "Stuart Russell and Melanie Mitchell on Munk Debates",
    "year": 2021,
    "category": "public_awareness",
    "description": "*Financial status: This is independent research, now supported by a grant.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/GDnFsyfKedevKHAuJ/stuart-russell-and-melanie-mitchell-on-munk-debates"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_befb7ec295714c5a",
    "title": "Apply to the ML for Alignment Bootcamp (MLAB) in Berkeley [Jan 3 - Jan 22]",
    "year": 2021,
    "category": "public_awareness",
    "description": "We ([Redwood Research](https://www.redwoodresearch.org/) and [Lightcone Infrastructure](https://www.lightconeinfrastructure.com/)) are organizing a bootcamp to bring people interested in AI Alignment up-to-speed with the state of modern ML engineering. We expect to invite about 20 technically talented effective altruists for three weeks of intense learning to Berkeley, taught by engineers working at AI Alignment organizations. The curriculum is designed by Buck Shlegeris (Redwood) and Ned Ruggeri (App Academy Co-founder). We will cover all expenses.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/YgpDYjTx7DCEgziG5/apply-to-the-ml-for-alignment-bootcamp-mlab-in-berkeley-jan"
    ],
    "tags": [
      "ai",
      "community",
      "practical"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_77466047310526fd",
    "title": "Modeling the impact of safety agendas",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This post, which deals with how safety research - that is, technical research agendas aiming to reduce AI existential risk - might impact risks from AI, is part 6 in our* [*sequence on Modeling Transformative AI Risk*](https://www.alignmentforum.org/s/aERZoriyHfCqvWkzg)*. In this series of posts, we are presenting a preliminary model of the relationships between key hypotheses in debates about catastrophic risks from AI. Previous posts in this sequence explained how different subtopics of this project, such as* [*AI takeoff*](https://www.alignmentforum.org/s/aERZoriyHfCqvWkzg/p/pGXR2ynhe5bBCCNqn) *and* [*mesa-optimization*](https://www.alignmentforum.org/s/aERZoriyHfCqvWkzg/p/T9oFjteStcE2ijCJi)*, are incorporated into our model.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/t7f6gF2kpafCMw6rv/modeling-the-impact-of-safety-agendas"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d8a9190e5a773b48",
    "title": "Drug addicts and deceptively aligned agents - a comparative analysis",
    "year": 2021,
    "category": "public_awareness",
    "description": "Co-authored by Nadia Mir-Montazeri and Jan Kirchner.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pFXEG9C5m2X5h2yiq/drug-addicts-and-deceptively-aligned-agents-a-comparative"
    ],
    "tags": [
      "ai",
      "ai risk"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_05960e5f9196393d",
    "title": "Comments on OpenPhil's Interpretability RFP",
    "year": 2021,
    "category": "public_awareness",
    "description": "I'm very excited about research that tries to deeply understand how neural networks are thinking, and especially to understand tiny parts of neural networks without too much concern for scalability, as described in [OpenPhil's recent RFP](https://www.alignmentforum.org/posts/CzZ6Fch4JSpwCpu6C/interpretability) or the [Circuits thread on Distill](https://distill.pub/2020/circuits/).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/oWN9fgYnFYJEWdAs9/comments-on-openphil-s-interpretability-rfp"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_becd92f65215697d",
    "title": "What are red flags for Neural Network suffering?",
    "year": 2021,
    "category": "policy_development",
    "description": "Epistemic status: High uncertainty; This is exploratory work; Our goal is to provide possible research directions rather than offering solutions.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Bpw2HXjMa3GaouDnC/what-are-red-flags-for-neural-network-suffering"
    ],
    "tags": [
      "ai",
      "ai risk",
      "neuroscience",
      "suffering",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c1ad46c2335d8cd5",
    "title": "How do we become confident in the safety of a machine learning system?",
    "year": 2021,
    "category": "public_awareness",
    "description": "*Thanks to Rohin Shah, Ajeya Cotra, Richard Ngo, Paul Christiano, Jon Uesato, Kate Woolverton, Beth Barnes, and William Saunders for helpful comments and feedback.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_198fe3304796041a",
    "title": "Possible research directions to improve the mechanistic explanation of neural networks",
    "year": 2021,
    "category": "public_awareness",
    "description": "Why I like the Circuits approach\n================================",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/HiutLvY2x7zrsTQkx/possible-research-directions-to-improve-the-mechanistic"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b6adb5a87398306d",
    "title": "What exactly is GPT-3's base objective?",
    "year": 2021,
    "category": "public_awareness",
    "description": "*[Probably a noob question]*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Nq58w4SiZMjHdAPaX/what-exactly-is-gpt-3-s-base-objective"
    ],
    "tags": [
      "ai",
      "gpt",
      "inner alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7203d61a8d680a71",
    "title": "Discussion with Eliezer Yudkowsky on AGI interventions",
    "year": 2021,
    "category": "policy_development",
    "description": "Thefollowing is a partially redacted and lightly edited transcript of a chat conversation about AGI between Eliezer Yudkowsky and a set of invitees in early September 2021. By default, all other participants are anonymized as \"Anonymous\".",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/CpvyhFy9WvCNsifkY/discussion-with-eliezer-yudkowsky-on-agi-interventions"
    ],
    "tags": [
      "ai",
      "ai risk",
      "existential risk",
      "transcripts",
      "world optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_be3d11a3c37493fa",
    "title": "Why I'm excited about Redwood Research's current project",
    "year": 2021,
    "category": "policy_development",
    "description": "[Redwood Research's current project](https://www.alignmentforum.org/posts/k7oxdbNaGATZbtEg3/redwood-research-s-current-project) is to train a model that completes short snippets of fiction without outputting text where someone gets injured. I'm excited about this direction and wanted to explain why.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pXLqpguHJzxSjDdx7/why-i-m-excited-about-redwood-research-s-current-project"
    ],
    "tags": [
      "ai",
      "redwood research"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_80d5e3793fa00140",
    "title": "Comments on Carlsmith's \"Is power-seeking AI an existential risk?\"",
    "year": 2021,
    "category": "policy_development",
    "description": "The following are some comments I gave on Open Philanthropy Senior Research Analyst Joe Carlsmith's Apr. 2021 \"[Is power-seeking AI an existential risk?](https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit#)\", published with permission and lightly edited. Joe replied; his comments are included inline. I gave a few quick replies in response, that I didn't want to worry about cleaning up; Rob Bensinger has summarized a few of them and those have also been added inline.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential"
    ],
    "tags": [
      "ai",
      "ai timelines",
      "nanotechnology"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9544e210063eff81",
    "title": "What would we do if alignment were futile?",
    "year": 2021,
    "category": "policy_development",
    "description": "This piece, which predates ChatGPT, is no longer endorsed by its author.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Xv77XjuZEkjRsvkJp/what-would-we-do-if-alignment-were-futile"
    ],
    "tags": [
      "ai",
      "ai risk"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8ca703776527939d",
    "title": "Attempted Gears Analysis of AGI Intervention Discussion With Eliezer",
    "year": 2021,
    "category": "technical_research_breakthrough",
    "description": "Recently, a discussion of potential AGI interventions and potential futures [was posted to LessWrong](https://www.lesswrong.com/posts/CpvyhFy9WvCNsifkY/discussion-with-eliezer-yudkowsky-on-agi-interventions). The picture Eliezer presented was broadly consistent with my existing model of Eliezer's model of reality, and most of it was also consistent with my own model of reality.",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/xHnuX42WNZ9hq53bz/attempted-gears-analysis-of-agi-intervention-discussion-with-1"
    ],
    "tags": [
      "ai",
      "gears-level"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d3566036e4035f00",
    "title": "My understanding of the alignment problem",
    "year": 2021,
    "category": "public_awareness",
    "description": "I've been clarifying my own understanding of the alignment problem over the past few months, and wanted to share my first writeups with folks here in case they're useful:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/WckCfXpfrb9Bms8aA/my-understanding-of-the-alignment-problem"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_57f1e8aa142b2c49",
    "title": "Ngo and Yudkowsky on alignment difficulty",
    "year": 2021,
    "category": "policy_development",
    "description": "This post is the first in a series of transcribed Discord conversations between Richard Ngo and Eliezer Yudkowsky, moderated by Nate Soares. We've also added Richard and Nate's running summaries of the conversation (and others' replies) from Google Docs.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/7im8at9PmhbT4JHsW/ngo-and-yudkowsky-on-alignment-difficulty"
    ],
    "tags": [
      "ai",
      "ai risk",
      "ai-assisted/ai automated alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d9a1ab5d908b2543",
    "title": "A positive case for how we might succeed at prosaic AI alignment",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This post is my attempt at something like a response to [Eliezer Yudkowsky's recent discussion on AGI interventions](https://www.lesswrong.com/posts/CpvyhFy9WvCNsifkY/discussion-with-eliezer-yudkowsky-on-agi-interventions).*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/5ciYedyQDDqAcrDLr/a-positive-case-for-how-we-might-succeed-at-prosaic-ai"
    ],
    "tags": [
      "ai",
      "ai success models",
      "outer alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0fc341e460df88dd",
    "title": "Applications for AI Safety Camp 2022 Now Open!",
    "year": 2021,
    "category": "policy_development",
    "description": "If you've read about alignment research and you want to start contributing, the new iteration of the AI Safety Camp is a great opportunity!",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/QeetPm8yvFf7mAGj9/applications-for-ai-safety-camp-2022-now-open"
    ],
    "tags": [
      "ai",
      "ai risk",
      "ai safety camp",
      "community"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e21f2f8dd816f716",
    "title": "Satisficers Tend To Seek Power: Instrumental Convergence Via Retargetability",
    "year": 2021,
    "category": "policy_development",
    "description": "**Summary**:  Why exactly should smart agents tend to usurp their creators? Previous results only apply to optimal agents tending to stay alive and preserve their future options. I extend the power-seeking theorems to apply to many kinds of policy-selection procedures, ranging from planning agents which choose plans with expected utility closest to a randomly generated number, to satisficers, to policies trained by some reinforcement learning algorithms. The key property is not agent optimality--as previously supposed--but is instead the *retargetability of the policy-selection procedure*. These results hint at which kinds of agent cognition and of agent-producing processes are dangerous by default.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/nZY8Np759HYFawdjH/satisficers-tend-to-seek-power-instrumental-convergence-via"
    ],
    "tags": [
      "ai",
      "instrumental convergence",
      "satisficer"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_81f2f03ae927da33",
    "title": "Ngo and Yudkowsky on AI capability gains",
    "year": 2021,
    "category": "policy_development",
    "description": "This is the second post in a series of transcribed conversations about AGI forecasting and alignment. See the [first post](https://www.lesswrong.com/posts/7im8at9PmhbT4JHsW/ngo-and-yudkowsky-on-alignment-difficulty) for prefaces and more information about the format.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/hwxj4gieR7FWNwYfa/ngo-and-yudkowsky-on-ai-capability-gains-1"
    ],
    "tags": [
      "ai",
      "ai governance",
      "ai takeoff",
      "effective altruism",
      "modest epistemology",
      "optimization",
      "recursive self-improvement",
      "utility functions"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0460d70bee5fc146",
    "title": "How To Get Into Independent Research On Alignment/Agency",
    "year": 2021,
    "category": "policy_development",
    "description": "I'm an independent researcher working on AI alignment and the theory of agency. I'm 29 years old, will make about $90k this year, and set my own research agenda. I deal with basically zero academic bullshit - my grant applications each take about one day's attention to write (and decisions typically come back in ~1 month), and I publish the bulk of my work right here on [LessWrong](https://www.lesswrong.com/)/[AF](https://www.alignmentforum.org/). Best of all, I work on some [really](https://www.lesswrong.com/tag/embedded-agency) [cool](https://www.lesswrong.com/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans) [technical](https://www.lesswrong.com/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro) [problems](https://www.lesswrong.com/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents) which I expect are central to the future of humanity.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency"
    ],
    "tags": [
      "ai",
      "careers",
      "practical"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b945bcf834d18813",
    "title": "Goodhart: Endgame",
    "year": 2021,
    "category": "public_awareness",
    "description": "**I - Digression, aggression**",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/dmp9PZjpSSX5NeXHM/goodhart-endgame"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a32e7b60bedcf927",
    "title": "More detailed proposal for measuring alignment of current models",
    "year": 2021,
    "category": "public_awareness",
    "description": "Here's a more detailed proposal for a project someone could do on [measuring](https://docs.google.com/document/d/1cPwcUSl0Y8TyZxCumGPBhdVUN0Yyyw9AR1QshlRI3gc/edit) [alignment](https://www.alignmentforum.org/posts/7Rvctxk73BrKqEaqh/call-for-research-on-evaluating-alignment-funding-advice) in current models.\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/GbAymLbJdGbqTumCN/more-detailed-proposal-for-measuring-alignment-of-current"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_15afd6bda96c9efb",
    "title": "A Certain Formalization of Corrigibility Is VNM-Incoherent",
    "year": 2021,
    "category": "policy_development",
    "description": "Edit, 5/16/23: I think this post is beautiful, correct in its narrow technical claims, and practically irrelevant to alignment. This post presents an unrealistic picture of the role of reward functions in reinforcement learning, conflating \"utility\" with \"reward\" in a type-incorrect fashion. Reward functions are not \"goals\", real-world policies are not \"optimal\", and the mechanistic function of reward is (usually) to provide policy gradients to update the policy network.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/WCX3EwnWAx7eyucqH/a-certain-formalization-of-corrigibility-is-vnm-incoherent"
    ],
    "tags": [
      "ai",
      "corrigibility",
      "instrumental convergence",
      "open problems"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9da9dad787f00f40",
    "title": "From language to ethics by automated reasoning",
    "year": 2021,
    "category": "public_awareness",
    "description": "*Posted also on the* [*EA Forum*](https://forum.effectivealtruism.org/posts/u3a6vuP9GapmKhhjR/from-language-to-ethics-by-automated-reasoning)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/uyvnjaRaKdGXoKrv7/from-language-to-ethics-by-automated-reasoning"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3a415b7a1e3da150",
    "title": "Some real examples of gradient hacking",
    "year": 2021,
    "category": "policy_development",
    "description": "*Speculation and an invitation to make further suggestions or clarifications*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/GPoPKk2wr2r4uPwxe/some-real-examples-of-gradient-hacking"
    ],
    "tags": [
      "adaptation executors",
      "ai",
      "evolution",
      "gradient hacking",
      "mesa-optimization",
      "seri mats"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1a16faa07a137313",
    "title": "Morally underdefined situations can be deadly",
    "year": 2021,
    "category": "public_awareness",
    "description": "**Thanks to Rebecca Gorman for help with this post.**",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/7q6jQ7y9xQNAkbTtt/morally-underdefined-situations-can-be-deadly"
    ],
    "tags": [
      "ai",
      "moral uncertainty",
      "value learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e6d4806bd5d886b0",
    "title": "Yudkowsky and Christiano discuss \"Takeoff Speeds\"",
    "year": 2021,
    "category": "policy_development",
    "description": "This is a transcription of Eliezer Yudkowsky responding to Paul Christiano's [Takeoff Speeds](https://sideways-view.com/2018/02/24/takeoff-speeds/) live on Sep. 14, followed by a conversation between Eliezer and Paul. This discussion took place after Eliezer's [conversation](https://www.lesswrong.com/posts/hwxj4gieR7FWNwYfa/ngo-and-yudkowsky-on-ai-capability-gains-1) with Richard Ngo.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/vwLxd6hhFvPbvKmBH/yudkowsky-and-christiano-discuss-takeoff-speeds"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "ai timelines",
      "forecasting & prediction",
      "general intelligence",
      "inside/outside view"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_82e9c8eba641dc6b",
    "title": "Potential Alignment mental tool: Keeping track of the types",
    "year": 2021,
    "category": "public_awareness",
    "description": "*Epistemic status: Came up with what I was thinking of as a notation. Realised that there was a potentially useful conceptual tool behind it.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/QEHb8tWLztMyvrv6f/potential-alignment-mental-tool-keeping-track-of-the-types"
    ],
    "tags": [
      "ai",
      "aixi"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_81ebfbe85f31075c",
    "title": "Integrating Three Models of (Human) Cognition",
    "year": 2021,
    "category": "public_awareness",
    "description": "You may have heard a few things about \"[predictive processing](https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/)\" or \"[the global neuronal workspace](https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip/p/x4n4jcoDP7xh5LWLq),\" and you may have read some of [Steve Byrnes' excellent posts](https://www.alignmentforum.org/users/steve2152) about [what's going on computationally in the human brain](https://www.alignmentforum.org/posts/diruo47z32eprenTg/my-computational-framework-for-the-brain). But how does it all fit together? How can we begin to arrive at a unified computational picture of human intelligence, and how can it inform our efforts in AI safety and alignment? In this post, I endeavor to take steps towards these ends by integrating insights from three computational frameworks for modeling what's going on in the human brain, with the hope of laying an adequate foundation for more precisely talking about how the brain cognitively implements goal-directed behaviors:",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/6chtMKXpLcJ26t7n5/integrating-three-models-of-human-cognition"
    ],
    "tags": [
      "ai",
      "consciousness",
      "intentionality",
      "motivations",
      "neocortex",
      "neuroscience",
      "rationality",
      "subagents",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9ef84728ce9e1318",
    "title": "AI Safety Needs Great Engineers",
    "year": 2021,
    "category": "public_awareness",
    "description": "**Top line: If you think you could write a substantial pull request for a major machine learning library, then major AI safety labs want to interview you** ***today*****.**",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/YDF7XhMThhNfHfim9/ai-safety-needs-great-engineers"
    ],
    "tags": [
      "ai",
      "careers",
      "community"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2eabc76154d4dcd1",
    "title": "AI Tracker: monitoring current and near-future risks from superscale models",
    "year": 2021,
    "category": "public_awareness",
    "description": "**TLDR:** We've put together a website to track recent releases of superscale models, and comment on the immediate and near-term safety risks they may pose. The website is little more than a view of an Airtable spreadsheet at the moment, but we'd greatly appreciate any feedback you might have on the content. Check it out at [aitracker.org](http://aitracker.org).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/eELny7y7JtCs6fQaA/ai-tracker-monitoring-current-and-near-future-risks-from"
    ],
    "tags": [
      "ai",
      "ai capabilities",
      "ai governance",
      "ai risk",
      "ai timelines"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_acf1f304a8a10fde",
    "title": "[AN #169]: Collaborating with humans without human data",
    "year": 2021,
    "category": "policy_development",
    "description": "Listen to this newsletter on **[The Alignment Newsletter Podcast](http://alignment-newsletter.libsyn.com/)**.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/BT7yLvSdyuerqzPpc/an-169-collaborating-with-humans-without-human-data"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_30956b8a3f0775f6",
    "title": "Christiano, Cotra, and Yudkowsky on AI progress",
    "year": 2021,
    "category": "policy_development",
    "description": "This post is a transcript of a discussion between Paul Christiano, Ajeya Cotra, and Eliezer Yudkowsky on AGI forecasting, following up on Paul and Eliezer's [\"Takeoff Speeds\" discussion](https://www.lesswrong.com/s/n945eovrA3oDueqtq/p/vwLxd6hhFvPbvKmBH).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/7MCqRnZzvszsxgtJi/christiano-cotra-and-yudkowsky-on-ai-progress"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "ai timelines",
      "forecasting & prediction",
      "inside/outside view",
      "technological forecasting"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_32d51265cbf03e5e",
    "title": "EfficientZero: How It Works",
    "year": 2021,
    "category": "policy_development",
    "description": "The goal of this essay is to help you understand [EfficientZero](https://github.com/YeWR/EfficientZero), a reinforcement learning agent that obtains better-than-human median performance on a set of 26 Atari games after just two hours of real-time experience playing each game.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/mRwJce3npmzbKfxws/efficientzero-how-it-works"
    ],
    "tags": [
      "ai",
      "ai capabilities",
      "efficientzero",
      "machine learning  (ml)",
      "reinforcement learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4e14ac4852602238",
    "title": "larger language models may disappoint you [or, an eternally unfinished draft]",
    "year": 2021,
    "category": "public_awareness",
    "description": "what this post is\n=================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pv7Qpu8WSge8NRbpB/larger-language-models-may-disappoint-you-or-an-eternally"
    ],
    "tags": [
      "ai",
      "gpt",
      "language models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2aa212eb12217a13",
    "title": "Solve Corrigibility Week",
    "year": 2021,
    "category": "public_awareness",
    "description": "A low-hanging fruit for solving alignment is to dedicate a chunk of time actually trying to solve a sub-problem collectively.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Lv3emECEjkCSHG7L7/solve-corrigibility-week"
    ],
    "tags": [
      "ai",
      "corrigibility"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_61dd2a2cff141a0e",
    "title": "Comments on Allan Dafoe on AI Governance",
    "year": 2021,
    "category": "policy_development",
    "description": "*Financial status: This is independent research, now supported by a grant.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/rjvLpRzd8mqDyZmcF/comments-on-allan-dafoe-on-ai-governance"
    ],
    "tags": [
      "ai",
      "ai governance"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_05cacbf2af48063e",
    "title": "Soares, Tallinn, and Yudkowsky discuss AGI cognition",
    "year": 2021,
    "category": "policy_development",
    "description": "This is a collection of follow-up discussions in the wake of Richard Ngo and Eliezer Yudkowsky's [Sep. 5-8](https://www.lesswrong.com/posts/7im8at9PmhbT4JHsW/ngo-and-yudkowsky-on-alignment-difficulty) and [Sep. 14](https://www.lesswrong.com/s/n945eovrA3oDueqtq/p/hwxj4gieR7FWNwYfa) conversations.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/oKYWbXioKaANATxKY/soares-tallinn-and-yudkowsky-discuss-agi-cognition"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "general intelligence",
      "treacherous turn"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1a40b320e2cd1dff",
    "title": "Visible Thoughts Project and Bounty Announcement",
    "year": 2021,
    "category": "policy_development",
    "description": "(**Update Jan. 12**: We released an [FAQ](https://docs.google.com/document/d/1sxNOxvcBWw7XC6MSUUpAjO4Rbu3VcUmCy7PJWQ9Vh5o/edit) last month, with more details. Last updated Jan. 7.)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/zRn6cLtxyNodudzhw/visible-thoughts-project-and-bounty-announcement"
    ],
    "tags": [
      "ai",
      "bounties & prizes (active)",
      "project announcement"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_55edfb8dbf7a2393",
    "title": "My take on higher-order game theory",
    "year": 2021,
    "category": "technical_research_breakthrough",
    "description": "This is how I currently think about [higher-order game theory](https://www.alignmentforum.org/posts/FPML8k4QtjJxk3Y4M/confusions-re-higher-level-game-theory), the study of agents thinking about agents thinking about agents....",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Cw84NJXAma85AmpnH/my-take-on-higher-order-game-theory"
    ],
    "tags": [
      "domain theory",
      "game theory",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5c1c4c2061d2ada6",
    "title": "Infra-Bayesian physicalism: a formal theory of naturalized induction",
    "year": 2021,
    "category": "policy_development",
    "description": "*This is joint work by Vanessa Kosoy and Alexander \"Diffractor\" Appel.*\n*For the proofs, see [1](https://www.alignmentforum.org/posts/cj3PRu8QoFm4BA8oc/infra-bayesian-physicalism-proofs-part-i) and [2](https://www.alignmentforum.org/posts/CPr8bRGekTyvh7nGC/infra-bayesian-physicalism-proofs-part-ii).*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/gHgs2e2J5azvGFatb/infra-bayesian-physicalism-a-formal-theory-of-naturalized"
    ],
    "tags": [
      "ai",
      "infra-bayesianism"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ef34a2c0213a72f2",
    "title": "Infra-Bayesian physicalism: proofs part I",
    "year": 2021,
    "category": "technical_research_breakthrough",
    "description": "*This post is an appendix to \"[Infra-Bayesian physicalism: a formal theory of naturalized induction](https://www.alignmentforum.org/posts/gHgs2e2J5azvGFatb/infra-bayesian-physicalism-a-formal-theory-of-naturalized)\".*",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/cj3PRu8QoFm4BA8oc/infra-bayesian-physicalism-proofs-part-i"
    ],
    "tags": [
      "infra-bayesianism",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_572cb5340b036136",
    "title": "Infra-Bayesian physicalism: proofs part II",
    "year": 2021,
    "category": "technical_research_breakthrough",
    "description": "*This post is an appendix to \"[Infra-Bayesian physicalism: a formal theory of naturalized induction](https://www.alignmentforum.org/posts/gHgs2e2J5azvGFatb/infra-bayesian-physicalism-a-formal-theory-of-naturalized)\".*",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/CPr8bRGekTyvh7nGC/infra-bayesian-physicalism-proofs-part-ii"
    ],
    "tags": [
      "infra-bayesianism",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_5910429cff09cd9e",
    "title": "AXRP Episode 12 - AI Existential Risk with Paul Christiano",
    "year": 2021,
    "category": "policy_development",
    "description": "[Google Podcasts link](https://podcasts.google.com/feed/aHR0cHM6Ly9heHJwb2RjYXN0LmxpYnN5bi5jb20vcnNz/episode/NmM5NzI3MjUtYWU3MC00YjgwLTg0MTgtODM2MzgxMmNlZGJk)",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/krsjmpDB4kgDq6pdu/axrp-episode-12-ai-existential-risk-with-paul-christiano"
    ],
    "tags": [
      "ai",
      "audio",
      "axrp",
      "existential risk",
      "interviews",
      "outer alignment"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_933db021caf33abd",
    "title": "Morality is Scary",
    "year": 2021,
    "category": "public_awareness",
    "description": "I'm worried that many AI alignment researchers and other LWers have a view of how human morality works, that really only applies to a small fraction of all humans (notably moral philosophers and themselves). In this view, people know or at least suspect that they are confused about morality, and are eager or willing to apply reason and deliberation to find out what their real values are, or to correct their moral beliefs. Here's [an example](https://www.greaterwrong.com/posts/FSmPtu7foXwNYpWiB/on-the-limits-of-idealized-values) of someone who fits this view:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/y5jAuKqkShdjMNZab/morality-is-scary"
    ],
    "tags": [
      "ai",
      "ethics & morality",
      "human-ai safety"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_8c8ecad500ce185a",
    "title": "Sydney AI Safety Fellowship",
    "year": 2021,
    "category": "policy_development",
    "description": "I'm excited to announce the launch of the Sydney AI Safety Fellowship which will provide fellows from Australia and New Zealand the opportunity to pursue projects in AI Safety or spend time upskilling. These projects may be technical projects, projects related to policy, or movement building projects.  \n  \nThe fellowship will take place at WeWork in Sydney which provides fantastic views, plus free coffee, beer and energy drinks. It will take place during the 7 weeks 10th of January to the 25th of February, but there will be the option to start a week earlier if so desired.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/jotEekAQxmrwcMf9e/sydney-ai-safety-fellowship"
    ],
    "tags": [
      "ai",
      "community"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_47a22fa911712a1a",
    "title": "$100/$50 rewards for good references",
    "year": 2021,
    "category": "public_awareness",
    "description": "*With thanks to Rohin Shah.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ex2qcux8TQXigGAfv/usd100-usd50-rewards-for-good-references"
    ],
    "tags": [
      "ai",
      "reward functions"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_dbbb1cc15f2ba41a",
    "title": "[Linkpost] A General Language Assistant as a Laboratory for Alignment",
    "year": 2021,
    "category": "public_awareness",
    "description": "This is a linkpost for a recent paper from Anthropic: [A General Language Assistant as a Laboratory for Alignment](https://arxiv.org/abs/2112.00861).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/dktT3BiinsBZLw96h/linkpost-a-general-language-assistant-as-a-laboratory-for"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7e8d75ca634b5d9f",
    "title": "Shulman and Yudkowsky on AI progress",
    "year": 2021,
    "category": "policy_development",
    "description": "This post is a transcript of a discussion between Carl Shulman and Eliezer Yudkowsky, following up on [a conversation with Paul Christiano and Ajeya Cotra](https://www.lesswrong.com/posts/7MCqRnZzvszsxgtJi/christiano-cotra-shulman-and-yudkowsky-on-ai-progress).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/sCCdCLPN9E3YvdZhj/shulman-and-yudkowsky-on-ai-progress"
    ],
    "tags": [
      "ai",
      "ai takeoff",
      "ai timelines"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_31728989c28401bd",
    "title": "Misc. questions about EfficientZero",
    "year": 2021,
    "category": "public_awareness",
    "description": "Perhaps these can be thought of as homework questions -- when I imagine us successfully making AI go well, I imagine us building expertise such that we can answer these questions quickly and easily. Before I read the answers I'm going to think for 10min or so about each one and post my own guesses.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/rtBpBNgXjwtsLJDbG/misc-questions-about-efficientzero-1"
    ],
    "tags": [
      "ai",
      "efficientzero"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_7a69975ac358d6de",
    "title": "Agency: What it is and why it matters",
    "year": 2021,
    "category": "public_awareness",
    "description": "*[ETA: I'm deprioritizing completing this sequence because it seems that other people are writing good similar stuff. In particular, see e.g. <https://www.lesswrong.com/posts/kpPnReyBC54KESiSn/optimality-is-the-tiger-and-agents-are-its-teeth> and <https://www.lesswrong.com/posts/pdJQYxCy29d7qYZxG/agency-and-coherence> ]*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qJBkcGW4GitfQ4BBy/agency-what-it-is-and-why-it-matters"
    ],
    "tags": [
      "agency",
      "ai",
      "rationality"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3b3270a5f8eebb7b",
    "title": "Behavior Cloning is Miscalibrated",
    "year": 2021,
    "category": "policy_development",
    "description": "Behavior cloning (BC) is, put simply, when you have a bunch of human expert demonstrations and you train your policy to maximize likelihood over the human expert demonstrations. It's the simplest possible approach under the broader umbrella of Imitation Learning, which also includes more complicated things like Inverse Reinforcement Learning or Generative Adversarial Imitation Learning. Despite its simplicity, it's a fairly strong baseline. In fact, prompting GPT-3 to act agent-y is essentially also BC, just rather than cloning on a specific task, you're cloning against all of the task demonstration-like data in the training set--but fundamentally, it's a scaled up version of the exact same thing. The problem with BC that leads to miscalibration is that the human demonstrator may know more or less than the model, which would result in the model systematically being over/underconfident for its own knowledge and abilities.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/BgoKdAzogxmgkuuAt/behavior-cloning-is-miscalibrated"
    ],
    "tags": [
      "ai",
      "calibration",
      "machine learning  (ml)",
      "outer alignment",
      "reinforcement learning"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_fd8c6444de11fb40",
    "title": "Interpreting Yudkowsky on Deep vs Shallow Knowledge",
    "year": 2021,
    "category": "policy_development",
    "description": "*Here is an exploration of what Eliezer Yudkowsky means when he writes about deep vs shallow patterns (although I'll be using \"knowledge\" instead of \"pattern\" for reasons explained in the next section). Not about any specific pattern Yudkowsky is discussing, mind you, about what deep and shallow patterns are at all. In doing so, I don't make any criticism of his ideas and instead focus on quoting him (seriously, this post is like 70% quotes) and interpreting him by finding the best explanation I can of his words (that still fit them, obviously). Still, there's a risk that my interpretation misses some of his points and ideas-- I'm building a lower-bound on his argument's power that is as high as I can get, not an upper-bound. Also, I might just be completely wrong, in which case defer to Yudkowsky if he points out that I'm completely missing the point.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/GSBCw94DsxLgDat6r/interpreting-yudkowsky-on-deep-vs-shallow-knowledge"
    ],
    "tags": [
      "ai",
      "ai capabilities"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_716cacc7558e742f",
    "title": "Are limited-horizon agents a good heuristic for the off-switch problem?",
    "year": 2021,
    "category": "public_awareness",
    "description": "(This is my first post, sorry if this is covered elsewhere.)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/itTLCFj5NCHhFbK2Q/are-limited-horizon-agents-a-good-heuristic-for-the-off"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_bcb28e29f0752ada",
    "title": "ML Alignment Theory Program under Evan Hubinger",
    "year": 2021,
    "category": "public_awareness",
    "description": "In the past six weeks, the Stanford Existential Risks Initiative (SERI) has been running a trial for the \"ML Alignment Theory Scholars\" (MATS) program. Our goal is to increase the number of people working on alignment theory, and to do this, we're running a scholars program that provides mentorship, funding, and community to promising new alignment theorists. This program is run in partnership with Evan Hubinger, who has been providing all of the mentorship to each of the scholars for their trial.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FpokmCnbP3CEZ5h4t/ml-alignment-theory-program-under-evan-hubinger"
    ],
    "tags": [
      "ai",
      "ai alignment fieldbuilding",
      "project announcement"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f5794e8b7bcfd27a",
    "title": "A Framework to Explain Bayesian Models",
    "year": 2021,
    "category": "public_awareness",
    "description": "[Bayesian Networks](https://en.wikipedia.org/wiki/Bayesian_network#:~:text=A%20Bayesian%20network%20(also%20known,directed%20acyclic%20graph%20(DAG).) are used to represent uncertainty and probabilistic relations between variables. They have an appealing graphical representation and a clear statistical meaning, which makes them a popular approach to manage uncertainty.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/SPfZiEwHotPncJBLz/a-framework-to-explain-bayesian-models"
    ],
    "tags": [
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0d6e227fda9784f4",
    "title": "Modeling Failure Modes of High-Level Machine Intelligence",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This post, which deals with some widely discussed failure modes of transformative AI, is part 7 in our* [*sequence on Modeling Transformative AI Risk*](https://www.alignmentforum.org/s/aERZoriyHfCqvWkzg)*. In this series of posts, we are presenting a preliminary model of the relationships between key hypotheses in debates about catastrophic risks from AI. Previous posts in this sequence explained how different subtopics of this project, such as* [*mesa-optimization*](https://www.alignmentforum.org/s/aERZoriyHfCqvWkzg/p/T9oFjteStcE2ijCJi) *and* [*safety research agendas*](https://www.alignmentforum.org/posts/t7f6gF2kpafCMw6rv/modeling-the-impact-of-safety-agendas)*, are incorporated into our model.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/3Eq5Rq5uQ97kt8B8f/modeling-failure-modes-of-high-level-machine-intelligence"
    ],
    "tags": [
      "ai",
      "ai risk",
      "threat models",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d696fa5d6209bc85",
    "title": "Are there alternative to solving value transfer and extrapolation?",
    "year": 2021,
    "category": "public_awareness",
    "description": "*Thanks to Rebecca Gorman for the discussions that inspired these ideas.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/DjTKMEwRqpuKkJzTo/are-there-alternative-to-solving-value-transfer-and"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_483a513fbb763c54",
    "title": "Considerations on interaction between AI and expected value of the future",
    "year": 2021,
    "category": "policy_development",
    "description": "Some thoughts about the 'default' trajectory of civilisation and how AI will affect the likelihood of different outcomes.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Dr3owdPqEAFK4pq8S/considerations-on-interaction-between-ai-and-expected-value"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Tangentially related to core safety concerns",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0e931ec789fa9d6b",
    "title": "Theoretical Neuroscience For Alignment Theory",
    "year": 2021,
    "category": "policy_development",
    "description": "*This post was written under Evan Hubinger's direct guidance and mentorship, as a part of the* [*Stanford Existential Risks Institute ML Alignment Theory Scholars (MATS) program*](https://www.alignmentforum.org/posts/FpokmCnbP3CEZ5h4t/ml-alignment-theory-program-under-evan-hubinger)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/ZJY3eotLdfBPCLP3z/theoretical-neuroscience-for-alignment-theory"
    ],
    "tags": [
      "ai",
      "inner alignment",
      "neuroscience",
      "seri mats"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_cf60dc934588ea9f",
    "title": "Some thoughts on why adversarial training might be useful",
    "year": 2021,
    "category": "public_awareness",
    "description": "### What are the reasons we might want to do adversarial training?",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Gi8HPM8iYZcdAEteJ/some-thoughts-on-why-adversarial-training-might-be-useful"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2483be04da9f735d",
    "title": "[AN #170]: Analyzing the argument for risk from power-seeking AI",
    "year": 2021,
    "category": "policy_development",
    "description": "Listen to this newsletter on **[The Alignment Newsletter Podcast](http://alignment-newsletter.libsyn.com/)**.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/PC6QavgNDQjHbutAq/an-170-analyzing-the-argument-for-risk-from-power-seeking-ai"
    ],
    "tags": [
      "ai",
      "newsletters",
      "power seeking (ai)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4e681425d8ec08f0",
    "title": "Finding the multiple ground truths of CoinRun and image classification",
    "year": 2021,
    "category": "public_awareness",
    "description": "Research projects\n-----------------",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/oCWk8QpjgyqbFHKtK/finding-the-multiple-ground-truths-of-coinrun-and-image"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e6892d68f153b886",
    "title": "Introduction to inaccessible information",
    "year": 2021,
    "category": "policy_development",
    "description": "*This post was written under Evan Hubinger's direct guidance and mentorship, as a part of the [Stanford Existential Risks Institute ML Alignment Theory Scholars (MATS) program](https://www.lesswrong.com/posts/FpokmCnbP3CEZ5h4t/ml-alignment-theory-program-under-evan-hubinger).*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/CYKeDjD7FEvAnzBBF/introduction-to-inaccessible-information"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2d833750b9457d59",
    "title": "Supervised learning and self-modeling: What's \"superhuman?\"",
    "year": 2021,
    "category": "policy_development",
    "description": "Research publication: Supervised learning and self-modeling: What's \"superhuman?\"",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/pz84sQKsgg3GBHQpd/supervised-learning-and-self-modeling-what-s-superhuman"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Background research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_258dc14301bd7b1f",
    "title": "[MLSN #2]: Adversarial Training",
    "year": 2021,
    "category": "public_awareness",
    "description": "As part of a larger community building effort, I am writing a safety newsletter which is designed to cover empirical safety research and be palatable to the broader machine learning research community. You can [subscribe here](https://newsletter.mlsafety.org/) or follow the newsletter on [twitter](https://twitter.com/ml_safety) here.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/7GQZyooNi5nqgoyyJ/mlsn-2-adversarial-training"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_edcca076ef370bed",
    "title": "Conversation on technology forecasting and gradualism",
    "year": 2021,
    "category": "public_awareness",
    "description": "This post is a transcript of a multi-day discussion between Paul Christiano, Richard Ngo, Eliezer Yudkowsky, Rob Bensinger, Holden Karnofsky, Rohin Shah, Carl Shulman, Nate Soares, and Jaan Tallinn, following up on the Yudkowsky/Christiano debate in [1](https://www.lesswrong.com/posts/vwLxd6hhFvPbvKmBH/yudkowsky-and-christiano-discuss-takeoff-speeds), [2](https://www.lesswrong.com/posts/7MCqRnZzvszsxgtJi/christiano-cotra-shulman-and-yudkowsky-on-ai-progress), [3](https://www.lesswrong.com/posts/sCCdCLPN9E3YvdZhj/shulman-and-yudkowsky-on-ai-progress), and [4](https://www.lesswrong.com/posts/fS7Zdj2e2xMqE6qja/more-christiano-cotra-and-yudkowsky-on-ai-progress).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/nPauymrHwpoNr6ipx/conversation-on-technology-forecasting-and-gradualism"
    ],
    "tags": [
      "ai takeoff",
      "ai timelines",
      "progress studies",
      "technological forecasting"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_667fcae760f32ff2",
    "title": "The Promise and Peril of Finite Sets",
    "year": 2021,
    "category": "public_awareness",
    "description": "Has this ever happened to you?",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qLaShfcnXGnYeKFJW/the-promise-and-peril-of-finite-sets"
    ],
    "tags": [
      "logic & mathematics",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_dc5380225685c309",
    "title": "There is essentially one best-validated theory of cognition.",
    "year": 2021,
    "category": "public_awareness",
    "description": "There are many theories of cognition. But if you want to work within a framework with the following properties:",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/NB9QrBa335GDijuyn/there-is-essentially-one-best-validated-theory-of-cognition"
    ],
    "tags": [
      "cognitive science",
      "world modeling"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9271d4fa53466cca",
    "title": "Understanding Gradient Hacking",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This post was written under Evan Hubinger's direct guidance and mentorship, as a part of the* [*Stanford Existential Risks Institute ML Alignment Theory Scholars (MATS) program*](https://www.lesswrong.com/posts/FpokmCnbP3CEZ5h4t/ml-alignment-theory-program-under-evan-hubinger)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/bdayaswyewjxxrQmB/understanding-gradient-hacking"
    ],
    "tags": [
      "ai",
      "gradient hacking",
      "inner alignment",
      "mesa-optimization",
      "optimization"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_4e266af91b9a601a",
    "title": "The Plan",
    "year": 2021,
    "category": "policy_development",
    "description": "This is a high-level overview of the reasoning behind my research priorities, written as a Q&A.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/3L46WGauGpr7nYubu/the-plan"
    ],
    "tags": [
      "ai",
      "deconfusion",
      "research agendas"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_0f5e944dc9cbdabb",
    "title": "Moore's Law, AI, and the pace of progress",
    "year": 2021,
    "category": "public_awareness",
    "description": "It seems to be a minority view nowadays to believe in Moore's Law, the routine doubling of transistor density roughly every couple of years, or even the much gentler claim, that [There's Plenty [more] Room at the Bottom](https://en.wikipedia.org/wiki/There%27s_Plenty_of_Room_at_the_Bottom). There's even a quip for it: *the number of people predicting the death of Moore's law doubles every two years*. This is not merely a populist view by the uninformed. Jensen Huang, CEO of NVIDIA, a GPU company, has talked about Moore's Law failing.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/aNAFrGbzXddQBMDqh/moore-s-law-ai-and-the-pace-of-progress"
    ],
    "tags": [
      "ai",
      "computer science",
      "moore's law",
      "nanotechnology"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_dbab4dfd016e9fce",
    "title": "Transforming myopic optimization to ordinary optimization - Do we want to seek convergence for myopic optimization problems?",
    "year": 2021,
    "category": "public_awareness",
    "description": "While reading some papers, I found a way to take a [myopic](https://www.lesswrong.com/tag/myopia) optimization problem L.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!...",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Ayt24gxcjfY3tDmwK/transforming-myopic-optimization-to-ordinary-optimization-do"
    ],
    "tags": [
      "ai",
      "myopia",
      "optimization"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ea906b55b438ea9b",
    "title": "Some abstract, non-technical reasons to be non-maximally-pessimistic about AI alignment",
    "year": 2021,
    "category": "policy_development",
    "description": "I basically agree with Eliezer's picture of things in the [AGI interventions post](https://www.lesswrong.com/posts/CpvyhFy9WvCNsifkY/discussion-with-eliezer-yudkowsky-on-agi-interventions).",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/vT4tsttHgYJBoKi4n/some-abstract-non-technical-reasons-to-be-non-maximally"
    ],
    "tags": [
      "ai",
      "ai risk"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1ca705c2acff9552",
    "title": "Redwood's Technique-Focused Epistemic Strategy",
    "year": 2021,
    "category": "policy_development",
    "description": "Imagine you're part of a team of ML engineers and research scientists, and you want to help with alignment. Everyone is ready to jump in the fray; there's only one problem -- how are you supposed to do applied research when you don't really know how AGI will be built, what it will look like, not even the architecture or something like that? What you have is the current state of ML, and a lot of conceptual and theoretical arguments.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/2xrBxhRhde7Xddt38/redwood-s-technique-focused-epistemic-strategy"
    ],
    "tags": [
      "ai",
      "redwood research"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ed83df6d87a662d8",
    "title": "Hard-Coding Neural Computation",
    "year": 2021,
    "category": "public_awareness",
    "description": "Previously: [Teaser: Hard-coding Transformer Models](https://www.lesswrong.com/posts/Lq6jo5j9ty4sezT7r/teaser-hard-coding-transformer-models)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/HkghiK6Rt35nbgwKA/hard-coding-neural-computation"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)",
      "language models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_e7a9c88cdadf5235",
    "title": "Summary of the Acausal Attack Issue for AIXI",
    "year": 2021,
    "category": "public_awareness",
    "description": "**Attention conservation notice:** To a large extent, this is redundant with [Paul's previous post about](https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/) [this](this{]}(https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-lo,), but I figured that some people might be interested in my restatement of the argument in my own words, as I did not start out believing that it was an issue, or start out agreeing with Vanessa about the attack operating via [bridge rules.](https://www.alignmentforum.org/posts/gHgs2e2J5azvGFatb/infra-bayesian-physicalism-a-formal-theory-of-naturalized)",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/YbahERfcjTu7LZNQ6/summary-of-the-acausal-attack-issue-for-aixi"
    ],
    "tags": [
      "ai",
      "ai risk",
      "aixi",
      "solomonoff induction"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d4b929360a7722a0",
    "title": "Understanding and controlling auto-induced distributional shift",
    "year": 2021,
    "category": "policy_development",
    "description": "*This post was written under Evan Hubinger's direct guidance and mentorship, as a part of the* [*Stanford Existential Risks Initiative ML Alignment Theory Scholars (MATS) program*](https://www.lesswrong.com/posts/FpokmCnbP3CEZ5h4t/ml-alignment-theory-program-under-evan-hubinger).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/rTYGMbmEsFkxyyXuR/understanding-and-controlling-auto-induced-distributional"
    ],
    "tags": [
      "ai",
      "machine learning  (ml)",
      "myopia",
      "seri mats"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_90c07c6e3ab2e50a",
    "title": "Solving Interpretability Week",
    "year": 2021,
    "category": "public_awareness",
    "description": "For original motivation, see [solving corrigibility week](https://www.lesswrong.com/posts/Lv3emECEjkCSHG7L7/solve-corrigibility-week). I'll state what I learned from corrigibility week, why I didn't post one last week, and the updated format for interpretability.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/jbdDxmhxBygDqbQMD/solving-interpretability-week"
    ],
    "tags": [
      "ai",
      "interpretability (ml & ai)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a33f85fbeebf1bba",
    "title": "Language Model Alignment Research Internships",
    "year": 2021,
    "category": "public_awareness",
    "description": "I'm [Ethan Perez](https://ethanperez.net/), a final year PhD student at NYU, working on aligning language models with human preferences. I'm looking to hire research interns to work on projects in this space, starting early 2022. I expect candidates to have strong software engineering ability, for ML engineering (e.g., to finetune GPT2 to good performance on a new task) or data engineering (e.g., to quickly find high quality subsets of text within petabytes of Common Crawl data). Ideal candidates will have experience doing ML and/or NLP research, reading papers, and coming up with ideas to test. I'm looking for people who'd be able to work full-time (remotely), with compensation of $70/hour. I expect each project to take 4-8 months to complete and lead to a first-author publication at a top-tier ML or NLP conference.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/vHcGGrnzcshybrCJD/language-model-alignment-research-internships"
    ],
    "tags": [
      "ai",
      "language models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_dbdacb864176b93c",
    "title": "Consequentialism & corrigibility",
    "year": 2021,
    "category": "public_awareness",
    "description": "Background 1: Preferences-over-future-states (a.k.a. consequentialism) vs ~~Preferences-over-trajectories~~ other kinds of preferences\n======================================================================================================================================",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/KDMLJEXTWtkZWheXt/consequentialism-and-corrigibility"
    ],
    "tags": [
      "ai",
      "corrigibility",
      "utility functions"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_046f1cbaaae13a0c",
    "title": "Interlude: Agents as Automobiles",
    "year": 2021,
    "category": "public_awareness",
    "description": "I ended up writing a long rant about agency in my review of Joe Carlsmith's report on x-risk from power-seeking AI. I've polished the rant a bit and posted it into this sequence. The central analogy between APS-AI and self-propelled machines (\"Auto-mobiles\") is a fun one, and I suspect the analogy runs much deeper than I've explored so far.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/cxkwQmys6mCB6bjDA/interlude-agents-as-automobiles"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d081be6f16c9dcec",
    "title": "ARC is hiring!",
    "year": 2021,
    "category": "technical_research_breakthrough",
    "description": "The [Alignment Research Center](https://alignmentresearchcenter.org/) is hiring researchers; if you are interested, please apply!",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/dLoK6KGcHAoudtwdo/arc-is-hiring"
    ],
    "tags": [
      "ai",
      "alignment research center",
      "community"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_2812f20a00bd120b",
    "title": "ARC's first technical report: Eliciting Latent Knowledge",
    "year": 2021,
    "category": "public_awareness",
    "description": "ARC has published a report on [Eliciting Latent Knowledge](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit?usp=sharing), an open problem which we believe is central to alignment. We think reading this report is the clearest way to understand what problems we are working on, how they fit into our plan for solving alignment in the worst case, and our research methodology.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge"
    ],
    "tags": [
      "ai",
      "alignment research center",
      "eliciting latent knowledge (elk)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_d76cb64c3406f0c2",
    "title": "Should we rely on the speed prior for safety?",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This post was written under Evan Hubinger's direct guidance and mentorship, as a part of the* [*Stanford Existential Risks Institute ML Alignment Theory Scholars (MATS) program*](https://www.lesswrong.com/posts/FpokmCnbP3CEZ5h4t/ml-alignment-theory-program-under-evan-hubinger)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/iALu99gYbodt4mLqg/should-we-rely-on-the-speed-prior-for-safety"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_dcd7d7cfdf913787",
    "title": "Ngo's view on alignment difficulty",
    "year": 2021,
    "category": "policy_development",
    "description": "This post features a write-up by Richard Ngo on his views, with inline comments.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/gf9hhmSvpZfyfS34B/ngo-s-view-on-alignment-difficulty"
    ],
    "tags": [
      "ai",
      "ai governance"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_329e36a9b878be1b",
    "title": "The Natural Abstraction Hypothesis: Implications and Evidence",
    "year": 2021,
    "category": "technical_research_breakthrough",
    "description": "*This post was written under Evan Hubinger's direct guidance and mentorship, as a part of the [Stanford Existential Risks Institute ML Alignment Theory Scholars (MATS) program](https://www.lesswrong.com/posts/FpokmCnbP3CEZ5h4t/ml-alignment-theory-program-under-evan-hubinger).*",
    "impacts": [
      {
        "variable": "research",
        "change": 15,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Fut8dtFsBYRz8atFF/the-natural-abstraction-hypothesis-implications-and-evidence"
    ],
    "tags": [
      "abstraction",
      "ai",
      "has diagram",
      "interpretability (ml & ai)",
      "natural abstraction",
      "seri mats"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f7c9a137407fbfec",
    "title": "My Overview of the AI Alignment Landscape: A Bird's Eye View",
    "year": 2021,
    "category": "policy_development",
    "description": "***Disclaimer:** I recently started as an interpretability researcher at Anthropic, but I wrote this doc before starting, and it entirely represents my personal views not those of my employer*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/SQ9cZtfrzDJmw9A2m/my-overview-of-the-ai-alignment-landscape-a-bird-s-eye-view"
    ],
    "tags": [
      "ai",
      "ai risk",
      "ai takeoff",
      "ai timelines",
      "coordination / cooperation",
      "debate (ai safety technique)",
      "existential risk",
      "inner alignment",
      "interpretability (ml & ai)",
      "iterated amplification"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_61d23a26ed2019bc",
    "title": "Universality and the \"Filter\"",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This post was written under Evan Hubinger's direct guidance and mentorship, as a part of the* [*Stanford Existential Risks Institute ML Alignment Theory Scholars (MATS) program*](https://www.lesswrong.com/posts/FpokmCnbP3CEZ5h4t/ml-alignment-theory-program-under-evan-hubinger)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/R67qpBj5doGcjQzmc/universality-and-the-filter"
    ],
    "tags": [
      "ai",
      "ai risk",
      "humans consulting hch"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_aa5c7992702a5cec",
    "title": "Motivations, Natural Selection, and Curriculum Engineering",
    "year": 2021,
    "category": "policy_development",
    "description": "*Epistemic status: I am professionally familiar with contemporary machine learning theory and practice, and have a long-standing amateur interest in natural history. But (despite being a long-term lurker on LW and AF) I feel very uncertain about the nature and status of goals and motivations! This post is an attempt, heavily inspired by Richard Ngo's* [Shaping Safer Goals sequence](https://www.lesswrong.com/s/boLPsyNwd6teK5key), *to lay out a few ways of thinking about goals and motivations in natural and artificial systems, to point to a class of possible prosaic development paths to AGI where general capabilities emerge from extensive open-ended training, and finally observe some desiderata and gaps in our understanding regarding alignment conditional on this situation.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/RwYh4grJs4pbJdTh3/motivations-natural-selection-and-curriculum-engineering"
    ],
    "tags": [
      "adaptation executors",
      "ai",
      "corrigibility",
      "evolution",
      "general intelligence",
      "mesa-optimization",
      "seri mats"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_f6ca4b0d7b115773",
    "title": "Elicitation for Modeling Transformative AI Risks",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This post is part 8 in our sequence on Modeling Transformative AI Risk. We are building a model to understand debates around existential risks from advanced AI. The model is made with Analytica software, and consists of nodes (representing key hypotheses and cruxes) and edges (representing the relationships between these cruxes), with final output corresponding to the likelihood of various potential failure scenarios. You can read more about the motivation for our project and how the model works in the Introduction post. Unlike other posts in the sequence, this discusses the related but distinct work around Elicitation.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Kz9NHBMeJxzSwb7R9/elicitation-for-modeling-transformative-ai-risks"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_6b47c1aaf4432a6e",
    "title": "Evidence Sets: Towards Inductive-Biases based Analysis of Prosaic AGI",
    "year": 2021,
    "category": "public_awareness",
    "description": "***Epistemic Status**:* [*Exploratory*](https://www.alignmentforum.org/posts/Hrm59GdN2yDPWbtrd/feature-idea-epistemic-status)*. My current but-changing outlook with limited exploration & understanding for ~60-80hrs.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/aW5CPtqtvs2EYKMMK/evidence-sets-towards-inductive-biases-based-analysis-of"
    ],
    "tags": [
      "adversarial examples",
      "ai",
      "inner alignment",
      "language models",
      "machine learning  (ml)"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_3ff3c2c59f3ad1ba",
    "title": "Introducing the Principles of Intelligent Behaviour in Biological and Social Systems (PIBBSS) Fellowship",
    "year": 2021,
    "category": "policy_development",
    "description": "*Cross-posted to* [*EA Forum*](https://forum.effectivealtruism.org/posts/Ckont9EtqkenegLYv/introducing-the-principles-of-intelligent-behaviour-in)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/4Tjz4EJ8DozE9z5nQ/introducing-the-principles-of-intelligent-behaviour-in"
    ],
    "tags": [
      "ai",
      "pibbss"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_66819fa6e5892022",
    "title": "Disentangling Perspectives On Strategy-Stealing in AI Safety",
    "year": 2021,
    "category": "policy_development",
    "description": "*This post was written under Evan Hubinger's direct guidance and mentorship, as a part of the Stanford Existential Risks Institute ML Alignment Theory Scholars (MATS) program.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/CtGwGgxfoefiwfcor/disentangling-perspectives-on-strategy-stealing-in-ai-safety"
    ],
    "tags": [
      "ai",
      "existential risk"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_a0505f642fcc51bb",
    "title": "Exploring Decision Theories With Counterfactuals and Dynamic Agent Self-Pointers",
    "year": 2021,
    "category": "public_awareness",
    "description": "This is a follow-up to [A Possible Resolution To Spurious Counterfactuals](https://www.alignmentforum.org/posts/TnkDtTAqCGetvLsgr/a-possible-resolution-to-spurious-counterfactuals), which was addressing a technical problem in self-proof. See the original post for the suggestion provided for a solution to the 5 and 10 problem as described in [the Embedded Agency paper](https://arxiv.org/pdf/1902.09469.pdf).",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/zupqBxrNKpT5dhFQb/exploring-decision-theories-with-counterfactuals-and-dynamic"
    ],
    "tags": [
      "ai",
      "decision theory",
      "embedded agency"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_b356a1b15f398dd0",
    "title": "Don't Influence the Influencers!",
    "year": 2021,
    "category": "public_awareness",
    "description": "*This post was written under Evan Hubinger's direct guidance and mentorship, as a part of the* [*Stanford Existential Risks Institute ML Alignment Theory Scholars (MATS) program*](https://www.lesswrong.com/posts/FpokmCnbP3CEZ5h4t/ml-alignment-theory-program-under-evan-hubinger)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/Eg9FE2iYp3ngySsMD/don-t-influence-the-influencers"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_9aca335953cbe7c4",
    "title": "Demanding and Designing Aligned Cognitive Architectures",
    "year": 2021,
    "category": "policy_development",
    "description": "This post is to announce my new paper [Demanding and Designing Aligned\nCognitive Architectures](https://arxiv.org/abs/2112.10190), which I\nrecently presented in the [PERLS](https://perls-workshop.github.io/)\nworkshop (Political Economy of Reinforcement Learning)] at NeurIPS\n2021.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/cDR8GkzCaxXoovPwh/demanding-and-designing-aligned-cognitive-architectures"
    ],
    "tags": [
      "academic papers",
      "ai",
      "ai governance",
      "reinforcement learning"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_18cb73b261db7d44",
    "title": "Transformer Circuits",
    "year": 2021,
    "category": "public_awareness",
    "description": "[Chris Olah](https://www.alignmentforum.org/users/christopher-olah), [Neel Nanda](https://www.alignmentforum.org/users/neel-nanda-1), [Catherine Olsson](https://www.lesswrong.com/users/catherio), Nelson Elhage, and a bunch of other people at [Anthropic](https://www.anthropic.com/) just published \"Transformer Circuits,\" an application of the [*Circuits*-style](https://www.alignmentforum.org/posts/MG4ZjWQDrdpgeu8wG/zoom-in-an-introduction-to-circuits) interpretability paradigm to transformer-based language models. From their very top level summary:",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/2269iGRnWruLHsZ5r/transformer-circuits"
    ],
    "tags": [
      "ai",
      "anthropic (org)",
      "interpretability (ml & ai)",
      "language models"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_06c9bcc89b7a248c",
    "title": "Worst-case thinking in AI alignment",
    "year": 2021,
    "category": "policy_development",
    "description": "**Alternative title: \"When should you assume that what could go wrong, will go wrong?\"**",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/yTvBSFrXhZfL8vr5a/worst-case-thinking-in-ai-alignment"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_aee50e85550d593f",
    "title": "2021 AI Alignment Literature Review and Charity Comparison",
    "year": 2021,
    "category": "policy_development",
    "description": "*cross-posted to the EA forum* [*here*](https://forum.effectivealtruism.org/posts/BNQMyWGCNWDdP2WyG/2021-ai-alignment-literature-review-and-charity-comparison)*.*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/C4tR3BEpuWviT7Sje/2021-ai-alignment-literature-review-and-charity-comparison"
    ],
    "tags": [
      "academic papers",
      "ai",
      "literature reviews"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Important work advancing our understanding of AI safety",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ce6dcd5640e1cfec",
    "title": "Reply to Eliezer on Biological Anchors",
    "year": 2021,
    "category": "public_awareness",
    "description": "The [\"biological anchors\" method for forecasting transformative AI](https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/) is the biggest non-[trust-based](https://www.cold-takes.com/minimal-trust-investigations/#navigating-trust) input into my thinking about likely timelines for transformative AI. While I'm sympathetic to parts of [Eliezer Yudkowsky's recent post on it](https://www.lesswrong.com/posts/ax695frGJEzGxFBK4/biology-inspired-agi-timelines-the-trick-that-never-works), I overall disagree with the post, and think it's easy to get a misimpression of the \"biological anchors\" report (which I'll abbreviate as **\"Bio Anchors\"**) - and Open Philanthropy's take on it - by reading it.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/nNqXfnjiezYukiMJi/reply-to-eliezer-on-biological-anchors"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Provides general AI context",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_56b9fe0d46e19d86",
    "title": "Risks from AI persuasion",
    "year": 2021,
    "category": "policy_development",
    "description": "A case for why persuasive AI might pose risks somewhat distinct from the normal power-seeking alignment failure scenarios.",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/5cWtwATHL6KyzChck/risks-from-ai-persuasion"
    ],
    "tags": [
      "ai",
      "ai persuasion"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "This is a significant contribution to alignment research",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_ad7ef9ccdffaf1fd",
    "title": "My Overview of the AI Alignment Landscape: Threat Models",
    "year": 2021,
    "category": "policy_development",
    "description": "*This is the second post in a sequence mapping out the AI Alignment research landscape. The sequence will likely never be completed, but you can read a draft [here](https://docs.google.com/document/d/1X3XyS6CtZShwaJHMxQBvgKPUs7qlt74WxhmNnSDesXE/edit).*",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 5,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/3DFBbPFZyscrAiTKS/my-overview-of-the-ai-alignment-landscape-threat-models"
    ],
    "tags": [
      "ai",
      "ai risk",
      "coordination / cooperation",
      "existential risk",
      "goodhart's law",
      "inner alignment",
      "outer alignment",
      "power seeking (ai)",
      "threat models",
      "world modeling"
    ],
    "rarity": "rare",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Critical insights for the field",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_536aced9d622f8bc",
    "title": "Gradient Hacking via Schelling Goals",
    "year": 2021,
    "category": "public_awareness",
    "description": "*Thanks to several people for useful discussion, including Evan Hubinger, Leo Gao, Beth Barnes, Richard Ngo, William Saunders, Daniel Ziegler, and probably others -- let me know if I'm leaving you out.*",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/A9eAPjpFjPwNW2rku/gradient-hacking-via-schelling-goals"
    ],
    "tags": [
      "ai",
      "gradient hacking",
      "inner alignment"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_21ab14346c2b728d",
    "title": "Reverse-engineering using interpretability",
    "year": 2021,
    "category": "public_awareness",
    "description": "Building a model for which you're confident your interpretability is correct, by reverse-engineering each part of the model to work how your interpretability says it should work. (Based on discussion in alignment reading group, ideas from William, Adam, dmz, Evan, Leo, maybe others)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/qrn2dRSwNratuM3tq/reverse-engineering-using-interpretability"
    ],
    "tags": [
      "ai"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Interesting perspective on safety challenges",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_1e0acf18349ce60b",
    "title": "Eliciting Latent Knowledge Via Hypothetical Sensors",
    "year": 2021,
    "category": "policy_development",
    "description": "This is a response to [ARC's first technical report: Eliciting Latent Knowledge](https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge). But it should be fairly understandable even if you didn't read ARC's report, since I summarize relevant parts of their report as necessary. Here I propose some approaches to the problem ARC outlines which are very different from the approaches they explore.",
    "impacts": [
      {
        "variable": "research",
        "change": 5,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/H7v5yyXAmmgu9DJmi/eliciting-latent-knowledge-via-hypothetical-sensors"
    ],
    "tags": [
      "ai",
      "eliciting latent knowledge (elk)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Adds to our knowledge base",
    "media_reaction": "Discussed in AI safety community"
  },
  {
    "id": "alignmentforum_c8607fce7a636978",
    "title": "Counterexamples to some ELK proposals",
    "year": 2021,
    "category": "public_awareness",
    "description": "(*This post was written as part of my work at the* [*Alignment Research Center*](https://alignment.org/)*.*)",
    "impacts": [
      {
        "variable": "research",
        "change": 10,
        "condition": null
      },
      {
        "variable": "vibey_doom",
        "change": 2,
        "condition": null
      },
      {
        "variable": "ethics_risk",
        "change": -5,
        "condition": null
      }
    ],
    "sources": [
      "https://www.alignmentforum.org/posts/FnZws8NuKw6BJzmvZ/counterexamples-to-some-elk-proposals"
    ],
    "tags": [
      "ai",
      "alignment research center",
      "eliciting latent knowledge (elk)"
    ],
    "rarity": "common",
    "pdoom_impact": null,
    "safety_researcher_reaction": "Useful research for the community",
    "media_reaction": "Discussed in AI safety community"
  }
]