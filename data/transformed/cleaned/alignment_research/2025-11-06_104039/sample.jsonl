{"id": "555121ab60501803606a56ca07bd552e", "source": "alignmentforum", "title": "[AN #79]: Recursive reward modeling as an alignment technique integrated with deep RL", "text": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.\n\nHappy New Year!  \n   \nAudio version [here](http://alignment-newsletter.libsyn.com/alignment-newsletter-79) (may not be up yet).\n\n**Highlights**\n--------------\n\n[AI Alignment Podcast: On DeepMind, AI Safety, and Recursive Reward Modeling](https://futureoflife.org/2019/12/16/ai-alignment-podcast-on-deepmind-ai-safety-and-recursive-reward-modeling-with-jan-leike/) *(Lucas Perry and Jan Leike)* (summarized by Rohin): While Jan originally worked on theory (specifically AIXI), DQN, AlphaZero and others demonstrated that deep RL was a plausible path to AGI, and so now Jan works on more empirical approaches. In particular, when selecting research directions, he looks for techniques that are deeply integrated with the current paradigm, that could scale to AGI and beyond. He also wants the technique to work for agents in general, rather than just question answering systems, since people will want to build agents that can act, at least in the digital world (e.g. composing emails). This has led him to work on [recursive reward modeling](https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84) ([AN #34](https://mailchi.mp/f1947668b183/alignment-newsletter-34)), which tries to solve the specification problem in the [SRA framework](https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1) ([AN #26](https://mailchi.mp/1ecd1b775703/alignment-newsletter-26)).\n\nReward functions are useful because they allow the AI to find novel solutions that we wouldn't think of (e.g. AlphaGo's move 37), but often are incorrectly specified, leading to reward hacking. This suggests that we should do *reward modeling*, where we learn a model of the reward function from human feedback. Of course, such a model is still likely to have errors leading to reward hacking, and so to avoid this, the reward model needs to be updated online. As long as it is **easier to evaluate behavior than to produce behavior**, reward modeling should allow AIs to find novel solutions that we wouldn't think of.\n\nHowever, we would eventually like to apply reward modeling to tasks where evaluation is also hard. In this case, we can decompose the evaluation task into smaller tasks, and recursively apply reward modeling to train AI systems that can perform those small helper tasks. Then, assisted by these helpers, the human should be able to evaluate the original task. This is essentially forming a \"tree\" of reward modeling agents that are all building up to the reward model for the original, hard task. While currently the decomposition would be done by a human, you could in principle also use recursive reward modeling to automate the decomposition. Assuming that we can get regular reward modeling working robustly, we then need to make sure that the tree of reward models doesn't introduce new problems. In particular, it might be the case that as you go up the tree, the errors compound: errors in the reward model at the leaves lead to slightly worse helper agents, which lead to worse evaluations for the second layer, and so on.\n\nHe recommends that rather than spending a lot of time figuring out the theoretically optimal way to address a problem, AI safety researchers should alternate between conceptual thinking and trying to make something work. The ML community errs on the other side, where they try out lots of techniques, but don't think as much about how their systems will be deployed in the real world. Jan also wants the community to focus more on clear, concrete technical explanations, rather than vague blog posts that are difficult to critique and reason about. This would allow us to more easily build on past work, rather than reasoning from first principles and reinventing the wheel many times.\n\nDeepMind is taking a portfolio approach to AI safety: they are trying many different lines of attack, and hoping that some of them will pan out. Currently, there are teams for agent alignment (primarily recursive reward modeling), incentive theory, trained agent analysis, policy, and ethics. They have also spent some time thinking about AI safety benchmarks, as in [AI Safety Gridworlds](https://arxiv.org/abs/1711.09883), since progress in machine learning is driven by benchmarks, though Jan does think it is quite hard to create a well-made benchmark.\n\n**Rohin's opinion:** I've become more optimistic about recursive reward modeling since the [original paper](https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84) ([AN #34](https://mailchi.mp/f1947668b183/alignment-newsletter-34)), primarily (I think) because I now see more value in approaches that can be used to perform specific tasks (relative to approaches that try to infer \"human values\").\n\nI also appreciated the recommendations for the AI safety community, and agree with them quite a lot. Relative to Jan, I see more value in conceptual work described using fuzzy intuitions, but I do think that more effort should be put into exposition of that kind of work.\n\n**Technical AI alignment**\n==========================\n\n### **Learning human intent**\n\n[Learning human objectives by evaluating hypothetical behaviours](https://deepmind.com/blog/article/learning-human-objectives-by-evaluating-hypothetical-behaviours) *(Siddharth Reddy et al)* (summarized by Rohin): [Deep RL from Human Preferences](https://deepmind.com/blog/learning-through-human-feedback/) updated its reward model by collecting human comparisons on on-policy trajectories where the reward model ensemble was most uncertain about what the reward should be. However, we want our reward model to be accurate off policy as well, even in unsafe states. To this end, we would like to train our reward model on *hypothetical* trajectories. This paper proposes learning a generative model of trajectories from some dataset of environment dynamics, such as safe expert demonstrations or rollouts from a random policy, and then finding trajectories that are \"useful\" for training the reward model. They consider four different criteria for usefulness of a trajectory: *uncertain rewards* (which intuitively are areas where the reward model needs training), *high rewards* (which could indicate reward hacking), *low rewards* (which increases the number of unsafe states that the reward model is trained on), and *novelty* (which covers more of the state space). Once a trajectory is generated, they have a human label it as good, neutral, or unsafe, and then train the reward model on these labels.\n\nThe authors are targeting an agent that can *explore safely*: since they already have a world model and a reward model, they use a model-based RL algorithm to act in the environment. Specifically, to act, they use gradient descent to optimize a trajectory in the latent space that maximizes expected rewards under the reward model and world model, and then take the first action of that trajectory. They argue that the world model can be trained on a dataset of safe human demonstrations (though in their experiments they use rollouts from a random policy), and then since the reward model is trained on hypothetical behavior and the model-based RL algorithm doesn't need any training, we get an agent that acts without us ever getting to an unsafe state.\n\n**Rohin's opinion:** I like the focus on integrating active selection of trajectory queries into reward model training, and especially the four different kinds of active criteria that they consider, and the detailed experiments (including an ablation study) on the benefits of these criteria. These seem important for improving the efficiency of reward modeling.\n\nHowever, I don't buy the argument that this allows us to train an agent without visiting unsafe states. In their actual experiments, they use a dataset gathered from a random policy, which certainly will visit unsafe states. If you instead use a dataset of safe human demonstrations, your generative model will only place probability mass on safe demonstrations, and so you'll never generate trajectories that visit unsafe states, and your reward model won't know that they are unsafe. (*Maybe* your generative model will generalize properly to the unsafe states, but that seems unlikely to me.) Such a reward model will either be limited to imitation learning (sticking to the same trajectories as in the demonstrations, and never finding something like AlphaGo's move 37), or it will eventually visit unsafe states.\n\n**Read more:** [Paper: Learning Human Objectives by Evaluating Hypothetical Behavior](https://arxiv.org/abs/1912.05652)\n\n[Causal Confusion in Imitation Learning](https://arxiv.org/abs/1905.11979) *(Pim de Haan et al)* (summarized by Asya): This paper argues that *causal misidentification* is a big problem in imitation learning. When the agent doesn't have a good model of what actions cause what state changes, it may mismodel the effects of a state change as a cause-- e.g., an agent learning to drive a car may incorrectly learn that it should turn on the brakes whenever the brake light on the dashboard is on. This leads to undesirable behavior where more information actually causes the agent to perform worse.\n\nThe paper presents an approach for resolving causal misidentification by (1) Training a specialized network to generate a \"disentangled\" representation of the state as variables, (2) Representing causal relationships between those variables in a graph structure, (3) Learning policies corresponding to each possible causal graph, and (4) Performing targeted interventions, either by querying an expert, or by executing a policy and observing the reward, to find the correct causal graph model.\n\nThe paper experiments with this method by testing it in environments artificially constructed to have confounding variables that correlate with actions but do not cause them. It finds that this method is successfully able to improve performance with confounding variables, and that it performs significantly better per number of queries (to an expert or of executing a policy) than any existing methods. It also finds that directly executing a policy and observing the reward is a more efficient strategy for narrowing down the correct causal graph than querying an expert.\n\n**Asya's opinion:** This paper goes into detail arguing why causal misidentification is a huge problem in imitation learning and I find its argument compelling. I am excited about attempts to address the problem, and I am tentatively excited about the method the paper proposes for finding representative causal graphs, with the caveat that I don't feel equipped to evaluate whether it could efficiently generalize past the constrained experiments presented in the paper.\n\n**Rohin's opinion:** While the conclusion that more information hurts sounds counterintuitive, it is actually straightforward: you *don't* get more data (in the sense of the size of your training dataset); you instead have *more features* in the input state data. This increases the number of possible policies (e.g. once you add the car dashboard, you can now express the policy \"if brake light is on, apply brakes\", which you couldn't do before), which can make you generalize worse. Effectively, there are more opportunities for the model to pick up on spurious correlations instead of the true relationships. This would happen in other areas of ML as well; surely someone has analyzed this effect for fairness, for example.\n\nThe success of their method over DAgger comes from improved *policy exploration* (for their environments): if your learned policy is primarily paying attention to the brake light, it's a very large change to instead focus on whether there is an obstacle visible, and so gradient descent is not likely to ever try that policy once it has gotten to the local optimum of paying attention to the brake light. In contrast, their algorithm effectively trains separate policies for scenarios in which different parts of the input are masked, which means that it is forced to explore policies that depend only on the brake light, and policies that depend only on the view outside the windshield, and so on. So, the desired policy has been explored already, and it only requires a little bit of active learning to identify the correct policy.\n\nLike Asya, I like the approach, but I don't know how well it will generalize to other environments. It seems like an example of [quality diversity](https://www.frontiersin.org/articles/10.3389/frobt.2016.00040/full), which I am generally optimistic about.\n\n[Humans Are Embedded Agents Too](https://www.alignmentforum.org/posts/WJzsTmsDctYCCyMfy/humans-are-embedded-agents-too) *(John S Wentworth)* (summarized by Rohin): [Embedded agency](https://www.alignmentforum.org/posts/p7x32SEt43ZMC9r7r/embedded-agents) ([AN #31](https://mailchi.mp/7d0e3916e3d9/alignment-newsletter-31)) is not just a problem for AI systems: humans are embedded agents too; many problems in understanding human values stem from this fact. For example, humans don't have a well-defined output channel: we can't say \"anything that comes from this keyboard is direct output from the human\", because the AI could seize control of the keyboard and wirehead, or a cat could walk over the keyboard, etc. Similarly, humans can \"self-modify\", e.g. by drinking, which often modifies their \"values\": what does that imply for value learning? Based on these and other examples, the post concludes that \"a better understanding of embedded agents in general will lead to substantial insights about the nature of human values\".\n\n**Rohin's opinion:** I certainly agree that many problems with figuring out what to optimize stem from embedded agency issues with humans, and any [formal account](https://www.alignmentforum.org/posts/zvrZi95EHqJPxdgps/why-we-need-a-theory-of-human-values) ([AN #36](https://mailchi.mp/6751e45fbb48/alignment-newsletter-36)) of this will benefit from general progress in understanding embeddedness. Unlike many others, I do not think we need a formal account of human values, and that a \"common-sense\" understanding will suffice, including for the embeddedness problems detailed in this post. (See also this [comment thread](https://www.alignmentforum.org/posts/WJzsTmsDctYCCyMfy/humans-are-embedded-agents-too#opd3EeESfWiEyekDh) and the next summary.)\n\n[What's the dream for giving natural language commands to AI?](https://www.alignmentforum.org/posts/Bxxh9GbJ6WuW5Hmkj/what-s-the-dream-for-giving-natural-language-commands-to-ai) *(Charlie Steiner)* (summarized by Rohin): We could try creating AI systems that take the \"artificial intentional stance\" towards humans: that is, they model humans as agents that are trying to achieve some goals, and then we get the AI system to optimize for those inferred goals. We could do this by training an agent that jointly models the world and understands natural language, in order to ground the language into actual states of the world. The hope is that with this scheme, as the agent gets more capable, its understanding of what we want improves as well, so that it is robust to scaling up. However, the scheme has no protection against Goodharting, and doesn't obviously care about metaethics.\n\n**Rohin's opinion:** I agree with the general spirit of \"get the AI system to understand common sense; then give it instructions that it interprets correctly\". I usually expect future ML research to figure out the common sense part, so I don't look for particular implementations (in this case, simultaneous training on vision and natural language), but just assume we'll have that capability somehow. The hard part is then how to leverage that capability to provide *correctly interpreted* instructions. It may be as simple as providing instructions in natural language, as this post suggests. I'm much less worried about instrumental subgoals in such a scenario, since part of \"understanding what we mean\" includes \"and don't pursue this instruction literally to extremes\". But we still need to figure out how to translate natural language instructions into actions.\n\n### **Forecasting**\n\n[Might humans not be the most intelligent animals?](https://www.lesswrong.com/posts/XjuT9vgBfwXPxsdfN/might-humans-not-be-the-most-intelligent-animals) *(Matthew Barnett)* (summarized by Rohin): We can roughly separate intelligence into two categories: *raw innovative capability* (the ability to figure things out from scratch, without the benefit of those who came before you), and *culture processing* (the ability to learn from accumulated human knowledge). It's not clear that humans have the highest raw innovative capability; we may just have much better culture. For example, feral children raised outside of human society look very \"unintelligent\", [The Secret of Our Success](https://www.amazon.com/dp/B00WY4OXAS/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1) documents cases where culture trumped innovative capability, and humans actually *don't* have the most neurons, or the most neurons in the forebrain.\n\n(Why is this relevant to AI alignment? Matthew claims that it has implications on AI takeoff speeds, though he doesn't argue for that claim in the post.)\n\n**Rohin's opinion:** It seems very hard to actually make a principled distinction between these two facets of intelligence, because culture has such an influence over our \"raw innovative capability\" in the sense of our ability to make original discoveries / learn new things. While feral children might be less intelligent than animals (I wouldn't know), the appropriate comparison would be against \"feral animals\" that also didn't get opportunities to explore their environment and learn from their parents, and even so I'm not sure how much I'd trust results from such a \"weird\" (evolutionarily off-distribution) setup.\n\n[Walsh 2017 Survey](https://aiimpacts.org/walsh-2017-survey/) *(Charlie Giattino)* (summarized by Rohin): In this survey, AI experts, robotics experts, and the public estimated a 50% chance of high-level machine intelligence (HLMI) by 2061, 2065, and 2039 respectively. The post presents other similar data from the survey.\n\n**Rohin's opinion:** While I expected that the public would expect HLMI sooner than AI experts, I was surprised that AI and robotics experts agreed so closely -- I would have thought that robotics experts would have longer timelines.\n\n### **Field building**\n\n[What I talk about when I talk about AI x-risk: 3 core claims I want machine learning researchers to address.](https://www.alignmentforum.org/posts/bJdaB2Mz4mBvwFBeb/what-i-talk-about-when-i-talk-about-ai-x-risk-3-core-claims-1) *(David Krueger)* (summarized by Rohin): When making the case for work on AI x-risk to other ML researchers, what should we focus on? This post suggests arguing for three core claims:\n\n1. Due to Goodhart's law, instrumental goals, and safety-performance trade-offs, the development of advanced AI increases the risk of human extinction non-trivially.\n\n2. To mitigate this x-risk, we need to know how to build safe systems, know that we know how to build safe systems, and prevent people from building unsafe systems.\n\n3. So, we should mitigate AI x-risk, as it is impactful, neglected, and challenging but tractable.\n\n**Rohin's opinion:** This is a nice concise case to make, but I think the bulk of the work is in splitting the first claim into subclaims: this is the part that is usually a sticking point (see also the next summary).\n\n### **Miscellaneous (Alignment)**\n\n[A list of good heuristics that the case for AI x-risk fails](https://www.alignmentforum.org/posts/bd2K3Jdz82csjCFob/a-list-of-good-heuristics-that-the-case-for-ai-x-risk-fails) *(David Krueger)* (summarized by Flo): Because human attention is limited and a lot of people try to convince us of the importance of their favourite cause, we cannot engage with everyone's arguments in detail. Thus we have to rely on heuristics to filter out insensible arguments. Depending on the form of exposure, the case for AI risks can fail on many of these generally useful heuristics, eight of which are detailed in this post. Given this outside view perspective, it is unclear whether we should actually expect ML researchers to spend time evaluating the arguments for AI risk.\n\n**Flo's opinion:** I can remember being critical of AI risk myself for similar reasons and think that it is important to be careful with the framing of pitches to avoid these heuristics from firing. This is not to say that we should avoid criticism of the idea of AI risk, but criticism is a lot more helpful if it comes from people who have actually engaged with the arguments.\n\n**Rohin's opinion:** Even after knowing the arguments, I find six of the heuristics quite compelling: technology doomsayers have usually been wrong in the past, there isn't a concrete threat model, it's not empirically testable, it's too extreme, it isn't well grounded in my experience with existing AI systems, and it's too far off to do useful work now. All six make me distinctly more skeptical of AI risk.\n\n**Other progress in AI**\n========================\n\n### **Reinforcement learning**\n\n[Procgen Benchmark](https://openai.com/blog/procgen-benchmark/) *(Karl Cobbe et al)* (summarized by Asya): Existing game-based benchmarks for reinforcement learners suffer from the problem that agents constantly encounter near-identical states, meaning that the agents may be overfitting and memorizing specific trajectories rather than learning a general set of skills. In an attempt to remedy this, in this post OpenAI introduces Procgen Benchmark, 16 procedurally-generated video game environments used to measure how quickly a reinforcement learning agent learns generalizable skills.\n\nThe authors conduct several experiments using the benchmark. Notably, they discover that:\n\n- Agents strongly overfit to small training sets and need access to as many as 10,000 levels to generalize appropriately.\n\n- After a certain threshold, training performance improves as the training set grows, counter to trends in other supervised learning tasks.\n\n- Using a fixed series of levels for each training sample (as other benchmarks do) makes agents fail to generalize to randomly generated series of levels at test time.\n\n- Larger models improve sample efficiency and generalization.\n\n**Asya's opinion:** This seems like a useful benchmark. I find it particularly interesting that their experiment testing non-procedurally generated levels as training samples implies huge overfitting effects in existing agents trained in video-game environments.\n\n**Read more:** [Paper: Leveraging Procedural Generation to Benchmark Reinforcement Learning](http://arxiv.org/abs/1912.01588)\n\n[Adaptive Online Planning for Continual Lifelong Learning](http://arxiv.org/abs/1912.01188) *(Kevin Lu et al)* (summarized by Nicholas): Lifelong learning is distinct from standard RL benchmarks because\n\n1. The environment is *sequential* rather than *episodic*; it is never reset to a new start state.\n\n2. The current *transition* and *reward* function are given, but they change over time.\n\nGiven this setup, there are two basic approaches: first, run model-free learning on simulated future trajectories and rerun it every time the dynamics change, and second, run model-based planning on the current model. If you ignore computational constraints, these should be equivalent; however, in practice, the second option tends to be more computationally efficient. The contribution of this work is to make this more efficient, rather than improving final performance, by starting with the second option and then using model-free learning to \"distill\" the knowledge produced by the model-based planner allowing for more efficient planning in the future.\n\nSpecifically, Adaptive Online Planning (AOP) balances between the model-based planner MPPI (a variant of MPC) and the model-free algorithm TD3. MPPI uses the given model to generate a trajectory up to a horizon and then uses an ensemble of value functions to estimate the cumulative reward. This knowledge is then distilled into TD3 for later use as a prior for MPPI. During future rollouts, the variance and Bellman error of the value function ensemble are used to determine how long the horizon should be, and therefore how much computation is used.\n\n**Nicholas's opinion:** I agree that episodic training and fixed world dynamics seem like unlikely conditions for most situations we would expect agents to encounter in the real world. Accounting for them seems particularly important to ensure safe exploration and robustness to distributional shift, and I think that these environments could serve as useful benchmarks for these safety problems as well.", "url": "https://www.alignmentforum.org/posts/EoY6P6mpz7ZozhAxm/an-79-recursive-reward-modeling-as-an-alignment-technique", "date_published": "2020-01-01T18:00:02Z", "authors": ["Rohin Shah"], "tags": ["ai", "newsletters"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:40:42.282311+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "fcbad849b33212ac3c517bb0f7859b8c", "source": "alignmentforum", "title": "[AN #80]: Why AI risk might be solved without additional intervention from longtermists", "text": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.\n\nAudio version [here](http://alignment-newsletter.libsyn.com/alignment-newsletter-80) (may not be up yet).\n\nWelcome to another special edition of the newsletter! In this edition, I summarize four conversations that AI Impacts had with researchers who were optimistic that AI safety would be solved \"by default\". (Note that one of the conversations was with me.)\n\nWhile all four of these conversations covered very different topics, I think there were three main points of convergence. First, we were relatively **unconvinced by the traditional arguments for AI risk**, and **find discontinuities relatively unlikely**. Second, we were more optimistic about **solving the problem in the future**, when we know more about the problem and have more evidence about powerful AI systems. And finally, we were more optimistic that as we get more evidence of the problem in the future, **the existing ML community will actually try to fix that problem**.\n\n[Conversation with Paul Christiano](https://aiimpacts.org/conversation-with-paul-christiano/) *(Paul Christiano, Asya Bergal, Ronny Fernandez, and Robert Long)* (summarized by Rohin): There can't be too many things that reduce the expected value of the future by 10%; if there were, there would be no expected value left (ETA: see [this comment](https://www.lesswrong.com/posts/QknPz9JQTQpGdaWDp/an-80-why-ai-risk-might-be-solved-without-additional#WWufqvbyESxNDEtDJ)). So, the prior that any particular thing has such an impact should be quite low. With AI in particular, obviously we're going to try to make AI systems that do what we want them to do. So starting from this position of optimism, we can then evaluate the arguments for doom. The two main arguments: first, we can't distinguish ahead of time between AIs that are trying to do the right thing, and AIs that are trying to kill us, because the latter will behave nicely until they can execute a treacherous turn. Second, since we don't have a crisp concept of \"doing the right thing\", we can't select AI systems on whether they are doing the right thing.\n\nHowever, there are many \"saving throws\", or ways that the argument could break down, avoiding doom. Perhaps there's no problem at all, or perhaps we can cope with it with a little bit of effort, or perhaps we can coordinate to not build AIs that destroy value. Paul assigns a decent amount of probability to each of these (and other) saving throws, and any one of them suffices to avoid doom. This leads Paul to estimate that AI risk reduces the expected value of the future by roughly 10%, a relatively optimistic number. Since it is so neglected, concerted effort by longtermists could reduce it to 5%, making it still a very valuable area for impact. The main way he expects to change his mind is from evidence from more powerful AI systems, e.g. as we build more powerful AI systems, perhaps inner optimizer concerns will materialize and we'll see examples where an AI system executes a non-catastrophic treacherous turn.\n\nPaul also believes that clean algorithmic problems are usually solvable in 10 years, or provably impossible, and early failures to solve a problem don't provide much evidence of the difficulty of the problem (unless they generate proofs of impossibility). So, the fact that we don't know how to solve alignment now doesn't provide very strong evidence that the problem is impossible. Even if the clean versions of the problem were impossible, that would suggest that the problem is much more messy, which requires more concerted effort to solve but also tends to be just a long list of relatively easy tasks to do. (In contrast, MIRI thinks that prosaic AGI alignment is probably impossible.)\n\nNote that even finding out that the problem is impossible can help; it makes it more likely that we can all coordinate to not build dangerous AI systems, since no one *wants* to build an unaligned AI system. Paul thinks that right now the case for AI risk is not very compelling, and so people don't care much about it, but if we could generate more compelling arguments, then they would take it more seriously. If instead you think that the case is already compelling (as MIRI does), then you would be correspondingly more pessimistic about others taking the arguments seriously and coordinating to avoid building unaligned AI.\n\nOne potential reason MIRI is more doomy is that they take a somewhat broader view of AI safety: in particular, in addition to building an AI that is trying to do what you want it to do, they would also like to ensure that when the AI builds successors, it does so well. In contrast, Paul simply wants to leave the next generation of AI systems in at least as good a situation as we find ourselves in now, since they will be both better informed and more intelligent than we are. MIRI has also previously defined aligned AI as one that produces good outcomes when run, which is a much broader conception of the problem than Paul has. But probably the main disagreement between MIRI and ML researchers and that ML researchers expect that we'll try a bunch of stuff, and something will work out, whereas MIRI expects that the problem is really hard, such that trial and error will only get you solutions that *appear* to work.\n\n**Rohin's opinion:** A general theme here seems to be that MIRI feels like they have very strong arguments, while Paul thinks that they're plausible arguments, but aren't extremely strong evidence. Simply having a lot more uncertainty leads Paul to be much more optimistic. I agree with most of this.\n\nHowever, I do disagree with the point about \"clean\" problems. I agree that clean algorithmic problems are usually solved within 10 years or are provably impossible, but it doesn't seem to me like AI risk counts as a clean algorithmic problem: we don't have a nice formal statement of the problem that doesn't rely on intuitive concepts like \"optimization\", \"trying to do something\", etc. This suggests to me that AI risk is more \"messy\", and so may require more time to solve.\n\n[Conversation with Rohin Shah](https://aiimpacts.org/conversation-with-rohin-shah/) *(Rohin Shah, Asya Bergal, Robert Long, and Sara Haxhia)* (summarized by Rohin): The main reason I am optimistic about AI safety is that we will see problems in advance, and we will solve them, because nobody wants to build unaligned AI. A likely crux is that I think that the ML community will actually solve the problems, as opposed to applying a bandaid fix that doesn't scale. I don't know why there are different underlying intuitions here.\n\nIn addition, many of the classic arguments for AI safety involve a system that can be decomposed into an objective function and a world model, which I suspect will not be a good way to model future AI systems. In particular, current systems trained by RL look like a grab bag of heuristics that correlate well with obtaining high reward. I think that as AI systems become more powerful, the heuristics will become more and more general, but they still won't decompose naturally into an objective function, a world model, and search. In addition, we can look at humans as an example: we don't fully pursue convergent instrumental subgoals; for example, humans can be convinced to pursue different goals. This makes me more skeptical of traditional arguments.\n\nI would guess that AI systems will become *more* interpretable in the future, as they start using the features / concepts / abstractions that humans are using. Eventually, sufficiently intelligent AI systems will probably find even better concepts that are alien to us, but if we only consider AI systems that are (say) 10x more intelligent than us, they will probably still be using human-understandable concepts. This should make alignment and oversight of these systems significantly easier. For significantly stronger systems, we should be delegating the problem to the AI systems that are 10x more intelligent than us. (This is very similar to the picture painted in [Chris Olah's views on AGI safety](https://www.alignmentforum.org/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety) ([AN #72](https://mailchi.mp/cac125522aa3/an-72-alignment-robustness-methodology-and-system-building-as-research-priorities-for-ai-safety)), but that had not been published and I was not aware of Chris's views at the time of this conversation.)\n\nI'm also less worried about race dynamics increasing *accident* risk than the median researcher. The benefit of racing a little bit faster is to have a little bit more power / control over the future, while also increasing the risk of extinction a little bit. This seems like a bad trade from each agent's perspective. (That is, the Nash equilibrium is for all agents to be cautious, because the potential upside of racing is small and the potential downside is large.) I'd be more worried if [AI risk is real AND not everyone agrees AI risk is real when we have powerful AI systems], or if the potential upside was larger (e.g. if racing a little more made it much more likely that you could achieve a decisive strategic advantage).\n\nOverall, it feels like there's around 90% chance that AI would not cause x-risk without additional intervention by longtermists. The biggest disagreement between me and more pessimistic researchers is that I think gradual takeoff is much more likely than discontinuous takeoff (and in fact, the first, third and fourth paragraphs above are quite weak if there's a discontinuous takeoff). If I condition on discontinuous takeoff, then I mostly get very confused about what the world looks like, but I also get a lot more worried about AI risk, especially because the \"AI is to humans as humans are to ants\" analogy starts looking more accurate. In the interview I said 70% chance of doom in this world, but with *way* more uncertainty than any of the other credences, because I'm really confused about what that world looks like. Two other disagreements, besides the ones above: I don't buy [Realism about rationality](https://www.lesswrong.com/posts/suxvE2ddnYMPJN9HD/realism-about-rationality) ([AN #25](https://mailchi.mp/0c5eeec28f75/alignment-newsletter-25)), whereas I expect many pessimistic researchers do. I may also be more pessimistic about our ability to write proofs about fuzzy concepts like those that arise in alignment.\n\nOn timelines, I estimated a very rough 50% chance of AGI within 20 years, and 30-40% chance that it would be using \"essentially current techniques\" (which is obnoxiously hard to define). Conditional on both of those, I estimated 70% chance that it would be something like a mesa optimizer; mostly because optimization is a very useful instrumental strategy for solving many tasks, especially because gradient descent and other current algorithms are very weak optimization algorithms (relative to e.g. humans), and so learned optimization algorithms will be necessary to reach human levels of sample efficiency.\n\n**Rohin's opinion:** Looking over this again, I'm realizing that I didn't emphasize enough that most of my optimism comes from the more outside view type considerations: that we'll get warning signs that the ML community won't ignore, and that the AI risk arguments are not watertight. The other parts are particular inside view disagreements that make me more optimistic, but they don't factor in much into my optimism besides being examples of how the meta considerations could play out. I'd recommend [this comment of mine](https://www.lesswrong.com/posts/mdau2DBSMi5bWXPGA/useful-does-not-mean-secure#xccsZeboCNcNJeGas) to get more of a sense of how the meta considerations factor into my thinking.\n\nI was also glad to see that I still broadly agree with things I said ~5 months ago (since no major new opposing evidence has come up since then), though as I mentioned above, I would now change what I place emphasis on.\n\n[Conversation with Robin Hanson](https://aiimpacts.org/conversation-with-robin-hanson/) *(Robin Hanson, Asya Bergal, and Robert Long)* (summarized by Rohin): The main theme of this conversation is that AI safety does not look particularly compelling on an outside view. Progress in most areas is relatively incremental and continuous; we should expect the same to be true for AI, suggesting that timelines should be quite long, on the order of centuries. The current AI boom looks similar to previous AI booms, which didn't amount to much in the past.\n\nTimelines could be short if progress in AI were \"lumpy\", as in a FOOM scenario. This could happen if intelligence was one simple thing that just has to be discovered, but Robin expects that intelligence is actually a bunch of not-very-general tools that together let us do many things, and we simply have to find all of these tools, which will presumably not be lumpy. Most of the value from tools comes from more specific, narrow tools, and intelligence should be similar. In addition, the literature on human uniqueness suggests that it wasn't \"raw intelligence\" or small changes to brain architecture that makes humans unique, it's our ability to process culture (communicating via language, learning from others, etc).\n\nIn any case, many researchers are now distancing themselves from the FOOM scenario, and are instead arguing that AI risk occurs due to standard principal-agency problems, in the situation where the agent (AI) is much smarter than the principal (human). Robin thinks that this doesn't agree with the existing literature on principal-agent problems, in which losses from principal-agent problems tend to be bounded, even when the agent is smarter than the principal.\n\nYou might think that since the stakes are so high, it's worth working on it anyway. Robin agrees that it's worth having a few people (say a hundred) pay attention to the problem, but doesn't think it's worth spending a lot of effort on it right now. Effort is much more effective and useful once the problem becomes clear, or once you are working with a concrete design; we have neither of these right now and so we should expect that most effort ends up being ineffective. It would be better if we saved our resources for the future, or if we spent time thinking about other ways that the future could go (as in his book, Age of Em).\n\nIt's especially bad that AI safety has thousands of \"fans\", because this leads to a \"crying wolf\" effect -- even if the researchers have subtle, nuanced beliefs, they cannot control the message that the fans convey, which will not be nuanced and will instead confidently predict doom. Then when doom doesn't happen, people will learn not to believe arguments about AI risk.\n\n**Rohin's opinion:** Interestingly, I agree with almost all of this, even though it's (kind of) arguing that I shouldn't be doing AI safety research at all. The main place I disagree is that losses from principal-agent problems with perfectly rational agents are bounded -- this seems crazy to me, and I'd be interested in specific paper recommendations (though note [I](https://www.lesswrong.com/posts/ktDKfKqukTPRiuEPM/robin-hanson-on-the-futurist-focus-on-ai#fecYAwjmMSZ9KRfPL) [and](https://www.lesswrong.com/posts/ktDKfKqukTPRiuEPM/robin-hanson-on-the-futurist-focus-on-ai#yRLaEzT57K4q5Qz5H) [others](https://www.lesswrong.com/posts/ktDKfKqukTPRiuEPM/robin-hanson-on-the-futurist-focus-on-ai#p6CGEFbqaYJb49jME) have searched and not found many).\n\nOn the point about lumpiness, my model is that there are only a few underlying factors (such as the ability to process culture) that allow humans to so quickly learn to do so many tasks, and almost all tasks require near-human levels of these factors to be done well. So, once AI capabilities on these factors reach approximately human level, we will \"suddenly\" start to see AIs beating humans on many tasks, resulting in a \"lumpy\" increase on the metric of \"number of tasks on which AI is superhuman\" (which seems to be the metric that people often use, though I don't like it, precisely because it seems like it wouldn't measure progress well until AI becomes near-human-level).\n\n[Conversation with Adam Gleave](https://aiimpacts.org/conversation-with-adam-gleave/) *(Adam Gleave et al)* (summarized by Rohin): Adam finds the traditional arguments for AI risk unconvincing. First, it isn't clear that we will build an AI system that is so capable that it can fight all of humanity from its initial position where it doesn't have any resources, legal protections, etc. While discontinuous progress in AI could cause this, Adam doesn't see much reason to expect such discontinuous progress: it seems like AI is progressing by using more computation rather than finding fundamental insights. Second, we don't know how difficult AI safety will turn out to be; he gives a probability of ~10% that the problem is as hard as (a caricature of) MIRI suggests, where any design not based on mathematical principles will be unsafe. This is especially true because as we get closer to AGI we'll have many more powerful AI techniques that we can leverage for safety. Thirdly, Adam does expect that AI researchers will eventually solve safety problems; they don't right now because it seems premature to work on those problems. Adam would be more worried if there were more arms race dynamics, or more empirical evidence or solid theoretical arguments in support of speculative concerns like inner optimizers. He would be less worried if AI researchers spontaneously started to work on relative problems (more than they already do).\n\nAdam makes the case for AI safety work differently. At the highest level, it seems possible to build AGI, and some organizations are trying very hard to build AGI, and if they succeed it would be transformative. That alone is enough to justify some effort into making sure such a technology is used well. Then, looking at the field itself, it seems like the field is not currently focused on doing good science and engineering to build safe, reliable systems. So there is an opportunity to have an impact by pushing on safety and reliability. Finally, there are several technical problems that we do need to solve before AGI, such as how we get information about what humans actually want.\n\nAdam also thinks that it's 40-50% likely that when we build AGI, a PhD thesis describing it would be understandable by researchers today without too much work, but ~50% that it's something radically different. However, it's only 10-20% likely that AGI comes only from small variations of current techniques (i.e. by vastly increasing data and compute). He would see this as more likely if we hit additional milestones by investing more compute and data (OpenAI Five was an example of such a milestone).\n\n**Rohin's opinion:** I broadly agree with all of this, with two main differences. First, I am less worried about some of the technical problems that Adam mentions, such as how to get information about what humans want, or how to improve the robustness of AI systems, and more concerned about the more traditional problem of how to create an AI system that is *trying* to do what you want. Second, I am more bullish on the creation of AGI using small variations on current techniques, but vastly increasing compute and data (I'd assign ~30%, while Adam assigns 10-20%).", "url": "https://www.alignmentforum.org/posts/QknPz9JQTQpGdaWDp/an-80-why-ai-risk-might-be-solved-without-additional", "date_published": "2020-01-02T18:20:02Z", "authors": ["Rohin Shah"], "tags": ["ai", "ai risk", "newsletters"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:40:42.283091+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "daea13bad1d31b1c033d9498ab2461e7", "source": "alignmentforum", "title": "Exploring safe exploration", "text": "*This post is an attempt at reformulating some of the points I wanted to make in \"[Safe exploration and corrigibility](https://www.alignmentforum.org/posts/87Y7w73phjBxnPyPD/safe-exploration-and-corrigibility)\" in a clearer way. This post is standalone and does not assume that post as background.*\n\n\n[In a previous comment thread, Rohin argued that safe exploration is currently defined as being about the agent not making \"an accidental mistake.\"](https://www.alignmentforum.org/posts/87Y7w73phjBxnPyPD/safe-exploration-and-corrigibility#aeXgHHx3v5rRMG5MP) I think that definition is wrong, at least to the extent that I think it both doesn't make much sense and doesn't describe how I actually expect current safe exploration work to be useful.\n\n\nFirst, what does it mean for a failure to be an \"accident?\" This question is simple from the perspective of an engineer outside the whole system--any unintended failure is an accident, encapsulating the majority of AI safety concerns (i.e. \"[accident risk](https://arxiv.org/abs/1606.06565)\"). But that's clearly not what the term \"accidental mistake\" is pointing at in this context--rather, the question here is *what is an accident from the perspective of the model?* Intuitively, an accident from the perspective of the model should be some failure that the model didn't intend or wouldn't retroactively endorse. But that sort of a definition only makes sense for [highly coherent mesa-optimizers](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/FkgsxrGf3QxhfLWHG) that actually have some notion of intent. Maybe instead we should be thinking of this from the perspective of the base optimizer/loss function? That is, maybe a failure is an accidental failure if the loss function wouldn't retroactively endorse it (e.g. the model got a very low reward for making the mistake). By this definition, however, *every generalization failure is an accidental failure* such that safe exploration would just be the problem of generalization.\n\n\nOf all of these definitions, the definition defining an accidental failure from the perspective of the model as a failure that the model didn't intend or wouldn't endorse seems the most sensical to me. Even assuming that your model is a highly coherent mesa-optimizer such that this definition makes sense, however, I still don't think it describes current safe exploration work, and in fact I don't think it's even really a safety problem. The problem of producing models which don't make mistakes from the perspective of their own internal goals is precisely the problem of making powerful, capable models--that is, it's precisely the problem of [capability generalization](https://www.alignmentforum.org/posts/2mhFMgtAjFJesaSYR/2-d-robustness). Thus, to the extent that it's reasonable to say this for any ML problem, the problem of accidental mistakes under this definition is just a capabilities problem. However, I don't think that at all invalidates the utility of current safe exploration work, as I don't think that current safe exploration work is actually best understood as avoiding \"accidental mistakes.\"\n\n\nIf safe exploration work isn't about avoiding accidental mistakes, however, then what is it about? Well, let's take a look at an example. [Safety Gym](https://cdn.openai.com/safexp-short.pdf) has a variety of different environments containing both goal states that the agent is supposed to reach and unsafe states that the agent is supposed to avoid. From [OpenAI's blog post](https://openai.com/blog/safety-gym/): \"If deep reinforcement learning is applied to the real world, whether in robotics or internet-based tasks, it will be important to have algorithms that are safe even while learning--like a self-driving car that can learn to avoid accidents without actually having to experience them.\" Why wouldn't this happen naturally, though--shouldn't an agent in a [POMDP](https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process) always want to be careful? Well, not quite. When we do RL, there are really two different forms of exploration happening:[[1]](#fn-HtGxjApz9usEYDaSs-1)\n\n\n* **Within-episode exploration,** where the agent tries to identify what particular environment/state it's in, and\n* **Across-episode exploration,** which is the problem of making your agent explore enough to gather all the data necessary to train it properly.\n\n\nIn your standard episodic POMDP setting, you get within-episode exploration naturally, but not across-episode exploration, which you have to explicitly incentivize.[[2]](#fn-HtGxjApz9usEYDaSs-2) Because we have to explicitly incentivize across-episode exploration, however, it can often lead to behaviors which are contrary to the goal of actually trying to achieve the greatest possible reward in the current episode. Fundamentally, I think current safe exploration research is about trying to fix that problem--that is, it's about trying to make across-episode exploration less detrimental to reward acquisition. This sort of a problem is most important in an online learning setting where bad across-episode exploration could lead to catastrophic consequences (e.g. crashing an actual car to get more data about car crashes).\n\n\nThus, rather than define safe exploration as \"avoiding accidental mistakes,\" I think the right definition is something more like \"improving across-episode exploration.\" However, I think that this framing makes clear that there are other types of safe exploration problems--that is, there are other problems in the general domain of making across-episode exploration better. For example, I would love to see an exploration of how different across-episode exploration techniques impact [capability generalization vs. objective generalization](https://www.alignmentforum.org/posts/2mhFMgtAjFJesaSYR/2-d-robustness)--that is, when is across-episode exploration helping you collect data which improves the model's ability to achieve its current goal versus helping you collect data which improves the model's goal?[[3]](#fn-HtGxjApz9usEYDaSs-3) Because across-episode exploration is explicitly incentivized, it seems entirely possible to me that we'll end up getting the incentives wrong somehow, so it seems quite important to me to think about how to get them right--and I think that the problem of getting them right is the right way to think about safe exploration.\n\n\n\n\n---\n\n\n\n1. This terminology is borrowed from [Rohin's first comment](https://www.alignmentforum.org/posts/87Y7w73phjBxnPyPD/safe-exploration-and-corrigibility#aeXgHHx3v5rRMG5MP) in the same comment chain I mentioned previously. [??](#fnref-HtGxjApz9usEYDaSs-1)\n2. With some caveats--in fact, I think a form of across-episode exploration will be instrumentally incentivized for an agent that is aware of the training process it resides in, though that's a bit of a tricky question that I won't try to fully address now (I tried talking about this somewhat in \"[Safe exploration and corrigibility](https://www.alignmentforum.org/posts/87Y7w73phjBxnPyPD/safe-exploration-and-corrigibility),\" though I don't think I really succeeded there). [??](#fnref-HtGxjApz9usEYDaSs-2)\n3. This is what I somewhat confusingly called the \"objective exploration problem\" in \"[Safe exploration and corrigibility](https://www.alignmentforum.org/posts/87Y7w73phjBxnPyPD/safe-exploration-and-corrigibility).\" [??](#fnref-HtGxjApz9usEYDaSs-3)", "url": "https://www.alignmentforum.org/posts/NBffcjqm2P4dNbjrE/exploring-safe-exploration", "date_published": "2020-01-06T21:07:38Z", "authors": ["evhub"], "tags": ["ai"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:40:42.283624+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "2ade106204f7ec23f0d3274622ebde0f", "source": "alignmentforum", "title": "(Double-)Inverse Embedded Agency Problem", "text": "MIRI [has said a lot](https://intelligence.org/embedded-agency/) about the issue of embedded agency over the last year. However, I am yet to see them trying to make progress in what I see as the most promising areas. \n\nHow does one attack a problem that is new, complicated and non-obvious? By **constructing toy models** and **inverting hard questions** to make them more tractable. \n\nIn general an [inverse problem](https://en.wikipedia.org/wiki/Inverse_problem) is harder than the \"direct\" one, because we are trying to infer unobservables from observables. Wikipedia gives an example of figuring out the position of Neptune from the perturbations in the orbit of Uranus. Another popular example is NP-complete problems: they are famously hard to solve but it is easy to verify a solution. Another example: you take **a multiple-choice math quiz**, it is often faster and easier to get the right answer by plugging the 4 or 5 potential solutions into the stated problem than to solve the problem directly.\n\nI'll give an example from my own area. The equations of **general relativity** are hard to solve except in a few highly symmetric cases. It is a classic inverse problem. But! Any spacetime metric is actually a solution of the Einstein equations, so all one needs to do is to write down a metric and calculate its Einstein tensor to see what kind of a matter distribution (and boundary conditions) it is a solution of. **Inverting the inverse problem!** Of course, life is not that easy. Most solutions correspond to \"unphysical\" matter, usually with negative energy density, superluminal flows, singularities, infinities, weird topologies etc. However, it is a useful approach if one wants to study some general properties of the equations, and get a feel for (or sometimes a theorem about) what goes wrong, why and how. After a few iterations one can get better at guessing what form a \"good\" solution might take, and write up an ansatz that can help solve the original, not the inverse problem in some cases.\n\nAnother, more familiar example: **arithmetic division**. Until you learn or figure out the rules, it's hard. But its inverse problem, multiplication, is actually much easier! So to learn more about division, it pays to try to start with potential solutions and see what kind of multiplication actually solve the division problem. Eventually one can come up with the long division algorithm, that uses nothing but multiplication and subtraction. And voila, inverting an inverse problem helps us solve the original one.\n\nThis approach is common in computer science, as well. Plenty of algorithms, like **search**, actually rely on solving smaller and simpler inverse problems.\n\nI contend that a similar approach could be useful for making progress in understanding embedded agency. To that end, let's first restate the original problem of embedded agency ([copied from the alignment forum page](https://www.alignmentforum.org/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version)):\n\n**How can one make good models of the world that are able to fit within an agent that is much smaller than the world?**\n\nThis is a hard inverse problem! There are many faucets of it, such as the oft-mentioned problem of logical counterfactuals, that do not seem to yield to direct attacks. So, it seem natural to learn to \"seek under the light\" before stepping into the darkness, and that includes, you guessed it, constructing toy models and inverting the inverse problems.\n\nWhat would inverting this problem look like? There are multiple possible formulations, just like an inverse of the operation of power a^b is both n-th root and logarithm. Here is a couple of ideas:\n\n* Create a toy universe and look for its representations inside.\n* Create a toy model and construct a world around it such that the model represents the world in some way.\n\nHere is an example: a **fractal** is self-similar, so any subset of it can be thought of as a near-perfect model of the whole. Of course, a model is not enough, one has to figure out what would constitute an agent using this model in this fractal world. But at least it can be a promising and potentially illuminating direction to explore. There are plenty more ideas one can come up after thinking about it for 5 minutes.\n\nI hope someone at MIRI is either thinking along these directions, or is ready to try to, instead of being stuck analyzing the messy and complicated inverse problem that is the \"real world\".", "url": "https://www.alignmentforum.org/posts/itGmH2AknmjWyAwj8/double-inverse-embedded-agency-problem", "date_published": "2020-01-08T04:30:25Z", "authors": ["shminux"], "tags": ["embedded agency"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:40:42.284732+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "303d09e63768206ddd97dd4c25b9336e", "source": "alignmentforum", "title": "[AN #81]: Universality as a potential solution to conceptual difficulties in intent alignment", "text": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.\n\nAudio version [here](http://alignment-newsletter.libsyn.com/alignment-newsletter-81) (may not be up yet).\n\nPublished a year ago, [this](https://ai-alignment.com/towards-formalizing-universality-409ab893a456) [sequence](https://ai-alignment.com/informed-oversight-18fcb5d3d1e1) [of](https://ai-alignment.com/universality-and-model-based-rl-b08701394ddd) [five](https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd) [posts](https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d) introduced the idea of *ascription universality*. I didn't really get it on a first reading, and only recently read it in enough detail that I think I understand the main ideas. This entire newsletter will focus on ascription universality; treat all of it as a \"Highlight\".\n\nThe key idea of these posts is that of *universality*: when we can say that some agent \"knows everything that any other agent could know\". Of course, there will always be some agent with arbitrarily complex beliefs, but we could hope to have agents that know everything that is known by any agent of some complexity class (e.g. agents that run in polynomial time). However, there are still simple programs that can have complex *true* beliefs: we could make agents that believe that P != NP and that P = NP, and one of those agents will fall into this class. However, these agents are somewhat degenerate -- they don't have a good reason to *trust* that their belief is true, and so we don't want to worry about them. Since we want to talk about *justified* belief, let's consider a property based on which of the two agents *we* would trust.\n\nSo, for now let's adopt an informal definition: a program A[C] is universal with respect to some class of programs C if we would trust any beliefs reported by A[C], no matter what beliefs we hear reported by programs in C. (I'll talk about the specific notion of \"ascription universality\" later.) Notably, we need to trust A[C] even over our own beliefs, that is, if A[C] believes something, we discard our position and adopt A[C]'s belief.\n\nNote that this is still a very strong criterion. Consider the question of whether AI will lead to x-risk: if one agent thinks about fixed-point theorems, while another studies neural nets, and a third tries to predict the response of democratic institutions, the agent A[C] must compete with *everything* that these agents (justifiably) conclude. For now, let's set aside the question of how to build a universal agent, and instead understand what it enables us to do.\n\n[Informed oversight (revisited)](https://ai-alignment.com/informed-oversight-18fcb5d3d1e1) *(Paul Christiano)* (summarized by Rohin): Introduced in [this older post](https://ai-alignment.com/the-informed-oversight-problem-1b51b4f66b35), we consider a setting in which an overseer is training an agent, and the overseer is smarter than the agent. Intuitively, this should work out well, because if the agent ever tries to do something bad, the overseer should notice this (since it is smarter), and penalize it. However, the problem is that the agent might *in the course of computing an answer* come up with considerations that the overseer can't easily find. For example, an agent tasked with creating novel literature might end up plagiarizing from one book; the overseer may not be able to search through all the books to detect the plagiarism. However, if the overseer is universal, then it should also know that the agent's output is plagiarized from the book (because the agent knows that), and it can penalize it appropriately.\n\nWe can even construct a semi-formal argument (see the post for the math). Let us assume that there is some true utility function U, and when the agent takes some action a, the overseer provides it with reward r = E[U(a)], where the expectation is relative to the overseer's beliefs. Let us assume away problems of distribution shift: in particular, we'll assume that when choosing between actions a1 and a2, the agent always chooses the action with higher reward. Now suppose that the agent chooses action a1. We can conclude that a1 has higher reward than a2. This means that the observer believes that a1 has higher true utility than a2. But by universality, we always trust the observer over the agent and ourselves, and so we must also believe that a1 has higher true utility than a2, which means that we believe the agent is choosing the right action.\n\n**Rohin's opinion:** This application of universality is the one I believe the most; even intuitively it seems like the entire informed oversight problem is that the overseer *doesn't* know everything that the agent knows, and universality straightforwardly fixes that. So I see this as the motivating example for caring about universality at all.\n\n[Worst-case guarantees (Revisited)](https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d) *(Paul Christiano)* (summarized by Rohin): Let's start considering distribution shift again: we imagine that the agent has already been trained, and there is no overseer any more. When the agent goes off distribution, it may not choose the action that would have gotten the highest reward, and in fact it could be chosen maliciously. As a silly motivating example, perhaps the agent learned the policy \"behave well until I see that RSA-2048 (a product of two very large primes) has been factored\"; such a policy will behave perfectly during training but will probably commit a treacherous turn eventually.\n\nIn this setting, we would like to have a worst-case guarantee, that the model will never behave unacceptably. However, we need to enforce this guarantee at training time, *before* we know what situations will arise at test time. Universality of the overseer isn't enough to guarantee this, because universality only means that the overseer knows what the agent *currently* knows; the agent can't really be said to \"know\" that it will deceive us in the future.\n\nNonetheless, it seems like the solution is very related to universality, in that we would like to \"peer inside\" the agent to figure out how it works, to make sure there aren't situations in which it would behave unacceptably. We could use adversarial training for this purpose: train an adversary to find inputs on which the model behaves unacceptably, and then train the model on those inputs to not do that. This will require interpretability techniques, so that the adversary actually stands a chance of finding the relevant inputs. In order to handle cases like RSA-2048 above, the adversary should also be able to provide \"pseudo-inputs\" like \"the world in which the agent sees that RSA-2048 is factored\" (otherwise the adversary would have to factor RSA-2048 itself to demonstrate an input on which the agent would fail, which is far too difficult), while not being allowed to provide pseudo-inputs like \"the world in which the agent kills all humans\" (which no agent can behave acceptably on). Ultimately, we need to make sure the adversary will always be able to find a bad pseudo-input if one exists, without making it too difficult for the agent to behave acceptably on all pseudo-inputs. See also [relaxed adversarial training](https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment) ([AN #70](https://mailchi.mp/732eaa192df0/an-70-agents-that-help-humans-who-are-still-learning-about-their-own-preferences)).\n\n**Rohin's opinion:** I was skeptical of the [previous iteration of this post](https://www.alignmentforum.org/posts/E2aZ9Xwdz3i2ghPtn/techniques-for-optimizing-worst-case-performance) ([AN #43](https://mailchi.mp/768a8130013f/alignment-newsletter-43)), which proposed adversarial training, verification, and interpretability, because it seemed like none of them would allow us to get a *guarantee* on performance. I'm significantly more optimistic after reading this post, mainly because a couple of previous confusions have been resolved:\n\n1. The point of verification is not that we can prove a theorem saying \"this agent is beneficial\"; the point is that by making *relaxations* (pseudo-inputs), a technique commonly used in formal verification, we can reduce the burden on the other methods being used (such as adversarial training).\n\n2. Similarly, the point of interpretability is not to help *us* understand what the agent is doing or will do, it's to help the *overseer* (or adversary in adversarial training) understand that. Unlike us, the overseer / adversary can scale up along with the agent itself.\n\nI still think that it would be hard to get a guarantee with adversarial training, given that adversarial training has to eliminate *all* vulnerabilities. On the other hand, it only has to find all of the settings where the agent is *maliciously optimizing against us*, which you might hope is a more natural category that is easier to identify without looking too much at particular inputs. This seems like an empirical question on which we'll hopefully get data, though even if it works in all cases that we see, that doesn't rule out the possibility that we failed to notice some issue that will only be triggered in the future (as in the RSA-2048 example).\n\n[Universality and model-based RL](https://ai-alignment.com/universality-and-model-based-rl-b08701394ddd) *(Paul Christiano)* (summarized by Rohin): So far, we've been talking about the model-free setting, where the overseer provides the incentives. What about model-based RL? Here, we might want to learn separate distributions over models and utility functions using iterated amplification or HCH, and then plan using any off-the-shelf algorithm, such as MCTS. The first new problem that arises is that our distribution over models may be malicious (see [benign induction](https://agentfoundations.org/item?id=1263)). Using universality, we can solve this problem in two different ways. First, we could use HCH to filter out malicious models. By universality, since a malicious model knows what its goals are, so does HCH, and thus HCH will detect it. Second, we could ask HCH to give us a good prior over models. By universality, this prior over models must be better than the prior used by the malicious models, and so the malicious models' predictions will be worse than that of HCH as a whole, and so the malicious models will not be able to affect the computation much.\n\nThe second new problem in the model-based setting is that it isn't clear how our utility function can extract all the relevant information from the model. For example, if the model is a detailed simulation of physics, how do you find the \"humans\" and evaluate how \"happy\" they are? Universality is at least necessary for this: if there are facts the model \"knows\", those facts may be relevant to the utility calculation, and so the utility function also needs to know them. We might hope that universality is sufficient: in the previous case, we might think that the physics simulation \"knows\" whether humans are happy. But this depends on details of what \"know\" means that haven't yet been fleshed out, and it depends on what \"kind\" of universality we can achieve in practice.\n\n[Universality and consequentialism within HCH](https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd) *(Paul Christiano)* (summarized by Rohin): In all the settings discussed so far, we hope to use [HCH](https://www.alignmentforum.org/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch) ([AN #34](https://mailchi.mp/f1947668b183/alignment-newsletter-34)). What if HCH itself contains malicious optimization? Specifically, recall that HCH involves an arbitrarily large tree of humans breaking questions down into subquestions. Memetic selection on 10^100 humans should be *much* stronger than the selection done by evolution to produce us, and so could be quite dangerous. We would like to use HCH to check whether its own computations are malicious.\n\nSo, we'll define a pair of mutually recursive functions, Filter and Filtered-HCH. Filter takes as input a transcript from HCH, and using filtered-HCH it finds the best argument suggesting that the transcript is problematic. It then uses filtered-HCH to ask whether, in light of this argument, the transcript should be treated as problematic. This check is repeated thrice for robustness. (Note that the original HCH treats the transcript as trusted computations, while the checker treats it as an untrusted input.) Filtered-HCH has the same type signature as HCH (i.e. it produces answers to questions). Just like HCH, it computes an answer to the question, with the help of recursive calls to filtered-HCH to get answers to subquestions. Instead of returning the answer directly, it calls Filter to check that the computation done was problematic, and returns the special symbol ? instead of the answer if so.\n\nThe basic hope is that if there's some malicious subcomputation implemented \"on top of\" the base agents in HCH, and it tries to break HCH, then it \"knows\" why that attempt could work, and so if filtered-HCH is universal with respect to computations that can be instantiated by (smaller versions of) HCH, then filtered-HCH will also \"know\" that the computation could break HCH, and will censor it as a result. Consider the threat model from [Reliability amplification](https://www.alignmentforum.org/posts/6fMvGoyy3kgnonRNM/reliability-amplification) ([AN #44](https://mailchi.mp/6bfac400a0c3/alignment-newsletter-44)), where the chance of any particular HCH computation with budget n becoming malicious is ?, independent of the input or any past HCH calls. Then, intuitively, filtered-HCH (which needs a higher budget, say kn) has a k? chance of becoming malicious, but since the check is repeated three times and a majority vote is taken, the chance of a malicious computation getting through is k^2?^2, which will be much smaller than the original ? if k^2 ? << 1. (This is significantly better than the scheme in [Reliability amplification](https://www.alignmentforum.org/posts/6fMvGoyy3kgnonRNM/reliability-amplification) ([AN #44](https://mailchi.mp/6bfac400a0c3/alignment-newsletter-44)), because the question \"is there a problem?\" is a binary yes/no question, and so majority vote works a lot better.) The harder case is when specific inputs cause very high probabilities of failure (which I believe is the threat model from [Security amplification](https://www.alignmentforum.org/posts/hjEaZgyQ2iprDhkg8/security-amplification) ([AN #45](https://mailchi.mp/35b451cb4d70/alignment-newsletter-45))). I particularly don't understand this section, but I believe the hope is that since the malicious computation is implemented \"on top of\" the (aligned) humans, only a small \"part\" of the computation can be malicious, and this can be detected by a *small* instance of Filtered-HCH on which the malicious computation cannot be implemented.\n\n**Rohin's opinion:** I don't understand this post that well, so I'll refrain from opining on it. I think I would benefit from a more concrete example of how malicious optimization could be implemented \"on top of\" HCH, even when the humans are restricted to a small set of inputs on which they are \"secure\".\n\n[Towards formalizing universality](https://ai-alignment.com/towards-formalizing-universality-409ab893a456) *(Paul Christiano)* (summarized by Rohin): We've now seen a bunch of applications of universality. So what is *ascription* universality? It's a specific formalization of the concept of universality. Recall our informal definition so far: a program A[C] is universal with respect to some class of programs C if we would trust any beliefs reported by A[C], no matter what beliefs we hear reported by programs in C. But how can a program \"report beliefs\"? For HCH, to find out its belief for some question, we simply ask HCH the question and take its answer as its \"belief\". However, we can't do this for arbitrary computations: in particular, if our agent is deliberately hiding a belief from us (and so wouldn't report it if asked), we shouldn't simply say that it doesn't have that belief. This is where *ascriptions* come in: an ascription is any way of ascribing beliefs to the agent. Simply asking the agent and trusting its answer is one possible ascription. Another class of ascriptions would look at the agent's code and memory, and make inferences about the agent's beliefs from that. We want to say that HCH will dominate *any* \"reasonable\" ascription of beliefs to the agent, where an ascription procedure is \"reasonable\" if it isn't \"doing the hard work\"; for example, if an agent always says \"Yes\", it is unreasonable to ascribe to it the belief that Fermat's Last Theorem is true (even though when given the question \"Is Fermat's Last Theorem true\" it answers yes).\n\nPutting it all together, A[C] is *ascription universal* with respect to some class of programs C if, for every \"reasonable\" ascription procedure and program c in C, A[C]'s beliefs (which we find by asking it) *epistemically dominate* the beliefs ascribed to c. A set of beliefs B1 *epistemically dominates* another set of beliefs B2 *from our perspective* if, given arbitrary information about B1 and B2, for every question our best guess at the answer is what we expect B1 would answer.\n\nSo far we've been talking about A[C] dominating an entire class of programs C. We might call this *idealized* ascription universality, and hope that it applies to systems with perfect optimization, such as [debate](https://blog.openai.com/debate/) ([AN #5](https://mailchi.mp/0ae5d69de63b/alignment-newsletter-5)) under optimal play. In practice, we'll never get perfect optimization, and our overseer isn't going to be able to think of *everything* that any agent could be thinking about. Instead, we could aim for *practical* ascription universality, in which A[c] dominates a particular computation c, and it gets to look at the computation c as a hint. This would allow an overseer to train an agent, while epistemically dominating that agent. Effectively we're asking two questions: first, would our procedure be universal if the optimization was performed perfectly? Second, is weight sharing / interpretability strong enough to solve the optimization problems needed for universality?\n\nThe blog post also has some informal thoughts on why we should expect that a careful Bayesian should be able to epistemically dominate computations found via machine learning (which is why we might hope that HCH could be ascription universal).\n\n**Rohin's opinion:** I was initially a bit confused by the decomposition into the idealized and the practical variant, but I think it makes more sense now: the assumption of *perfect* optimization gives you a lot of mileage. For example, A[C] might choose a particular computation c in C, and then use its beliefs; if that *choice of computation* were already \"perfectly optimized\" for convincing us, then we would always be at least as convinced of A[C]'s answer as the answer of any other c in C. Debate under optimal play is pretty similar: under optimal play, the agent only tells us about the most important considerations, and only brings up other considerations if its opponent asks about them; you would hope that this is like selecting the best c in C and reporting its reasons for its beliefs.\n\nWhat's happening here is that the perfect optimization allows us to select the best c in C, or the best argument, etc. \"for free\" (i.e. without using any computation), giving us huge savings; everything else just has to present the results properly. Consider again the question of whether AI will lead to x-risk: while there are different agents thinking about fixed-point theorems, neural nets, and democratic institutions, perfect optimization lets us \"pick\" the agent whose arguments are most important. Then, we should only need about as much computation as the agent itself.\n\nHowever, \"presenting the results properly\" is non-trivial: A[C] needs to report the beliefs *in plain English*, even if the computation c in C has its beliefs \"encoded\" (as in the case of a detailed physics simulation that can be said to \"know\" that humans are happy, but doesn't say that in plain English). It's not clear to me whether this will work out.\n\nYou might very reasonably be worried that perfect optimization of \"the most convincing argument\" will get you something that is convincing for the wrong reasons, and is not actually correlated with the truth. The definition of universality doesn't preclude this: it simply says that we trust A[C]'s beliefs over our own beliefs and any other agent's beliefs; this does hold for convincing but invalid arguments. I believe (but am not sure) that the hope is to deal with this by restricting inputs to the human to a small secure space, and by the techniques discussed above in [Universality and consequentialism within HCH](https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd).\n\n[Nuances with ascription universality](https://www.alignmentforum.org/posts/R5Euq7gZgobJi5S25/nuances-with-ascription-universality) *(evhub)* (summarized by Rohin): Consider a formalization of honesty / transparency related to ascription universality: a system is honest, if relative to our expectations, its explicitly stated beliefs give us just as much information as any other way of ascribing beliefs to the system. A system that searches for ways to commit a treacherous turn would not be honest, since it would not state that it was performing this search. However, what if this search was \"distilled\" into the weights of a neural net (akin to a lookup table)? In that case, since we wouldn't ascribe beliefs to the lookup table, the system could be deemed honest by this criterion. So, what we actually need is for the entire training process to be honest. (A similar critique applies directly to ascription universality.)\n\n**Rohin's opinion:** This is a good point; it shows that you need your overseer to be ascription universal with respect to the entire training process, not just the final agent.", "url": "https://www.alignmentforum.org/posts/3kzFPA5uuaGZWg4PS/an-81-universality-as-a-potential-solution-to-conceptual", "date_published": "2020-01-08T18:00:02Z", "authors": ["Rohin Shah"], "tags": ["ai", "newsletters"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:40:42.285143+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "088f3d3eb0aa6beb183f7b52831c5d55", "source": "alignmentforum", "title": "Outer alignment and imitative amplification", "text": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\nUnderstanding the outer alignment problem\n-----------------------------------------\n\n\nWhat really is outer alignment? In \"[Risks from Learned Optimization](https://arxiv.org/abs/1906.01820),\" we defined outer alignment in the context of machine learning as \"aligning the specified loss function with the intended goal.\" But that's not a perfectly well-defined statement--what does it mean for a loss function to be \"aligned\" with the intended goal? If the goal we care about is maximizing U, do we need exactly L=?aU+b for constants a,b? That's a pretty high bar.\n\n\nWell, what exactly do we want outer alignment for? At the end of the day, we care about whether the model that pops out the other end of our training procedure will be safe, which is a complicated question involving the loss function, the architecture, the implicit inductive biases, and so on. In what sense, then, is it even reasonable to look at just the lost function in isolation and ask whether it's aligned or not?\n\n\nI think the strongest case for outer alignment being a meaningful problem in isolation comes from the argument that loss functions seem to scale pretty well with generic machine learning progress. If, as a silly example, your outer alignment scheme is to \"train image classification models,\" that's something that ML has progressively gotten better at over time. Compare that to the silly inner alignment scheme of \"train a straightforward CNN\"--that's something that ML has passed by pretty rapidly in favor of architectural improvements like residual connections even just for the task of image classification. Of course, outer alignment alone does not an aligned AGI make, so you still have to have some notion of how you're going to do inner alignment in mind--but loss functions scaling better is still a perfectly valid reason for focusing on outer alignment.[[1]](#fn-wY7Ni5JBsYFJxwg7g-1)\n\n\nThus, it does seem quite reasonable to me to put effort into finding \"aligned\" loss functions. But that still brings us back to the question of what exactly makes a loss function \"aligned.\" In the context of a specific training/inner alignment scheme, we can say that a loss function is aligned if, when plugged into that training scheme, it produces models which are aligned with our goals. But in the absence of any specific training scheme, what does it mean to say that a loss function is aligned in isolation? We can of course ask for L=?aU+b as I stated previously, though in my opinion I think achieving something like that is likely to be nearly impossible.\n\n\nOuter alignment at optimum\n--------------------------\n\n\nI think there is another version of \"outer aligned in isolation,\" however, which is both meaningful and (at least somewhat) achievable which I will call *outer aligned at optimum.* Intuitively, I will say that a loss function is *outer aligned at optimum* if all the possible models that perform optimally according to that loss function are aligned with our goals--that is, they are at least [trying to do what we want](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6). More precisely, let M=X?A and L=(X?A)?R=M?R. For a given loss function L?L, let l?=minM?M L(M). Then, L is *outer aligned at optimum* if, for all M??M such that L(M?)=l?, M? is trying to do what we want.\n\n\nThat's the definition--now why should we care? In basically any practical setting we're never going to reach perfect loss, so why should it matter if those functions which do have perfect loss are aligned or not? In my opinion, I think there is a strong argument for loss functions which are aligned at optimum being significantly less susceptible to [Goodhart's Law](https://en.wikipedia.org/wiki/Goodhart%27s_law) as we scale up ML capabilities. Suppose you know that a loss function L is aligned for current ML capabilities. When you then scale up those capabilities and push harder on minimizing L, you immediately run into all the issues of Goodhart's Law where L can quickly cease to be a good proxy for alignment as you push harder on it. If you have a guarantee that L is aligned at optimum, however, then, while still quite possible, it's a lot harder for Goodhart's Law to bite you. In particular, if you think about the [Goodhart taxonomy](https://www.alignmentforum.org/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy), alignment at optimum almost entirely rules out both Causal and Extremal Goodhart--since you know the relationship is valid and doesn't break down at the extremes--and ensures that Regressional and Adversarial Goodhart won't show up in the limit, though you could still see them before that point. Though this obviously doesn't just give you an alignment guarantee--before you get to the true optimum, you can still get Regressional Goodhart biting you through [proxy alignment](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/pL56xPoniLvtMDQ4J) or Adversarial Goodhart biting you through [deceptive alignment](https://www.alignmentforum.org/posts/zthDPAjh9w6Ytbeks/deceptive-alignment), for example--I think it is nevertheless still a very nice thing to have.\n\n\nThe case for imitative amplification\n------------------------------------\n\n\nWith all of that being said, I can get to the reason that I want to talk about all of this: I think that specifically what I will call *imitative amplification*--in contrast to other [amplification](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd)-based approaches or [debate](https://arxiv.org/abs/1805.00899)-based approaches--has a strong claim to being outer aligned at optimum.[[2]](#fn-wY7Ni5JBsYFJxwg7g-2) Specifically, when I say *imitative amplification,* I mean the class of training procedures which are attempting to produce models which approximate [HCH](https://ai-alignment.com/strong-hch-bedb0dc08d4e) as closely as possible. As a concrete example, consider the scheme where you train a model to minimize the difference between its output and the output of a human consulting that model. I want to contrast this with *approval-based amplification,* by which I mean the class of training procedures where the loss is generated using an approval signal from an amplified overseer. As a concrete example, consider the scheme where you train a model to maximize the extent to which a human consulting that model would approve of that model's output.[[3]](#fn-wY7Ni5JBsYFJxwg7g-3)\n\n\nSo, why does imitative amplification have a stronger case for being outer aligned at optimum than approval-based amplification or debate? Well, precisely because we know what the optimum of imitative amplification is--it's HCH--whereas we really don't know what perfect approval-based amplification or perfect debate look like.[[4]](#fn-wY7Ni5JBsYFJxwg7g-4) Though [some challenges have been raised regarding whether HCH is actually aligned or not](https://www.alignmentforum.org/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal), I tend to be fairly skeptical of these challenges--HCH is just a bunch of humans after all and if you can instruct them not to do things like instantiate arbitrary Turing machines, then I think a bunch of humans put together has a strong case for being aligned.[[5]](#fn-wY7Ni5JBsYFJxwg7g-5) That being said, the same argument does not at all apply to approval-based amplification or debate.\n\n\nFirst, let's consider approval-based amplification.[[6]](#fn-wY7Ni5JBsYFJxwg7g-6) We know what the optimum of imitative amplification looks like--but what is the optimum of approval-based amplification? At first glance, one might imagine that the optimum of approval-based amplification looks like a model whose output is selected to be maximally approved of by HCH. That's very much not the case for the approval-based scheme I described earlier, however. If each step of training is done via maximizing an approval signal, then instead of a tree of just humans you get a tree of humans interspersed with models trying to maximize the approval that their descendants in the tree would assign to their answers. And if you think that human approval can be gamed--which seems extremely likely in my opinion given that we see exactly that sort of gaming happening in our world already all the time--then this is very much not a safe tree. Now, one could make the argument that approval-based amplification can just become imitative amplification if the humans determine their approval by computing a distance function between what they would have said and what the model produced as its output. For example, you could ask your humans to come up with their answers first, then show them the model's answer and ask them to rate how close it was. I'm pretty skeptical of this approach, however--it doesn't seem at all clear to me that this gets around the approval-gaming problem, since the humans still get to see the model's answer and doing so could significantly change how they're thinking about the rating problem.[[7]](#fn-wY7Ni5JBsYFJxwg7g-7)\n\n\nNow, second, let's consider debate with a human judge. In many ways, debate was designed as an approach meant to fix the problems of approval-based reward signals. With a standard approval-based reward signal, the argument goes, it's easy to be tricked by a bad argument that you don't fully understand. In a debate setup, however, you get the benefit of having two competing systems trying to point out flaws in each other's arguments, which hopefully should prevent you from being tricked by bad arguments and thus fix the approval-gaming problem. I'm not convinced, though--false things can be significantly easier to argue for than true things just because there are fewer ways to attack them, they're more rhetorically powerful, or any other number of possible ways in which an argument can be subtly wrong yet still persuasive.[[8]](#fn-wY7Ni5JBsYFJxwg7g-8) Regardless, however, I think the more fundamental objection is just that we really have no way of knowing what optimal play in debate looks like, which makes it very difficult to ever know whether it is outer aligned at optimum or not. With HCH, we know that it just looks like a tree of humans, which at least means we can reason about the parts and how they interact. With optimal debate, however, we have to somehow analyze, understand, and be confident in the alignment of superhuman play on a game involving humans assessing arbitrary strings of characters, which is something that in my opinion seems extremely difficult to do.\n\n\nAddressing competitiveness concerns\n-----------------------------------\n\n\nAll of that is an argument for why we should prefer imitative amplification from an alignment standpoint. However, there's also the problem of imitative amplification just not being competitive in terms of capabilities with other approaches. First of all, I think it's important to remember the importance of putting safety first--*if something isn't safe, then we shouldn't build it.* Of course, arms race dynamics could end up pushing one's hand into going with a best available current option in order to beat some other team which one believes will produce an AI which is even less likely to be safe, though I think it's important to remember that that's a *last resort,* not the default option. Furthermore, even in such a situation, it's still probably fine to eat an overhead cost that is just something like a constant factor worse.\n\n\nWith that being said, I still think there are strong arguments to be made for why imitative amplification can be done competitively. First, like the silly outer alignment scheme of \"just train an image classification model\" from earlier, imitative amplification gets to piggy-back off of generic ML progress. Imitative amplification is just a language modeling problem, which means generic progress on language modeling should generally be transferable to imitative amplification. Second, I think there is a strong case for language being sufficiently rich as a dataset for training an AGI (EDIT: where \"language\" is construed to also include embedded images, videos, etc.), at least for the sorts of tasks which I think you will want to use your first AGI for.[[9]](#fn-wY7Ni5JBsYFJxwg7g-9) For example, if the primary/most important purpose of your first AGI is to help you build your second AGI by helping you improve your AGI design, that's the sort of highly cognitive task which I think language is sufficient for. Certainly, if you needed your first AGI to be able to do fine motor control to be competitive, then imitative amplification probably won't get you there--but it seems pretty unlikely to me that ability to do fine motor control will be an important desiderata. Third, a common criticism of imitative amplification is that because imitation treats all data points the same, it won't be able to focus on the most important ones. However, that's not something that's fundamental to the task of imitation. For example, you could use [active learning](https://en.wikipedia.org/wiki/Active_learning_(machine_learning)) to select the most important data points rather than just using a fixed curriculum.[[10]](#fn-wY7Ni5JBsYFJxwg7g-10) Or, you could even weight different data points in your imitation loss using some outside importance criterion while still maintaining the guarantee of perfect imitation at optimum.\n\n\nRegardless, I think the case for imitative amplification's safety is a strong argument in favor of at least focusing on figuring out whether it works and is safe first, before we give up and move to other approaches. Furthermore, even if imitative amplification on its own isn't competitive, I don't think that means we have to abandon it completely--there are modifications to imitative amplification that can be made to help improve competitiveness without sacrificing all of its benefits. For example, you could do reward-modeling-based distillation (e.g. RL + IRL as the distillation step) instead of imitation-based distillation, which, while not imitative (as the optimum isn't HCH anymore), also isn't based on human approval, which could be a nice property. Alternatively, you could first train an HCH model, and then use that model as the judge to train a debate model, which could have significant benefits over just using a human judge. While I don't think we should be focusing on those sorts of things now, the fact that such options exist makes it more likely that imitative amplification work can transfer to future approaches even if imitative amplification itself ends up not being competitive. In any event, I think the case for focusing on imitative amplification right now both from an outer alignment perspective as well as from a competitiveness perspective is quite strong.[[11]](#fn-wY7Ni5JBsYFJxwg7g-11)\n\n\n\n\n---\n\n\n\n1. There is still lots of potential for outer alignment work to be outdated by machine learning progress, however--see, for example, Daniel Kokotajlo's \"[A dilemma for prosaic AI alignment](https://www.lesswrong.com/posts/jYdAxH8BarPT4fqnb/a-dilemma-for-prosaic-ai-alignment).\" [??](#fnref-wY7Ni5JBsYFJxwg7g-1)\n2. I mentioned this previously a bit in [this comment](https://www.alignmentforum.org/posts/jYdAxH8BarPT4fqnb/a-dilemma-for-prosaic-ai-alignment#FhHeRrkrvJp5fYELj). [??](#fnref-wY7Ni5JBsYFJxwg7g-2)\n3. Note that the two categories of \"imitative\" and \"approval-based\" amplification do not cover the entire space of possible amplification-based approaches--there are other possible schemes in this domain as well. For example, you could use imitative amplification to train an HCH approximator, then do RL to produce a model which maximizes that model's approval--or even use your HCH model as a judge in a debate. Alternatively, you could do imitative amplification but instead of using standard imitation learning you could do IRL + RL instead. All of these different approaches have different alignment properties--I have singled out imitative amplification, approval-based amplification, and debate with a human judge because they are the approaches I'm most interested in talking about there, though they are far from the only ones. [??](#fnref-wY7Ni5JBsYFJxwg7g-3)\n4. Note that for the optimum of imitative amplification to be precisely HCH, you need it to be the case that you progressively enlarge your training data as you go along. The fact that you don't get good guarantees for finite datasets is certainly a problem, though it's one that you basically have to solve via inner alignment techniques and thus not one I want to focus on right now. [??](#fnref-wY7Ni5JBsYFJxwg7g-4)\n5. The question of whether theoretical HCH is aligned or not is a pretty complicated question that I don't really want to go into in full detail right now, so if you strongly disagree just take it as a given for this post. [??](#fnref-wY7Ni5JBsYFJxwg7g-5)\n6. Though there was [a previous claim by William Saunders that RL amplification and imitative amplification are equivalent](https://www.alignmentforum.org/posts/fq7Ehb2oWwXtZic8S/reinforcement-learning-in-the-iterated-amplification#vtMDKELmpdJS7n7JT), I think that both of William's proposals there fall into my approval-based category, not my imitative category. See [Rohin Shah's](https://www.alignmentforum.org/posts/fq7Ehb2oWwXtZic8S/reinforcement-learning-in-the-iterated-amplification#vtMDKELmpdJS7n7JT) and [Wei Dai's](https://www.alignmentforum.org/posts/fq7Ehb2oWwXtZic8S/reinforcement-learning-in-the-iterated-amplification#Gfd3H6ZS8dYxQvCJE) comments on William's post to that effect. [??](#fnref-wY7Ni5JBsYFJxwg7g-6)\n7. In particular, this breaks the [analogy to counterfactual oracles](https://www.lesswrong.com/posts/yAiqLmLFxvyANSfs2/counterfactual-oracles-online-supervised-learning-with). [??](#fnref-wY7Ni5JBsYFJxwg7g-7)\n8. I have a lot more to say on this point regarding reasons why false arguments can be more persuasive than true ones, though that's not something I want to go into in too much detail right now. [??](#fnref-wY7Ni5JBsYFJxwg7g-8)\n9. Much of my thinking here owes a debt to Geoffrey Irving. I also talked about the case for language being all you need a bit previously in [this comment](https://www.lesswrong.com/posts/jYdAxH8BarPT4fqnb/a-dilemma-for-prosaic-ai-alignment#aCAwppMhHorp42rkD). [??](#fnref-wY7Ni5JBsYFJxwg7g-9)\n10. Such an active learning scheme could even borrow lessons from [across-episode exploration in RL](https://www.alignmentforum.org/posts/NBffcjqm2P4dNbjrE/exploring-safe-exploration). [??](#fnref-wY7Ni5JBsYFJxwg7g-10)\n11. I also think imitative amplification has some nice inner alignment properties as well, since it gives you an amplified overseer to use for [transparency](https://www.alignmentforum.org/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety)/[relaxed adversarial training](https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment). [??](#fnref-wY7Ni5JBsYFJxwg7g-11)", "url": "https://www.alignmentforum.org/posts/33EKjmAdKFn3pbKPJ/outer-alignment-and-imitative-amplification", "date_published": "2020-01-10T00:26:40Z", "authors": ["evhub"], "tags": ["ai", "goodhart's law", "outer alignment"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:40:42.286127+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "3dcc44b42816621c1d8013a8f0b1adf4", "source": "alignmentforum", "title": "Of arguments and wagers", "text": "*Automatically crossposted from* [*ai-alignment.com*](https://ai-alignment.com/)\n\n \n\n(In which I explore an unusual way of combining the two.)\n\nSuppose that Alice and Bob disagree, and both care about Judy's opinion. Perhaps Alice wants to convince Judy that raising the minimum wage is a cost-effective way to fight poverty, and Bob wants to convince Judy that it isn't.\n\nIf Judy has the same background knowledge as Alice and Bob, and is willing to spend as much time thinking about the issue as they have, then she can hear all of their arguments and decide for herself whom she believes.\n\nBut in many cases Judy will have much less time than Alice or Bob, and is missing a lot of relevant background knowledge. Often Judy can't even understand the key considerations in the argument; how can she hope to arbitrate it?\n\nWagers\n\nFor a warm-up, imagine that Judy could evaluate the arguments if she spent a long enough thinking about them.\n\nTo save time, she could make Alice and Bob wager on the result. If both of them believe they'll win the argument, then they should be happy to agree to the deal: \"If I win the argument I get $100; if I lose I pay $100.\" (Note: by the end of the post, no dollars will need to be involved.)\n\nIf either side isn't willing to take the bet, then Judy could declare the case settled without wasting her time. If they are both willing to bet, then Judy can hear them out and decide who she agrees with. That person \"wins\" the argument, and the bet: **Alice and Bob are betting about what Judy will believe, not about the facts on the ground**.\n\nOf course we don't have to stick with 1:1 bets. Judy wants to know the probability that she will be convinced, and so wants to know at what odds the two parties are both willing to bet. Based on that probability, she can decide if she wants to hear the arguments.\n\nIt may be that both parties are happy to take 2:1 bets, i.e. each believes they have a 2/3 chance of being right. What should Judy believe? (In fact this should always happen at small stakes: both participants are willing to pay some premium to try to convince Judy. For example, no matter what Alice believes, she would probably be willing to take a bet of $0.10 against $0.01, if doing so would help her convince Judy.)\n\nIf this happens, there is an arbitrage opportunity: Judy can make 2:1 bets with both of them, and end up with a guaranteed profit. So we can continuously raise the required stakes for each wager, until either (1) the market approximately clears, i.e. the two are willing to bet at nearly the same odds, or (2) the arbitrage gap is large enough to compensate Judy for the time of hearing the argument. If (2) happens, then Judy implements the arbitrage and hears the arguments. (In this case Judy gets paid for her time, but the pay is independent of what she decides.)\n\nRecursion\n\nBetting about the whole claim saved us some time (at best). Betting about parts of the claim might get us much further.\n\nIn the course of arguing, Alice and Bob will probably rely on intermediate claims or summaries of particular evidence. For example, Alice might provide a short report describing what we should infer from study Z, or Bob might claim \"The analysis in study Z is so problematic that we should ignore it.\"\n\nLet's allow anyone to make a claim at any time. But if Alice makes a claim, Bob can make a counterclaim that he feels better represents the evidence. Then we have a recursive argument to decide which version better represents the evidence.\n\nThe key idea is that **this recursive argument can also be settled by betting**. So one of two things happens: (1) Judy is told the market-clearing odds, and can use that information to help settle the original argument, or (2) there is an arbitrage opportunity, so Judy hears out the argument and collects the profits to compensate her for the time.\n\nThis recursive argument is made in context: that is, Judy evaluates which of the two claims she feels would be a more helpful summary within the original argument. Sometimes this will be a question of fact about which Alice and Bob disagree, but sometimes it will be a more complicated judgment call. For example, we could even have a recursive argument about which wording better reflects the nuances of the situation.\n\nWhen making this evaluation, Judy uses facts she learned over the course of the argument, but she interprets the claim as she would have interpreted it at the beginning of the argument. For example, if Bob asserts \"The ellipsoid algorithm is efficient\" and Alice disagrees, Bob cannot win the argument by explaining that \"efficient\" is a technical term which in context means \"polynomial time\"--unless that's how Judy would have understood the statement to start with.\n\nThis allows Judy to arbitrate disagreements that are too complex for her to evaluate in their entirety, by showing her what she \"would have believed\" about a number of intermediate claims, if she had bothered to check. Each of these intermediate claims might itself be too complicated for Judy to evaluate directly--if Judy needed to evaluate it, she would use the same trick again.\n\nBetting with attention\n\nIf Alice and Bob are betting about many claims over the course of a long argument, we can replace dollars by \"attention points,\" which represent Judy's time thinking about the argument (perhaps 1 attention point = 1 minute of Judy's time). Judy considers an arbitrage opportunity \"good enough\" if the profit is more than the time required to evaluate the argument. The initial allocation of attention points reflects the total amount of time Judy is willing to spend thinking about the issue. If someone runs out of attention points, then they can no longer make any claims or use up any of Judy's time.\n\nThis removes some of the problems of using dollars, and introduces a new set of problems. The modified system works best when the total stock of attention points is large compared to the number at stake for each claim. Intuitively, if there are N comparable claims to wager about, the stakes of each should not be more than a 1/sqrt(N) of the total attention pool -- or else random chance will be too large a factor. This requirement still allows a large gap between the time actually required to evaluate an argument (i.e. the initial bankroll of attention points) and the total time that would have been required to evaluate all of the claims made in the argument (the total stake of all of the bets). If each claim is itself supported by a recursive argument, this gap can grow exponentially.\n\nTalking it out\n\nIf Alice and Bob disagree about a claim (rather, if they disagree about Judy's probability of accepting the claim) then they can have an incentive to \"talk it out\" rather than bringing the dispute to Judy.\n\nFor example, suppose that Alice and Bob each think they have a 60% chance of winning an argument. If they bring in Judy to arbitrate, both of them will get unfavorable odds. Because the surplus from the disagreement is going to Judy, both parties would be happy enough to see their counterparty wise up (and of course both would be happy to wise up themselves). This creates room for positive sum trades.\n\nRather than bringing in Judy to arbitrate their disagreement, they could do further research, consult an expert, pay Judy attention points to hear her opinion on a key issue, talk to Judy's friends--whatever is the most cost-effective way to resolve the disagreement. Once they have this information, their betting odds can reflect it.\n\nAn example\n\nSuppose that Alice and Bob are arguing about how many trees are in North America; both are experts on the topic, but Judy knows nothing about it.\n\nThe easiest case is if Alice and Bob know all of the relevant facts, but one of them wants to mislead Judy. In this case, the truth will quickly prevail. Alice and Bob can begin by breaking down the issue into \"How many trees are in each of Canada, the US, and Mexico?\" If Alice or Bob lie about any of these estimates, they will quickly be corrected. Neither should be willing to bet much for a lie, but if they do, the same thing will happen recursively -- the question will be broken down into \"how many trees are east and west of the Mississippi?\" and so on, until they disagree about how many trees are on a particular hill--a straightforward disagreement to resolve.\n\nIn reality, Alice and Bob will have different information about each of these estimates (and geography probably won't be the easiest way to break things down -- instead they might combine the different considerations that inform their views, the best guess suggested by different methodologies, approximate counts of each type of tree on each type of land, and so on). If Alice and Bob can reach a rational consensus on a given estimate, then Judy can use that consensus to inform her own view. If Alice and Bob can't resolve their disagreement, then we're back to the previous case. The only difference is that now Alice and Bob have probabilistic disagreements: if Alice disagrees with Bob she doesn't expect to win the ensuing argument with 100% probability, merely with a high probability.\n\nOdds and ends\n\nThis writeup leaves many details underspecified. In particular, how does Judy estimate how long it will take her to arbitrate a disagreement? This can be handled in several ways: by having Judy guess, by having Alice and Bob bet on the length of time until Judy reaches a conclusion, by having them make bets of the form \"Alice will agree with me with Z effort,\" or so on. I don't know what would work best.\n\nDespite my use of the word \"recursion,\" the estimate for \"time to settle an argument\" (which Judy uses to decide when the stakes are high enough to step in and resolve a disagreement) probably shouldn't include the time required to settle sub-arguments, since Judy is being paid separately for arbitrating each of those. The structure of the arguments and sub-arguments need not be a tree.\n\nThis is a simple enough proposal that it can be realistically implemented, so eventually we'll hopefully see how it works and why it fails.\n\nI expect this will work best if Alice and Bob often argue about similar topics.\n\nThis scheme was motivated by a particular exotic application: delegating decision-making to very intelligent machines. In that setting the goal is to scale to very complex disagreements, with very intelligent arguers, while being very efficient with the overseer's time (and more cavalier with the arguers' time).\n\n![](https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ee16a0e84cf7)\n\n---\n\n[Of arguments and wagers](https://ai-alignment.com/of-arguments-and-wagers-ee16a0e84cf7) was originally published in [AI Alignment](https://ai-alignment.com) on Medium, where people are continuing the conversation by highlighting and responding to this story.", "url": "https://www.alignmentforum.org/posts/aPsdGPCpcyPqkatgc/of-arguments-and-wagers", "date_published": "2020-01-10T22:20:02Z", "authors": ["paulfchristiano"], "tags": ["betting"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:40:42.287224+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "3b943f80b0c1f994cc3a22432fce8940", "source": "alignmentforum", "title": "Malign generalization without internal search", "text": "In [my last post](https://www.alignmentforum.org/posts/nFDXq7HTv9Xugcqaw/is-the-term-mesa-optimizer-too-narrow), I challenged the idea that inner alignment failures should be explained by appealing to agents which perform explicit internal search. By doing so, I argued that we should instead appeal to the more general concept of *malign generalization*, and treat mesa-misalignment as a special case. \n\nUnfortunately, the post was light on examples of what we should be worrying about instead of mesa-misalignment. Evan Hubinger wrote,\n\n\n> Personally, I think there is a meaningful sense in which all the models I'm most worried about do some sort of search internally (at least to the same extent that humans do search internally), but I'm definitely uncertain about that.\n\nWei Dai expressed confusion why I would want to retreat to malign generalization without some sort of concrete failure mode in mind,\n\n\n> Can you give some realistic examples/scenarios of \"malign generalization\" that does not involve mesa optimization? I'm not sure what kind of thing you're actually worried about here.\n\nIn this post, I will outline a general category of agents which may exhibit malign generalization without internal search, and then will provide a concrete example of an agent in the category. Then I will argue that, rather than being a very narrow counterexample, this class of agents could be competitive with search-based agents. \n\n**The switch case agent**\n-------------------------\n\nConsider an agent governed by the following general behavior, \n\n.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\nLOOP:State = GetStateOfWorld(Observation)IF State == 1:PerformActionSequence1()IF State == 2:PerformActionSequence2()...END\\_LOOP \n\nIt's clear that this agent does not perform any internal search for strategies: it doesn't operate by choosing actions which rank highly according to some sort of internal objective function. While you *could* potentially rationalize its behavior according to some observed-utility function, this would generally lead to more confusion than clarity.\n\nHowever, this agent could still be malign in the following way. Suppose the agent is 'mistaken' about the state of the world. Say that it believes that the state of the world is 1, whereas the actual state of the world is 2. Then it could take the wrong action, almost like a person who is confident in a falsehood and makes catastrophic mistakes because of their error.\n\nTo see how this could manifest as bad behavior in our artificial agents, I will use a motivating example.\n\n**The red-seeking lunar lander**\n--------------------------------\n\nSuppose we train a deep reinforcement learning agent on the lunar lander environment from OpenAI's Gym. \n\n![](https://miro.medium.com/max/1200/0*OeoznZ1fcM56p0Jm.gif)We make one crucial modification to our environment. During training, we make it so the landing pad is always painted red, and this is given to the agent as part of its observation of the world. We still reward the agent like normally for successfully landing in a landing pad.\n\n![](https://i.imgur.com/pVFu3dL.png)Suppose what really determines whether a patch of ground is a landing pad is whether it is enclosed by two flags. Nevertheless, instead of picking up on the true indicator of whether something is a landing pad, the agent may instead pick up the proxy that held during training -- namely, that landing pads are parts of the ground that are painted red.\n\nUsing the psuedocode earlier and filling in some details, we could describe the agent's behavior something like this. LOOP:State = GetStateOfWorld(Observation)IF State == RedIsToTheLeft:ApplyLeftThruster(45%)ApplyRightThruster(50%)IF State == RedIsToTheRight:ApplyLeftThruster(50%)ApplyRightThruster(45%)IF State == RedIsDirectlyBelow:ApplyLeftThruster(35%)ApplyRightThruster(35%)END\\_LOOP \n\nDuring deployment, this could end catastrophically. Assume that some crater is painted red but our landing pads is painted blue. Now, the agent will guide itself competently towards the crater and miss the real landing pad entirely. That's not what we wanted.\n\n(ETA: If you think I'm using the term 'catastrophically' too loosely here, since the agent actually lands safely in a crater rather than crashing into the ground, we could instead imagine a lunar vehicle which veers off into the red crater rather than just sitting still and awaiting further instruction since it's confused.)\n\n**What made the agent become malign**\n-------------------------------------\n\nAbove, I pointed to the reason why agents like ours could be malign. Specifically, it was 'mistaken' about what counted as a landing pad. However, it's worth noting that saying the agent is mistaken about the state of the world is really an anthropomorphization. It was actually perfectly correct in inferring where the red part of the world was -- we just didn't want it to go to that part of the world. We model the agent as being 'mistaken' about where the landing pad is, but it works equally well to model the agent as having goals that are counter to ours.\n\nSince the malign failure doesn't come from a pure epistemic error, we can't merely expect that the agent will self-correct as it gains more knowledge about the world. Saying that it is making an epistemic mistake is just a model of what's going on that helps us interpret its behavior, and it does not imply that this error is benign.\n\n**Imagining more complex agents**\n---------------------------------\n\nBut what's to worry about if this sort of thing only happens in very simple agents? Perhaps you think that only agents which perform internal search could ever reach the level of competence required to perform a real-world catastrophe?\n\nI think that these concerns about my example are valid, but I don't believe they are compelling. As a reply, I think the general agent superstructure I outlined in the initial pseudocode could reach very high levels of competence.\n\nConsider an agent that could, during its operation, call upon a vast array of subroutines. Some of these subroutines can accomplish extremely complicated actions, such as \"Prove this theorem: [...]\" or \"Compute the fastest route to Paris.\" We then imagine that this agent still shares the basic superstructure of the pseudocode I gave initially above. In effect, the agent has an outer loop, during which it takes in observations from the real world, and outputs action sequences depending on which state of the world it thinks its in, and using the subroutines it has available.\n\nSince the subroutines are arbitrarily complex, I don't think there is any fundamental barrier for this agent to achieve high levels of competence in the real world. Moreover, some subroutines could themselves perform powerful internal searches, pretty clearly obviating the competitive advantage that explicit search agents offer.\n\nAnd even while some subroutines could perform powerful internal searches, these subroutines aren't the only source of our malign generalization concern. The behavior of the agent is still well-described as a switch-case agent, and this means that the failure mode of the agent being 'mistaken' about the state of the world remains. Therefore, it's inaccurate to say that the source of malign generalization *must* come from an internal search being misaligned with the objective function we used during training.", "url": "https://www.alignmentforum.org/posts/ynt9TD6PrYw6iT49m/malign-generalization-without-internal-search", "date_published": "2020-01-12T18:03:43Z", "authors": ["Matthew Barnett"], "tags": ["ai", "inner alignment"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:40:42.287749+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "e62549b14685db3321871fe505fa8813", "source": "alignmentforum", "title": "Update on Ought's experiments on factored evaluation of arguments", "text": "[Ought](https://ought.org/) has written a detailed update and analysis of recent experiments on factored cognition. These are experiments with human participants and don't involve any machine learning. The goal is to learn about the viability of [IDA](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd?_ga=2.44167719.2055190071.1578702594-1142780176.1552454685), [Debate](https://openai.com/blog/debate/), and related [approaches](https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84) to AI alignment. For background, here are some prior LW posts on Ought: [Ought: Why it Matters and How to Help](https://www.lesswrong.com/posts/cpewqG3MjnKJpCr7E/ought-why-it-matters-and-ways-to-help)), [Factored Cognition presentation](https://www.lesswrong.com/posts/DFkGStzvj3jgXibFG/factored-cognition).\n\nHere is the opening of the research update:\n\n\n> **Evaluating Arguments One Step at a Time** \n\n\n> We're studying [factored cognition](https://ought.org/research/factored-cognition): under what conditions can a group of people accomplish complex cognitive tasks if each person only has minimal context?\n\n\n> In a recent experiment, we focused on dividing up the task of evaluating arguments. We created short, structured arguments for claims about movie reviews. We then tried to distinguish valid from invalid arguments by showing each participant only one step of the argument, not the review or the other steps.\n\n\n> In this experiment, we found that:\n\n\n> 1. Factored evaluation of arguments can distinguish some valid from invalid arguments by identifying implausible steps in arguments for false claims.\n\n\n> 2. However, experiment participants disagreed a lot about whether steps were valid or invalid. This method is therefore brittle in its current form, even for arguments which only have 1-5 steps.\n\n\n> 3. More diverse argument and evidence types (besides direct quotes from the text), larger trees, and different participant guidelines should improve results.\n\n\n> In this technical progress update, we describe these findings in depth.\n\nThe rest of the post is [here](https://ought.org/updates/2020-01-11-arguments).", "url": "https://www.alignmentforum.org/posts/pH3eKEAEupx8c2ep9/update-on-ought-s-experiments-on-factored-evaluation-of", "date_published": "2020-01-12T21:20:42Z", "authors": ["Owain_Evans"], "tags": ["factored cognition", "ought"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:40:42.288095+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "ae5c0c8af2c74f2abfcfda9cb2fc8b58", "source": "alignmentforum", "title": "[AN #82]: How OpenAI Five distributed their training computation", "text": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.\n\nAudio version [here](http://alignment-newsletter.libsyn.com/alignment-newsletter-82) (may not be up yet).\n\n**Highlights**\n--------------\n\n[Dota 2 with Large Scale Deep Reinforcement Learning](http://arxiv.org/abs/1912.06680) *(OpenAI et al)* (summarized by Nicholas): In April, [OpenAI Five](https://openai.com/blog/how-to-train-your-openai-five/) ([AN #54](https://mailchi.mp/3e2f43012b07/an-54-boxing-a-finite-horizon-ai-system-to-keep-it-unambitious)) defeated the world champion Dota 2 team, OG. This paper describes its training process. OpenAI et al. hand-engineered the reward function as well as some features, actions, and parts of the policy. The rest of the policy was trained using PPO with an LSTM architecture at a massive scale. They trained this in a distributed fashion as follows:\n\n- The *Controller* receives and distributes the updated parameters.\n\n- The *Rollout Worker CPUs* simulate the game, send observations to the *Forward Pass GPUs* and publish samples to the *Experience Buffer*.\n\n- The *Forward Pass GPUs* determine the actions to use and send them to the *Rollout Workers*.\n\n- The *Optimizer GPUs* sample experience from the *Experience Buffer*, calculate gradient updates, and then publish updated parameters to the *Controller*.\n\nThe model trained over 296 days. In that time, OpenAI needed to adapt it to changes in the code and game mechanics. This was done via model \"surgery\", in which they would try to initialize a new model to maintain the same input-output mapping as the old one. When this was not possible, they gradually increased the proportion of games played with the new version over time.\n\n**Nicholas's opinion:** I feel similarly to my opinion on [AlphaStar](https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning) ([AN #73](https://mailchi.mp/ef55eb52b0fd/an-73-detecting-catastrophic-failures-by-learning-how-agents-tend-to-break)) here. The result is definitely impressive and a major step up in complexity from shorter, discrete games like chess or go. However, I don't see how the approach of just running PPO at a large scale brings us closer to AGI because we can't run massively parallel simulations of real world tasks. Even for tasks that can be simulated, this seems prohibitively expensive for most use cases (I couldn't find the exact costs, but I'd estimate this model cost tens of millions of dollars). I'd be quite excited to see an example of deep RL being used for a complex real world task without training in simulation.\n\n**Technical AI alignment**\n==========================\n\n### **Technical agendas and prioritization**\n\n[Just Imitate Humans?](https://www.alignmentforum.org/posts/LTFaD96D9kWuTibWr/just-imitate-humans) *(Michael Cohen)* (summarized by Rohin): This post asks whether it is safe to build AI systems that just imitate humans. The comments have a lot of interesting debate.\n\n### **Agent foundations**\n\n[Conceptual Problems with UDT and Policy Selection](https://www.alignmentforum.org/posts/9sYzoRnmqmxZm4Whf/conceptual-problems-with-udt-and-policy-selection) *(Abram Demski)* (summarized by Rohin): In Updateless Decision Theory (UDT), the agent decides \"at the beginning of time\" exactly how it will respond to every possible sequence of observations it could face, so as to maximize the expected value it gets with respect to its prior over how the world evolves. It is updateless because it decides ahead of time how it will respond to evidence, rather than updating once it sees the evidence. This works well when the agent can consider the full environment and react to it, and often gets the right result even when the environment can model the agent (as in Newcomblike problems), as long as the agent knows how the environment will model it.\n\nHowever, it seems unlikely that UDT will generalize to logical uncertainty and multiagent settings. Logical uncertainty occurs when you haven't computed all the consequences of your actions and is reduced by thinking longer. However, this effectively is a form of updating, whereas UDT tries to know everything upfront and never update, and so it seems hard to make it compatible with logical uncertainty. With multiagent scenarios, the issue is that UDT wants to decide on its policy \"before\" any other policies, which may not always be possible, e.g. if another agent is also using UDT. The philosophy behind UDT is to figure out how you will respond to everything ahead of time; as a result, UDT aims to precommit to strategies assuming that other agents will respond to its commitments; so two UDT agents are effectively \"racing\" to make their commitments as fast as possible, reducing the time taken to consider those commitments as much as possible. This seems like a bad recipe if we want UDT agents to work well with each other.\n\n**Rohin's opinion:** I am no expert in decision theory, but these objections seem quite strong and convincing to me.\n\n[A Critique of Functional Decision Theory](https://www.alignmentforum.org/posts/ySLYSsNeFL5CoAQzN/a-critique-of-functional-decision-theory) *(Will MacAskill)* (summarized by Rohin): *This summary is more editorialized than most.* This post critiques [Functional Decision Theory](https://arxiv.org/abs/1710.05060) (FDT). I'm not going to go into detail, but I think the arguments basically fall into two camps. First, there are situations in which there is no uncertainty about the consequences of actions, and yet FDT chooses actions that do not have the highest utility, because of their impact on counterfactual worlds which \"could have happened\" (but ultimately, the agent is just leaving utility on the table). Second, FDT relies on the ability to tell when someone is \"running an algorithm that is similar to you\", or is \"logically correlated with you\". But there's no such crisp concept, and this leads to all sorts of problems with FDT as a decision theory.\n\n**Rohin's opinion:** Like [Buck from MIRI](https://forum.effectivealtruism.org/posts/tDk57GhrdK54TWzPY/i-m-buck-shlegeris-i-do-research-and-outreach-at-miri-ama#iX6knDPMXZb696tDc), I feel like I understand these objections and disagree with them. On the first argument, I agree with [Abram](https://www.alignmentforum.org/posts/ySLYSsNeFL5CoAQzN/a-critique-of-functional-decision-theory#y8zRwcpNeu2ZhM3yE) that a decision should be evaluated based on how well the agent performs with respect to the probability distribution used to define the problem; FDT only performs badly if you evaluate on a decision problem produced by conditioning on a highly improbable event. On the second class of arguments, I certainly agree that there isn't (yet) a crisp concept for \"logical similarity\"; however, I would be shocked if the *intuitive concept* of logical similarity was not relevant in the general way that FDT suggests. If your goal is to hardcode FDT into an AI agent, or your goal is to write down a decision theory that in principle (e.g. with infinite computation) defines the correct action, then it's certainly a problem that we have no crisp definition yet. However, FDT can still be useful for getting more clarity on how one ought to reason, without providing a full definition.\n\n### **Learning human intent**\n\n[Learning to Imitate Human Demonstrations via CycleGAN](https://bair.berkeley.edu/blog/2019/12/13/humans-cyclegan/) *(Laura Smith et al)* (summarized by Zach): Most methods for imitation learning, where robots learn from a demonstration, assume that the actions of the demonstrator and robot are the same. This means that expensive techniques such as teleoperation have to be used to generate demonstrations. **This paper presents a method to engage in automated visual instruction-following with demonstrations (AVID) that works by translating video demonstrations done by a human into demonstrations done by a robot.** To do this, the authors use [CycleGAN](https://junyanz.github.io/CycleGAN/), a method to translate an image from one domain to another domain using unpaired images as training data. CycleGAN allows them to translate videos of humans performing the task into videos of the robot performing the task, which the robot can then imitate. In order to make learning tractable, the demonstrations had to be divided up into 'key stages' so that the robot can learn a sequence of more manageable tasks. In this setup, the robot only needs supervision to ensure that it's copying each stage properly before moving on to the next one. To test the method, the authors have the robot retrieve a coffee cup and make coffee. AVID significantly outperforms other imitation learning methods and can achieve 70% / 80% success rate on the tasks, respectively.\n\n**Zach's opinion:** In general, I like the idea of 'translating' demonstrations from one domain into another. It's worth noting that there do exist methods for translating visual demonstrations into latent policies. I'm a bit surprised that we didn't see any comparisons with other adversarial methods like [GAIfO](https://arxiv.org/pdf/1807.06158.pdf), but I understand that those methods have high sample complexity so perhaps the methods weren't useful in this context. It's also important to note that these other methods would still require demonstration translation. Another criticism is that AVID is not fully autonomous since it relies on human feedback to progress between stages. However, compared to kinetic teaching or teleoperation, sparse feedback from a human overseer is a minor inconvenience.\n\n**Read more:** [Paper: AVID: Learning Multi-Stage Tasks via Pixel-Level Translation of Human Videos](https://arxiv.org/abs/1912.04443)\n\n### **Preventing bad behavior**\n\n[When Goodharting is optimal: linear vs diminishing returns, unlikely vs likely, and other factors](https://www.alignmentforum.org/posts/megKzKKsoecdYqwb7/when-goodharting-is-optimal-linear-vs-diminishing-returns) *(Stuart Armstrong)* (summarized by Flo): Suppose we were uncertain about which arm in a bandit provides reward (and we don't get to observe the rewards after choosing an arm). Then, maximizing expected value under this uncertainty is equivalent to picking the most likely reward function as a proxy reward and optimizing that; Goodhart's law doesn't apply and is thus not universal. This means that our fear of Goodhart effects is actually informed by more specific intuitions about the structure of our preferences. If there are actions that contribute to multiple possible rewards, optimizing the most likely reward does not need to maximize the expected reward. Even if we optimize for that, we have a problem if value is complex and the way we do reward learning implicitly penalizes complexity. Another problem arises if the correct reward is comparatively difficult to optimize: if we want to maximize the average, it can make sense to only care about rewards that are both likely and easy to optimize. Relatedly, we could fail to correctly account for diminishing marginal returns in some of the rewards.\n\nGoodhart effects are a lot less problematic if we can deal with all of the mentioned factors. Independent of that, Goodhart effects are most problematic when there is little middle ground that all rewards can agree on.\n\n**Flo's opinion:** I enjoyed this article and the proposed factors match my intuitions. There seem to be two types of problems: extreme beliefs and concave Pareto boundaries. Dealing with the second is more important since a concave Pareto boundary favours extreme policies, even for moderate beliefs. Luckily, diminishing returns can be used to bend the Pareto boundary. However, I expect it to be hard to find the correct rate of diminishing returns, especially in novel situations.\n\n**Rohin's opinion:** Note that this post considers the setting where we have uncertainty over the true reward function, but *we can't learn about the true reward function*. If you can gather information about the true reward function, which [seems necessary to me](https://www.alignmentforum.org/posts/4783ufKpx8xvLMPc6/human-ai-interaction) ([AN #41](https://mailchi.mp/8c3f02cabccd/alignment-newsletter-41)), then it is almost always worse to take the most likely reward or expected reward as a proxy reward to optimize.\n\n### **Robustness**\n\n[AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty](https://openreview.net/pdf?id=S1gmrxHFvB) *(Dan Hendrycks, Norman Mu et al)* (summarized by Dan H): This paper introduces a data augmentation technique to improve robustness and uncertainty estimates. The idea is to take various random augmentations such as random rotations, produce several augmented versions of an image with compositions of random augmentations, and then pool the augmented images into a single image by way of an elementwise convex combination. Said another way, the image is augmented with various traditional augmentations, and these augmented images are \"averaged\" together. This produces highly diverse augmentations that have similarity to the original image. Unlike techniques such as AutoAugment, this augmentation technique uses typical resources, not 15,000 GPU hours. It also greatly improves generalization to unforeseen corruptions, and it makes models more stable under small perturbations. Most importantly, even as the distribution shifts and accuracy decreases, this technique produces models that can [remain calibrated under distributional shift](https://openreview.net/pdf?id=S1gmrxHFvB#page=8&zoom=100,144,298).\n\n### **Miscellaneous (Alignment)**\n\n[Defining and Unpacking Transformative AI](https://arxiv.org/abs/1912.00747) *(Ross Gruetzemacher et al)* (summarized by Flo): The notion of **transformative AI** (TAI) is used to highlight that even narrow AI systems can have large impacts on society. This paper offers a clearer definition of TAI and distinguishes it from **radical transformative AI** (RTAI).\n\n\"Discontinuities or other anomalous patterns in metrics of human progress, as well as *irreversibility* are common indicators of transformative change. TAI is then broadly defined as an AI technology, which leads to an irreversible change of some important aspects of society, making it a (multi-dimensional) spectrum along the axes of **extremity**, **generality** and **fundamentality**. \" For example, advanced AI weapon systems might have strong implications for great power conflicts but limited effects on people's daily lives; extreme change of limited generality, similar to nuclear weapons. There are two levels: while TAI is comparable to general-purpose technologies (GPTs) like the internal combustion engine, RTAI leads to changes that are comparable to the agricultural or industrial revolution. Both revolutions have been driven by GPTs like the domestication of plants and the steam engine. Similarly, we will likely see TAI before RTAI. The scenario where we don't is termed a **radical shift**.\n\nNon-radical TAI could still contribute to existential risk in conjunction with other factors. Furthermore, if TAI precedes RTAI, our management of TAI can affect the risks RTAI will pose.\n\n**Flo's opinion:** Focusing on the impacts on society instead of specific features of AI systems makes sense and I do believe that the shape of RTAI as well as the risks it poses will depend on the way we handle TAI at various levels. More precise terminology can also help to prevent misunderstandings, for example between people forecasting AI and decision maker.\n\n[Six AI Risk/Strategy Ideas](https://www.alignmentforum.org/posts/dt4z82hpvvPFTDTfZ/six-ai-risk-strategy-ideas) *(Wei Dai)* (summarized by Rohin): This post briefly presents three ways that power can become centralized in a world with [Comprehensive AI Services](https://www.fhi.ox.ac.uk/reframing/) ([AN #40](https://mailchi.mp/b649f32b07da/alignment-newsletter-40)), argues that under risk aversion \"logical\" risks can be more concerning than physical risks because they are more correlated, proposes combining human imitations and oracles to remove the human in the loop and become competitive, and suggests doing research to generate evidence of difficulty of a particular strand of research.", "url": "https://www.alignmentforum.org/posts/6tikKda9LBzrkLfBJ/an-82-how-openai-five-distributed-their-training-computation", "date_published": "2020-01-15T18:20:01Z", "authors": ["Rohin Shah"], "tags": ["ai", "newsletters"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:40:42.289363+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
