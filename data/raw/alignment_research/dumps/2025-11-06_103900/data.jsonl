{"id": "555121ab60501803606a56ca07bd552e", "source": "alignmentforum", "title": "[AN #79]: Recursive reward modeling as an alignment technique integrated with deep RL", "text": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.\n\nHappy New Year!  \n   \nAudio version [here](http://alignment-newsletter.libsyn.com/alignment-newsletter-79) (may not be up yet).\n\n**Highlights**\n--------------\n\n[AI Alignment Podcast: On DeepMind, AI Safety, and Recursive Reward Modeling](https://futureoflife.org/2019/12/16/ai-alignment-podcast-on-deepmind-ai-safety-and-recursive-reward-modeling-with-jan-leike/) *(Lucas Perry and Jan Leike)* (summarized by Rohin): While Jan originally worked on theory (specifically AIXI), DQN, AlphaZero and others demonstrated that deep RL was a plausible path to AGI, and so now Jan works on more empirical approaches. In particular, when selecting research directions, he looks for techniques that are deeply integrated with the current paradigm, that could scale to AGI and beyond. He also wants the technique to work for agents in general, rather than just question answering systems, since people will want to build agents that can act, at least in the digital world (e.g. composing emails). This has led him to work on [recursive reward modeling](https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84) ([AN #34](https://mailchi.mp/f1947668b183/alignment-newsletter-34)), which tries to solve the specification problem in the [SRA framework](https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1) ([AN #26](https://mailchi.mp/1ecd1b775703/alignment-newsletter-26)).\n\nReward functions are useful because they allow the AI to find novel solutions that we wouldn't think of (e.g. AlphaGo's move 37), but often are incorrectly specified, leading to reward hacking. This suggests that we should do *reward modeling*, where we learn a model of the reward function from human feedback. Of course, such a model is still likely to have errors leading to reward hacking, and so to avoid this, the reward model needs to be updated online. As long as it is **easier to evaluate behavior than to produce behavior**, reward modeling should allow AIs to find novel solutions that we wouldn't think of.\n\nHowever, we would eventually like to apply reward modeling to tasks where evaluation is also hard. In this case, we can decompose the evaluation task into smaller tasks, and recursively apply reward modeling to train AI systems that can perform those small helper tasks. Then, assisted by these helpers, the human should be able to evaluate the original task. This is essentially forming a \"tree\" of reward modeling agents that are all building up to the reward model for the original, hard task. While currently the decomposition would be done by a human, you could in principle also use recursive reward modeling to automate the decomposition. Assuming that we can get regular reward modeling working robustly, we then need to make sure that the tree of reward models doesn't introduce new problems. In particular, it might be the case that as you go up the tree, the errors compound: errors in the reward model at the leaves lead to slightly worse helper agents, which lead to worse evaluations for the second layer, and so on.\n\nHe recommends that rather than spending a lot of time figuring out the theoretically optimal way to address a problem, AI safety researchers should alternate between conceptual thinking and trying to make something work. The ML community errs on the other side, where they try out lots of techniques, but don't think as much about how their systems will be deployed in the real world. Jan also wants the community to focus more on clear, concrete technical explanations, rather than vague blog posts that are difficult to critique and reason about. This would allow us to more easily build on past work, rather than reasoning from first principles and reinventing the wheel many times.\n\nDeepMind is taking a portfolio approach to AI safety: they are trying many different lines of attack, and hoping that some of them will pan out. Currently, there are teams for agent alignment (primarily recursive reward modeling), incentive theory, trained agent analysis, policy, and ethics. They have also spent some time thinking about AI safety benchmarks, as in [AI Safety Gridworlds](https://arxiv.org/abs/1711.09883), since progress in machine learning is driven by benchmarks, though Jan does think it is quite hard to create a well-made benchmark.\n\n**Rohin's opinion:** I've become more optimistic about recursive reward modeling since the [original paper](https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84) ([AN #34](https://mailchi.mp/f1947668b183/alignment-newsletter-34)), primarily (I think) because I now see more value in approaches that can be used to perform specific tasks (relative to approaches that try to infer \"human values\").\n\nI also appreciated the recommendations for the AI safety community, and agree with them quite a lot. Relative to Jan, I see more value in conceptual work described using fuzzy intuitions, but I do think that more effort should be put into exposition of that kind of work.\n\n**Technical AI alignment**\n==========================\n\n### **Learning human intent**\n\n[Learning human objectives by evaluating hypothetical behaviours](https://deepmind.com/blog/article/learning-human-objectives-by-evaluating-hypothetical-behaviours) *(Siddharth Reddy et al)* (summarized by Rohin): [Deep RL from Human Preferences](https://deepmind.com/blog/learning-through-human-feedback/) updated its reward model by collecting human comparisons on on-policy trajectories where the reward model ensemble was most uncertain about what the reward should be. However, we want our reward model to be accurate off policy as well, even in unsafe states. To this end, we would like to train our reward model on *hypothetical* trajectories. This paper proposes learning a generative model of trajectories from some dataset of environment dynamics, such as safe expert demonstrations or rollouts from a random policy, and then finding trajectories that are \"useful\" for training the reward model. They consider four different criteria for usefulness of a trajectory: *uncertain rewards* (which intuitively are areas where the reward model needs training), *high rewards* (which could indicate reward hacking), *low rewards* (which increases the number of unsafe states that the reward model is trained on), and *novelty* (which covers more of the state space). Once a trajectory is generated, they have a human label it as good, neutral, or unsafe, and then train the reward model on these labels.\n\nThe authors are targeting an agent that can *explore safely*: since they already have a world model and a reward model, they use a model-based RL algorithm to act in the environment. Specifically, to act, they use gradient descent to optimize a trajectory in the latent space that maximizes expected rewards under the reward model and world model, and then take the first action of that trajectory. They argue that the world model can be trained on a dataset of safe human demonstrations (though in their experiments they use rollouts from a random policy), and then since the reward model is trained on hypothetical behavior and the model-based RL algorithm doesn't need any training, we get an agent that acts without us ever getting to an unsafe state.\n\n**Rohin's opinion:** I like the focus on integrating active selection of trajectory queries into reward model training, and especially the four different kinds of active criteria that they consider, and the detailed experiments (including an ablation study) on the benefits of these criteria. These seem important for improving the efficiency of reward modeling.\n\nHowever, I don't buy the argument that this allows us to train an agent without visiting unsafe states. In their actual experiments, they use a dataset gathered from a random policy, which certainly will visit unsafe states. If you instead use a dataset of safe human demonstrations, your generative model will only place probability mass on safe demonstrations, and so you'll never generate trajectories that visit unsafe states, and your reward model won't know that they are unsafe. (*Maybe* your generative model will generalize properly to the unsafe states, but that seems unlikely to me.) Such a reward model will either be limited to imitation learning (sticking to the same trajectories as in the demonstrations, and never finding something like AlphaGo's move 37), or it will eventually visit unsafe states.\n\n**Read more:** [Paper: Learning Human Objectives by Evaluating Hypothetical Behavior](https://arxiv.org/abs/1912.05652)\n\n[Causal Confusion in Imitation Learning](https://arxiv.org/abs/1905.11979) *(Pim de Haan et al)* (summarized by Asya): This paper argues that *causal misidentification* is a big problem in imitation learning. When the agent doesn't have a good model of what actions cause what state changes, it may mismodel the effects of a state change as a cause-- e.g., an agent learning to drive a car may incorrectly learn that it should turn on the brakes whenever the brake light on the dashboard is on. This leads to undesirable behavior where more information actually causes the agent to perform worse.\n\nThe paper presents an approach for resolving causal misidentification by (1) Training a specialized network to generate a \"disentangled\" representation of the state as variables, (2) Representing causal relationships between those variables in a graph structure, (3) Learning policies corresponding to each possible causal graph, and (4) Performing targeted interventions, either by querying an expert, or by executing a policy and observing the reward, to find the correct causal graph model.\n\nThe paper experiments with this method by testing it in environments artificially constructed to have confounding variables that correlate with actions but do not cause them. It finds that this method is successfully able to improve performance with confounding variables, and that it performs significantly better per number of queries (to an expert or of executing a policy) than any existing methods. It also finds that directly executing a policy and observing the reward is a more efficient strategy for narrowing down the correct causal graph than querying an expert.\n\n**Asya's opinion:** This paper goes into detail arguing why causal misidentification is a huge problem in imitation learning and I find its argument compelling. I am excited about attempts to address the problem, and I am tentatively excited about the method the paper proposes for finding representative causal graphs, with the caveat that I don't feel equipped to evaluate whether it could efficiently generalize past the constrained experiments presented in the paper.\n\n**Rohin's opinion:** While the conclusion that more information hurts sounds counterintuitive, it is actually straightforward: you *don't* get more data (in the sense of the size of your training dataset); you instead have *more features* in the input state data. This increases the number of possible policies (e.g. once you add the car dashboard, you can now express the policy \"if brake light is on, apply brakes\", which you couldn't do before), which can make you generalize worse. Effectively, there are more opportunities for the model to pick up on spurious correlations instead of the true relationships. This would happen in other areas of ML as well; surely someone has analyzed this effect for fairness, for example.\n\nThe success of their method over DAgger comes from improved *policy exploration* (for their environments): if your learned policy is primarily paying attention to the brake light, it's a very large change to instead focus on whether there is an obstacle visible, and so gradient descent is not likely to ever try that policy once it has gotten to the local optimum of paying attention to the brake light. In contrast, their algorithm effectively trains separate policies for scenarios in which different parts of the input are masked, which means that it is forced to explore policies that depend only on the brake light, and policies that depend only on the view outside the windshield, and so on. So, the desired policy has been explored already, and it only requires a little bit of active learning to identify the correct policy.\n\nLike Asya, I like the approach, but I don't know how well it will generalize to other environments. It seems like an example of [quality diversity](https://www.frontiersin.org/articles/10.3389/frobt.2016.00040/full), which I am generally optimistic about.\n\n[Humans Are Embedded Agents Too](https://www.alignmentforum.org/posts/WJzsTmsDctYCCyMfy/humans-are-embedded-agents-too) *(John S Wentworth)* (summarized by Rohin): [Embedded agency](https://www.alignmentforum.org/posts/p7x32SEt43ZMC9r7r/embedded-agents) ([AN #31](https://mailchi.mp/7d0e3916e3d9/alignment-newsletter-31)) is not just a problem for AI systems: humans are embedded agents too; many problems in understanding human values stem from this fact. For example, humans don't have a well-defined output channel: we can't say \"anything that comes from this keyboard is direct output from the human\", because the AI could seize control of the keyboard and wirehead, or a cat could walk over the keyboard, etc. Similarly, humans can \"self-modify\", e.g. by drinking, which often modifies their \"values\": what does that imply for value learning? Based on these and other examples, the post concludes that \"a better understanding of embedded agents in general will lead to substantial insights about the nature of human values\".\n\n**Rohin's opinion:** I certainly agree that many problems with figuring out what to optimize stem from embedded agency issues with humans, and any [formal account](https://www.alignmentforum.org/posts/zvrZi95EHqJPxdgps/why-we-need-a-theory-of-human-values) ([AN #36](https://mailchi.mp/6751e45fbb48/alignment-newsletter-36)) of this will benefit from general progress in understanding embeddedness. Unlike many others, I do not think we need a formal account of human values, and that a \"common-sense\" understanding will suffice, including for the embeddedness problems detailed in this post. (See also this [comment thread](https://www.alignmentforum.org/posts/WJzsTmsDctYCCyMfy/humans-are-embedded-agents-too#opd3EeESfWiEyekDh) and the next summary.)\n\n[What's the dream for giving natural language commands to AI?](https://www.alignmentforum.org/posts/Bxxh9GbJ6WuW5Hmkj/what-s-the-dream-for-giving-natural-language-commands-to-ai) *(Charlie Steiner)* (summarized by Rohin): We could try creating AI systems that take the \"artificial intentional stance\" towards humans: that is, they model humans as agents that are trying to achieve some goals, and then we get the AI system to optimize for those inferred goals. We could do this by training an agent that jointly models the world and understands natural language, in order to ground the language into actual states of the world. The hope is that with this scheme, as the agent gets more capable, its understanding of what we want improves as well, so that it is robust to scaling up. However, the scheme has no protection against Goodharting, and doesn't obviously care about metaethics.\n\n**Rohin's opinion:** I agree with the general spirit of \"get the AI system to understand common sense; then give it instructions that it interprets correctly\". I usually expect future ML research to figure out the common sense part, so I don't look for particular implementations (in this case, simultaneous training on vision and natural language), but just assume we'll have that capability somehow. The hard part is then how to leverage that capability to provide *correctly interpreted* instructions. It may be as simple as providing instructions in natural language, as this post suggests. I'm much less worried about instrumental subgoals in such a scenario, since part of \"understanding what we mean\" includes \"and don't pursue this instruction literally to extremes\". But we still need to figure out how to translate natural language instructions into actions.\n\n### **Forecasting**\n\n[Might humans not be the most intelligent animals?](https://www.lesswrong.com/posts/XjuT9vgBfwXPxsdfN/might-humans-not-be-the-most-intelligent-animals) *(Matthew Barnett)* (summarized by Rohin): We can roughly separate intelligence into two categories: *raw innovative capability* (the ability to figure things out from scratch, without the benefit of those who came before you), and *culture processing* (the ability to learn from accumulated human knowledge). It's not clear that humans have the highest raw innovative capability; we may just have much better culture. For example, feral children raised outside of human society look very \"unintelligent\", [The Secret of Our Success](https://www.amazon.com/dp/B00WY4OXAS/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1) documents cases where culture trumped innovative capability, and humans actually *don't* have the most neurons, or the most neurons in the forebrain.\n\n(Why is this relevant to AI alignment? Matthew claims that it has implications on AI takeoff speeds, though he doesn't argue for that claim in the post.)\n\n**Rohin's opinion:** It seems very hard to actually make a principled distinction between these two facets of intelligence, because culture has such an influence over our \"raw innovative capability\" in the sense of our ability to make original discoveries / learn new things. While feral children might be less intelligent than animals (I wouldn't know), the appropriate comparison would be against \"feral animals\" that also didn't get opportunities to explore their environment and learn from their parents, and even so I'm not sure how much I'd trust results from such a \"weird\" (evolutionarily off-distribution) setup.\n\n[Walsh 2017 Survey](https://aiimpacts.org/walsh-2017-survey/) *(Charlie Giattino)* (summarized by Rohin): In this survey, AI experts, robotics experts, and the public estimated a 50% chance of high-level machine intelligence (HLMI) by 2061, 2065, and 2039 respectively. The post presents other similar data from the survey.\n\n**Rohin's opinion:** While I expected that the public would expect HLMI sooner than AI experts, I was surprised that AI and robotics experts agreed so closely -- I would have thought that robotics experts would have longer timelines.\n\n### **Field building**\n\n[What I talk about when I talk about AI x-risk: 3 core claims I want machine learning researchers to address.](https://www.alignmentforum.org/posts/bJdaB2Mz4mBvwFBeb/what-i-talk-about-when-i-talk-about-ai-x-risk-3-core-claims-1) *(David Krueger)* (summarized by Rohin): When making the case for work on AI x-risk to other ML researchers, what should we focus on? This post suggests arguing for three core claims:\n\n1. Due to Goodhart's law, instrumental goals, and safety-performance trade-offs, the development of advanced AI increases the risk of human extinction non-trivially.\n\n2. To mitigate this x-risk, we need to know how to build safe systems, know that we know how to build safe systems, and prevent people from building unsafe systems.\n\n3. So, we should mitigate AI x-risk, as it is impactful, neglected, and challenging but tractable.\n\n**Rohin's opinion:** This is a nice concise case to make, but I think the bulk of the work is in splitting the first claim into subclaims: this is the part that is usually a sticking point (see also the next summary).\n\n### **Miscellaneous (Alignment)**\n\n[A list of good heuristics that the case for AI x-risk fails](https://www.alignmentforum.org/posts/bd2K3Jdz82csjCFob/a-list-of-good-heuristics-that-the-case-for-ai-x-risk-fails) *(David Krueger)* (summarized by Flo): Because human attention is limited and a lot of people try to convince us of the importance of their favourite cause, we cannot engage with everyone’s arguments in detail. Thus we have to rely on heuristics to filter out insensible arguments. Depending on the form of exposure, the case for AI risks can fail on many of these generally useful heuristics, eight of which are detailed in this post. Given this outside view perspective, it is unclear whether we should actually expect ML researchers to spend time evaluating the arguments for AI risk.\n\n**Flo's opinion:** I can remember being critical of AI risk myself for similar reasons and think that it is important to be careful with the framing of pitches to avoid these heuristics from firing. This is not to say that we should avoid criticism of the idea of AI risk, but criticism is a lot more helpful if it comes from people who have actually engaged with the arguments.\n\n**Rohin's opinion:** Even after knowing the arguments, I find six of the heuristics quite compelling: technology doomsayers have usually been wrong in the past, there isn't a concrete threat model, it's not empirically testable, it's too extreme, it isn't well grounded in my experience with existing AI systems, and it's too far off to do useful work now. All six make me distinctly more skeptical of AI risk.\n\n**Other progress in AI**\n========================\n\n### **Reinforcement learning**\n\n[Procgen Benchmark](https://openai.com/blog/procgen-benchmark/) *(Karl Cobbe et al)* (summarized by Asya): Existing game-based benchmarks for reinforcement learners suffer from the problem that agents constantly encounter near-identical states, meaning that the agents may be overfitting and memorizing specific trajectories rather than learning a general set of skills. In an attempt to remedy this, in this post OpenAI introduces Procgen Benchmark, 16 procedurally-generated video game environments used to measure how quickly a reinforcement learning agent learns generalizable skills.\n\nThe authors conduct several experiments using the benchmark. Notably, they discover that:\n\n- Agents strongly overfit to small training sets and need access to as many as 10,000 levels to generalize appropriately.\n\n- After a certain threshold, training performance improves as the training set grows, counter to trends in other supervised learning tasks.\n\n- Using a fixed series of levels for each training sample (as other benchmarks do) makes agents fail to generalize to randomly generated series of levels at test time.\n\n- Larger models improve sample efficiency and generalization.\n\n**Asya's opinion:** This seems like a useful benchmark. I find it particularly interesting that their experiment testing non-procedurally generated levels as training samples implies huge overfitting effects in existing agents trained in video-game environments.\n\n**Read more:** [Paper: Leveraging Procedural Generation to Benchmark Reinforcement Learning](http://arxiv.org/abs/1912.01588)\n\n[Adaptive Online Planning for Continual Lifelong Learning](http://arxiv.org/abs/1912.01188) *(Kevin Lu et al)* (summarized by Nicholas): Lifelong learning is distinct from standard RL benchmarks because\n\n1. The environment is *sequential* rather than *episodic*; it is never reset to a new start state.\n\n2. The current *transition* and *reward* function are given, but they change over time.\n\nGiven this setup, there are two basic approaches: first, run model-free learning on simulated future trajectories and rerun it every time the dynamics change, and second, run model-based planning on the current model. If you ignore computational constraints, these should be equivalent; however, in practice, the second option tends to be more computationally efficient. The contribution of this work is to make this more efficient, rather than improving final performance, by starting with the second option and then using model-free learning to “distill” the knowledge produced by the model-based planner allowing for more efficient planning in the future.\n\nSpecifically, Adaptive Online Planning (AOP) balances between the model-based planner MPPI (a variant of MPC) and the model-free algorithm TD3. MPPI uses the given model to generate a trajectory up to a horizon and then uses an ensemble of value functions to estimate the cumulative reward. This knowledge is then distilled into TD3 for later use as a prior for MPPI. During future rollouts, the variance and Bellman error of the value function ensemble are used to determine how long the horizon should be, and therefore how much computation is used.\n\n**Nicholas's opinion:** I agree that episodic training and fixed world dynamics seem like unlikely conditions for most situations we would expect agents to encounter in the real world. Accounting for them seems particularly important to ensure safe exploration and robustness to distributional shift, and I think that these environments could serve as useful benchmarks for these safety problems as well.", "url": "https://www.alignmentforum.org/posts/EoY6P6mpz7ZozhAxm/an-79-recursive-reward-modeling-as-an-alignment-technique", "date_published": "2020-01-01T18:00:02Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.692543+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "fcbad849b33212ac3c517bb0f7859b8c", "source": "alignmentforum", "title": "[AN #80]: Why AI risk might be solved without additional intervention from longtermists", "text": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.\n\nAudio version [here](http://alignment-newsletter.libsyn.com/alignment-newsletter-80) (may not be up yet).\n\nWelcome to another special edition of the newsletter! In this edition, I summarize four conversations that AI Impacts had with researchers who were optimistic that AI safety would be solved \"by default\". (Note that one of the conversations was with me.)\n\nWhile all four of these conversations covered very different topics, I think there were three main points of convergence. First, we were relatively **unconvinced by the traditional arguments for AI risk**, and **find discontinuities relatively unlikely**. Second, we were more optimistic about **solving the problem in the future**, when we know more about the problem and have more evidence about powerful AI systems. And finally, we were more optimistic that as we get more evidence of the problem in the future, **the existing ML community will actually try to fix that problem**.\n\n[Conversation with Paul Christiano](https://aiimpacts.org/conversation-with-paul-christiano/) *(Paul Christiano, Asya Bergal, Ronny Fernandez, and Robert Long)* (summarized by Rohin): There can't be too many things that reduce the expected value of the future by 10%; if there were, there would be no expected value left (ETA: see [this comment](https://www.lesswrong.com/posts/QknPz9JQTQpGdaWDp/an-80-why-ai-risk-might-be-solved-without-additional#WWufqvbyESxNDEtDJ)). So, the prior that any particular thing has such an impact should be quite low. With AI in particular, obviously we're going to try to make AI systems that do what we want them to do. So starting from this position of optimism, we can then evaluate the arguments for doom. The two main arguments: first, we can't distinguish ahead of time between AIs that are trying to do the right thing, and AIs that are trying to kill us, because the latter will behave nicely until they can execute a treacherous turn. Second, since we don't have a crisp concept of \"doing the right thing\", we can't select AI systems on whether they are doing the right thing.\n\nHowever, there are many \"saving throws\", or ways that the argument could break down, avoiding doom. Perhaps there's no problem at all, or perhaps we can cope with it with a little bit of effort, or perhaps we can coordinate to not build AIs that destroy value. Paul assigns a decent amount of probability to each of these (and other) saving throws, and any one of them suffices to avoid doom. This leads Paul to estimate that AI risk reduces the expected value of the future by roughly 10%, a relatively optimistic number. Since it is so neglected, concerted effort by longtermists could reduce it to 5%, making it still a very valuable area for impact. The main way he expects to change his mind is from evidence from more powerful AI systems, e.g. as we build more powerful AI systems, perhaps inner optimizer concerns will materialize and we'll see examples where an AI system executes a non-catastrophic treacherous turn.\n\nPaul also believes that clean algorithmic problems are usually solvable in 10 years, or provably impossible, and early failures to solve a problem don't provide much evidence of the difficulty of the problem (unless they generate proofs of impossibility). So, the fact that we don't know how to solve alignment now doesn't provide very strong evidence that the problem is impossible. Even if the clean versions of the problem were impossible, that would suggest that the problem is much more messy, which requires more concerted effort to solve but also tends to be just a long list of relatively easy tasks to do. (In contrast, MIRI thinks that prosaic AGI alignment is probably impossible.)\n\nNote that even finding out that the problem is impossible can help; it makes it more likely that we can all coordinate to not build dangerous AI systems, since no one *wants* to build an unaligned AI system. Paul thinks that right now the case for AI risk is not very compelling, and so people don't care much about it, but if we could generate more compelling arguments, then they would take it more seriously. If instead you think that the case is already compelling (as MIRI does), then you would be correspondingly more pessimistic about others taking the arguments seriously and coordinating to avoid building unaligned AI.\n\nOne potential reason MIRI is more doomy is that they take a somewhat broader view of AI safety: in particular, in addition to building an AI that is trying to do what you want it to do, they would also like to ensure that when the AI builds successors, it does so well. In contrast, Paul simply wants to leave the next generation of AI systems in at least as good a situation as we find ourselves in now, since they will be both better informed and more intelligent than we are. MIRI has also previously defined aligned AI as one that produces good outcomes when run, which is a much broader conception of the problem than Paul has. But probably the main disagreement between MIRI and ML researchers and that ML researchers expect that we'll try a bunch of stuff, and something will work out, whereas MIRI expects that the problem is really hard, such that trial and error will only get you solutions that *appear* to work.\n\n**Rohin's opinion:** A general theme here seems to be that MIRI feels like they have very strong arguments, while Paul thinks that they're plausible arguments, but aren't extremely strong evidence. Simply having a lot more uncertainty leads Paul to be much more optimistic. I agree with most of this.\n\nHowever, I do disagree with the point about \"clean\" problems. I agree that clean algorithmic problems are usually solved within 10 years or are provably impossible, but it doesn't seem to me like AI risk counts as a clean algorithmic problem: we don't have a nice formal statement of the problem that doesn't rely on intuitive concepts like \"optimization\", \"trying to do something\", etc. This suggests to me that AI risk is more \"messy\", and so may require more time to solve.\n\n[Conversation with Rohin Shah](https://aiimpacts.org/conversation-with-rohin-shah/) *(Rohin Shah, Asya Bergal, Robert Long, and Sara Haxhia)* (summarized by Rohin): The main reason I am optimistic about AI safety is that we will see problems in advance, and we will solve them, because nobody wants to build unaligned AI. A likely crux is that I think that the ML community will actually solve the problems, as opposed to applying a bandaid fix that doesn't scale. I don't know why there are different underlying intuitions here.\n\nIn addition, many of the classic arguments for AI safety involve a system that can be decomposed into an objective function and a world model, which I suspect will not be a good way to model future AI systems. In particular, current systems trained by RL look like a grab bag of heuristics that correlate well with obtaining high reward. I think that as AI systems become more powerful, the heuristics will become more and more general, but they still won't decompose naturally into an objective function, a world model, and search. In addition, we can look at humans as an example: we don't fully pursue convergent instrumental subgoals; for example, humans can be convinced to pursue different goals. This makes me more skeptical of traditional arguments.\n\nI would guess that AI systems will become *more* interpretable in the future, as they start using the features / concepts / abstractions that humans are using. Eventually, sufficiently intelligent AI systems will probably find even better concepts that are alien to us, but if we only consider AI systems that are (say) 10x more intelligent than us, they will probably still be using human-understandable concepts. This should make alignment and oversight of these systems significantly easier. For significantly stronger systems, we should be delegating the problem to the AI systems that are 10x more intelligent than us. (This is very similar to the picture painted in [Chris Olah’s views on AGI safety](https://www.alignmentforum.org/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety) ([AN #72](https://mailchi.mp/cac125522aa3/an-72-alignment-robustness-methodology-and-system-building-as-research-priorities-for-ai-safety)), but that had not been published and I was not aware of Chris's views at the time of this conversation.)\n\nI'm also less worried about race dynamics increasing *accident* risk than the median researcher. The benefit of racing a little bit faster is to have a little bit more power / control over the future, while also increasing the risk of extinction a little bit. This seems like a bad trade from each agent's perspective. (That is, the Nash equilibrium is for all agents to be cautious, because the potential upside of racing is small and the potential downside is large.) I'd be more worried if [AI risk is real AND not everyone agrees AI risk is real when we have powerful AI systems], or if the potential upside was larger (e.g. if racing a little more made it much more likely that you could achieve a decisive strategic advantage).\n\nOverall, it feels like there's around 90% chance that AI would not cause x-risk without additional intervention by longtermists. The biggest disagreement between me and more pessimistic researchers is that I think gradual takeoff is much more likely than discontinuous takeoff (and in fact, the first, third and fourth paragraphs above are quite weak if there's a discontinuous takeoff). If I condition on discontinuous takeoff, then I mostly get very confused about what the world looks like, but I also get a lot more worried about AI risk, especially because the \"AI is to humans as humans are to ants\" analogy starts looking more accurate. In the interview I said 70% chance of doom in this world, but with *way* more uncertainty than any of the other credences, because I'm really confused about what that world looks like. Two other disagreements, besides the ones above: I don't buy [Realism about rationality](https://www.lesswrong.com/posts/suxvE2ddnYMPJN9HD/realism-about-rationality) ([AN #25](https://mailchi.mp/0c5eeec28f75/alignment-newsletter-25)), whereas I expect many pessimistic researchers do. I may also be more pessimistic about our ability to write proofs about fuzzy concepts like those that arise in alignment.\n\nOn timelines, I estimated a very rough 50% chance of AGI within 20 years, and 30-40% chance that it would be using \"essentially current techniques\" (which is obnoxiously hard to define). Conditional on both of those, I estimated 70% chance that it would be something like a mesa optimizer; mostly because optimization is a very useful instrumental strategy for solving many tasks, especially because gradient descent and other current algorithms are very weak optimization algorithms (relative to e.g. humans), and so learned optimization algorithms will be necessary to reach human levels of sample efficiency.\n\n**Rohin's opinion:** Looking over this again, I'm realizing that I didn't emphasize enough that most of my optimism comes from the more outside view type considerations: that we'll get warning signs that the ML community won't ignore, and that the AI risk arguments are not watertight. The other parts are particular inside view disagreements that make me more optimistic, but they don't factor in much into my optimism besides being examples of how the meta considerations could play out. I'd recommend [this comment of mine](https://www.lesswrong.com/posts/mdau2DBSMi5bWXPGA/useful-does-not-mean-secure#xccsZeboCNcNJeGas) to get more of a sense of how the meta considerations factor into my thinking.\n\nI was also glad to see that I still broadly agree with things I said ~5 months ago (since no major new opposing evidence has come up since then), though as I mentioned above, I would now change what I place emphasis on.\n\n[Conversation with Robin Hanson](https://aiimpacts.org/conversation-with-robin-hanson/) *(Robin Hanson, Asya Bergal, and Robert Long)* (summarized by Rohin): The main theme of this conversation is that AI safety does not look particularly compelling on an outside view. Progress in most areas is relatively incremental and continuous; we should expect the same to be true for AI, suggesting that timelines should be quite long, on the order of centuries. The current AI boom looks similar to previous AI booms, which didn't amount to much in the past.\n\nTimelines could be short if progress in AI were \"lumpy\", as in a FOOM scenario. This could happen if intelligence was one simple thing that just has to be discovered, but Robin expects that intelligence is actually a bunch of not-very-general tools that together let us do many things, and we simply have to find all of these tools, which will presumably not be lumpy. Most of the value from tools comes from more specific, narrow tools, and intelligence should be similar. In addition, the literature on human uniqueness suggests that it wasn't \"raw intelligence\" or small changes to brain architecture that makes humans unique, it's our ability to process culture (communicating via language, learning from others, etc).\n\nIn any case, many researchers are now distancing themselves from the FOOM scenario, and are instead arguing that AI risk occurs due to standard principal-agency problems, in the situation where the agent (AI) is much smarter than the principal (human). Robin thinks that this doesn't agree with the existing literature on principal-agent problems, in which losses from principal-agent problems tend to be bounded, even when the agent is smarter than the principal.\n\nYou might think that since the stakes are so high, it's worth working on it anyway. Robin agrees that it's worth having a few people (say a hundred) pay attention to the problem, but doesn't think it's worth spending a lot of effort on it right now. Effort is much more effective and useful once the problem becomes clear, or once you are working with a concrete design; we have neither of these right now and so we should expect that most effort ends up being ineffective. It would be better if we saved our resources for the future, or if we spent time thinking about other ways that the future could go (as in his book, Age of Em).\n\nIt's especially bad that AI safety has thousands of \"fans\", because this leads to a \"crying wolf\" effect -- even if the researchers have subtle, nuanced beliefs, they cannot control the message that the fans convey, which will not be nuanced and will instead confidently predict doom. Then when doom doesn't happen, people will learn not to believe arguments about AI risk.\n\n**Rohin's opinion:** Interestingly, I agree with almost all of this, even though it's (kind of) arguing that I shouldn't be doing AI safety research at all. The main place I disagree is that losses from principal-agent problems with perfectly rational agents are bounded -- this seems crazy to me, and I'd be interested in specific paper recommendations (though note [I](https://www.lesswrong.com/posts/ktDKfKqukTPRiuEPM/robin-hanson-on-the-futurist-focus-on-ai#fecYAwjmMSZ9KRfPL) [and](https://www.lesswrong.com/posts/ktDKfKqukTPRiuEPM/robin-hanson-on-the-futurist-focus-on-ai#yRLaEzT57K4q5Qz5H) [others](https://www.lesswrong.com/posts/ktDKfKqukTPRiuEPM/robin-hanson-on-the-futurist-focus-on-ai#p6CGEFbqaYJb49jME) have searched and not found many).\n\nOn the point about lumpiness, my model is that there are only a few underlying factors (such as the ability to process culture) that allow humans to so quickly learn to do so many tasks, and almost all tasks require near-human levels of these factors to be done well. So, once AI capabilities on these factors reach approximately human level, we will \"suddenly\" start to see AIs beating humans on many tasks, resulting in a \"lumpy\" increase on the metric of \"number of tasks on which AI is superhuman\" (which seems to be the metric that people often use, though I don't like it, precisely because it seems like it wouldn't measure progress well until AI becomes near-human-level).\n\n[Conversation with Adam Gleave](https://aiimpacts.org/conversation-with-adam-gleave/) *(Adam Gleave et al)* (summarized by Rohin): Adam finds the traditional arguments for AI risk unconvincing. First, it isn't clear that we will build an AI system that is so capable that it can fight all of humanity from its initial position where it doesn't have any resources, legal protections, etc. While discontinuous progress in AI could cause this, Adam doesn't see much reason to expect such discontinuous progress: it seems like AI is progressing by using more computation rather than finding fundamental insights. Second, we don't know how difficult AI safety will turn out to be; he gives a probability of ~10% that the problem is as hard as (a caricature of) MIRI suggests, where any design not based on mathematical principles will be unsafe. This is especially true because as we get closer to AGI we'll have many more powerful AI techniques that we can leverage for safety. Thirdly, Adam does expect that AI researchers will eventually solve safety problems; they don't right now because it seems premature to work on those problems. Adam would be more worried if there were more arms race dynamics, or more empirical evidence or solid theoretical arguments in support of speculative concerns like inner optimizers. He would be less worried if AI researchers spontaneously started to work on relative problems (more than they already do).\n\nAdam makes the case for AI safety work differently. At the highest level, it seems possible to build AGI, and some organizations are trying very hard to build AGI, and if they succeed it would be transformative. That alone is enough to justify some effort into making sure such a technology is used well. Then, looking at the field itself, it seems like the field is not currently focused on doing good science and engineering to build safe, reliable systems. So there is an opportunity to have an impact by pushing on safety and reliability. Finally, there are several technical problems that we do need to solve before AGI, such as how we get information about what humans actually want.\n\nAdam also thinks that it's 40-50% likely that when we build AGI, a PhD thesis describing it would be understandable by researchers today without too much work, but ~50% that it's something radically different. However, it's only 10-20% likely that AGI comes only from small variations of current techniques (i.e. by vastly increasing data and compute). He would see this as more likely if we hit additional milestones by investing more compute and data (OpenAI Five was an example of such a milestone).\n\n**Rohin's opinion:** I broadly agree with all of this, with two main differences. First, I am less worried about some of the technical problems that Adam mentions, such as how to get information about what humans want, or how to improve the robustness of AI systems, and more concerned about the more traditional problem of how to create an AI system that is *trying* to do what you want. Second, I am more bullish on the creation of AGI using small variations on current techniques, but vastly increasing compute and data (I'd assign ~30%, while Adam assigns 10-20%).", "url": "https://www.alignmentforum.org/posts/QknPz9JQTQpGdaWDp/an-80-why-ai-risk-might-be-solved-without-additional", "date_published": "2020-01-02T18:20:02Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI Risk", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.693779+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "daea13bad1d31b1c033d9498ab2461e7", "source": "alignmentforum", "title": "Exploring safe exploration", "text": "*This post is an attempt at reformulating some of the points I wanted to make in “[Safe exploration and corrigibility](https://www.alignmentforum.org/posts/87Y7w73phjBxnPyPD/safe-exploration-and-corrigibility)” in a clearer way. This post is standalone and does not assume that post as background.*\n\n\n[In a previous comment thread, Rohin argued that safe exploration is currently defined as being about the agent not making “an accidental mistake.”](https://www.alignmentforum.org/posts/87Y7w73phjBxnPyPD/safe-exploration-and-corrigibility#aeXgHHx3v5rRMG5MP) I think that definition is wrong, at least to the extent that I think it both doesn't make much sense and doesn't describe how I actually expect current safe exploration work to be useful.\n\n\nFirst, what does it mean for a failure to be an “accident?” This question is simple from the perspective of an engineer outside the whole system—any unintended failure is an accident, encapsulating the majority of AI safety concerns (i.e. “[accident risk](https://arxiv.org/abs/1606.06565)”). But that's clearly not what the term “accidental mistake” is pointing at in this context—rather, the question here is *what is an accident from the perspective of the model?* Intuitively, an accident from the perspective of the model should be some failure that the model didn't intend or wouldn't retroactively endorse. But that sort of a definition only makes sense for [highly coherent mesa-optimizers](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/FkgsxrGf3QxhfLWHG) that actually have some notion of intent. Maybe instead we should be thinking of this from the perspective of the base optimizer/loss function? That is, maybe a failure is an accidental failure if the loss function wouldn't retroactively endorse it (e.g. the model got a very low reward for making the mistake). By this definition, however, *every generalization failure is an accidental failure* such that safe exploration would just be the problem of generalization.\n\n\nOf all of these definitions, the definition defining an accidental failure from the perspective of the model as a failure that the model didn't intend or wouldn't endorse seems the most sensical to me. Even assuming that your model is a highly coherent mesa-optimizer such that this definition makes sense, however, I still don't think it describes current safe exploration work, and in fact I don't think it's even really a safety problem. The problem of producing models which don't make mistakes from the perspective of their own internal goals is precisely the problem of making powerful, capable models—that is, it's precisely the problem of [capability generalization](https://www.alignmentforum.org/posts/2mhFMgtAjFJesaSYR/2-d-robustness). Thus, to the extent that it's reasonable to say this for any ML problem, the problem of accidental mistakes under this definition is just a capabilities problem. However, I don't think that at all invalidates the utility of current safe exploration work, as I don't think that current safe exploration work is actually best understood as avoiding “accidental mistakes.”\n\n\nIf safe exploration work isn't about avoiding accidental mistakes, however, then what is it about? Well, let's take a look at an example. [Safety Gym](https://cdn.openai.com/safexp-short.pdf) has a variety of different environments containing both goal states that the agent is supposed to reach and unsafe states that the agent is supposed to avoid. From [OpenAI's blog post](https://openai.com/blog/safety-gym/): “If deep reinforcement learning is applied to the real world, whether in robotics or internet-based tasks, it will be important to have algorithms that are safe even while learning—like a self-driving car that can learn to avoid accidents without actually having to experience them.” Why wouldn't this happen naturally, though—shouldn't an agent in a [POMDP](https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process) always want to be careful? Well, not quite. When we do RL, there are really two different forms of exploration happening:[[1]](#fn-HtGxjApz9usEYDaSs-1)\n\n\n* **Within-episode exploration,** where the agent tries to identify what particular environment/state it's in, and\n* **Across-episode exploration,** which is the problem of making your agent explore enough to gather all the data necessary to train it properly.\n\n\nIn your standard episodic POMDP setting, you get within-episode exploration naturally, but not across-episode exploration, which you have to explicitly incentivize.[[2]](#fn-HtGxjApz9usEYDaSs-2) Because we have to explicitly incentivize across-episode exploration, however, it can often lead to behaviors which are contrary to the goal of actually trying to achieve the greatest possible reward in the current episode. Fundamentally, I think current safe exploration research is about trying to fix that problem—that is, it's about trying to make across-episode exploration less detrimental to reward acquisition. This sort of a problem is most important in an online learning setting where bad across-episode exploration could lead to catastrophic consequences (e.g. crashing an actual car to get more data about car crashes).\n\n\nThus, rather than define safe exploration as “avoiding accidental mistakes,” I think the right definition is something more like “improving across-episode exploration.” However, I think that this framing makes clear that there are other types of safe exploration problems—that is, there are other problems in the general domain of making across-episode exploration better. For example, I would love to see an exploration of how different across-episode exploration techniques impact [capability generalization vs. objective generalization](https://www.alignmentforum.org/posts/2mhFMgtAjFJesaSYR/2-d-robustness)—that is, when is across-episode exploration helping you collect data which improves the model's ability to achieve its current goal versus helping you collect data which improves the model's goal?[[3]](#fn-HtGxjApz9usEYDaSs-3) Because across-episode exploration is explicitly incentivized, it seems entirely possible to me that we'll end up getting the incentives wrong somehow, so it seems quite important to me to think about how to get them right—and I think that the problem of getting them right is the right way to think about safe exploration.\n\n\n\n\n---\n\n\n\n1. This terminology is borrowed from [Rohin's first comment](https://www.alignmentforum.org/posts/87Y7w73phjBxnPyPD/safe-exploration-and-corrigibility#aeXgHHx3v5rRMG5MP) in the same comment chain I mentioned previously. [↩︎](#fnref-HtGxjApz9usEYDaSs-1)\n2. With some caveats—in fact, I think a form of across-episode exploration will be instrumentally incentivized for an agent that is aware of the training process it resides in, though that's a bit of a tricky question that I won't try to fully address now (I tried talking about this somewhat in “[Safe exploration and corrigibility](https://www.alignmentforum.org/posts/87Y7w73phjBxnPyPD/safe-exploration-and-corrigibility),” though I don't think I really succeeded there). [↩︎](#fnref-HtGxjApz9usEYDaSs-2)\n3. This is what I somewhat confusingly called the “objective exploration problem” in “[Safe exploration and corrigibility](https://www.alignmentforum.org/posts/87Y7w73phjBxnPyPD/safe-exploration-and-corrigibility).” [↩︎](#fnref-HtGxjApz9usEYDaSs-3)", "url": "https://www.alignmentforum.org/posts/NBffcjqm2P4dNbjrE/exploring-safe-exploration", "date_published": "2020-01-06T21:07:38Z", "authors": ["evhub"], "tags": ["AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.694117+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "2ade106204f7ec23f0d3274622ebde0f", "source": "alignmentforum", "title": "(Double-)Inverse Embedded Agency Problem", "text": "MIRI [has said a lot](https://intelligence.org/embedded-agency/) about the issue of embedded agency over the last year. However, I am yet to see them trying to make progress in what I see as the most promising areas. \n\nHow does one attack a problem that is new, complicated and non-obvious? By **constructing toy models** and **inverting hard questions** to make them more tractable. \n\nIn general an [inverse problem](https://en.wikipedia.org/wiki/Inverse_problem) is harder than the \"direct\" one, because we are trying to infer unobservables from observables. Wikipedia gives an example of figuring out the position of Neptune from the perturbations in the orbit of Uranus. Another popular example is NP-complete problems: they are famously hard to solve but it is easy to verify a solution. Another example: you take **a multiple-choice math quiz**, it is often faster and easier to get the right answer by plugging the 4 or 5 potential solutions into the stated problem than to solve the problem directly.\n\nI'll give an example from my own area. The equations of **general relativity** are hard to solve except in a few highly symmetric cases. It is a classic inverse problem. But! Any spacetime metric is actually a solution of the Einstein equations, so all one needs to do is to write down a metric and calculate its Einstein tensor to see what kind of a matter distribution (and boundary conditions) it is a solution of. **Inverting the inverse problem!** Of course, life is not that easy. Most solutions correspond to \"unphysical\" matter, usually with negative energy density, superluminal flows, singularities, infinities, weird topologies etc. However, it is a useful approach if one wants to study some general properties of the equations, and get a feel for (or sometimes a theorem about) what goes wrong, why and how. After a few iterations one can get better at guessing what form a \"good\" solution might take, and write up an ansatz that can help solve the original, not the inverse problem in some cases.\n\nAnother, more familiar example: **arithmetic division**. Until you learn or figure out the rules, it's hard. But its inverse problem, multiplication, is actually much easier! So to learn more about division, it pays to try to start with potential solutions and see what kind of multiplication actually solve the division problem. Eventually one can come up with the long division algorithm, that uses nothing but multiplication and subtraction. And voila, inverting an inverse problem helps us solve the original one.\n\nThis approach is common in computer science, as well. Plenty of algorithms, like **search**, actually rely on solving smaller and simpler inverse problems.\n\nI contend that a similar approach could be useful for making progress in understanding embedded agency. To that end, let's first restate the original problem of embedded agency ([copied from the alignment forum page](https://www.alignmentforum.org/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version)):\n\n**How can one make good models of the world that are able to fit within an agent that is much smaller than the world?**\n\nThis is a hard inverse problem! There are many faucets of it, such as the oft-mentioned problem of logical counterfactuals, that do not seem to yield to direct attacks. So, it seem natural to learn to \"seek under the light\" before stepping into the darkness, and that includes, you guessed it, constructing toy models and inverting the inverse problems.\n\nWhat would inverting this problem look like? There are multiple possible formulations, just like an inverse of the operation of power a^b is both n-th root and logarithm. Here is a couple of ideas:\n\n* Create a toy universe and look for its representations inside.\n* Create a toy model and construct a world around it such that the model represents the world in some way.\n\nHere is an example: a **fractal** is self-similar, so any subset of it can be thought of as a near-perfect model of the whole. Of course, a model is not enough, one has to figure out what would constitute an agent using this model in this fractal world. But at least it can be a promising and potentially illuminating direction to explore. There are plenty more ideas one can come up after thinking about it for 5 minutes.\n\nI hope someone at MIRI is either thinking along these directions, or is ready to try to, instead of being stuck analyzing the messy and complicated inverse problem that is the \"real world\".", "url": "https://www.alignmentforum.org/posts/itGmH2AknmjWyAwj8/double-inverse-embedded-agency-problem", "date_published": "2020-01-08T04:30:25Z", "authors": ["shminux"], "tags": ["Embedded Agency"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.694940+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "303d09e63768206ddd97dd4c25b9336e", "source": "alignmentforum", "title": "[AN #81]: Universality as a potential solution to conceptual difficulties in intent alignment", "text": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.\n\nAudio version [here](http://alignment-newsletter.libsyn.com/alignment-newsletter-81) (may not be up yet).\n\nPublished a year ago, [this](https://ai-alignment.com/towards-formalizing-universality-409ab893a456) [sequence](https://ai-alignment.com/informed-oversight-18fcb5d3d1e1) [of](https://ai-alignment.com/universality-and-model-based-rl-b08701394ddd) [five](https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd) [posts](https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d) introduced the idea of *ascription universality*. I didn't really get it on a first reading, and only recently read it in enough detail that I think I understand the main ideas. This entire newsletter will focus on ascription universality; treat all of it as a \"Highlight\".\n\nThe key idea of these posts is that of *universality*: when we can say that some agent \"knows everything that any other agent could know\". Of course, there will always be some agent with arbitrarily complex beliefs, but we could hope to have agents that know everything that is known by any agent of some complexity class (e.g. agents that run in polynomial time). However, there are still simple programs that can have complex *true* beliefs: we could make agents that believe that P != NP and that P = NP, and one of those agents will fall into this class. However, these agents are somewhat degenerate -- they don't have a good reason to *trust* that their belief is true, and so we don't want to worry about them. Since we want to talk about *justified* belief, let's consider a property based on which of the two agents *we* would trust.\n\nSo, for now let's adopt an informal definition: a program A[C] is universal with respect to some class of programs C if we would trust any beliefs reported by A[C], no matter what beliefs we hear reported by programs in C. (I'll talk about the specific notion of \"ascription universality\" later.) Notably, we need to trust A[C] even over our own beliefs, that is, if A[C] believes something, we discard our position and adopt A[C]'s belief.\n\nNote that this is still a very strong criterion. Consider the question of whether AI will lead to x-risk: if one agent thinks about fixed-point theorems, while another studies neural nets, and a third tries to predict the response of democratic institutions, the agent A[C] must compete with *everything* that these agents (justifiably) conclude. For now, let's set aside the question of how to build a universal agent, and instead understand what it enables us to do.\n\n[Informed oversight (revisited)](https://ai-alignment.com/informed-oversight-18fcb5d3d1e1) *(Paul Christiano)* (summarized by Rohin): Introduced in [this older post](https://ai-alignment.com/the-informed-oversight-problem-1b51b4f66b35), we consider a setting in which an overseer is training an agent, and the overseer is smarter than the agent. Intuitively, this should work out well, because if the agent ever tries to do something bad, the overseer should notice this (since it is smarter), and penalize it. However, the problem is that the agent might *in the course of computing an answer* come up with considerations that the overseer can't easily find. For example, an agent tasked with creating novel literature might end up plagiarizing from one book; the overseer may not be able to search through all the books to detect the plagiarism. However, if the overseer is universal, then it should also know that the agent's output is plagiarized from the book (because the agent knows that), and it can penalize it appropriately.\n\nWe can even construct a semi-formal argument (see the post for the math). Let us assume that there is some true utility function U, and when the agent takes some action a, the overseer provides it with reward r = E[U(a)], where the expectation is relative to the overseer's beliefs. Let us assume away problems of distribution shift: in particular, we'll assume that when choosing between actions a1 and a2, the agent always chooses the action with higher reward. Now suppose that the agent chooses action a1. We can conclude that a1 has higher reward than a2. This means that the observer believes that a1 has higher true utility than a2. But by universality, we always trust the observer over the agent and ourselves, and so we must also believe that a1 has higher true utility than a2, which means that we believe the agent is choosing the right action.\n\n**Rohin's opinion:** This application of universality is the one I believe the most; even intuitively it seems like the entire informed oversight problem is that the overseer *doesn't* know everything that the agent knows, and universality straightforwardly fixes that. So I see this as the motivating example for caring about universality at all.\n\n[Worst-case guarantees (Revisited)](https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d) *(Paul Christiano)* (summarized by Rohin): Let's start considering distribution shift again: we imagine that the agent has already been trained, and there is no overseer any more. When the agent goes off distribution, it may not choose the action that would have gotten the highest reward, and in fact it could be chosen maliciously. As a silly motivating example, perhaps the agent learned the policy \"behave well until I see that RSA-2048 (a product of two very large primes) has been factored\"; such a policy will behave perfectly during training but will probably commit a treacherous turn eventually.\n\nIn this setting, we would like to have a worst-case guarantee, that the model will never behave unacceptably. However, we need to enforce this guarantee at training time, *before* we know what situations will arise at test time. Universality of the overseer isn't enough to guarantee this, because universality only means that the overseer knows what the agent *currently* knows; the agent can't really be said to \"know\" that it will deceive us in the future.\n\nNonetheless, it seems like the solution is very related to universality, in that we would like to \"peer inside\" the agent to figure out how it works, to make sure there aren't situations in which it would behave unacceptably. We could use adversarial training for this purpose: train an adversary to find inputs on which the model behaves unacceptably, and then train the model on those inputs to not do that. This will require interpretability techniques, so that the adversary actually stands a chance of finding the relevant inputs. In order to handle cases like RSA-2048 above, the adversary should also be able to provide \"pseudo-inputs\" like \"the world in which the agent sees that RSA-2048 is factored\" (otherwise the adversary would have to factor RSA-2048 itself to demonstrate an input on which the agent would fail, which is far too difficult), while not being allowed to provide pseudo-inputs like \"the world in which the agent kills all humans\" (which no agent can behave acceptably on). Ultimately, we need to make sure the adversary will always be able to find a bad pseudo-input if one exists, without making it too difficult for the agent to behave acceptably on all pseudo-inputs. See also [relaxed adversarial training](https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment) ([AN #70](https://mailchi.mp/732eaa192df0/an-70-agents-that-help-humans-who-are-still-learning-about-their-own-preferences)).\n\n**Rohin's opinion:** I was skeptical of the [previous iteration of this post](https://www.alignmentforum.org/posts/E2aZ9Xwdz3i2ghPtn/techniques-for-optimizing-worst-case-performance) ([AN #43](https://mailchi.mp/768a8130013f/alignment-newsletter-43)), which proposed adversarial training, verification, and interpretability, because it seemed like none of them would allow us to get a *guarantee* on performance. I'm significantly more optimistic after reading this post, mainly because a couple of previous confusions have been resolved:\n\n1. The point of verification is not that we can prove a theorem saying \"this agent is beneficial\"; the point is that by making *relaxations* (pseudo-inputs), a technique commonly used in formal verification, we can reduce the burden on the other methods being used (such as adversarial training).\n\n2. Similarly, the point of interpretability is not to help *us* understand what the agent is doing or will do, it's to help the *overseer* (or adversary in adversarial training) understand that. Unlike us, the overseer / adversary can scale up along with the agent itself.\n\nI still think that it would be hard to get a guarantee with adversarial training, given that adversarial training has to eliminate *all* vulnerabilities. On the other hand, it only has to find all of the settings where the agent is *maliciously optimizing against us*, which you might hope is a more natural category that is easier to identify without looking too much at particular inputs. This seems like an empirical question on which we'll hopefully get data, though even if it works in all cases that we see, that doesn't rule out the possibility that we failed to notice some issue that will only be triggered in the future (as in the RSA-2048 example).\n\n[Universality and model-based RL](https://ai-alignment.com/universality-and-model-based-rl-b08701394ddd) *(Paul Christiano)* (summarized by Rohin): So far, we've been talking about the model-free setting, where the overseer provides the incentives. What about model-based RL? Here, we might want to learn separate distributions over models and utility functions using iterated amplification or HCH, and then plan using any off-the-shelf algorithm, such as MCTS. The first new problem that arises is that our distribution over models may be malicious (see [benign induction](https://agentfoundations.org/item?id=1263)). Using universality, we can solve this problem in two different ways. First, we could use HCH to filter out malicious models. By universality, since a malicious model knows what its goals are, so does HCH, and thus HCH will detect it. Second, we could ask HCH to give us a good prior over models. By universality, this prior over models must be better than the prior used by the malicious models, and so the malicious models' predictions will be worse than that of HCH as a whole, and so the malicious models will not be able to affect the computation much.\n\nThe second new problem in the model-based setting is that it isn't clear how our utility function can extract all the relevant information from the model. For example, if the model is a detailed simulation of physics, how do you find the \"humans\" and evaluate how \"happy\" they are? Universality is at least necessary for this: if there are facts the model \"knows\", those facts may be relevant to the utility calculation, and so the utility function also needs to know them. We might hope that universality is sufficient: in the previous case, we might think that the physics simulation \"knows\" whether humans are happy. But this depends on details of what \"know\" means that haven't yet been fleshed out, and it depends on what \"kind\" of universality we can achieve in practice.\n\n[Universality and consequentialism within HCH](https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd) *(Paul Christiano)* (summarized by Rohin): In all the settings discussed so far, we hope to use [HCH](https://www.alignmentforum.org/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch) ([AN #34](https://mailchi.mp/f1947668b183/alignment-newsletter-34)). What if HCH itself contains malicious optimization? Specifically, recall that HCH involves an arbitrarily large tree of humans breaking questions down into subquestions. Memetic selection on 10^100 humans should be *much* stronger than the selection done by evolution to produce us, and so could be quite dangerous. We would like to use HCH to check whether its own computations are malicious.\n\nSo, we'll define a pair of mutually recursive functions, Filter and Filtered-HCH. Filter takes as input a transcript from HCH, and using filtered-HCH it finds the best argument suggesting that the transcript is problematic. It then uses filtered-HCH to ask whether, in light of this argument, the transcript should be treated as problematic. This check is repeated thrice for robustness. (Note that the original HCH treats the transcript as trusted computations, while the checker treats it as an untrusted input.) Filtered-HCH has the same type signature as HCH (i.e. it produces answers to questions). Just like HCH, it computes an answer to the question, with the help of recursive calls to filtered-HCH to get answers to subquestions. Instead of returning the answer directly, it calls Filter to check that the computation done was problematic, and returns the special symbol ⟂ instead of the answer if so.\n\nThe basic hope is that if there's some malicious subcomputation implemented \"on top of\" the base agents in HCH, and it tries to break HCH, then it \"knows\" why that attempt could work, and so if filtered-HCH is universal with respect to computations that can be instantiated by (smaller versions of) HCH, then filtered-HCH will also \"know\" that the computation could break HCH, and will censor it as a result. Consider the threat model from [Reliability amplification](https://www.alignmentforum.org/posts/6fMvGoyy3kgnonRNM/reliability-amplification) ([AN #44](https://mailchi.mp/6bfac400a0c3/alignment-newsletter-44)), where the chance of any particular HCH computation with budget n becoming malicious is ε, independent of the input or any past HCH calls. Then, intuitively, filtered-HCH (which needs a higher budget, say kn) has a kε chance of becoming malicious, but since the check is repeated three times and a majority vote is taken, the chance of a malicious computation getting through is k^2ε^2, which will be much smaller than the original ε if k^2 ε << 1. (This is significantly better than the scheme in [Reliability amplification](https://www.alignmentforum.org/posts/6fMvGoyy3kgnonRNM/reliability-amplification) ([AN #44](https://mailchi.mp/6bfac400a0c3/alignment-newsletter-44)), because the question \"is there a problem?\" is a binary yes/no question, and so majority vote works a lot better.) The harder case is when specific inputs cause very high probabilities of failure (which I believe is the threat model from [Security amplification](https://www.alignmentforum.org/posts/hjEaZgyQ2iprDhkg8/security-amplification) ([AN #45](https://mailchi.mp/35b451cb4d70/alignment-newsletter-45))). I particularly don't understand this section, but I believe the hope is that since the malicious computation is implemented \"on top of\" the (aligned) humans, only a small \"part\" of the computation can be malicious, and this can be detected by a *small* instance of Filtered-HCH on which the malicious computation cannot be implemented.\n\n**Rohin's opinion:** I don't understand this post that well, so I'll refrain from opining on it. I think I would benefit from a more concrete example of how malicious optimization could be implemented \"on top of\" HCH, even when the humans are restricted to a small set of inputs on which they are \"secure\".\n\n[Towards formalizing universality](https://ai-alignment.com/towards-formalizing-universality-409ab893a456) *(Paul Christiano)* (summarized by Rohin): We've now seen a bunch of applications of universality. So what is *ascription* universality? It's a specific formalization of the concept of universality. Recall our informal definition so far: a program A[C] is universal with respect to some class of programs C if we would trust any beliefs reported by A[C], no matter what beliefs we hear reported by programs in C. But how can a program \"report beliefs\"? For HCH, to find out its belief for some question, we simply ask HCH the question and take its answer as its \"belief\". However, we can't do this for arbitrary computations: in particular, if our agent is deliberately hiding a belief from us (and so wouldn't report it if asked), we shouldn't simply say that it doesn't have that belief. This is where *ascriptions* come in: an ascription is any way of ascribing beliefs to the agent. Simply asking the agent and trusting its answer is one possible ascription. Another class of ascriptions would look at the agent's code and memory, and make inferences about the agent's beliefs from that. We want to say that HCH will dominate *any* \"reasonable\" ascription of beliefs to the agent, where an ascription procedure is \"reasonable\" if it isn't \"doing the hard work\"; for example, if an agent always says \"Yes\", it is unreasonable to ascribe to it the belief that Fermat's Last Theorem is true (even though when given the question \"Is Fermat's Last Theorem true\" it answers yes).\n\nPutting it all together, A[C] is *ascription universal* with respect to some class of programs C if, for every \"reasonable\" ascription procedure and program c in C, A[C]'s beliefs (which we find by asking it) *epistemically dominate* the beliefs ascribed to c. A set of beliefs B1 *epistemically dominates* another set of beliefs B2 *from our perspective* if, given arbitrary information about B1 and B2, for every question our best guess at the answer is what we expect B1 would answer.\n\nSo far we've been talking about A[C] dominating an entire class of programs C. We might call this *idealized* ascription universality, and hope that it applies to systems with perfect optimization, such as [debate](https://blog.openai.com/debate/) ([AN #5](https://mailchi.mp/0ae5d69de63b/alignment-newsletter-5)) under optimal play. In practice, we'll never get perfect optimization, and our overseer isn't going to be able to think of *everything* that any agent could be thinking about. Instead, we could aim for *practical* ascription universality, in which A[c] dominates a particular computation c, and it gets to look at the computation c as a hint. This would allow an overseer to train an agent, while epistemically dominating that agent. Effectively we're asking two questions: first, would our procedure be universal if the optimization was performed perfectly? Second, is weight sharing / interpretability strong enough to solve the optimization problems needed for universality?\n\nThe blog post also has some informal thoughts on why we should expect that a careful Bayesian should be able to epistemically dominate computations found via machine learning (which is why we might hope that HCH could be ascription universal).\n\n**Rohin's opinion:** I was initially a bit confused by the decomposition into the idealized and the practical variant, but I think it makes more sense now: the assumption of *perfect* optimization gives you a lot of mileage. For example, A[C] might choose a particular computation c in C, and then use its beliefs; if that *choice of computation* were already \"perfectly optimized\" for convincing us, then we would always be at least as convinced of A[C]'s answer as the answer of any other c in C. Debate under optimal play is pretty similar: under optimal play, the agent only tells us about the most important considerations, and only brings up other considerations if its opponent asks about them; you would hope that this is like selecting the best c in C and reporting its reasons for its beliefs.\n\nWhat's happening here is that the perfect optimization allows us to select the best c in C, or the best argument, etc. \"for free\" (i.e. without using any computation), giving us huge savings; everything else just has to present the results properly. Consider again the question of whether AI will lead to x-risk: while there are different agents thinking about fixed-point theorems, neural nets, and democratic institutions, perfect optimization lets us \"pick\" the agent whose arguments are most important. Then, we should only need about as much computation as the agent itself.\n\nHowever, \"presenting the results properly\" is non-trivial: A[C] needs to report the beliefs *in plain English*, even if the computation c in C has its beliefs \"encoded\" (as in the case of a detailed physics simulation that can be said to \"know\" that humans are happy, but doesn't say that in plain English). It's not clear to me whether this will work out.\n\nYou might very reasonably be worried that perfect optimization of \"the most convincing argument\" will get you something that is convincing for the wrong reasons, and is not actually correlated with the truth. The definition of universality doesn't preclude this: it simply says that we trust A[C]'s beliefs over our own beliefs and any other agent's beliefs; this does hold for convincing but invalid arguments. I believe (but am not sure) that the hope is to deal with this by restricting inputs to the human to a small secure space, and by the techniques discussed above in [Universality and consequentialism within HCH](https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd).\n\n[Nuances with ascription universality](https://www.alignmentforum.org/posts/R5Euq7gZgobJi5S25/nuances-with-ascription-universality) *(evhub)* (summarized by Rohin): Consider a formalization of honesty / transparency related to ascription universality: a system is honest, if relative to our expectations, its explicitly stated beliefs give us just as much information as any other way of ascribing beliefs to the system. A system that searches for ways to commit a treacherous turn would not be honest, since it would not state that it was performing this search. However, what if this search was \"distilled\" into the weights of a neural net (akin to a lookup table)? In that case, since we wouldn't ascribe beliefs to the lookup table, the system could be deemed honest by this criterion. So, what we actually need is for the entire training process to be honest. (A similar critique applies directly to ascription universality.)\n\n**Rohin's opinion:** This is a good point; it shows that you need your overseer to be ascription universal with respect to the entire training process, not just the final agent.", "url": "https://www.alignmentforum.org/posts/3kzFPA5uuaGZWg4PS/an-81-universality-as-a-potential-solution-to-conceptual", "date_published": "2020-01-08T18:00:02Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.695231+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "088f3d3eb0aa6beb183f7b52831c5d55", "source": "alignmentforum", "title": "Outer alignment and imitative amplification", "text": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\nUnderstanding the outer alignment problem\n-----------------------------------------\n\n\nWhat really is outer alignment? In “[Risks from Learned Optimization](https://arxiv.org/abs/1906.01820),” we defined outer alignment in the context of machine learning as “aligning the specified loss function with the intended goal.” But that's not a perfectly well-defined statement—what does it mean for a loss function to be “aligned” with the intended goal? If the goal we care about is maximizing U, do we need exactly L=−aU+b for constants a,b? That's a pretty high bar.\n\n\nWell, what exactly do we want outer alignment for? At the end of the day, we care about whether the model that pops out the other end of our training procedure will be safe, which is a complicated question involving the loss function, the architecture, the implicit inductive biases, and so on. In what sense, then, is it even reasonable to look at just the lost function in isolation and ask whether it's aligned or not?\n\n\nI think the strongest case for outer alignment being a meaningful problem in isolation comes from the argument that loss functions seem to scale pretty well with generic machine learning progress. If, as a silly example, your outer alignment scheme is to “train image classification models,” that's something that ML has progressively gotten better at over time. Compare that to the silly inner alignment scheme of “train a straightforward CNN”—that's something that ML has passed by pretty rapidly in favor of architectural improvements like residual connections even just for the task of image classification. Of course, outer alignment alone does not an aligned AGI make, so you still have to have some notion of how you're going to do inner alignment in mind—but loss functions scaling better is still a perfectly valid reason for focusing on outer alignment.[[1]](#fn-wY7Ni5JBsYFJxwg7g-1)\n\n\nThus, it does seem quite reasonable to me to put effort into finding “aligned” loss functions. But that still brings us back to the question of what exactly makes a loss function “aligned.” In the context of a specific training/inner alignment scheme, we can say that a loss function is aligned if, when plugged into that training scheme, it produces models which are aligned with our goals. But in the absence of any specific training scheme, what does it mean to say that a loss function is aligned in isolation? We can of course ask for L=−aU+b as I stated previously, though in my opinion I think achieving something like that is likely to be nearly impossible.\n\n\nOuter alignment at optimum\n--------------------------\n\n\nI think there is another version of “outer aligned in isolation,” however, which is both meaningful and (at least somewhat) achievable which I will call *outer aligned at optimum.* Intuitively, I will say that a loss function is *outer aligned at optimum* if all the possible models that perform optimally according to that loss function are aligned with our goals—that is, they are at least [trying to do what we want](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6). More precisely, let M=X→A and L=(X→A)→R=M→R. For a given loss function L∈L, let l∗=minM∈M L(M). Then, L is *outer aligned at optimum* if, for all M∗∈M such that L(M∗)=l∗, M∗ is trying to do what we want.\n\n\nThat's the definition—now why should we care? In basically any practical setting we're never going to reach perfect loss, so why should it matter if those functions which do have perfect loss are aligned or not? In my opinion, I think there is a strong argument for loss functions which are aligned at optimum being significantly less susceptible to [Goodhart's Law](https://en.wikipedia.org/wiki/Goodhart%27s_law) as we scale up ML capabilities. Suppose you know that a loss function L is aligned for current ML capabilities. When you then scale up those capabilities and push harder on minimizing L, you immediately run into all the issues of Goodhart's Law where L can quickly cease to be a good proxy for alignment as you push harder on it. If you have a guarantee that L is aligned at optimum, however, then, while still quite possible, it's a lot harder for Goodhart's Law to bite you. In particular, if you think about the [Goodhart taxonomy](https://www.alignmentforum.org/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy), alignment at optimum almost entirely rules out both Causal and Extremal Goodhart—since you know the relationship is valid and doesn't break down at the extremes—and ensures that Regressional and Adversarial Goodhart won't show up in the limit, though you could still see them before that point. Though this obviously doesn't just give you an alignment guarantee—before you get to the true optimum, you can still get Regressional Goodhart biting you through [proxy alignment](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/pL56xPoniLvtMDQ4J) or Adversarial Goodhart biting you through [deceptive alignment](https://www.alignmentforum.org/posts/zthDPAjh9w6Ytbeks/deceptive-alignment), for example—I think it is nevertheless still a very nice thing to have.\n\n\nThe case for imitative amplification\n------------------------------------\n\n\nWith all of that being said, I can get to the reason that I want to talk about all of this: I think that specifically what I will call *imitative amplification*—in contrast to other [amplification](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd)-based approaches or [debate](https://arxiv.org/abs/1805.00899)-based approaches—has a strong claim to being outer aligned at optimum.[[2]](#fn-wY7Ni5JBsYFJxwg7g-2) Specifically, when I say *imitative amplification,* I mean the class of training procedures which are attempting to produce models which approximate [HCH](https://ai-alignment.com/strong-hch-bedb0dc08d4e) as closely as possible. As a concrete example, consider the scheme where you train a model to minimize the difference between its output and the output of a human consulting that model. I want to contrast this with *approval-based amplification,* by which I mean the class of training procedures where the loss is generated using an approval signal from an amplified overseer. As a concrete example, consider the scheme where you train a model to maximize the extent to which a human consulting that model would approve of that model's output.[[3]](#fn-wY7Ni5JBsYFJxwg7g-3)\n\n\nSo, why does imitative amplification have a stronger case for being outer aligned at optimum than approval-based amplification or debate? Well, precisely because we know what the optimum of imitative amplification is—it's HCH—whereas we really don't know what perfect approval-based amplification or perfect debate look like.[[4]](#fn-wY7Ni5JBsYFJxwg7g-4) Though [some challenges have been raised regarding whether HCH is actually aligned or not](https://www.alignmentforum.org/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal), I tend to be fairly skeptical of these challenges—HCH is just a bunch of humans after all and if you can instruct them not to do things like instantiate arbitrary Turing machines, then I think a bunch of humans put together has a strong case for being aligned.[[5]](#fn-wY7Ni5JBsYFJxwg7g-5) That being said, the same argument does not at all apply to approval-based amplification or debate.\n\n\nFirst, let's consider approval-based amplification.[[6]](#fn-wY7Ni5JBsYFJxwg7g-6) We know what the optimum of imitative amplification looks like—but what is the optimum of approval-based amplification? At first glance, one might imagine that the optimum of approval-based amplification looks like a model whose output is selected to be maximally approved of by HCH. That's very much not the case for the approval-based scheme I described earlier, however. If each step of training is done via maximizing an approval signal, then instead of a tree of just humans you get a tree of humans interspersed with models trying to maximize the approval that their descendants in the tree would assign to their answers. And if you think that human approval can be gamed—which seems extremely likely in my opinion given that we see exactly that sort of gaming happening in our world already all the time—then this is very much not a safe tree. Now, one could make the argument that approval-based amplification can just become imitative amplification if the humans determine their approval by computing a distance function between what they would have said and what the model produced as its output. For example, you could ask your humans to come up with their answers first, then show them the model's answer and ask them to rate how close it was. I'm pretty skeptical of this approach, however—it doesn't seem at all clear to me that this gets around the approval-gaming problem, since the humans still get to see the model's answer and doing so could significantly change how they're thinking about the rating problem.[[7]](#fn-wY7Ni5JBsYFJxwg7g-7)\n\n\nNow, second, let's consider debate with a human judge. In many ways, debate was designed as an approach meant to fix the problems of approval-based reward signals. With a standard approval-based reward signal, the argument goes, it's easy to be tricked by a bad argument that you don't fully understand. In a debate setup, however, you get the benefit of having two competing systems trying to point out flaws in each other's arguments, which hopefully should prevent you from being tricked by bad arguments and thus fix the approval-gaming problem. I'm not convinced, though—false things can be significantly easier to argue for than true things just because there are fewer ways to attack them, they're more rhetorically powerful, or any other number of possible ways in which an argument can be subtly wrong yet still persuasive.[[8]](#fn-wY7Ni5JBsYFJxwg7g-8) Regardless, however, I think the more fundamental objection is just that we really have no way of knowing what optimal play in debate looks like, which makes it very difficult to ever know whether it is outer aligned at optimum or not. With HCH, we know that it just looks like a tree of humans, which at least means we can reason about the parts and how they interact. With optimal debate, however, we have to somehow analyze, understand, and be confident in the alignment of superhuman play on a game involving humans assessing arbitrary strings of characters, which is something that in my opinion seems extremely difficult to do.\n\n\nAddressing competitiveness concerns\n-----------------------------------\n\n\nAll of that is an argument for why we should prefer imitative amplification from an alignment standpoint. However, there's also the problem of imitative amplification just not being competitive in terms of capabilities with other approaches. First of all, I think it's important to remember the importance of putting safety first—*if something isn't safe, then we shouldn't build it.* Of course, arms race dynamics could end up pushing one's hand into going with a best available current option in order to beat some other team which one believes will produce an AI which is even less likely to be safe, though I think it's important to remember that that's a *last resort,* not the default option. Furthermore, even in such a situation, it's still probably fine to eat an overhead cost that is just something like a constant factor worse.\n\n\nWith that being said, I still think there are strong arguments to be made for why imitative amplification can be done competitively. First, like the silly outer alignment scheme of “just train an image classification model” from earlier, imitative amplification gets to piggy-back off of generic ML progress. Imitative amplification is just a language modeling problem, which means generic progress on language modeling should generally be transferable to imitative amplification. Second, I think there is a strong case for language being sufficiently rich as a dataset for training an AGI (EDIT: where “language” is construed to also include embedded images, videos, etc.), at least for the sorts of tasks which I think you will want to use your first AGI for.[[9]](#fn-wY7Ni5JBsYFJxwg7g-9) For example, if the primary/most important purpose of your first AGI is to help you build your second AGI by helping you improve your AGI design, that's the sort of highly cognitive task which I think language is sufficient for. Certainly, if you needed your first AGI to be able to do fine motor control to be competitive, then imitative amplification probably won't get you there—but it seems pretty unlikely to me that ability to do fine motor control will be an important desiderata. Third, a common criticism of imitative amplification is that because imitation treats all data points the same, it won't be able to focus on the most important ones. However, that's not something that's fundamental to the task of imitation. For example, you could use [active learning](https://en.wikipedia.org/wiki/Active_learning_(machine_learning)) to select the most important data points rather than just using a fixed curriculum.[[10]](#fn-wY7Ni5JBsYFJxwg7g-10) Or, you could even weight different data points in your imitation loss using some outside importance criterion while still maintaining the guarantee of perfect imitation at optimum.\n\n\nRegardless, I think the case for imitative amplification's safety is a strong argument in favor of at least focusing on figuring out whether it works and is safe first, before we give up and move to other approaches. Furthermore, even if imitative amplification on its own isn't competitive, I don't think that means we have to abandon it completely—there are modifications to imitative amplification that can be made to help improve competitiveness without sacrificing all of its benefits. For example, you could do reward-modeling-based distillation (e.g. RL + IRL as the distillation step) instead of imitation-based distillation, which, while not imitative (as the optimum isn't HCH anymore), also isn't based on human approval, which could be a nice property. Alternatively, you could first train an HCH model, and then use that model as the judge to train a debate model, which could have significant benefits over just using a human judge. While I don't think we should be focusing on those sorts of things now, the fact that such options exist makes it more likely that imitative amplification work can transfer to future approaches even if imitative amplification itself ends up not being competitive. In any event, I think the case for focusing on imitative amplification right now both from an outer alignment perspective as well as from a competitiveness perspective is quite strong.[[11]](#fn-wY7Ni5JBsYFJxwg7g-11)\n\n\n\n\n---\n\n\n\n1. There is still lots of potential for outer alignment work to be outdated by machine learning progress, however—see, for example, Daniel Kokotajlo's “[A dilemma for prosaic AI alignment](https://www.lesswrong.com/posts/jYdAxH8BarPT4fqnb/a-dilemma-for-prosaic-ai-alignment).” [↩︎](#fnref-wY7Ni5JBsYFJxwg7g-1)\n2. I mentioned this previously a bit in [this comment](https://www.alignmentforum.org/posts/jYdAxH8BarPT4fqnb/a-dilemma-for-prosaic-ai-alignment#FhHeRrkrvJp5fYELj). [↩︎](#fnref-wY7Ni5JBsYFJxwg7g-2)\n3. Note that the two categories of “imitative” and “approval-based” amplification do not cover the entire space of possible amplification-based approaches—there are other possible schemes in this domain as well. For example, you could use imitative amplification to train an HCH approximator, then do RL to produce a model which maximizes that model's approval—or even use your HCH model as a judge in a debate. Alternatively, you could do imitative amplification but instead of using standard imitation learning you could do IRL + RL instead. All of these different approaches have different alignment properties—I have singled out imitative amplification, approval-based amplification, and debate with a human judge because they are the approaches I'm most interested in talking about there, though they are far from the only ones. [↩︎](#fnref-wY7Ni5JBsYFJxwg7g-3)\n4. Note that for the optimum of imitative amplification to be precisely HCH, you need it to be the case that you progressively enlarge your training data as you go along. The fact that you don't get good guarantees for finite datasets is certainly a problem, though it's one that you basically have to solve via inner alignment techniques and thus not one I want to focus on right now. [↩︎](#fnref-wY7Ni5JBsYFJxwg7g-4)\n5. The question of whether theoretical HCH is aligned or not is a pretty complicated question that I don't really want to go into in full detail right now, so if you strongly disagree just take it as a given for this post. [↩︎](#fnref-wY7Ni5JBsYFJxwg7g-5)\n6. Though there was [a previous claim by William Saunders that RL amplification and imitative amplification are equivalent](https://www.alignmentforum.org/posts/fq7Ehb2oWwXtZic8S/reinforcement-learning-in-the-iterated-amplification#vtMDKELmpdJS7n7JT), I think that both of William's proposals there fall into my approval-based category, not my imitative category. See [Rohin Shah's](https://www.alignmentforum.org/posts/fq7Ehb2oWwXtZic8S/reinforcement-learning-in-the-iterated-amplification#vtMDKELmpdJS7n7JT) and [Wei Dai's](https://www.alignmentforum.org/posts/fq7Ehb2oWwXtZic8S/reinforcement-learning-in-the-iterated-amplification#Gfd3H6ZS8dYxQvCJE) comments on William's post to that effect. [↩︎](#fnref-wY7Ni5JBsYFJxwg7g-6)\n7. In particular, this breaks the [analogy to counterfactual oracles](https://www.lesswrong.com/posts/yAiqLmLFxvyANSfs2/counterfactual-oracles-online-supervised-learning-with). [↩︎](#fnref-wY7Ni5JBsYFJxwg7g-7)\n8. I have a lot more to say on this point regarding reasons why false arguments can be more persuasive than true ones, though that's not something I want to go into in too much detail right now. [↩︎](#fnref-wY7Ni5JBsYFJxwg7g-8)\n9. Much of my thinking here owes a debt to Geoffrey Irving. I also talked about the case for language being all you need a bit previously in [this comment](https://www.lesswrong.com/posts/jYdAxH8BarPT4fqnb/a-dilemma-for-prosaic-ai-alignment#aCAwppMhHorp42rkD). [↩︎](#fnref-wY7Ni5JBsYFJxwg7g-9)\n10. Such an active learning scheme could even borrow lessons from [across-episode exploration in RL](https://www.alignmentforum.org/posts/NBffcjqm2P4dNbjrE/exploring-safe-exploration). [↩︎](#fnref-wY7Ni5JBsYFJxwg7g-10)\n11. I also think imitative amplification has some nice inner alignment properties as well, since it gives you an amplified overseer to use for [transparency](https://www.alignmentforum.org/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety)/[relaxed adversarial training](https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment). [↩︎](#fnref-wY7Ni5JBsYFJxwg7g-11)", "url": "https://www.alignmentforum.org/posts/33EKjmAdKFn3pbKPJ/outer-alignment-and-imitative-amplification", "date_published": "2020-01-10T00:26:40Z", "authors": ["evhub"], "tags": ["Outer Alignment", "Goodhart's Law", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.695971+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "3dcc44b42816621c1d8013a8f0b1adf4", "source": "alignmentforum", "title": "Of arguments and wagers", "text": "*Automatically crossposted from* [*ai-alignment.com*](https://ai-alignment.com/)\n\n \n\n(In which I explore an unusual way of combining the two.)\n\nSuppose that Alice and Bob disagree, and both care about Judy’s opinion. Perhaps Alice wants to convince Judy that raising the minimum wage is a cost-effective way to fight poverty, and Bob wants to convince Judy that it isn’t.\n\nIf Judy has the same background knowledge as Alice and Bob, and is willing to spend as much time thinking about the issue as they have, then she can hear all of their arguments and decide for herself whom she believes.\n\nBut in many cases Judy will have much less time than Alice or Bob, and is missing a lot of relevant background knowledge. Often Judy can’t even understand the key considerations in the argument; how can she hope to arbitrate it?\n\nWagers\n\nFor a warm-up, imagine that Judy could evaluate the arguments if she spent a long enough thinking about them.\n\nTo save time, she could make Alice and Bob wager on the result. If both of them believe they’ll win the argument, then they should be happy to agree to the deal: “If I win the argument I get $100; if I lose I pay $100.” (Note: by the end of the post, no dollars will need to be involved.)\n\nIf either side isn’t willing to take the bet, then Judy could declare the case settled without wasting her time. If they are both willing to bet, then Judy can hear them out and decide who she agrees with. That person “wins” the argument, and the bet: **Alice and Bob are betting about what Judy will believe, not about the facts on the ground**.\n\nOf course we don’t have to stick with 1:1 bets. Judy wants to know the probability that she will be convinced, and so wants to know at what odds the two parties are both willing to bet. Based on that probability, she can decide if she wants to hear the arguments.\n\nIt may be that both parties are happy to take 2:1 bets, i.e. each believes they have a 2/3 chance of being right. What should Judy believe? (In fact this should always happen at small stakes: both participants are willing to pay some premium to try to convince Judy. For example, no matter what Alice believes, she would probably be willing to take a bet of $0.10 against $0.01, if doing so would help her convince Judy.)\n\nIf this happens, there is an arbitrage opportunity: Judy can make 2:1 bets with both of them, and end up with a guaranteed profit. So we can continuously raise the required stakes for each wager, until either (1) the market approximately clears, i.e. the two are willing to bet at nearly the same odds, or (2) the arbitrage gap is large enough to compensate Judy for the time of hearing the argument. If (2) happens, then Judy implements the arbitrage and hears the arguments. (In this case Judy gets paid for her time, but the pay is independent of what she decides.)\n\nRecursion\n\nBetting about the whole claim saved us some time (at best). Betting about parts of the claim might get us much further.\n\nIn the course of arguing, Alice and Bob will probably rely on intermediate claims or summaries of particular evidence. For example, Alice might provide a short report describing what we should infer from study Z, or Bob might claim “The analysis in study Z is so problematic that we should ignore it.”\n\nLet’s allow anyone to make a claim at any time. But if Alice makes a claim, Bob can make a counterclaim that he feels better represents the evidence. Then we have a recursive argument to decide which version better represents the evidence.\n\nThe key idea is that **this recursive argument can also be settled by betting**. So one of two things happens: (1) Judy is told the market-clearing odds, and can use that information to help settle the original argument, or (2) there is an arbitrage opportunity, so Judy hears out the argument and collects the profits to compensate her for the time.\n\nThis recursive argument is made in context: that is, Judy evaluates which of the two claims she feels would be a more helpful summary within the original argument. Sometimes this will be a question of fact about which Alice and Bob disagree, but sometimes it will be a more complicated judgment call. For example, we could even have a recursive argument about which wording better reflects the nuances of the situation.\n\nWhen making this evaluation, Judy uses facts she learned over the course of the argument, but she interprets the claim as she would have interpreted it at the beginning of the argument. For example, if Bob asserts “The ellipsoid algorithm is efficient” and Alice disagrees, Bob cannot win the argument by explaining that “efficient” is a technical term which in context means “polynomial time”—unless that’s how Judy would have understood the statement to start with.\n\nThis allows Judy to arbitrate disagreements that are too complex for her to evaluate in their entirety, by showing her what she “would have believed” about a number of intermediate claims, if she had bothered to check. Each of these intermediate claims might itself be too complicated for Judy to evaluate directly—if Judy needed to evaluate it, she would use the same trick again.\n\nBetting with attention\n\nIf Alice and Bob are betting about many claims over the course of a long argument, we can replace dollars by “attention points,” which represent Judy’s time thinking about the argument (perhaps 1 attention point = 1 minute of Judy’s time). Judy considers an arbitrage opportunity “good enough” if the profit is more than the time required to evaluate the argument. The initial allocation of attention points reflects the total amount of time Judy is willing to spend thinking about the issue. If someone runs out of attention points, then they can no longer make any claims or use up any of Judy’s time.\n\nThis removes some of the problems of using dollars, and introduces a new set of problems. The modified system works best when the total stock of attention points is large compared to the number at stake for each claim. Intuitively, if there are N comparable claims to wager about, the stakes of each should not be more than a 1/sqrt(N) of the total attention pool — or else random chance will be too large a factor. This requirement still allows a large gap between the time actually required to evaluate an argument (i.e. the initial bankroll of attention points) and the total time that would have been required to evaluate all of the claims made in the argument (the total stake of all of the bets). If each claim is itself supported by a recursive argument, this gap can grow exponentially.\n\nTalking it out\n\nIf Alice and Bob disagree about a claim (rather, if they disagree about Judy’s probability of accepting the claim) then they can have an incentive to “talk it out” rather than bringing the dispute to Judy.\n\nFor example, suppose that Alice and Bob each think they have a 60% chance of winning an argument. If they bring in Judy to arbitrate, both of them will get unfavorable odds. Because the surplus from the disagreement is going to Judy, both parties would be happy enough to see their counterparty wise up (and of course both would be happy to wise up themselves). This creates room for positive sum trades.\n\nRather than bringing in Judy to arbitrate their disagreement, they could do further research, consult an expert, pay Judy attention points to hear her opinion on a key issue, talk to Judy’s friends—whatever is the most cost-effective way to resolve the disagreement. Once they have this information, their betting odds can reflect it.\n\nAn example\n\nSuppose that Alice and Bob are arguing about how many trees are in North America; both are experts on the topic, but Judy knows nothing about it.\n\nThe easiest case is if Alice and Bob know all of the relevant facts, but one of them wants to mislead Judy. In this case, the truth will quickly prevail. Alice and Bob can begin by breaking down the issue into “How many trees are in each of Canada, the US, and Mexico?” If Alice or Bob lie about any of these estimates, they will quickly be corrected. Neither should be willing to bet much for a lie, but if they do, the same thing will happen recursively — the question will be broken down into “how many trees are east and west of the Mississippi?” and so on, until they disagree about how many trees are on a particular hill—a straightforward disagreement to resolve.\n\nIn reality, Alice and Bob will have different information about each of these estimates (and geography probably won’t be the easiest way to break things down — instead they might combine the different considerations that inform their views, the best guess suggested by different methodologies, approximate counts of each type of tree on each type of land, and so on). If Alice and Bob can reach a rational consensus on a given estimate, then Judy can use that consensus to inform her own view. If Alice and Bob can’t resolve their disagreement, then we’re back to the previous case. The only difference is that now Alice and Bob have probabilistic disagreements: if Alice disagrees with Bob she doesn’t expect to win the ensuing argument with 100% probability, merely with a high probability.\n\nOdds and ends\n\nThis writeup leaves many details underspecified. In particular, how does Judy estimate how long it will take her to arbitrate a disagreement? This can be handled in several ways: by having Judy guess, by having Alice and Bob bet on the length of time until Judy reaches a conclusion, by having them make bets of the form “Alice will agree with me with Z effort,” or so on. I don’t know what would work best.\n\nDespite my use of the word “recursion,” the estimate for “time to settle an argument” (which Judy uses to decide when the stakes are high enough to step in and resolve a disagreement) probably shouldn’t include the time required to settle sub-arguments, since Judy is being paid separately for arbitrating each of those. The structure of the arguments and sub-arguments need not be a tree.\n\nThis is a simple enough proposal that it can be realistically implemented, so eventually we’ll hopefully see how it works and why it fails.\n\nI expect this will work best if Alice and Bob often argue about similar topics.\n\nThis scheme was motivated by a particular exotic application: delegating decision-making to very intelligent machines. In that setting the goal is to scale to very complex disagreements, with very intelligent arguers, while being very efficient with the overseer’s time (and more cavalier with the arguers’ time).\n\n![](https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ee16a0e84cf7)\n\n---\n\n[Of arguments and wagers](https://ai-alignment.com/of-arguments-and-wagers-ee16a0e84cf7) was originally published in [AI Alignment](https://ai-alignment.com) on Medium, where people are continuing the conversation by highlighting and responding to this story.", "url": "https://www.alignmentforum.org/posts/aPsdGPCpcyPqkatgc/of-arguments-and-wagers", "date_published": "2020-01-10T22:20:02Z", "authors": ["paulfchristiano"], "tags": ["Betting"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.696770+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "3b943f80b0c1f994cc3a22432fce8940", "source": "alignmentforum", "title": "Malign generalization without internal search", "text": "In [my last post](https://www.alignmentforum.org/posts/nFDXq7HTv9Xugcqaw/is-the-term-mesa-optimizer-too-narrow), I challenged the idea that inner alignment failures should be explained by appealing to agents which perform explicit internal search. By doing so, I argued that we should instead appeal to the more general concept of *malign generalization*, and treat mesa-misalignment as a special case. \n\nUnfortunately, the post was light on examples of what we should be worrying about instead of mesa-misalignment. Evan Hubinger wrote,\n\n\n> Personally, I think there is a meaningful sense in which all the models I'm most worried about do some sort of search internally (at least to the same extent that humans do search internally), but I'm definitely uncertain about that.\n\nWei Dai expressed confusion why I would want to retreat to malign generalization without some sort of concrete failure mode in mind,\n\n\n> Can you give some realistic examples/scenarios of “malign generalization” that does not involve mesa optimization? I’m not sure what kind of thing you’re actually worried about here.\n\nIn this post, I will outline a general category of agents which may exhibit malign generalization without internal search, and then will provide a concrete example of an agent in the category. Then I will argue that, rather than being a very narrow counterexample, this class of agents could be competitive with search-based agents. \n\n**The switch case agent**\n-------------------------\n\nConsider an agent governed by the following general behavior, \n\n.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\nLOOP:State = GetStateOfWorld(Observation)IF State == 1:PerformActionSequence1()IF State == 2:PerformActionSequence2()...END\\_LOOP \n\nIt's clear that this agent does not perform any internal search for strategies: it doesn't operate by choosing actions which rank highly according to some sort of internal objective function. While you *could* potentially rationalize its behavior according to some observed-utility function, this would generally lead to more confusion than clarity.\n\nHowever, this agent could still be malign in the following way. Suppose the agent is 'mistaken' about the state of the world. Say that it believes that the state of the world is 1, whereas the actual state of the world is 2. Then it could take the wrong action, almost like a person who is confident in a falsehood and makes catastrophic mistakes because of their error.\n\nTo see how this could manifest as bad behavior in our artificial agents, I will use a motivating example.\n\n**The red-seeking lunar lander**\n--------------------------------\n\nSuppose we train a deep reinforcement learning agent on the lunar lander environment from OpenAI's Gym. \n\n![](https://miro.medium.com/max/1200/0*OeoznZ1fcM56p0Jm.gif)We make one crucial modification to our environment. During training, we make it so the landing pad is always painted red, and this is given to the agent as part of its observation of the world. We still reward the agent like normally for successfully landing in a landing pad.\n\n![](https://i.imgur.com/pVFu3dL.png)Suppose what really determines whether a patch of ground is a landing pad is whether it is enclosed by two flags. Nevertheless, instead of picking up on the true indicator of whether something is a landing pad, the agent may instead pick up the proxy that held during training -- namely, that landing pads are parts of the ground that are painted red.\n\nUsing the psuedocode earlier and filling in some details, we could describe the agent's behavior something like this. LOOP:State = GetStateOfWorld(Observation)IF State == RedIsToTheLeft:ApplyLeftThruster(45%)ApplyRightThruster(50%)IF State == RedIsToTheRight:ApplyLeftThruster(50%)ApplyRightThruster(45%)IF State == RedIsDirectlyBelow:ApplyLeftThruster(35%)ApplyRightThruster(35%)END\\_LOOP \n\nDuring deployment, this could end catastrophically. Assume that some crater is painted red but our landing pads is painted blue. Now, the agent will guide itself competently towards the crater and miss the real landing pad entirely. That's not what we wanted.\n\n(ETA: If you think I'm using the term 'catastrophically' too loosely here, since the agent actually lands safely in a crater rather than crashing into the ground, we could instead imagine a lunar vehicle which veers off into the red crater rather than just sitting still and awaiting further instruction since it's confused.)\n\n**What made the agent become malign**\n-------------------------------------\n\nAbove, I pointed to the reason why agents like ours could be malign. Specifically, it was 'mistaken' about what counted as a landing pad. However, it's worth noting that saying the agent is mistaken about the state of the world is really an anthropomorphization. It was actually perfectly correct in inferring where the red part of the world was -- we just didn't want it to go to that part of the world. We model the agent as being 'mistaken' about where the landing pad is, but it works equally well to model the agent as having goals that are counter to ours.\n\nSince the malign failure doesn't come from a pure epistemic error, we can't merely expect that the agent will self-correct as it gains more knowledge about the world. Saying that it is making an epistemic mistake is just a model of what's going on that helps us interpret its behavior, and it does not imply that this error is benign.\n\n**Imagining more complex agents**\n---------------------------------\n\nBut what's to worry about if this sort of thing only happens in very simple agents? Perhaps you think that only agents which perform internal search could ever reach the level of competence required to perform a real-world catastrophe?\n\nI think that these concerns about my example are valid, but I don't believe they are compelling. As a reply, I think the general agent superstructure I outlined in the initial pseudocode could reach very high levels of competence.\n\nConsider an agent that could, during its operation, call upon a vast array of subroutines. Some of these subroutines can accomplish extremely complicated actions, such as \"Prove this theorem: [...]\" or \"Compute the fastest route to Paris.\" We then imagine that this agent still shares the basic superstructure of the pseudocode I gave initially above. In effect, the agent has an outer loop, during which it takes in observations from the real world, and outputs action sequences depending on which state of the world it thinks its in, and using the subroutines it has available.\n\nSince the subroutines are arbitrarily complex, I don't think there is any fundamental barrier for this agent to achieve high levels of competence in the real world. Moreover, some subroutines could themselves perform powerful internal searches, pretty clearly obviating the competitive advantage that explicit search agents offer.\n\nAnd even while some subroutines could perform powerful internal searches, these subroutines aren't the only source of our malign generalization concern. The behavior of the agent is still well-described as a switch-case agent, and this means that the failure mode of the agent being 'mistaken' about the state of the world remains. Therefore, it's inaccurate to say that the source of malign generalization *must* come from an internal search being misaligned with the objective function we used during training.", "url": "https://www.alignmentforum.org/posts/ynt9TD6PrYw6iT49m/malign-generalization-without-internal-search", "date_published": "2020-01-12T18:03:43Z", "authors": ["Matthew Barnett"], "tags": ["Inner Alignment", "AI"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.697145+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "e62549b14685db3321871fe505fa8813", "source": "alignmentforum", "title": "Update on Ought's experiments on factored evaluation of arguments", "text": "[Ought](https://ought.org/) has written a detailed update and analysis of recent experiments on factored cognition. These are experiments with human participants and don’t involve any machine learning. The goal is to learn about the viability of [IDA](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd?_ga=2.44167719.2055190071.1578702594-1142780176.1552454685), [Debate](https://openai.com/blog/debate/), and related [approaches](https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84) to AI alignment. For background, here are some prior LW posts on Ought: [Ought: Why it Matters and How to Help](https://www.lesswrong.com/posts/cpewqG3MjnKJpCr7E/ought-why-it-matters-and-ways-to-help)), [Factored Cognition presentation](https://www.lesswrong.com/posts/DFkGStzvj3jgXibFG/factored-cognition).\n\nHere is the opening of the research update:\n\n\n> **Evaluating Arguments One Step at a Time** \n\n\n> We’re studying [factored cognition](https://ought.org/research/factored-cognition): under what conditions can a group of people accomplish complex cognitive tasks if each person only has minimal context?\n\n\n> In a recent experiment, we focused on dividing up the task of evaluating arguments. We created short, structured arguments for claims about movie reviews. We then tried to distinguish valid from invalid arguments by showing each participant only one step of the argument, not the review or the other steps.\n\n\n> In this experiment, we found that:\n\n\n> 1. Factored evaluation of arguments can distinguish some valid from invalid arguments by identifying implausible steps in arguments for false claims.\n\n\n> 2. However, experiment participants disagreed a lot about whether steps were valid or invalid. This method is therefore brittle in its current form, even for arguments which only have 1–5 steps.\n\n\n> 3. More diverse argument and evidence types (besides direct quotes from the text), larger trees, and different participant guidelines should improve results.\n\n\n> In this technical progress update, we describe these findings in depth.\n\nThe rest of the post is [here](https://ought.org/updates/2020-01-11-arguments).", "url": "https://www.alignmentforum.org/posts/pH3eKEAEupx8c2ep9/update-on-ought-s-experiments-on-factored-evaluation-of", "date_published": "2020-01-12T21:20:42Z", "authors": ["Owain_Evans"], "tags": ["Factored Cognition", "Ought"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.697402+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "ae5c0c8af2c74f2abfcfda9cb2fc8b58", "source": "alignmentforum", "title": "[AN #82]: How OpenAI Five distributed their training computation", "text": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.\n\nAudio version [here](http://alignment-newsletter.libsyn.com/alignment-newsletter-82) (may not be up yet).\n\n**Highlights**\n--------------\n\n[Dota 2 with Large Scale Deep Reinforcement Learning](http://arxiv.org/abs/1912.06680) *(OpenAI et al)* (summarized by Nicholas): In April, [OpenAI Five](https://openai.com/blog/how-to-train-your-openai-five/) ([AN #54](https://mailchi.mp/3e2f43012b07/an-54-boxing-a-finite-horizon-ai-system-to-keep-it-unambitious)) defeated the world champion Dota 2 team, OG. This paper describes its training process. OpenAI et al. hand-engineered the reward function as well as some features, actions, and parts of the policy. The rest of the policy was trained using PPO with an LSTM architecture at a massive scale. They trained this in a distributed fashion as follows:\n\n- The *Controller* receives and distributes the updated parameters.\n\n- The *Rollout Worker CPUs* simulate the game, send observations to the *Forward Pass GPUs* and publish samples to the *Experience Buffer*.\n\n- The *Forward Pass GPUs* determine the actions to use and send them to the *Rollout Workers*.\n\n- The *Optimizer GPUs* sample experience from the *Experience Buffer*, calculate gradient updates, and then publish updated parameters to the *Controller*.\n\nThe model trained over 296 days. In that time, OpenAI needed to adapt it to changes in the code and game mechanics. This was done via model “surgery”, in which they would try to initialize a new model to maintain the same input-output mapping as the old one. When this was not possible, they gradually increased the proportion of games played with the new version over time.\n\n**Nicholas's opinion:** I feel similarly to my opinion on [AlphaStar](https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning) ([AN #73](https://mailchi.mp/ef55eb52b0fd/an-73-detecting-catastrophic-failures-by-learning-how-agents-tend-to-break)) here. The result is definitely impressive and a major step up in complexity from shorter, discrete games like chess or go. However, I don’t see how the approach of just running PPO at a large scale brings us closer to AGI because we can’t run massively parallel simulations of real world tasks. Even for tasks that can be simulated, this seems prohibitively expensive for most use cases (I couldn’t find the exact costs, but I’d estimate this model cost tens of millions of dollars). I’d be quite excited to see an example of deep RL being used for a complex real world task without training in simulation.\n\n**Technical AI alignment**\n==========================\n\n### **Technical agendas and prioritization**\n\n[Just Imitate Humans?](https://www.alignmentforum.org/posts/LTFaD96D9kWuTibWr/just-imitate-humans) *(Michael Cohen)* (summarized by Rohin): This post asks whether it is safe to build AI systems that just imitate humans. The comments have a lot of interesting debate.\n\n### **Agent foundations**\n\n[Conceptual Problems with UDT and Policy Selection](https://www.alignmentforum.org/posts/9sYzoRnmqmxZm4Whf/conceptual-problems-with-udt-and-policy-selection) *(Abram Demski)* (summarized by Rohin): In Updateless Decision Theory (UDT), the agent decides \"at the beginning of time\" exactly how it will respond to every possible sequence of observations it could face, so as to maximize the expected value it gets with respect to its prior over how the world evolves. It is updateless because it decides ahead of time how it will respond to evidence, rather than updating once it sees the evidence. This works well when the agent can consider the full environment and react to it, and often gets the right result even when the environment can model the agent (as in Newcomblike problems), as long as the agent knows how the environment will model it.\n\nHowever, it seems unlikely that UDT will generalize to logical uncertainty and multiagent settings. Logical uncertainty occurs when you haven't computed all the consequences of your actions and is reduced by thinking longer. However, this effectively is a form of updating, whereas UDT tries to know everything upfront and never update, and so it seems hard to make it compatible with logical uncertainty. With multiagent scenarios, the issue is that UDT wants to decide on its policy \"before\" any other policies, which may not always be possible, e.g. if another agent is also using UDT. The philosophy behind UDT is to figure out how you will respond to everything ahead of time; as a result, UDT aims to precommit to strategies assuming that other agents will respond to its commitments; so two UDT agents are effectively \"racing\" to make their commitments as fast as possible, reducing the time taken to consider those commitments as much as possible. This seems like a bad recipe if we want UDT agents to work well with each other.\n\n**Rohin's opinion:** I am no expert in decision theory, but these objections seem quite strong and convincing to me.\n\n[A Critique of Functional Decision Theory](https://www.alignmentforum.org/posts/ySLYSsNeFL5CoAQzN/a-critique-of-functional-decision-theory) *(Will MacAskill)* (summarized by Rohin): *This summary is more editorialized than most.* This post critiques [Functional Decision Theory](https://arxiv.org/abs/1710.05060) (FDT). I'm not going to go into detail, but I think the arguments basically fall into two camps. First, there are situations in which there is no uncertainty about the consequences of actions, and yet FDT chooses actions that do not have the highest utility, because of their impact on counterfactual worlds which \"could have happened\" (but ultimately, the agent is just leaving utility on the table). Second, FDT relies on the ability to tell when someone is \"running an algorithm that is similar to you\", or is \"logically correlated with you\". But there's no such crisp concept, and this leads to all sorts of problems with FDT as a decision theory.\n\n**Rohin's opinion:** Like [Buck from MIRI](https://forum.effectivealtruism.org/posts/tDk57GhrdK54TWzPY/i-m-buck-shlegeris-i-do-research-and-outreach-at-miri-ama#iX6knDPMXZb696tDc), I feel like I understand these objections and disagree with them. On the first argument, I agree with [Abram](https://www.alignmentforum.org/posts/ySLYSsNeFL5CoAQzN/a-critique-of-functional-decision-theory#y8zRwcpNeu2ZhM3yE) that a decision should be evaluated based on how well the agent performs with respect to the probability distribution used to define the problem; FDT only performs badly if you evaluate on a decision problem produced by conditioning on a highly improbable event. On the second class of arguments, I certainly agree that there isn't (yet) a crisp concept for \"logical similarity\"; however, I would be shocked if the *intuitive concept* of logical similarity was not relevant in the general way that FDT suggests. If your goal is to hardcode FDT into an AI agent, or your goal is to write down a decision theory that in principle (e.g. with infinite computation) defines the correct action, then it's certainly a problem that we have no crisp definition yet. However, FDT can still be useful for getting more clarity on how one ought to reason, without providing a full definition.\n\n### **Learning human intent**\n\n[Learning to Imitate Human Demonstrations via CycleGAN](https://bair.berkeley.edu/blog/2019/12/13/humans-cyclegan/) *(Laura Smith et al)* (summarized by Zach): Most methods for imitation learning, where robots learn from a demonstration, assume that the actions of the demonstrator and robot are the same. This means that expensive techniques such as teleoperation have to be used to generate demonstrations. **This paper presents a method to engage in automated visual instruction-following with demonstrations (AVID) that works by translating video demonstrations done by a human into demonstrations done by a robot.** To do this, the authors use [CycleGAN](https://junyanz.github.io/CycleGAN/), a method to translate an image from one domain to another domain using unpaired images as training data. CycleGAN allows them to translate videos of humans performing the task into videos of the robot performing the task, which the robot can then imitate. In order to make learning tractable, the demonstrations had to be divided up into 'key stages' so that the robot can learn a sequence of more manageable tasks. In this setup, the robot only needs supervision to ensure that it's copying each stage properly before moving on to the next one. To test the method, the authors have the robot retrieve a coffee cup and make coffee. AVID significantly outperforms other imitation learning methods and can achieve 70% / 80% success rate on the tasks, respectively.\n\n**Zach's opinion:** In general, I like the idea of 'translating' demonstrations from one domain into another. It's worth noting that there do exist methods for translating visual demonstrations into latent policies. I'm a bit surprised that we didn't see any comparisons with other adversarial methods like [GAIfO](https://arxiv.org/pdf/1807.06158.pdf), but I understand that those methods have high sample complexity so perhaps the methods weren't useful in this context. It's also important to note that these other methods would still require demonstration translation. Another criticism is that AVID is not fully autonomous since it relies on human feedback to progress between stages. However, compared to kinetic teaching or teleoperation, sparse feedback from a human overseer is a minor inconvenience.\n\n**Read more:** [Paper: AVID: Learning Multi-Stage Tasks via Pixel-Level Translation of Human Videos](https://arxiv.org/abs/1912.04443)\n\n### **Preventing bad behavior**\n\n[When Goodharting is optimal: linear vs diminishing returns, unlikely vs likely, and other factors](https://www.alignmentforum.org/posts/megKzKKsoecdYqwb7/when-goodharting-is-optimal-linear-vs-diminishing-returns) *(Stuart Armstrong)* (summarized by Flo): Suppose we were uncertain about which arm in a bandit provides reward (and we don’t get to observe the rewards after choosing an arm). Then, maximizing expected value under this uncertainty is equivalent to picking the most likely reward function as a proxy reward and optimizing that; Goodhart’s law doesn’t apply and is thus not universal. This means that our fear of Goodhart effects is actually informed by more specific intuitions about the structure of our preferences. If there are actions that contribute to multiple possible rewards, optimizing the most likely reward does not need to maximize the expected reward. Even if we optimize for that, we have a problem if value is complex and the way we do reward learning implicitly penalizes complexity. Another problem arises if the correct reward is comparatively difficult to optimize: if we want to maximize the average, it can make sense to only care about rewards that are both likely and easy to optimize. Relatedly, we could fail to correctly account for diminishing marginal returns in some of the rewards.\n\nGoodhart effects are a lot less problematic if we can deal with all of the mentioned factors. Independent of that, Goodhart effects are most problematic when there is little middle ground that all rewards can agree on.\n\n**Flo's opinion:** I enjoyed this article and the proposed factors match my intuitions. There seem to be two types of problems: extreme beliefs and concave Pareto boundaries. Dealing with the second is more important since a concave Pareto boundary favours extreme policies, even for moderate beliefs. Luckily, diminishing returns can be used to bend the Pareto boundary. However, I expect it to be hard to find the correct rate of diminishing returns, especially in novel situations.\n\n**Rohin's opinion:** Note that this post considers the setting where we have uncertainty over the true reward function, but *we can't learn about the true reward function*. If you can gather information about the true reward function, which [seems necessary to me](https://www.alignmentforum.org/posts/4783ufKpx8xvLMPc6/human-ai-interaction) ([AN #41](https://mailchi.mp/8c3f02cabccd/alignment-newsletter-41)), then it is almost always worse to take the most likely reward or expected reward as a proxy reward to optimize.\n\n### **Robustness**\n\n[AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty](https://openreview.net/pdf?id=S1gmrxHFvB) *(Dan Hendrycks, Norman Mu et al)* (summarized by Dan H): This paper introduces a data augmentation technique to improve robustness and uncertainty estimates. The idea is to take various random augmentations such as random rotations, produce several augmented versions of an image with compositions of random augmentations, and then pool the augmented images into a single image by way of an elementwise convex combination. Said another way, the image is augmented with various traditional augmentations, and these augmented images are “averaged” together. This produces highly diverse augmentations that have similarity to the original image. Unlike techniques such as AutoAugment, this augmentation technique uses typical resources, not 15,000 GPU hours. It also greatly improves generalization to unforeseen corruptions, and it makes models more stable under small perturbations. Most importantly, even as the distribution shifts and accuracy decreases, this technique produces models that can [remain calibrated under distributional shift](https://openreview.net/pdf?id=S1gmrxHFvB#page=8&zoom=100,144,298).\n\n### **Miscellaneous (Alignment)**\n\n[Defining and Unpacking Transformative AI](https://arxiv.org/abs/1912.00747) *(Ross Gruetzemacher et al)* (summarized by Flo): The notion of **transformative AI** (TAI) is used to highlight that even narrow AI systems can have large impacts on society. This paper offers a clearer definition of TAI and distinguishes it from **radical transformative AI** (RTAI).\n\n\"Discontinuities or other anomalous patterns in metrics of human progress, as well as *irreversibility* are common indicators of transformative change. TAI is then broadly defined as an AI technology, which leads to an irreversible change of some important aspects of society, making it a (multi-dimensional) spectrum along the axes of **extremity**, **generality** and **fundamentality**. \" For example, advanced AI weapon systems might have strong implications for great power conflicts but limited effects on people's daily lives; extreme change of limited generality, similar to nuclear weapons. There are two levels: while TAI is comparable to general-purpose technologies (GPTs) like the internal combustion engine, RTAI leads to changes that are comparable to the agricultural or industrial revolution. Both revolutions have been driven by GPTs like the domestication of plants and the steam engine. Similarly, we will likely see TAI before RTAI. The scenario where we don't is termed a **radical shift**.\n\nNon-radical TAI could still contribute to existential risk in conjunction with other factors. Furthermore, if TAI precedes RTAI, our management of TAI can affect the risks RTAI will pose.\n\n**Flo's opinion:** Focusing on the impacts on society instead of specific features of AI systems makes sense and I do believe that the shape of RTAI as well as the risks it poses will depend on the way we handle TAI at various levels. More precise terminology can also help to prevent misunderstandings, for example between people forecasting AI and decision maker.\n\n[Six AI Risk/Strategy Ideas](https://www.alignmentforum.org/posts/dt4z82hpvvPFTDTfZ/six-ai-risk-strategy-ideas) *(Wei Dai)* (summarized by Rohin): This post briefly presents three ways that power can become centralized in a world with [Comprehensive AI Services](https://www.fhi.ox.ac.uk/reframing/) ([AN #40](https://mailchi.mp/b649f32b07da/alignment-newsletter-40)), argues that under risk aversion \"logical\" risks can be more concerning than physical risks because they are more correlated, proposes combining human imitations and oracles to remove the human in the loop and become competitive, and suggests doing research to generate evidence of difficulty of a particular strand of research.", "url": "https://www.alignmentforum.org/posts/6tikKda9LBzrkLfBJ/an-82-how-openai-five-distributed-their-training-computation", "date_published": "2020-01-15T18:20:01Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.698356+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "af9e73f2774260efb862acb368e17582", "source": "alignmentforum", "title": "Inner alignment requires making assumptions about human values", "text": "Many approaches to AI alignment require making assumptions about what humans want. On a first pass, it might appear that inner alignment is a sub-component of AI alignment that doesn't require making these assumptions. This is because if we define the problem of inner alignment to be the problem of how to train an AI to be aligned with arbitrary reward functions, then a solution would presumably have no dependence on any particular reward function. We could imagine an alien civilization solving the same problem, despite using very different reward functions to train their AIs.\n\nUnfortunately, the above argument fails because aligning an AI with our values requires giving the AI extra information that is not encoded directly in the reward function (under reasonable assumptions). The argument for my thesis is subtle, and so I will break it into pieces.\n\nFirst, I will more fully elaborate what I mean by inner alignment. Then I will argue that the definition implies that we can't come up with a full solution without some dependence on human values. Finally, I will provide an example, in order to make this discussion less abstract.\n\nCharacterizing inner alignment\n------------------------------\n\nIn the last few posts I wrote ([1](https://www.lesswrong.com/posts/nFDXq7HTv9Xugcqaw/is-the-term-mesa-optimizer-too-narrow), [2](https://www.lesswrong.com/posts/ynt9TD6PrYw6iT49m/malign-generalization-without-internal-search)), I attempted to frame the problem of inner alignment in a way that wasn't too theory-laden. My concern was that the [previous characterization](https://www.lesswrong.com/posts/pL56xPoniLvtMDQ4J/the-inner-alignment-problem) was dependent on a solving particular outcome where you have an AI that is using an explicit outer loop to evaluate strategies based on an explicit internal search.\n\nIn the absence of an explicit internal objective function, it is difficult to formally define whether an agent is \"aligned\" with the reward function that is used to train it. We might therefore define alignment as the ability of our agent to perform well on the test distribution. However, if the test set is sampled from the same distribution as the training data, this definition is equivalent to the performance of a model in standard machine learning, and we haven't actually defined the problem in a way that adds clarity.\n\nWhat we really care about is whether our agent performs well on a test distribution that doesn't match the training environment. In particular, we care about the agent's performance on during real-world deployment. We can estimate this real world performance ahead of time by giving the agent a test distribution that was artificially selected to emphasize important aspects of the real world more closely than the training distribution (eg. by using [relaxed adversarial training](https://www.lesswrong.com/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment)).\n\nTo distinguish the typical robustness problem from inner alignment, we evaluate the agent on this testing distribution by observing its behaviors and evaluating it very negatively if it does something catastrophic (defined as something so bad we'd prefer it to fail completely). This information is used to iterate on future versions of the agent. An inner aligned agent is therefore defined as an agent that avoids catastrophes during testing.\n\nThe reward function doesn't provide enough information\n------------------------------------------------------\n\nSince reward functions are defined as mappings between state-action pairs and a real number, our agent doesn't actually have enough information from the reward function alone to infer what good performance means on the test. This is because the test distribution contains states that were not available in the training distribution.\n\nTherefore, no matter how much the agent learns about the true reward function during training, it must perform some implicit extrapolation of the reward function to what we intended, in order to perform well on the test we gave it.\n\nWe can visualize this extrapolation as if we were asking a supervised learner what it predicts for inputs beyond the range it was provided in its training set. It will be forced to make some guesses for what rule determines what the function looks like outside of its normal range.\n\n![](https://media1.shmoop.com/images/algebra-ii/alg2_ch2_narr_graphik_4.png)One might assume that we could just use simplicity as the criterion for extrapolation. Perhaps we could just say, formally, the simplest possible reward function that encodes the values observed during training is the \"true reward\" function that we will use to test the agent. Then the problem of inner alignment reduces to the problem of creating an agent that is able to infer the true reward function from data, and then perform well according to it inside general environments. Framing the problem like this would minimize dependence on human values.\n\nThere are a number of problems with that framing, however. To start, there are boring problems associated with using simplicity to extrapolate the reward function, such as the fact that one's notion of simplicity is language dependent, often uncomputable, and [the universal prior is malign](https://www.lesswrong.com/posts/5bd75cc58225bf06703752a3/the-universal-prior-is-malign). Beyond these (arguably minor) issues, there's a deeper issue, which forces us to make assumptions about human values in order to ensure inner alignment.\n\nSince we assumed that the training environment was necessarily different from the testing environment, we cannot possibly provide the agent information about every possible scenario we consider catastrophic during training. Therefore, the metric we were using to judge the success of the agent during testing is not captured in training data alone. We must introduce additional information about what we consider catastrophic. This information comes in the form of our own preferences, as we prefer the agent to fail in some ways but not in others..mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\n[1]\n\nIt's also important to note that if we actually did provide the agent with the exact same data during training as it would experience during deployment, this is equivalent to simply letting the agent learn in the real world, and there would be no difference between training and testing. Since we normally assume providing such a perfect environment is either impossible or unsafe, the considerations in that case become quite different.\n\nAn example\n----------\n\nI worry my discussion was a bit too abstract to be useful, so I'll provide a specific example to show where my thinking lies. Consider the lunar lander example that I provided in the [last post](https://www.lesswrong.com/posts/ynt9TD6PrYw6iT49m/malign-generalization-without-internal-search).\n\nTo reiterate, we train an agent to land on a landing pad, but during training there is a perfect correlation between whether a landing pad is painted red and whether it is a real landing pad.\n\n![](https://i.imgur.com/pVFu3dL.png)During deployment, if the \"true\" factor that determined whether a patch of ground is a landing pad was whether it is enclosed by flags, and some faraway crater is painted red, then the agent might veer off into the crater rather than landing on the landing pad.\n\nSince there is literally not enough information during training to infer what property correctly determines whether a patch of ground is a landing pad, the agent is forced to infer whether its the flags or the red painting. It's not exactly clear what the \"simplest\" inference is here, but it's coherent to imagine that \"red painting determines whether something is a landing pad\" is the simplest inference.\n\nAs humans, we might have a preference for the flags being the true determinant, since that resonates more with what we think a landing pad should be, and whether something is painted red is not nearly as compelling to us.\n\nThe important point is to notice that our judgement here is determined by our preferences, and not something the agent could have learned during training using some value-neutral inferences. The agent *must* make further assumptions about human preferences for it to consistently perform well during testing.\n\n\n\n---\n\n1. You might wonder whether we could define catastrophe in a completely value-independent way, sidestepping this whole issue. This is the approach implicitly assumed by [impact measures](https://www.lesswrong.com/posts/pf48kg9xCxJAcHmQc/understanding-recent-impact-measures). However, if we want to avoid all types of situations where we'd prefer the system fail completely, I think this will require a different notion of catastrophe than \"something with a large impact.\" Furthermore, we would not want to penalize systems for having a large positive impact.", "url": "https://www.alignmentforum.org/posts/6m5qqkeBTrqQsegGi/inner-alignment-requires-making-assumptions-about-human", "date_published": "2020-01-20T18:38:27Z", "authors": ["Matthew Barnett"], "tags": ["Inner Alignment"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.698645+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "f29f96117ba41a267e070aaacfdec114", "source": "alignmentforum", "title": "[AN #83]: Sample-efficient deep learning with ReMixMatch", "text": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.\n\nAudio version [here](http://alignment-newsletter.libsyn.com/alignment-newsletter-83) (may not be up yet).\n\n**Highlights**\n--------------\n\n[ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring](https://arxiv.org/abs/1911.09785) *(David Berthelot et al)* (summarized by Dan H): A common criticism of deep learning is that it requires far too much training data. Some view this as a fundamental flaw that suggests we need a new approach. However, considerable data efficiency is possible with a new technique called ReMixMatch. ReMixMatch on CIFAR-10 obtains 84.92% accuracy using only 4 labeled examples per class. Using 250 labeled examples, or around 25 labeled examples per class, a ReMixMatch model on CIFAR-10 has 93.73% accuracy. This is approximately how well a vanilla ResNet does on CIFAR-10 with 50000 labeled examples. Two years ago, special techniques utilizing 250 CIFAR-10 labeled examples could enable an accuracy of approximately [53%](https://paperswithcode.com/sota/semi-supervised-image-classification-on-3). ReMixMatch builds on [MixMatch](https://arxiv.org/abs/1905.02249) and has several seemingly arbitrary design decisions, so I will refrain from describing its design. In short, deep networks do not necessarily require large labeled datasets.\n\nAnd just yesterday, after this summary was first written, the [FixMatch](https://arxiv.org/abs/2001.07685) paper got even better results.\n\n**Previous newsletters**\n========================\n\nIn last week's email, two of Flo's opinions were somehow scrambled together. See below for what they were supposed to be.\n\n[Defining and Unpacking Transformative AI](https://arxiv.org/abs/1912.00747) *(Ross Gruetzemacher et al)* (summarized by Flo): Focusing on the impacts on society instead of specific features of AI systems makes sense and I do believe that the shape of RTAI as well as the risks it poses will depend on the way we handle TAI at various levels. More precise terminology can also help to prevent misunderstandings, for example between people forecasting AI and decision makers.\n\n[When Goodharting is optimal: linear vs diminishing returns, unlikely vs likely, and other factors](https://www.alignmentforum.org/posts/megKzKKsoecdYqwb7/when-goodharting-is-optimal-linear-vs-diminishing-returns) *(Stuart Armstrong)* (summarized by Flo): I enjoyed this article and the proposed factors match my intuitions. There seem to be two types of problems: extreme beliefs and concave Pareto boundaries. Dealing with the second is more important since a concave Pareto boundary favours extreme policies, even for moderate beliefs. Luckily, diminishing returns can be used to bend the Pareto boundary. However, I expect it to be hard to find the correct rate of diminishing returns, especially in novel situations.\n\n**Technical AI alignment**\n==========================\n\n### **Iterated amplification**\n\n[AI Safety Debate and Its Applications](https://alignmentforum.org/posts/5Kv2qNfRyXXihNrx2/ai-safety-debate-and-its-applications) *(Vojta Kovarik)* (summarized by Rohin): This post defines the components of a [debate](https://blog.openai.com/debate/) ([AN #5](https://mailchi.mp/0ae5d69de63b/alignment-newsletter-5)) game, lists some of its applications, and defines truth-seeking as the property that we want. Assuming that the agent chooses randomly from the possible Nash equilibria, the truth-promoting likelihood is the probability that the agent picks the actually correct answer. The post then shows the results of experiments on MNIST and Fashion MNIST, seeing comparable results to the original paper.\n\n[(When) is Truth-telling Favored in AI debate?](https://medium.com/@RyanCarey/new-paper-when-is-truth-telling-favored-in-ai-debate-8f58f14562e5) *(Vojtěch Kovařík et al)* (summarized by Rohin): [Debate](https://blog.openai.com/debate/) ([AN #5](https://mailchi.mp/0ae5d69de63b/alignment-newsletter-5)) aims to train an AI system using self-play to win \"debates\" which aim to convincingly answer a question, as evaluated by a human judge. The main hope is that the equilibrium behavior of this game is for the AI systems to provide true, useful information. This paper studies this in a simple theoretical setting called *feature debates*. In this environment, a \"world\" is sampled from some distribution, and the agents (who have perfect information) are allowed to make claims about real-valued \"features\" of the world, in order to answer some question about the features of the world. The judge is allowed to check the value of a single feature before declaring a winner, but otherwise knows nothing about the world.\n\nIf either agent lies about the value of a feature, the other agent can point this out, which the judge can then check; so at the very least the agents are incentivized to honestly report the values of features. However, does this mean that they will try to answer the full question truthfully? If the debate has more rounds than there are features, then it certainly does: either agent can unilaterally reveal every feature, which uniquely determines the answer to the question. However, shorter debates need not lead to truthful answers. For example, if the question is whether the first K features are all 1, then if the debate length is shorter than K, there is no way for an agent to prove that the first K features are all 1.\n\n**Rohin's opinion:** While it is interesting to see what doesn't work with feature debates, I see two problems that make it hard to generalize these results to regular debate. First, I see debate as being truth-seeking in the sense that the answer you arrive at is (in expectation) more accurate than the answer the judge would have arrived at by themselves. However, this paper wants the answers to actually be *correct*. Thus, they claim that for sufficiently complicated questions, since the debate can't reach the right answer, the debate isn't truth-seeking -- but in these cases, the answer is still in expectation more accurate than the answer the judge would come up with by themselves.\n\nSecond, feature debate doesn't allow for decomposition of the question during the debate, and doesn't allow the agents to challenge each other on particular questions. I think this limits the \"expressive power\" of feature debate to P, while regular debate reaches PSPACE, and is thus able to do much more than feature debate. See this [comment](https://www.alignmentforum.org/posts/RQoSCs9SePDMLJvfz/new-paper-when-is-truth-telling-favored-in-ai-debate#gCeKuJ62HmLtPB9C9) for more details.\n\n**Read more:** [Paper: (When) Is Truth-telling Favored in AI Debate?](https://arxiv.org/abs/1911.04266)\n\n### **Mesa optimization**\n\n[Malign generalization without internal search](https://www.alignmentforum.org/posts/ynt9TD6PrYw6iT49m/malign-generalization-without-internal-search) *(Matthew Barnett)* (summarized by Rohin): This post argues that agents can have [capability generalization without objective generalization](https://www.alignmentforum.org/posts/2mhFMgtAjFJesaSYR/2-d-robustness) ([AN #66](https://mailchi.mp/c8ea4a5e842f/an-66-decomposing-robustness-into-capability-robustness-and-alignment-robustness)), *without* having an agent that does internal search in pursuit of a simple mesa objective. Consider an agent that learns different heuristics for different situations which it selects from using a switch statement. For example, in lunar lander, if at training time the landing pad is always red, the agent may learn a heuristic about which thrusters to apply based on the position of red ground relative to the lander. The post argues that this selection across heuristics could still happen with very complex agents (though the heuristics themselves may involve search).\n\n**Rohin's opinion:** I generally agree that you could get powerful agents that nonetheless are \"following heuristics\" rather than \"doing search\"; however, others with differing intuitions [did not find this post convincing](https://www.alignmentforum.org/posts/ynt9TD6PrYw6iT49m/malign-generalization-without-internal-search#F4CzHbopjRMLJXP3h).\n\n### **Agent foundations**\n\n[Embedded Agency via Abstraction](https://www.alignmentforum.org/posts/hLFD6qSN9MmQxKjG5/embedded-agency-via-abstraction) *(John S Wentworth)* (summarized by Asya): [Embedded agency problems](https://www.alignmentforum.org/posts/p7x32SEt43ZMC9r7r/embedded-agents) ([AN #31](https://mailchi.mp/7d0e3916e3d9/alignment-newsletter-31)) are a class of theoretical problems that arise as soon as an agent is part of the environment it is interacting with and modeling, rather than having a clearly-defined and separated relationship. This post makes the argument that before we can solve embedded agency problems, we first need to develop a theory of *abstraction*. *Abstraction* refers to the problem of throwing out some information about a system while still being able to make predictions about it. This problem can also be referred to as the problem of constructing a map for some territory.\n\nThe post argues that abstraction is key for embedded agency problems because the underlying challenge of embedded world models is that the agent (the map) is smaller than the environment it is modeling (the territory), and so inherently has to throw some information away.\n\nSome simple questions around abstraction that we might want to answer include:\n\n- Given a map-making process, characterize the queries whose answers the map can reliably predict.\n\n- Given some representation of the map-territory correspondence, translate queries from the territory-representation to the map-representation and vice versa.\n\n- Given a territory, characterize classes of queries which can be reliably answered using a map much smaller than the territory itself.\n\n- Given a territory and a class of queries, construct a map which throws out as much information as possible while still allowing accurate prediction over the query class.\n\nThe post argues that once we create the simple theory, we will have a natural way of looking at more challenging problems with embedded agency, like the problem of self-referential maps, the problem of other map-makers, and the problem of self-reasoning that arises when the produced map includes an abstraction of the map-making process itself.\n\n**Asya's opinion:** My impression is that embedded agency problems as a class of problems are very young, extremely entangled, and characterized by a lot of confusion. I am enthusiastic about attempts to decrease confusion and intuitively, abstraction does feel like a key component to doing that.\n\nThat being said, my guess is that it’s difficult to predictably suggest the most promising research directions in a space that’s so entangled. For example, [one thread in the comments of this post](https://www.alignmentforum.org/posts/hLFD6qSN9MmQxKjG5/embedded-agency-via-abstraction#qQwY6tyzSbEuGkrke) discusses the fact that this theory of abstraction as presented looks at “one-shot” agency where the system takes in some data once and then outputs it, rather than “dynamic” agency where a system takes in data and outputs decisions repeatedly over time. [Abram Demski argues](https://www.alignmentforum.org/posts/hLFD6qSN9MmQxKjG5/embedded-agency-via-abstraction#XenkPobqgAjonwios) that the “dynamic” nature of embedded agency is a [central part of the problem](https://www.alignmentforum.org/s/Rm6oQRJJmhGCcLvxh/p/iTpLAaPamcKyjmbFC) and that it may be more valuable and neglected to put research emphasis there.\n\n[Dissolving Confusion around Functional Decision Theory](https://www.lesswrong.com/posts/xoQRz8tBvsznMXTkt/dissolving-confusion-around-functional-decision-theory) *(Stephen Casper)* (summarized by Rohin): This post argues for functional decision theory (FDT) on the basis of the following two principles:\n\n1. Questions in decision theory are not about what \"choice\" you should make with your \"free will\", but about what source code you should be running.\n\n2. P \"subjunctively depends\" on A to the extent that P's predictions of A depend on correlations that can't be confounded by choosing the source code that A runs.\n\n**Rohin's opinion:** I liked these principles, especially the notion that subjunctive dependence should be cashed out as \"correlations that aren't destroyed by changing the source code\". This isn't a perfect criterion: FDT can and should apply to humans as well, but we *don't* have control over our source code.\n\n[Predictors exist: CDT going bonkers... forever](https://www.alignmentforum.org/posts/Kr76XzME7TFkN937z/predictors-exist-cdt-going-bonkers-forever) *(Stuart Armstrong)* (summarized by Rohin): Consider a setting in which an agent can play a game against a predictor. The agent can choose to say zero or one. It gets 3 utility if it says something different from the predictor, and -1 utility if it says the same thing. If the predictor is near-perfect, but the agent models its actions as independent of the predictor (since the prediction was made in the past), then the agent will have some belief about the prediction and will choose the less likely action for expected utility at least 1, and will continually lose.\n\n[ACDT: a hack-y acausal decision theory](https://www.alignmentforum.org/posts/9m2fzjNSJmd3yxxKG/acdt-a-hack-y-acausal-decision-theory) *(Stuart Armstrong)* (summarized by Rohin): The problem with the previous agent is that it never learns that it has the wrong causal model. If the agent is able to learn a better causal model from experience, then it can learn that the predictor can actually predict the agent successfully, and so will no longer expect a 50% chance of winning, and it will stop playing the game.\n\n### **Miscellaneous (Alignment)**\n\n[Clarifying The Malignity of the Universal Prior: The Lexical Update](https://www.lesswrong.com/posts/peebMuCuscjkNvTnE/clarifying-the-malignity-of-the-universal-prior-the-lexical) *(interstice)*\n\n**Other progress in AI**\n========================\n\n### **Reinforcement learning**\n\n[Reward-Conditioned Policies](http://arxiv.org/abs/1912.13465) *(Aviral Kumar et al)* (summarized by Nicholas): Standard RL algorithms create a policy that maximizes a reward function; the *Reward-Conditioned Policy* algorithm instead creates a policy that can achieve a particular reward value passed in as an input. This allows the policy to be trained via supervised regression on a dataset. Each example in the dataset consists of a state, action, and either a return or an advantage, referred to as *Z*. The network then predicts the action based on the state and *Z*. The learned model is able to generalize to policies for larger returns. During training, the target value is sampled from a distribution that gradually increases so that it continues to learn higher rewards.\n\nDuring evaluation, they then feed in the state and a high target value of *Z* (set one standard deviation above the average in their paper.) This enables them to achieve solid - but not state of the art - performance on a variety of the OpenAI Gym benchmark tasks. They also run ablation studies showing, among other things, that the policy is indeed accurate in achieving the target reward it aims for.\n\n**Nicholas's opinion:** One of the dangers of training powerful AI to maximize a reward function is that optimizing the function to extreme values may no longer correlate with what we want, as in the classic paperclip maximizer example. I think RCP provides an interesting solution to that problem; if we can instead specify a good, but reasonable, value, we may be able to avoid those extreme cases. We can then gradually increase the desired reward without retraining while continuously monitoring for issues. I think there are likely flaws in the above scheme, but I am optimistic in general about the potential of finding alternate ways to communicate goals to an agent.\n\nOne piece I am still curious about is whether the policy remembers how to achieve lower rewards as its training dataset updates towards higher rewards. They show in a heatmap that the target and actual rewards do match up well, but the target rewards are all sampled quite near each other; it would be interesting to see how well the final policy generalizes to the entire spectrum of target rewards.\n\n[Reinforcement Learning Upside Down: Don't Predict Rewards -- Just Map Them to Actions](http://arxiv.org/abs/1912.02875v1) and [Training Agents using Upside-Down Reinforcement Learning](http://arxiv.org/abs/1912.02877) *(Juergen Schmidhuber)* (summarized by Zach): It's a common understanding that using supervised learning to solve RL problems is challenging because supervised learning works directly with error signals while RL only has access to evaluation signals. The approach in these papers introduce 'upside-down' reinforcement learning (UDRL) as a way to bridge this gap. Instead of learning how to predict rewards, UDRL learns how to take actions when given a state and a desired reward. Then, to get good behavior, we simply ask the policy to take actions that lead to particularly high rewards. The main approach is to slowly increase the desired goal behavior as the agent learns in order to maximize agent performance. The authors evaluate UDRL on the Lunar Lander and the Take Cover environments. UDRL ultimately performs worse on Lunar Lander and better on Take Cover so it's unclear whether or not UDRL is an improvement over popular methods. However, when rewards are made to be sparse UDRL is able to significantly outperform other RL methods.\n\n**Zach's opinion:** This approach fits neatly with older work including [\"Learning to Reach Goals”](https://pdfs.semanticscholar.org/6df4/3f70f383007a946448122b75918e3a9d6682.pdf?_ga=2.78217424.59277899.1578587972-37524897.1574936748) and more recent work such as [Hindsight experience replay](https://arxiv.org/pdf/1707.01495.pdf) and [Goal-Conditioned Policies](https://arxiv.org/pdf/1912.06088.pdf). In particular, all of these methods seem to be effective at addressing the difficulty that comes with working with sparse rewards. I also found myself justifying the utility of selecting the objective of 'learning to achieve general goals' to be related to the idea that [seeking power is instrumentally convergent](https://www.alignmentforum.org/posts/6DuJxY8X45Sco4bS2/seeking-power-is-provably-instrumentally-convergent-in-mdps) ([AN #78](https://mailchi.mp/eef1d6c95d7c/an-78formalizing-power-and-instrumental-convergence-and-the-end-of-year-ai-safety-charity-comparison)).\n\n**Rohin's opinion:** Both this and the previous paper have explored the idea of conditioning on rewards and predicting actions, trained by supervised learning. While this doesn't hit state-of-the-art performance, it works reasonably well for a new approach.\n\n[Planning with Goal-Conditioned Policies](http://arxiv.org/abs/1911.08453) *(Soroush Nasiriany, Vitchyr H. Pong et al)* (summarized by Zach): Reinforcement learning can learn complex skills by interacting with the environment. However, temporally extended or long-range decision-making problems require more than just well-honed reactions. **In this paper, the authors investigate whether or not they can obtain the benefits of action planning found in model-based RL without the need to model the environment at the lowest level.** The authors propose a model-free planning framework that learns low-level goal-conditioned policies that use their value functions as implicit models. Goal-conditioned policies are policies that can be trained to reach a goal state provided as an additional input. Given a goal-conditioned policy, the agent can then plan over intermediate subgoals (goal states) using a goal-conditioned value function to estimate reachability. Since the state space is large, the authors propose what they call latent embeddings for abstracted planning (LEAP), which is able to find useful subgoals by first searching a much smaller latent representation space and then planning a sequence of reachable subgoals that reaches the target state. In experiments, LEAP significantly outperforms prior algorithms on 2D navigation and push/reach tasks. Moreover, their method can get a quadruped ant to navigate around walls which is difficult because much of the planning happens in configuration space. This shows that LEAP is able to be extended to non-visual domains.\n\n**Zach's opinion:** The presentation of the paper is clear. In particular, the idea of planning a sequence of maximally feasible subgoals seems particularly intuitive. In general, I think that LEAP relies on the clever idea of reusing trajectory data to augment the data-set for the goal-conditioned policy. As the authors noted, the question of exploration was mostly neglected. I wonder how well the idea of reusing trajectory data generalizes to the general exploration problem.\n\n**Rohin's opinion:** The general goal of inferring hierarchy and using this to plan more efficiently seems very compelling but hard to do well; this is the goal in most hierarchical RL algorithms and [Learning Latent Plans from Play](https://learning-from-play.github.io/) ([AN #65](https://mailchi.mp/3d4e6c2c206f/an-65learning-useful-skills-by-watching-humans-play)).\n\n[Dream to Control: Learning Behaviors by Latent Imagination](http://arxiv.org/abs/1912.01603) *(Danijar Hafner et al)* (summarized by Cody): In the past year or so, the idea of learning a transition model in a latent space has gained traction, motivated by the hope that such an approach could combine the best of the worlds of model-free and model-based learning. The central appeal of learning a latent transition model is that it allows you to imagine future trajectories in a potentially high-dimensional, structured observation space without actually having to generate those high-dimensional observations.\n\nDreamer builds on a prior model by the same authors, [PlaNet](http://arxiv.org/abs/1811.04551) ([AN #33](https://mailchi.mp/b6dc636f6a1b/alignment-newsletter-33)), which learned a latent representation of the observations, p(s|o), trained both through a VAE-style observation reconstruction loss, and also a transition model q(s-next|s, a), which is trained to predict the state at the next step given only the state at the prior one, with no next-step observation data. Together, these two models allow you to simulate action-conditioned trajectories through latent state space. If you then predict reward from state, you can use this to simulate the value of trajectories. Dreamer extends on this by also training an Actor Critic-style model on top of states to predict action and value, forcing the state representation to not only capture next-step transition information, but also information relevant to predicting future rewards. The authors claim this extension makes their model more able to solve long-horizon problems, because the predicted value function can capture far-future rewards without needing to simulate the entire way there. Empirically, there seems to be reasonable evidence that this claim plays out, at least within the fairly simple environments the model is tested in.\n\n**Cody's opinion:** The extension from PlaNet (adding actor-critic rather than direct single-step reward prediction) is relatively straightforward, but I think latent models are an interesting area - especially if they eventually become at all possible to interpret - and so I'm happy to see more work in this area.", "url": "https://www.alignmentforum.org/posts/ZrCsaCXrMTgrX9GzK/an-83-sample-efficient-deep-learning-with-remixmatch", "date_published": "2020-01-22T18:10:01Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.699379+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "95c18bf6745da9a2227bf21d24528cf5", "source": "alignmentforum", "title": "New paper: The Incentives that Shape Behaviour", "text": "Abstract: Which variables does an agent have an incentive to control with its decision, and which variables does it have an incentive to respond to? We formalise these incentives, and demonstrate unique graphical criteria for detecting them in any single decision causal influence diagram. To this end, we introduce structural causal influence models, a hybrid of the influence diagram and structural causal model frameworks. Finally, we illustrate how these incentives predict agent incentives in both fairness and AI safety applications.", "url": "https://www.alignmentforum.org/posts/TgPCet7m9DnkuxyKP/new-paper-the-incentives-that-shape-behaviour", "date_published": "2020-01-23T19:07:37Z", "authors": ["RyanCarey"], "tags": ["Causality", "Machine Learning  (ML)", "Mechanism Design", "Academic Papers"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.700241+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "aff57fe8a743c3d7dbfd902b5bddbcfa", "source": "alignmentforum", "title": "The two-layer model of human values, and problems with synthesizing preferences", "text": "I have been thinking about Stuart Armstrong's [preference synthesis research agenda](https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into), and have long had the feeling that there's something off about the way that it is currently framed. In the post I try to describe why. I start by describing my current model of human values, how I interpret Stuart's implicit assumptions to conflict with it, and then talk about my confusion with regard to reconciling the two views.\n\nThe two-layer/ULM model of human values\n=======================================\n\n[In Player vs. Character: A Two-Level Model of Ethics](https://www.lesswrong.com/posts/fyGEP4mrpyWEAfyqj/player-vs-character-a-two-level-model-of-ethics), Sarah Constantin describes a model where the mind is divided, in game terms, into a \"player\" and a \"character\". The character is everything that we consciously experience, but our conscious experiences are not our true reasons for acting. As Sarah puts it:\n\n\n>  In many games, such as Magic: The Gathering, Hearthstone, or Dungeons and Dragons, there’s a two-phase process. First, the player constructs a *deck* or *character* from a very large sample space of possibilities.  This is a particular combination of strengths and weaknesses and capabilities for action, which the player thinks can be successful against other decks/characters or at winning in the game universe.  The choice of deck or character often determines the strategies that deck or character can use in the second phase, which is actual gameplay.  In gameplay, the character (or deck) can only use the affordances that it’s been previously set up with.  This means that there are two separate places where a player needs to get things right: first, in designing a strong character/deck, and second, in executing the optimal strategies for that character/deck during gameplay. [...]\n\n\n> The idea is that human behavior works very much like a two-level game. [...] The player determines what we find rewarding or unrewarding.  The player determines what we notice and what we overlook; things come to our attention if it suits the player’s strategy, and not otherwise.  The player gives us emotions when it’s strategic to do so.  The player sets up our subconscious evaluations of what is good for us and bad for us, which we experience as “liking” or “disliking.”\n\n\n> The character is what *executing the player’s strategies feels like from the inside*.  If the player has decided that a task is unimportant, the character will experience “forgetting” to do it.  If the player has decided that alliance with someone will be in our interests, the character will experience “liking” that person.  Sometimes the player will notice and seize opportunities in a very strategic way that feels to the character like “being lucky” or “being in the right place at the right time.”\n\n\n> This is where confusion often sets in. People will often protest “but I *did*care about that thing, I just forgot” or “but I’m *not*that Machiavellian, I’m just doing what comes naturally.”  This is true, because when we talk about ourselves and our experiences, we’re speaking “in character”, as our character.  The strategy is not going on at a conscious level. In fact, I don’t believe we (characters) have direct access to the player; we can only *infer*what it’s doing, based on what patterns of behavior (or thought or emotion or perception) we observe in ourselves and others.\n\nI think that this model is basically correct, and that our emotional responses, preferences, etc. are all the result of a deeper-level optimization process. This optimization process, then, is something like that described in [The Brain as a Universal Learning Machine](https://www.lesswrong.com/posts/9Yc7Pp7szcjPgPsjf/the-brain-as-a-universal-learning-machine):\n\n\n> The universal learning hypothesis proposes that *all* significant mental algorithms are learned; nothing is innate except for the learning and reward machinery itself (which is somewhat complicated, involving a number of systems and mechanisms), the initial rough architecture (equivalent to a prior over mindspace), and a small library of simple innate circuits (analogous to the operating system layer in a computer).  In this view the mind (software) is distinct from the brain (hardware).  The mind is a complex software system built out of a general learning mechanism. [...]\n\n\n> An initial untrained seed ULM can be defined by 1.) a prior over the space of models (or equivalently, programs), 2.) an initial utility function, and 3.) the universal learning machinery/algorithm.  The machine is a real-time system that processes an input sensory/observation stream and produces an output motor/action stream to control the external world using a learned internal program that is the result of continuous self-optimization. [...]\n\n\n> The key defining characteristic of a ULM is that it uses its universal learning algorithm for continuous recursive self-improvement with regards to the utility function (reward system).  We can view this as second (and higher) order optimization: the ULM optimizes the external world (first order), and also optimizes its own internal optimization process (second order), and so on.  Without loss of generality, any system capable of computing a large number of decision variables can also compute internal self-modification decisions.\n\n\n> Conceptually the learning machinery computes a probability distribution over program-space that is proportional to the expected utility distribution.  At each timestep it receives a new sensory observation and expends some amount of computational energy to infer an updated (approximate) posterior distribution over its internal program-space: an approximate 'Bayesian' self-improvement.\n\nRephrasing these posts in terms of each other, in a person's brain \"the player\" is the underlying learning machinery, which is searching the space of programs (brains) in order to find a suitable configuration; the \"character\" is whatever set of emotional responses, aesthetics, identities, and so forth the learning program has currently hit upon.\n\nMany of the things about the character that seem fixed, can in fact be modified by the learning machinery. One's sense of aesthetics can be [updated by propagating new facts into it](https://www.lesswrong.com/posts/YN6daWakNnkXEeznB/propagating-facts-into-aesthetics), and strongly-held identities (such as \"I am a technical person\") [can change](https://www.lesswrong.com/posts/JTzxg7y5HFYBBWfBj/identities-are-subconscious-strategies) in response to new kinds of strategies becoming viable. [Unlocking the Emotional Brain describes](https://www.lesswrong.com/posts/i9xyZBS3qzA8nFXNQ/book-summary-unlocking-the-emotional-brain) a number of such updates, such as - in these terms - the ULM eliminating subprograms blocking confidence after receiving an update saying that the consequences of expressing confidence will not be as bad as previously predicted.\n\nAnother example of this kind of a thing was the framework that I sketched in [Building up to an Internal Family Systems model](https://www.lesswrong.com/posts/5gfqG3Xcopscta3st/building-up-to-an-internal-family-systems-model): if a system has certain kinds of bad experiences, it makes sense for it to spawn subsystems dedicated to ensuring that those experiences do not repeat. Moral psychology's [social intuitionist model](https://en.wikipedia.org/wiki/Social_intuitionism) claims that people often have an existing conviction that certain actions or outcomes are bad, and that they then level seemingly rational arguments for the sake of preventing those outcomes. Even if you rebut the arguments, the conviction remains. This kind of a model is compatible with an IFS/ULM style model, where the learning machinery sets the goal of preventing particular outcomes, and then applies the \"reasoning module\" for that purpose.\n\n[Qiaochu Yuan notes](https://twitter.com/QiaochuYuan/status/1219281477898244096) that once you see people being upset at their coworker for criticizing them and you do therapy approaches with them, and this gets to the point where they are crying about how their father never told them that they were proud of them... then it gets really hard to take people's reactions to things at face value. Many of our consciously experienced motivations, actually have nothing to do with our real motivations. (See also: [Nobody does the thing that they are supposedly doing](https://www.lesswrong.com/posts/8iAJ9QsST9X9nzfFy/nobody-does-the-thing-that-they-are-supposedly-doing), [The Elephant in the Brain](https://www.lesswrong.com/posts/BgBrXpByCSmCLjpwr/book-review-the-elephant-in-the-brain), [The Intelligent Social Web](https://www.lesswrong.com/posts/AqbWna2S85pFTsHH4/the-intelligent-social-web).)\n\nPreference synthesis as a character-level model\n===============================================\n\nWhile I like a lot of the work that Stuart Armstrong has done on [synthesizing human preferences](https://www.lesswrong.com/posts/m2bwD87ctjJDXC3SZ/ultra-simplified-research-agenda), I have a serious concern about it which is best described as: everything in it is based on the character level, rather than the player/ULM level. \n\nFor example, in \"[Our values are underdefined, changeable, and manipulable](https://www.lesswrong.com/posts/KCg7NeKQ7MycXWpYd/our-values-are-underdefined-changeable-and-manipulable)\", Stuart - in my view, correctly - argues for the claim stated in the title... except that, it is not clear to me to what extent the things we intuitively consider our \"values\", are actually our values. Stuart opens with this example:\n\n\n> When asked whether \"communist\" journalists could report freely from the USA, only 36% of 1950 Americans agreed. A follow up question about Amerian journalists reporting freely from the USSR got 66% agreement. When the order of the questions was reversed, 90% were in favour of American journalists - and an astounding 73% in favour of the communist ones.\n\nFrom this, Stuart suggests that people's values on these questions should be thought of as *underdetermined*. I think that this has a grain of truth to it, but that calling these opinions \"values\" in the first place is misleading. \n\nMy preferred framing would rather be that people's *values* - in the sense of some deeper set of rewards which the underlying machinery is optimizing for - are in fact underdetermined, *but* that is not what's going on in this particular example. The order of the questions does not change those values, which remain stable under this kind of a consideration. Rather, consciously-held political opinions are *strategies* for carrying out the underlying values. Receiving the questions in a different order caused the system to consider different kinds of information when it was choosing its initial strategy, causing different strategic choices. \n\nStuart's research agenda does talk about [incorporating meta-preferences](https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into#2_6_Synthesising_the_preference_function__meta_preferences), but as far as I can tell, all the meta-preferences are about the character level too. Stuart mentions \"I want to be more generous\" and \"I want to have consistent preferences\" as examples of meta-preferences; in actuality, these meta-preferences might exist because of something like \"the learning system has identified generosity as a socially admirable strategy and predicts that to lead to better social outcomes\" and \"the learning system has formulated consistency as a generally valuable heuristic and one which affirms the 'logical thinker' identity, which in turn is being optimized because of its predicted social outcomes\".\n\nMy confusion about a better theory of values\n============================================\n\nIf a \"purely character-level\" model of human values is wrong, how do we incorporate the player level?\n\nI'm not sure and am mostly confused about it, so I will just [babble](https://www.lesswrong.com/s/pC6DYFLPMTCbEwH8W/p/i42Dfoh4HtsCAfXxL) & [boggle](https://rationality.org/files/cfar-handbook.pdf#page=16) at my confusion for a while, in the hopes that it would help.\n\nThe optimistic take would be that there exists some set of universal human values which the learning machinery is optimizing for. There exist various therapy frameworks which claim to have found something like this. \n\nFor example, the [NEDERA model](https://bioemotiveframework.com/wp-content/uploads/woocommerce_uploads/2017/07/Nedera-Guidebook.pdf) claims that there exist nine negative core feelings whose avoidance humans are optimizing for: people may feel Alone, Bad, Helpless, Hopeless, Inadequate, Insignificant, Lost/Disoriented, Lost/Empty, and Worthless. And [pjeby mentions](https://www.lesswrong.com/posts/i9xyZBS3qzA8nFXNQ/book-summary-unlocking-the-emotional-brain?commentId=JKrsoMcRkvyJd4KZZ) that in his empirical work, he has found three clusters of underlying fears which seem similar to these nine:\n\n\n> For example, working with people on self-image problems, I've found that there appear to be only three critical \"flavors\" of self-judgment that create life-long low self-esteem in some area, and associated compulsive or avoidant behaviors:\n\n\n> Belief that one is bad, defective, or malicious (i.e. lacking in care/altruism for friends or family)\n\n\n> Belief that one is foolish, incapable, incompetent, unworthy, etc. (i.e. lacking in ability to learn/improve/perform)\n\n\n> Belief that one is selfish, irresponsible, careless, etc. (i.e. not respecting what the family or community values or believes important)\n\n\n> (Notice that these are things that, if you were bad enough at them in the ancestral environment, or if people only **thought** you were, you would lose reproductive opportunities and/or your life due to ostracism. So it's reasonable to assume that we have wiring biased to treat these as high-priority *long-term* drivers of compensatory signaling behavior.)\n\n\n> Anyway, when somebody gets taught that some behavior (e.g. showing off, not working hard, forgetting things) *equates* to one of these morality-like judgments as a *persistent quality* of themselves, they often develop a compulsive need to prove otherwise, which makes them choose their goals, not based on the goal's actual utility to themself or others, but rather based on the goal's perceived value as a means of virtue-signalling. (Which then leads to a pattern of continually trying to achieve similar goals and either failing, or feeling as though the goal was unsatisfactory despite succeeding at it.)\n\nSo - assuming for the sake of argument that these findings are correct - one might think something like \"okay, here are the things the brain is trying to avoid, we can take those as the basic human values\".\n\nBut not so fast. After all, emotions are all computed in the brain, so \"avoidance of these emotions\" can't be the only goal any more than \"optimizing happiness\" can. It would only lead to wireheading.\n\nFurthermore, it seems like one of the things that the underlying machinery *also* learns, is *situations in which it should trigger these feelings*. E.g. feelings of irresponsibility can be used as an internal carrot and stick scheme, in which the system comes to predict that if it will feel persistently bad, this will cause parts of it to pursue specific goals in an attempt to make those negative feelings go away. \n\nAlso, we are not *only* trying to avoid negative feelings. Empirically, it doesn't look like happy people end up doing *less* than unhappy people, and [guilt-free people may in fact do more](http://mindingourway.com/guilt-conclusion/) than guilt-driven people. The relationship is nowhere linear, but it seems like there are plenty of happy, energetic people who are happy *in part because* they are doing all kinds of fulfilling things.\n\nSo maybe we could look at the inverse of negative feelings: positive feelings. The current mainstream model of human motivation and basic needs is [self-determination theory](https://positivepsychology.com/self-determination-theory/), which explicitly holds that there exist three separate basic needs:\n\n\n> **Autonomy:** people have a need to feel that they are the masters of their own destiny and that they have at least some control over their lives; most importantly, people have a need to feel that they are in control of their own behavior.\n\n\n> **Competence:** another need concerns our achievements, knowledge, and skills; people have a need to build their competence and develop mastery over tasks that are important to them.\n\n\n> **Relatedness (also called Connection):** people need to have a sense of belonging and connectedness with others; each of us needs other people to some degree\n\nSo one model could be that the basic learning machinery is, first, optimizing for avoiding bad feelings; and then, optimizing for things that have been associated with good feelings (even when doing those things is locally unrewarding, e.g. taking care of your children even when it's unpleasant). But this too risks running into the wireheading issue.\n\nA problem here is that while it might make intuitive sense to say \"okay, if the character's values aren't the real values, let's use the player's values instead\", the split isn't actually anywhere that clean. In a sense the player's values are the real ones - but there's also a sense in which the player doesn't *have* anything that we could call values. It's just a learning system which observes a stream of rewards and optimizes it according to some set of mechanisms, and even the reward and optimization mechanisms themselves may end up getting at least partially rewritten. The underlying machinery has no idea about things like \"existential risk\" or \"avoiding wireheading\" or necessarily even \"personal survival\" - thinking about those is a character-level strategy, even if it is chosen by the player using criteria that it does not actually understand. \n\nFor a moment it felt like looking at the player level would help with the underdefinability and mutability of values, but the player's values seem like they could be *even less* defined and *even more* mutable. It's not clear to me that we can call *them* values in the first place, either - any more than it makes meaningful sense to say that a neuron in the brain \"values\" firing and releasing neurotransmitters. The player is just a set of code, or going one abstraction level down, just a bunch of cells. \n\nTo the extent that there exists something that intuitively resembles what we call \"human values\", it feels like it exists in some hybrid level which incorporates parts of the player *and* parts of the character. That is, assuming that the two can even be very clearly distinguished from each other in the first place.\n\nOr something. I'm confused.", "url": "https://www.alignmentforum.org/posts/2yLn8iTrvHoEgqXcJ/the-two-layer-model-of-human-values-and-problems-with", "date_published": "2020-01-24T15:17:34Z", "authors": ["Kaj_Sotala"], "tags": ["Value Learning", "Complexity of Value", "Motivations"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.700507+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "f7d37210ea313f847a4debca8f689234", "source": "alignmentforum", "title": "AI Alignment 2018-19 Review", "text": "Preamble\n========\n\n#### What this post is\n\nThis is a review post of public work in AI alignment over 2019, with some inclusions from 2018. It has this preamble (~700 words), a short version / summary (~1.6k words), and a long version (~8.3k words). It is available as a Google Doc [here](https://docs.google.com/document/d/1Fng1J_QPb7GEeLBMmWWfZOguw7yUTZot0egrCbKpVwk/edit#).\n\nThere are many areas of work that are relevant to AI alignment that I have barely touched on, such as interpretability, uncertainty estimation, adversarial examples, and assured autonomy, primarily because I have not been following these fields and wouldn’t be able to write a good summary of what has happened in them. I have also mostly focused on articles that provide some conceptual insight, and excluded or briefly linked to papers that primarily make quantitative improvements on important metrics. While such papers are obviously important (ultimately, our techniques need to work *well*), there isn’t much to say about them in a yearly review other than that the quantitative metric was improved.\n\nDespite these exclusions, there was still a ton of work to select from, perhaps around ~500 articles, of which over 300 have been linked to in this post. There are many interesting articles that I really enjoyed that get only a sentence of description, in which I ignore many of the points that the article makes. Most have been summarized in the [Alignment Newsletter](https://rohinshah.com/alignment-newsletter), so if you’d like to learn more about any particular link, but don’t want to read the entire thing, just search for its title in the [database](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit#gid=0).\n\n#### What you should know about the structure of this post\n\nI am *not* speaking for myself; by default I am trying to explain what has been said, in a way that the authors of the articles would agree with. Any extra opinion that I add will be in italics.\n\nAs a post, this is meant to be read sequentially, but the underlying structure is a graph (nodes are posts, edges connect posts that are very related). I arranged it in a sequence that highlights the most salient-to-me connections. This means that the order in which I present subtopics is very much *not* a reflection of what I think is most important in AI safety: in my presentation order, I focused on *edges* (connections) rather than *nodes* (subtopics).\n\nOther minor details:\n\n1. Any links from earlier than 2018 will have their year of publication right after the link (except for articles that were reposted as part of Alignment Forum sequences).\n2. I typically link to blog posts; in several cases there is also an associated paper that I have not linked.\n\n#### How to read this post\n\nI have put the most effort into making the prose of the long version read smoothly. The hierarchical organization is comparatively less coherent; this is partly because I optimized the prose, and partly because AI safety work is hard to cluster. As a result, for those willing to put in the effort, I’d recommend reading the long version directly, without paying too much attention to the hierarchy. If you have less time, or are less interested in the minutiae of AI alignment research, the short version is for you.\n\nSince I don’t name authors or organizations, you may want to take this as your opportunity to form beliefs about which arguments in AI alignment are important based on the ideas (as opposed to based on trust in the author of the post).\n\nPeople who keep up with AI alignment work might want to know which posts I’m referencing as they read, which is a bit hard since I don’t name the posts in the text. If this describes you, you should be reading this post on the Alignment Forum, where you can hover over most links to see what they link to. Alternatively, the [references section in the Google Doc](https://docs.google.com/document/d/1Fng1J_QPb7GEeLBMmWWfZOguw7yUTZot0egrCbKpVwk/edit#heading=h.kr223qj1g1y5) lists all links in the order that they appear in the post, along with the hierarchical organization, and so you can open the references in a new tab, and read through the post and the references together.\n\nI expect that if you aren’t already familiar with them, some articles will sound crazy from my summary here; please read at least the newsletter summary and ideally the full article before arguing that it’s crazy.\n\n#### Acknowledgements\n\nThanks to the [Alignment Newsletter team](https://rohinshah.com/alignment-newsletter), Ben Pace, Oliver Habryka, Jonathan Uesato, Tom Everitt, Luke Muehlhauser, Jan Leike, Rob Bensinger, Adam Gleave, Scott Emmons, Rachel Freedman, Andrew Critch, Victoria Krakovna, and probably a few others (I really should have kept better track of this). Thanks especially to Ben Pace for suggesting that I write this review in the first place.\n\nShort version (~1.6k words)\n===========================\n\nWhile the full text tries to accurately summarize different points of view, that is not a goal in this summary. Here I simply try to give a sense of the topics involved in the discussion, without saying what discussion actually happened.\n\n**Basic analysis of AI risk.** Traditional arguments for AI risk argue that since agentic AI systems will apply lots of optimization, they will lead to extreme outcomes that can’t be handled with normal engineering efforts. Powerful AI systems will not have their resources stolen from them, which by various dutch book theorems implies that they must be expected utility maximizers; since expected utility maximizers are goal-directed, they are dangerous.\n\nHowever, the VNM theorem [does not justify](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/NxF5G6CJiof6cemTw) the assumption that an AI system will be goal-directed: such an assumption is really based on intuitions and conceptual arguments (which are still quite strong).\n\n[Comprehensive AI Services](https://www.alignmentforum.org/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services-as) (CAIS) challenges the assumption that we will have a single agentic AI, instead suggesting that any task will be performed by a collection of modular services.\n\nThat being said, there are several other arguments for AI risk, such as the [argument](https://www.alignmentforum.org/posts/w6d7XBCegc96kz4n3/the-argument-from-philosophical-difficulty) that AI might cause “lock in” which may require us to solve hard philosophical problems before the development of AGI.\n\nNonetheless, there are [disjunctive reasons](https://www.alignmentforum.org/posts/QknPz9JQTQpGdaWDp/an-80-why-ai-risk-might-be-solved-without-additional) to expect that catastrophe does not occur: for example, there may not be a problem, or ML researchers may solve the problem after we get “warning shots”, or we could coordinate to not build unaligned AI.\n\n**Agency and optimization.** One proposed problem is that of [mesa optimization](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB), in which an optimization algorithm used to train an AI creates an agent that is *itself* performing optimization. In such a scenario, we need to ensure that the “inner” optimization is also aligned.\n\nTo better understand these and other situations, it would be useful to have a formalization of optimization. This is [hard](https://www.alignmentforum.org/posts/26eupx3Byc8swRS7f/bottle-caps-aren-t-optimisers): while we don’t want optimization to be about our beliefs about a system, if we try to define it mechanistically, it becomes hard to avoid defining a bottle cap as an optimizer of “water kept in the bottle”.\n\nUnderstanding agents is another hard task. While agents are relatively well understood under the Cartesian assumption, where the agent is separate from its environment, things become much more complex and poorly-understood when the agent is a [part of its environment](https://www.alignmentforum.org/s/Rm6oQRJJmhGCcLvxh).\n\n**Value learning.** Building an AI that learns all of human value has historically been thought to be very hard, because it requires you to decompose human behavior into the “beliefs and planning” part and the “values” part, and there’s no clear way to do this.\n\nAnother way of looking at it is to say that value learning [requires](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/h9DesGT3WT9u2k7Hr) a model that separates the given data into that which actually achieves the true “values” and that which is just “a mistake”, which seems hard to do. In addition, value learning seems quite fragile to mis-specification of this human model.\n\nNonetheless, there are reasons for optimism. We could try to build an [*adequate* utility function](https://www.alignmentforum.org/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into), which works well enough for our purposes. We can also have [*uncertainty* over the utility function](https://www.alignmentforum.org/posts/nd692YfFGfZDh9Mwz/an-69-stuart-russell-s-new-book-on-why-we-need-to-replace), and update the belief over time based on human behavior. If everything is specified correctly (a big if), as time goes on, the agent would become more and more aligned with human values. One major benefit of this is that it is *interactive* -- it doesn’t require us to specify everything perfectly ahead of time.\n\n**Robustness.** We would like our agents to be robust - that is, they shouldn’t fail catastrophically in situations slightly different from the ones they were designed for. Within reinforcement learning, safe reinforcement learning aims to avoid mistakes, even during training. This either requires analytical (i.e. not trial-and-error) reasoning about what a “mistake” is, which requires a formal specification of what a mistake is, or an overseer who can correct the agent before it makes a mistake.\n\nThe classic example of a failure of robustness is adversarial examples, in which a tiny change to an image can drastically affect its classification. Recent research has shown that these examples are [caused](https://distill.pub/2019/advex-bugs-discussion/) (at least in part) by real statistical correlations that generalize to the test set, that are nonetheless fragile to small changes. In addition, since robustness to one kind of adversary doesn’t make the classifier robust to other kinds of adversaries, there has been a lot of work done on improving adversarial evaluation in image classification. We’re also seeing some of this work in reinforcement learning.\n\nHowever, asking our agents to be robust to arbitrary mistakes seems to be too much -- humans certainly don’t meet this bar. For AI safety, it seems like we need to ensure that our agents are robustly [intent aligned](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd/p/ZeE7EKHTFMBs8eMxn), that is, they are always “trying” to do what we want. One particular way that our agents could be intent aligned is if they are [corrigible](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd/p/fkLYhTQteAu5SinAc), that is, they are trying to keep us “in control”. This seems like a particularly easy property to verify, as conceptually it seems to be independent of the domain in which the agent is deployed.\n\nSo, we would like to ensure that even in the worst case, our agent remains corrigible. One [proposal](https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment) would be to train an adversary to search for “relaxed” situations in which the agent behaves incorrigibly, and then train the agent not to do that.\n\n**Scaling to superhuman abilities.** If we’re building corrigible agents using adversarial training, our adversary should be more capable than the agent that it is training, so that it can find all the situations in which the agent behaves incorrigibly. This requires techniques that scale to superhuman abilities. Some techniques for this include [iterated amplification](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd) and [debate](https://arxiv.org/abs/1805.00899).\n\nIn iterated amplification, we start with an initial policy, and alternate between amplification and distillation, which increase capabilities and efficiency respectively. This can encode a range of algorithms, but often amplification is done by decomposing questions and using the agent to answer subquestions, and distillation can be done using supervised learning or reinforcement learning.\n\nIn debate, we train an agent through self-play in a zero-sum game in which the agent’s goal is to “win” a question-answering debate, as evaluated by a human judge. The hope is that since each “side” of the debate can point out flaws in the other side’s arguments, such a setup can use a human judge to train far more capable agents while still incentivizing them to provide honest, true information.\n\nBoth iterated amplification and debate aim to train an agent that approximates the answer that one would get from an exponentially large tree of humans deliberating. The [factored cognition](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd/p/DFkGStzvj3jgXibFG) hypothesis is that this sort of tree of humans is able to do any task we care about. This hypothesis is controversial: many have the intuition that cognition requires large contexts and flashes of intuition that couldn’t be replicated by a tree of time-limited humans.\n\n**Universality.** One [property](https://www.alignmentforum.org/posts/M8WdeNWacMrmorNdd/towards-formalizing-universality) we would hope to have is that if we use this tree of humans as an overseer for some simpler agent, then the tree would “know everything the agent knows”. If true, this property could allow us to build a significantly stronger conceptual argument for safety. It is also very related to…\n\n**Interpretability.** While interpretability can help us know what the agent knows, and what the agent would do in other situations (which can help us verify if it is corrigible), there are [other uses](https://www.alignmentforum.org/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety) for it as well: in general, it seems better if we can understand the things we’re building.\n\n**Impact regularization.** While relative reachability and attainable utility preservation were developed last year, this year saw them be [unified](https://medium.com/@deepmindsafetyresearch/designing-agent-incentives-to-avoid-side-effects-e1ac80ea6107) into a single framework. In addition, there was a new proposed [definition](https://www.alignmentforum.org/s/7CdoznhJaLEKHwvJW) of impact: change in our ability to get what we want. This notion of impact depends on knowing the utility function U. However, we might hope that we can penalize some “objective” notion, perhaps \"power\", that occurs regardless of the choice of U, for the same reasons that we expect instrumental convergence.\n\n**Causal modeling.** Causal models have been used recently to [model](https://arxiv.org/abs/1906.08663) the incentives for an agent under different AI safety frameworks, and to [argue](https://medium.com/@deepmindsafetyresearch/designing-agent-incentives-to-avoid-reward-tampering-4380c1bb6cd) that by evaluating plans with the current reward function, you can remove the incentive for an agent to tamper with its reward function.\n\n**Oracles.** Even if oracles are trying to maximize predictive accuracy, they could “choose” between different self-confirming predictions. We could avoid this using counterfactual oracles, which make predictions conditioning that their predictions do not influence the future.\n\n**Decision theory.** There was work on decision theory, that I haven’t followed very much.\n\n**Forecasting.** Several resources were developed to enable effective group forecasting, including an [AI forecasting dictionary](https://www.lesswrong.com/posts/8y7DcSF4eAkXoru4u/ai-forecasting-dictionary-forecasting-infrastructure-part-1-2) that defines terms, an [AI resolution council](https://www.lesswrong.com/posts/9G6CCNXkA7JZoorpY/ai-forecasting-resolution-council-forecasting-infrastructure) whose future opinions can be predicted, and a dataset of well-constructed [exemplar questions](https://www.lesswrong.com/posts/yy3FCmdAbgSLePD7H/how-to-write-good-ai-forecasting-questions-question-database) about AI.\n\nSeparately, the debate over takeoff speeds continued, with [two](https://sideways-view.com/2018/02/24/takeoff-speeds/) [posts](https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/) arguing forcefully for continuous takeoff, [without much response](https://www.lesswrong.com/posts/PzAnWgqvfESgQEvdg/any-rebuttals-of-christiano-and-ai-impacts-on-takeoff-speeds) (although many researchers do not agree with them). The continuity of takeoff is relevant for but doesn’t completely determine whether recursive self improvement will happen, or whether some actor acquires a decisive strategic advantage. The primary implication of the debate is whether we should expect that we will have enough time to react and fix problems as they arise.\n\nIt has also become clearer that recent progress in AI has been driven to a significant degree by increasing the [amount of compute](https://openai.com/blog/ai-and-compute/) devoted to AI, which suggests a more continuous takeoff. You could take the position that current methods can’t do <property X> (say, causal reasoning), and so it doesn’t matter how much compute you use.\n\n**AI Progress.** There was a lot of progress in AI.\n\n**Field building.** There were posts aiming to build the field, but they were all fairly disjointed.\n\nThe **long version** (~8.3k words) starts here.\n\nBasic analysis of AI risk\n=========================\n\nAgentic AI systems\n------------------\n\nMuch of the foundational writing about AI risk has focused on agentic AI systems. This approach (recently discussed in the post and comments [here](https://www.lesswrong.com/posts/mdau2DBSMi5bWXPGA/useful-does-not-mean-secure)) argues that since AI agents will be exerting a lot of optimization, there will be [extreme outcomes](https://www.lesswrong.com/posts/zEvqFtT4AtTztfYC4/optimization-amplifies) in which our regular arguments may not work. This implies that we must adopt a [security mindset](https://www.lesswrong.com/posts/8gqrbnW758qjHFTrH/security-mindset-and-ordinary-paranoia) (2017) to ensure alignment, and it suggests that proof-level guarantees may be [more important](https://www.alignmentforum.org/posts/aPwNaiSLjYP4XXZQW/ai-alignment-open-thread-august-2019#3TKtFmF3ZcQFgQNsA) at various stages of alignment research.\n\n#### Goal-directedness\n\nThe foundational writing then goes on to point out that since powerful AI systems should not be able to be dutch booked (i.e. have their resources stolen from them), they will be well [modeled](https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities) (2017) as expected utility maximisers. An AI system that maximizes expected utility is very likely to be dangerous. One reason was [recently](https://www.alignmentforum.org/posts/6DuJxY8X45Sco4bS2/seeking-power-is-provably-instrumentally-convergent-in-mdps) [formalized](https://www.alignmentforum.org/posts/cwpKagyTvqSyAJB7q/clarifying-power-seeking-and-instrumental-convergence) in MDPs in which the agent gets a *random* utility function: using formalizations of power and instrumental convergence, we find some suggestive results that agents seek control over their future (from which we might infer that they will try to wrest that control from us).\n\nHowever, it is [not mathematically necessary](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/NxF5G6CJiof6cemTw) that AI systems will have utility functions (except in a vacuous sense), and while there are [intuitive and conceptual reasons](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/9zpT9dikrrebdq3Jf) to think that we will build [goal-directed agents](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/DfcywmqRSkBaCB6Ma) by default, there are [alternative pathways](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/tHxXdAn8Yuiy9y2pZ) that might be taken instead, and that are valuable to explore and build out to ensure AI safety.\n\nThis challenge to the usual argument for utility maximizers has [prompted](https://www.alignmentforum.org/posts/pLZ3bdeng4u5W8Yft/let-s-talk-about-convergent-rationality-1) [a](https://www.alignmentforum.org/posts/vphFJzK3mWA4PJKAg/coherent-behaviour-in-the-real-world-is-an-incoherent) [series](https://www.alignmentforum.org/posts/4K52SS7fm9mp5rMdX/three-ways-that-sufficiently-optimized-agents-appear) [of](https://www.alignmentforum.org/posts/Q9JKKwSFybCTtMS9d/what-are-we-assuming-about-utility-functions) [articles](https://www.lesswrong.com/posts/EnN7cm3KaRrEAuWfa/comment-on-coherence-arguments-do-not-imply-goal-directed) exploring other variants of the argument, for example by [restricting](https://www.alignmentforum.org/posts/yGuo5R9fgrrFLYWuv/when-do-utility-functions-constrain-1) the class of utility functions to make it non-vacuous, or by saying that [optimization processes](https://www.alignmentforum.org/posts/vphFJzK3mWA4PJKAg/coherent-behaviour-in-the-real-world-is-an-incoherent#F2YB5aJgDdK9ZGspw) in general will lead to goal-directed agents.\n\n#### Comprehensive AI Services\n\n[Comprehensive AI Services](https://www.alignmentforum.org/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services-as) (CAIS) also takes issue with the model of a single AGI agent hyper-competently pursuing some goal, and instead proposes a model in which different tasks are solved by specialized, competing *AI services*. This is suggesting that modularity across tasks is sufficiently useful that it will apply to AI, in the same way that it applies to humans (e.g. I have specialized in AI research, and not plumbing). The aggregate of all the services can accomplish any task, including the development of new services, making it comprehensive (analogous to the “general” in AGI). Since AI services can also do basic AI R&D research, which leads to improvement in AI services generally, we should expect recursive *technological* improvement (as opposed to recursive *self* improvement). Note that CAIS does not necessarily suggest we will be *safe*, just that the traditional risks are not as likely as we may have thought, while other emergent risks are perhaps greater.\n\n[Critics](https://www.alignmentforum.org/posts/HvNAmkXPTSoA4dvzv/comments-on-cais) [often](https://www.lesswrong.com/posts/bXYtDfMTNbjCXFQPh/drexler-on-ai-risk) [argue](https://www.alignmentforum.org/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services-as#sXHXAfSKWPyEMhtcu) that end-to-end training and integrated agent-like architectures are likely to (eventually) outperform modular services. However, through coordination services can also be integrated. In addition, this [post](https://www.overcomingbias.com/2019/02/how-lumpy-ai-services.html) argues that this criticism mirrors old concerns that under capitalism firms will become too large -- a concern that the post argues did not pan out.\n\nCAIS *does* allow for AI systems that are capable of *learning* across many domains: it simply argues that these AI systems will specialize for efficiency reasons, and so will only be *competent* at a small subset of domains. This decomposition of intelligence into learning + competence has been used to explain the [variation in human abilities](https://www.lesswrong.com/posts/ZwSrTsP3YkgnmHWnJ/two-explanations-for-variability-in-human-abilities).\n\n(This conversation is related to much prior conversation on Tool AI, which is listed [here](https://www.lesswrong.com/posts/jkxkMTGfZDzBEaaY8/why-not-tool-ai#zECozzvnPz7XKvLLc).)\n\nArguments for AI risk\n---------------------\n\nThere are many arguments for AI risk, with each of [these](https://www.alignmentforum.org/posts/JbcWQCxKWn3y49bNB/disentangling-arguments-for-the-importance-of-ai-safety) [posts](https://www.alignmentforum.org/posts/WXvt8bxYnwBYpy9oT/the-main-sources-of-ai-risk-1) providing a list of such arguments. It is unclear whether from an outside perspective this should be taken as evidence *against* AI risk (since different researchers believe different arguments and are aiming for different [“success stories”](https://www.alignmentforum.org/posts/bnY3L48TtDrKTzGRb/ai-safety-success-stories)) or as evidence *for* AI risk (because there are so many different sources of AI risk).\n\nOne argument that saw a lot of discussion was that we must [figure out philosophy](https://www.alignmentforum.org/posts/w6d7XBCegc96kz4n3/the-argument-from-philosophical-difficulty) since the creation of AGI might “lock in” philosophical ideas. For example, we might [not want](https://arxiv.org/abs/1901.00064) to have AI systems with utility functions because of impossibility results in population ethics that suggest that every utility function would lead to some counterintuitive conclusion. Similarly, there are [many proposals](https://www.alignmentforum.org/posts/W95gbuognJu5WxkTW/the-value-definition-problem) for how to define values; it may be necessary to figure out the right definition ahead of time. Rather than solving these problems directly, we could solve [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy), or delegate to humans who [deliberate](https://www.alignmentforum.org/posts/ebdf8GZxt3L9grwwN/deliberation-as-a-method-to-find-the-actual-preferences-of), whether [idealized or real](https://www.alignmentforum.org/posts/vbtvgNXkufFRSrx4j/three-ai-safety-related-ideas).\n\nWe might also worry that AIs will economically outcompete humans, give us technologies we [aren’t ready for](https://www.alignmentforum.org/posts/vbtvgNXkufFRSrx4j/three-ai-safety-related-ideas), or amplify [human](https://www.lesswrong.com/posts/6RjL996E8Dsz3vHPk/two-more-decision-theory-problems-for-humans) [vulnerabilities](https://www.alignmentforum.org/posts/HTgakSs6JpnogD6c2/two-neglected-problems-in-human-ai-safety).\n\nUnder [continuous takeoff](https://sideways-view.com/2018/02/24/takeoff-speeds/), two scenarios have been proposed for [what failure looks like](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like). First, AI differentially improves society’s capability to optimize metrics that are easy to measure, rather than ones that we actually care about. Second, AI agents could accidentally be trained to seek influence, and then fail catastrophically at some point in the future once they are sufficiently capable. One [critique](http://www.overcomingbias.com/2019/04/agency-failure-ai-apocalypse.html) argues that these principal-agent problems only lead to bounded losses (i.e. they aren’t catastrophic), but [several](https://www.alignmentforum.org/posts/92J4zJHkqmXTduxzY/and-the-ai-would-have-got-away-with-it-too-if) [others](https://www.lesswrong.com/posts/ktDKfKqukTPRiuEPM/robin-hanson-on-the-futurist-focus-on-ai#W4Fr8WyoD6AtM4eEF) [disagree](https://www.lesswrong.com/posts/ktDKfKqukTPRiuEPM/robin-hanson-on-the-futurist-focus-on-ai#fecYAwjmMSZ9KRfPL).\n\n[This post](https://www.alignmentforum.org/posts/hubbRt4DPegiA5gRR/a-shift-in-arguments-for-ai-risk) argues that there has been a shift in the arguments that motivate new AI risk researchers, and calls for more explanation of these arguments so that they can be properly evaluated.\n\nArguments against AI risk\n-------------------------\n\nMany views that expect the problem to be solved by default have also been written up this year.\n\nA [series](https://aiimpacts.org/conversation-with-paul-christiano/) [of](https://aiimpacts.org/conversation-with-rohin-shah/) [four](https://aiimpacts.org/conversation-with-robin-hanson/) [conversations](https://aiimpacts.org/conversation-with-adam-gleave/) (summarized [here](https://www.alignmentforum.org/posts/QknPz9JQTQpGdaWDp/an-80-why-ai-risk-might-be-solved-without-additional)) suggested that some engaged people expect AI to go well by default, because they are unconvinced by the traditional arguments for AI risk, find discontinuities in AI capabilities relatively unlikely, and are hopeful that there will be “warning shots” that demonstrate problems, that the existing ML community will then successfully fix.\n\nOne [post](https://www.alignmentforum.org/posts/bd2K3Jdz82csjCFob/a-list-of-good-heuristics-that-the-case-for-ai-x-risk-fails) lists several good outside-view heuristics that argue against AI x-risk, while [another](https://www.alignmentforum.org/posts/xzFQp7bmkoKfnae9R/but-exactly-how-complex-and-fragile) questions why value being complex and fragile must lead to high AI risk.\n\n[This talk](https://forum.effectivealtruism.org/posts/9sBAW3qKppnoG3QPq/how-sure-are-we-about-this-ai-stuff) argues that while AGI will intuitively be a big deal, it’s not obvious that we can affect its impact, and so it’s not obvious that longtermists should focus on it. It gives an analogy to trying to influence the impact of electricity, before electricity was commonplace, and suggests there was little impact one could have had on its safe use. It argues that accident risks in particular draw on fuzzy, intuitive concepts, haven’t been engaged with much by critics, and don’t sway most AI researchers.\n\nDespite the seeming controversy in this and previous sections, it is worth noting that there is general agreement within the AI safety community on the following broader argument for work on AI safety:\n\n1. Superhuman agents are not *required* to treat humans well, in the same way that humans aren’t required to treat gorillas well.\n2. You should have a good *technical* reason to expect that superhuman agents *will* treat humans well.\n3. We do not currently have such a reason.\n\nAgency and optimization\n=======================\n\nMesa optimization\n-----------------\n\nThe problem of [mesa optimization](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB) was explained in significantly more detail (see also this less formal [summary](https://www.alignmentforum.org/posts/bG4PR9uSsZqHg2gYY/utility-reward)). In mesa optimization, we start with a *base optimizer* like gradient descent that searches for a policy that accomplishes some complex task. For sufficiently complex tasks, it seems likely that the best policy will *itself* be an optimizer. (Meta learning is explicitly trying to learn policies that are also optimizers.) However, the policy could be optimizing a different goal, called the *mesa objective*, rather than the *base objective*.\n\nOptimizing the mesa objective must lead to good base objective behavior on the training distribution (else gradient descent would not select it), but could be arbitrarily bad when off distribution. For example, a plausible mesa objective would be to seek influence: such an agent would initially do what we want it to do (since otherwise we would shut it down), but might turn against us once it has accumulated enough power.\n\nThis decomposes the overall alignment problem into *outer alignment* (ensuring that the base objective is aligned with “what we want”) and *inner alignment* (ensuring that the mesa objective is aligned with the base objective). This is somewhat [analogous](https://www.alignmentforum.org/posts/yXPT4nr4as7JvxLQa/classifying-specification-problems-as-variants-of-goodhart-s) to different [types](https://www.alignmentforum.org/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy) (2017) of Goodhart’s law.\n\nThe paper and [subsequent](https://www.alignmentforum.org/posts/iydwbZhATANhjoGP7/more-variations-on-pseudo-alignment) [analysis](https://www.alignmentforum.org/posts/uXH4r6MmKPedk8rMA/gradient-hacking) identify and categorize relationships between the base and mesa objectives, and explain how mesa optimizers could fail catastrophically. Of particular interest is that mesa optimizers should be fast, but could still be misaligned, suggesting that [penalizing compute](https://www.alignmentforum.org/posts/nyCHnY7T5PHPLjxmN/open-question-are-minimal-circuits-daemon-free) is [not enough](https://www.alignmentforum.org/posts/fM5ZWGDbnjb7ThNKJ/are-minimal-circuits-deceptive) to solve inner alignment.\n\nEffectively, the concern is that our AI systems will have capabilities that generalize, but objectives that [don’t](https://www.alignmentforum.org/posts/2mhFMgtAjFJesaSYR/2-d-robustness). Since this is what drives risk, some [suggest](https://www.alignmentforum.org/posts/nFDXq7HTv9Xugcqaw/is-the-term-mesa-optimizer-too-narrow) that we should talk about this phenomenon, without needing to bring in the baggage of “optimization”, a term we have yet to understand well, while others [argue](https://www.alignmentforum.org/posts/zCcmJzbenAXu6qugS/tabooing-agent-for-prosaic-alignment) that even if we start with this definition, it would be useful to reintroduce the notions of optimization and agency.\n\nOne advantage of the original definition is that it specifies a particular mechanism by which risk arises; this gives us a foothold into the problem that allows us to propose [potential](https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment) [solutions](https://www.alignmentforum.org/posts/Ca3sCRGfWvXvYC5YC/what-are-some-non-purely-sampling-ways-to-do-deep-rl) [and](https://www.alignmentforum.org/posts/uSdPa9nrSgmXCtdKN/concrete-experiments-in-inner-alignment) [empirical](https://www.alignmentforum.org/posts/AFdRGfYDWQqmkdhFq/a-simple-environment-for-showing-mesa-misalignment) [investigations](https://www.alignmentforum.org/posts/2GycxikGnepJbxfHT/towards-an-empirical-investigation-of-inner-alignment). Of course, this is actively counterproductive if the risk arises by some [other mechanism](https://www.alignmentforum.org/posts/osxNg6yBCJ4ur9hpi/does-agent-like-behavior-imply-agent-like-architecture), but we might expect optimization to be especially likely because optimization algorithms are simple, and the phenomenon of [double descent](https://www.alignmentforum.org/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent) [suggests](https://www.alignmentforum.org/posts/nGqzNC6uNueum2w8T/inductive-biases-stick-around) that neural nets have an inductive bias towards simplicity.\n\nWhat are optimization and agency, anyway?\n-----------------------------------------\n\nGiven the central importance of optimization to inner alignment and AI safety more broadly, we’d like to be able to formalize it. However, it’s not clear how to do so: while we want optimization to be about the *mechanical process* by which outcomes happen (as opposed to e.g. *our beliefs* about that process), we cannot simply say that X is an optimizer if it makes some quantity go up: by this definition, a [bottle cap](https://www.alignmentforum.org/posts/26eupx3Byc8swRS7f/bottle-caps-aren-t-optimisers) would be an optimizer for “keeping water in the bottle''.\n\nIt is also relevant how the system [interacts](https://www.alignmentforum.org/posts/rvxcSc6wdcCfaX6GZ/two-senses-of-optimizer) with its environment, rather than just being about whether some number is going up. The type of computation matters: while older models of optimization involve an agent that can search over possible actions and simulate their results, other optimization processes must [control](https://www.alignmentforum.org/posts/ZDZmopKquzHYPRNxq/selection-vs-control) their environment without being able to simulate the consequences of their choice.\n\nOur use of the word “agency” [might](https://www.alignmentforum.org/posts/ZigRhB4pAGdr6beQh/deconfuse-yourself-about-agency) [be](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/DfcywmqRSkBaCB6Ma) tied to our models or specific human architectures, rather than being a general concept that could describe a mechanical property of a computation. This would be particularly worrying since it would mean that arguments for AI risk are based on our flawed models of reality, rather than an objective property about reality. However, this is extremely speculative.\n\nEmbedded agency\n---------------\n\nDiscussions about AI usually assume that a notion of the “actions” that an agent can take. However, the [embedded agency](https://www.alignmentforum.org/s/Rm6oQRJJmhGCcLvxh) sequence points out that this “Cartesian boundary” does not actually exist: since any real agent is embedded in the real world, you cannot make many assumptions that are common in reinforcement learning, such as dedicated and perfectly trusted input-output channels, a perfect model of the environment, an agent architecture that is uninfluenced by the environment, etc.\n\nThis means you can never consider all of the important information, and optimize everything that could be optimized. This has led to a couple of hypotheses:\n\n1. Real learning algorithms require [modeling assumptions](https://www.alignmentforum.org/posts/Ajcq9xWi2fmgn8RBJ/the-credit-assignment-problem) to solve the credit assignment problem, and so can only lead to [partial agency](https://www.alignmentforum.org/posts/4hdHto3uHejhY2F3Q/partial-agency) or [myopia](https://www.alignmentforum.org/posts/qpZTWb2wvgSt5WQ4H/defining-myopia). (See also this [parable](https://www.alignmentforum.org/posts/SwcyMEgLyd4C3Dern/the-parable-of-predict-o-matic) and [associated thoughts](https://www.alignmentforum.org/posts/25288usP5B5ytnzA4/random-thoughts-on-predict-o-matic).)\n2. Embedded agency works via [abstraction](https://www.alignmentforum.org/posts/hLFD6qSN9MmQxKjG5/embedded-agency-via-abstraction), which is the key idea allowing you to [make maps](https://www.alignmentforum.org/posts/t5DFpygMqpnFsmJ3b/cartographic-processes) that are smaller than the territory.\n\nValue learning\n==============\n\nDescriptive embedded agency\n---------------------------\n\nWhile the embedded agency sequence is written from the perspective of *prescribing* how ideal agents should operate, we could also aim for a theory that can *describe* real agents like humans. This involves making your theory of agency correspondingly broader: for example, moving from utility functions to [markets](https://www.alignmentforum.org/posts/WmNeCipNwg9CmGy3T/markets-are-universal-for-logical-induction) or [subagents](https://www.alignmentforum.org/posts/3xF66BNSC5caZuKyC/why-subagents), which are more general. The development of such a theory is more grounded in concrete real systems, and more likely to generate theoretical insight or counterexamples, [making it a good research meta-strategy](https://www.alignmentforum.org/posts/9pZtvjegYKBALFnLk/characterizing-real-world-agents-as-a-research-meta-strategy).\n\nSuch a theory would be [useful](https://www.alignmentforum.org/posts/zQZcWkvEA8DLjKR7C/theory-of-ideal-agents-or-of-existing-agents) so that we can build AI systems that can model humans and human values while avoiding [embedded agency problems with humans](https://www.alignmentforum.org/posts/WJzsTmsDctYCCyMfy/humans-are-embedded-agents-too).\n\nThe difficulty of value learning\n--------------------------------\n\nEven if we ignore problems of embedded agency, there are obstacles to value learning. For example, there [need not be](https://www.alignmentforum.org/posts/AeHtdxHheMjHredaq/what-you-see-isn-t-always-what-you-want) a reward function over observations that leads to what we want in POMDP (though we could instead focus on [instrumental reward functions](https://www.alignmentforum.org/posts/aAzApjEpdYwAxnsAS/reinforcement-learning-with-imperceptible-rewards) defined on states).\n\nAnother key problem is that all you ever get to observe is behavior; this then needs to be decomposed into “beliefs” and “values”, but there is [no clear criterion](https://arxiv.org/abs/1712.05812) (2017) that separates them (although it [hasn’t been proven](https://www.alignmentforum.org/posts/pHWTNMESuAEjZg2Qn/occam-s-razor-may-be-sufficient-to-infer-the-preferences-of) that simplicity doesn’t work, and [human](https://www.alignmentforum.org/posts/2KLz6RQWkCj4Rozrk/is-my-result-wrong-maths-vs-intuition-vs-evolution-in) [priors](https://www.alignmentforum.org/posts/cnjWN4mzmWzggRnCJ/practical-consequences-of-impossibility-of-value-learning) [help](http://arxiv.org/abs/1905.09397)). This suggests that [ambitious value learning](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/5eX8ko7GCxwR5N9mN), in which you identify the one true utility function, is hard.\n\nHuman models\n------------\n\nFor an agent to outperform the process generating its data, it [must](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/h9DesGT3WT9u2k7Hr) understand the ways in which that process makes mistakes. So, to outperform humans at a task given only human demonstrations of that task, you need to detect human mistakes in the demonstrations. Modeling humans to this fidelity is an [unsolved problem](https://www.alignmentforum.org/posts/upP8PYgHfXgvgh3FF/training-human-models-is-an-unsolved-problem), though there is a little [progress](https://www.alignmentforum.org/posts/xxnPxELC4jLKaFKqG/learning-biases-and-rewards-simultaneously), and we might hope that we can [make assumptions](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/EhNCnCkmu7MwrQ7yz) about the structure of the model.\n\nAny such model is likely to be misspecified, and value learning algorithms are not currently [robust](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/gnvrixhDfG7S2TpNL) to [misspecification](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/cnC2RMWEGiGpJv8go): in one case, the simpler but less conceptually accurate model is [more robust](https://arxiv.org/abs/1903.03877).\n\nYou might hope that if we give up on *outperforming* humans and just *imitate* them, this would be [safe](https://www.alignmentforum.org/posts/LTFaD96D9kWuTibWr/just-imitate-humans). Even this is controversial, because perhaps humans themselves are [unsafe](https://www.alignmentforum.org/posts/vbtvgNXkufFRSrx4j/three-ai-safety-related-ideas#1__AI_design_as_opportunity_and_obligation_to_address_human_safety_problems), maybe imitating humans [leads](https://www.lesswrong.com/posts/whRPLBZNQm3JD5Zv8/imitation-learning-considered-unsafe) to mesa optimization, or possibly perfect imitation is [too hard](https://www.alignmentforum.org/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal) to achieve.\n\nYou might also hope that AI systems have good enough models that you can simply provide [natural language instructions](https://www.alignmentforum.org/posts/Bxxh9GbJ6WuW5Hmkj/what-s-the-dream-for-giving-natural-language-commands-to-ai) and the AI does what you mean.\n\nThe presence of human models in an AI system has a few [unfortunate effects](https://www.alignmentforum.org/posts/BKjJJH2cRpJcAnP7T/thoughts-on-human-models):\n\n1. We can’t test an AI system by seeing if it agrees with human judgment, because the AI system may be using its human model to (in the short term) optimize for agreement with human judgment\n2. A bug in the code is more likely to optimize for suffering (since the human model would include the concept of suffering)\n3. If humans are modeled with sufficient fidelity, these models may themselves be conscious and capable of suffering.\n\nLearning an adequate utility function\n-------------------------------------\n\nDespite the objections that learning values is hard, it seems like humans are pretty good at learning the values of other humans, even if not perfect. Perhaps we could replicate this, in order to learn an *adequate* utility function that leads to okay outcomes?\n\nThe main issue is that we are only good at predicting human values in *normal* situations, while powerful AI systems will likely put us in extreme situations where we will disagree much more about values. As a result, we need a [theory](https://www.alignmentforum.org/posts/zvrZi95EHqJPxdgps/why-we-need-a-theory-of-human-values) of human values that defines what to do in these situations. One [theory](https://www.alignmentforum.org/posts/qezBTig6p6p5xtL6G/a-theory-of-human-values), associated [value learning agenda](https://www.alignmentforum.org/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into), and [toy model](https://www.alignmentforum.org/posts/hcrFxeYYfbFrkKQEJ/full-toy-model-for-preference-learning) propose that we can extract partial preferences from human mental models, and synthesize them together into a full utility function, while respecting meta-preferences about preferences and the synthesis process and taking care to [properly](https://www.alignmentforum.org/posts/qudmaMyRuQk2pHxtj/normalising-utility-as-willingness-to-pay) [normalize](https://www.alignmentforum.org/posts/GfMGa9e79AfDMLj36/best-utility-normalisation-method-to-date) [utilities](https://www.alignmentforum.org/posts/5bd75cc58225bf06703753ef/intertheoretic-utility-comparison-examples).\n\nIn fact, the [core pieces](https://www.alignmentforum.org/posts/m2bwD87ctjJDXC3SZ/ultra-simplified-research-agenda) of such an approach seem [necessary](https://www.alignmentforum.org/posts/TR3eqQ2fnfKWzxxHL/research-agenda-in-reverse-what-would-a-solution-look-like) for any solution to the problem. [However](https://www.alignmentforum.org/posts/GHNokcgERpLJwJnLW/some-comments-on-stuart-armstrong-s-research-agenda-v0-9), this research agenda depends upon solving many hard problems explicitly in a human-understandable way, which doesn’t jive with the [bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) that ML progress primarily happens by using more compute to solve harder problems.\n\n*I don’t agree that the core pieces identified in this research agenda must be solved before creating powerful AI, nor that we must have explicit solutions to the problems.*\n\nUncertainty over the utility function\n-------------------------------------\n\nWe could also make the AI uncertain about the utility function, and ensure that it has a way to learn about the utility function that is grounded in human behavior. Then, as an instrumental goal for maximizing expected reward, the AI will choose actions with high [expected information gain](https://www.alignmentforum.org/posts/Pkr97mB9Y4rkx5DdZ/utility-uncertainty-vs-expected-information-gain). While this was proposed [earlier](https://arxiv.org/abs/1606.03137) (2016), the book [Human Compatible](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem-ebook/dp/B07N5J5FTS) ([summary](https://www.alignmentforum.org/posts/nd692YfFGfZDh9Mwz/an-69-stuart-russell-s-new-book-on-why-we-need-to-replace), [podcast 1](https://futureoflife.org/2019/10/08/ai-alignment-podcast-human-compatible-artificial-intelligence-and-the-problem-of-control-with-stuart-russell/), [podcast 2](https://futureoflife.org/2019/01/17/cooperative-inverse-reinforcement-learning-with-dylan-hadfield-menell/), [interview](https://www.vox.com/future-perfect/2019/10/26/20932289/ai-stuart-russell-human-compatible)) explores the idea in much more detail than previous writing, and it has now [made its way](https://interactive-learning.github.io/) into deep reinforcement learning as well.\n\nIntuitively, since the AI is [uncertain](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/ZiLLxaLB5CCofrzPp) about the true reward, it will behave conservatively and try to learn about the true reward, thus [avoiding](https://www.alignmentforum.org/posts/PADPJ3xac5ogjEGwA/defeating-goodhart-and-the-closest-unblocked-strategy) Goodhart’s law (see also [fuzziness](https://www.alignmentforum.org/posts/QJwnPRBBvgaeFeiLR/uncertainty-versus-fuzziness-versus-extrapolation-desiderata)). Of course, once the AI has learned everything there is to learn, it will [behave](https://arbital.com/p/updated_deference/) (2015?) just like a regular utility maximizer. In this setting, you would hope that the AI has [become aligned](https://www.alignmentforum.org/posts/Pkr97mB9Y4rkx5DdZ/utility-uncertainty-vs-expected-information-gain#FGSgRJdrewhEGXRW9) with the true utility function, as long as its initial distribution over utility functions contains the truth, and the observation model by which its distribution is updated is “correct”. However, it might be [quite](https://www.alignmentforum.org/posts/YJq6R9Wgk5Atjx54D/does-bayes-beat-goodhart) [difficult](https://www.alignmentforum.org/posts/FuGDYNvA6qh4qyFah/thoughts-on-human-compatible) to ensure that these actually hold. This also depends on the assumption that there *is* a true utility function, and that the human *knows* it, which is not the case, though this is [being addressed](https://arxiv.org/abs/1901.08654).\n\nOne important feature of this agenda is that rather than requiring a perfect utility function to begin with, the AI can learn the utility function by interacting with the human; such a feedback mechanism can make a problem [much easier](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/4783ufKpx8xvLMPc6). Interaction also opens up other possibilities, such as learning human [norms](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/eBd6WvzhuqduCkYv3) instead of values. However, it is computationally difficult, and so more [research](https://www.alignmentforum.org/posts/dBMC63hjkc5wPqTC7/human-ai-collaboration) would be needed to make it a viable solution.\n\nCurrent methods for learning human preferences\n----------------------------------------------\n\nThere has been a lot of practical work on learning human preferences, including:\n\n* Building a mistake model by comparing demonstrations of varying degrees of optimality (either by getting humans to [rank](https://arxiv.org/abs/1904.06387) demonstrations or by [introducing noise](https://arxiv.org/abs/1907.03976) into optimal demonstrations)\n* Learning human-interpretable [representations](https://arxiv.org/abs/1905.12686) in order to advise humans\n* New forms of human [guidance](http://arxiv.org/abs/1909.09906), such as having humans provide the [advantage](http://arxiv.org/abs/1902.04257) function (instead of the reward), following [natural language](https://research.fb.com/wp-content/uploads/2019/07/Why-Build-an-Assistant-in-Minecraft-v3.pdf) [instructions](https://arxiv.org/abs/1903.02020), and learning from the (human-optimized) [initial state](https://www.alignmentforum.org/posts/7f6DNZhracD7RvxMr/learning-preferences-by-looking-at-the-world)\n* Learning preferences [for natural language](https://openai.com/blog/fine-tuning-gpt-2/) (rather than the standard RL setup)\n* [Combining](https://arxiv.org/abs/1811.06521) [multiple types](https://rohinshah.com/wp-content/uploads/2019/12/Reward_Combination_NeurIPS_2019_Workshop_Camera_Ready.pdf) of human feedback\n* [General](http://arxiv.org/abs/1909.13392) [improvements](https://arxiv.org/abs/1905.12888) [in](https://arxiv.org/abs/1905.11979) [imitation](https://arxiv.org/abs/1905.11108) [learning](http://arxiv.org/abs/1810.06544) and [inverse](https://arxiv.org/abs/1911.00459) [reinforcement](http://arxiv.org/abs/1902.07742) [learning](http://arxiv.org/abs/1809.06404).\n\nThere are many recent papers that I haven’t cited here, as it is a very large area of work.\n\nRobustness\n==========\n\nSafe reinforcement learning\n---------------------------\n\nWe would like to ensure that our AI systems do not make mistakes during training. With preference learning, we can do this by learning human preferences over [hypothetical behaviors](https://deepmind.com/blog/article/learning-human-objectives-by-evaluating-hypothetical-behaviours) that are not actually executed. Another option is to provide safety constraints and ensure that the AI [never violates them](http://arxiv.org/abs/1910.01074) (even during training), or at least to [significantly](https://openai.com/blog/safety-gym/) [reduce](https://people.eecs.berkeley.edu/~jfisac/papers/Bridging_Safety_and_RL.pdf) such violations.\n\nAvoiding *all* mistakes would require us to have a formal specification of what a “mistake” is, or to have some [overseer](https://intelligence.org/2019/04/24/delegative-reinforcement-learning/) that can identify “mistakes” before execution, so that our AI could avoid the mistake even though it hasn’t seen this situation before. *This seems prohibitively hard to me if we include literally all “mistakes”.*\n\nAdversarial examples\n--------------------\n\nAdversarial examples are a clear demonstration of how the “cognition” of neural nets is different from our own: by making superficial changes to the input that would not matter to a human, you can completely change the output of the neural net. While I am not an expert here, and certainly have not read the huge mountains of work done over the last year, I do want to highlight a few things.\n\nFirst, while we might nominally think of adversarial examples as “bugs” in our neural net, this [paper](http://gradientscience.org/adv/) shows that image classifiers are picking up *real* imperceptible features that *do* generalize to the test set. The classifiers really are maximizing predictive accuracy; the problem is that we want them to predict labels based on the features that *we* use, instead of imperceptible (but predictive) features. Adversarial training removes these fragile features, leaving only the robust features; this makes [subsequent](http://gradientscience.org/robust_reps/) [applications](http://gradientscience.org/robust_apps/) easier.\n\nWhile the paper was controversial, I thought that its main thesis seemed to be supported even after reading [these six responses](https://distill.pub/2019/advex-bugs-discussion/).\n\nSecond, there has been a distinct shift away from the L-infinity norm ball threat model of adversarial examples. So far, it seems that robustness to one set of perturbations doesn’t grant robustness to other perturbations, prompting the development of [multiple perturbations](https://openai.com/blog/testing-robustness/), a benchmark of [natural adversarial examples](http://arxiv.org/abs/1907.07174), and new [evaluation metrics](https://arxiv.org/abs/1902.08265). While the L-infinity norm ball is an interesting [unsolved research problem](https://medium.com/@catherio/unsolved-research-problems-vs-real-world-threat-models-e270e256bc9e), it is in no way a realistic threat model.\n\nThird, [adversarial](https://arxiv.org/abs/1812.01647) [attacks](https://arxiv.org/abs/1905.10615) are now being proposed as a method for evaluating how robust an agent trained by reinforcement learning is. This seems especially important since in RL there is often no train-test split, and so it is hard to tell whether an agent has “memorized” a single trajectory or actually learned a policy that works well across a variety of circumstances.\n\nIntent alignment\n----------------\n\nUltimately, robustness [seeks to](https://medium.com/@deepmindsafetyresearch/towards-robust-and-verified-ai-specification-testing-robust-training-and-formal-verification-69bd1bc48bda) identify and eliminate all “bugs”, i.e. behaviors that are inconsistent with the specification (see also this [podcast](https://80000hours.org/podcast/episodes/pushmeet-kohli-deepmind-safety-research/)). Instead of considering all the mistakes, we could seek to only prevent [catastrophic mistakes](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd/p/qALeGJ9nPcs9eC9Af), and ensure that the AI is *intent aligned*, that is, it is always *[trying](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd/p/ZeE7EKHTFMBs8eMxn)* to do what we want. This goal [avoids](https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment#3ECKoYzFNW2ZqS6km) many of the pitfalls around the goal of designing an AI with the right utility function.\n\nCorrigibility\n-------------\n\nOne promising way in which an AI could be intent aligned is by being [corrigible](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd/p/fkLYhTQteAu5SinAc): roughly, the AI is not trying to deceive us, it clarifies its uncertainty by asking us, it learns about our preferences, it shuts down if we ask it to, etc. This is a *narrower* concept than intent alignment: an AI that infers our “true” utility function and optimizes it may wrest control away from us in order to expand faster, or make us safer; such an AI would be [aligned but not corrigible](https://www.lesswrong.com/posts/o22kP33tumooBtia3/can-corrigibility-be-learned-safely#YgLfCHknM4Ng2hGvB). There are a few benefits of using corrigibility:\n\n1. It can be achieved with relatively low levels of intelligence (we can imagine corrigible humans)\n2. It seems to have a positive feedback loop (that is, an AI that reaches some “threshold” of corrigibility would tend to become more corrigible)\n3. It doesn’t seem to require any domain expertise.\n\n(A similar [idea](https://www.alignmentforum.org/posts/F9vcbEMKW48j4Z6h9/non-consequentialist-cooperation) would be to build an AI system that only takes actions that the overseer has given informed consent for.)\n\nNote that [MIRI’s notion of corrigibility](http://intelligence.org/files/Corrigibility.pdf) (2015) is similar but much stricter. My guess is that MIRI wants the same intuitive corrigibility properties, but wants them to be created by a *simple* change to the *utility function*. Simplicity helps ensure that it cannot be gamed, and the utility function means that you are changing what the AI cares about, rather than trying to constrain a powerful superintelligence. For example, I’d guess that MIRI-corrigibility can depend on whether a shutdown button is pressed, but cannot depend on the *reasons* for which the shutdown button is pressed.\n\nIf you set aside the utility function requirement, then this property can be achieved using [constrained optimization](https://www.alignmentforum.org/posts/cGLgs3t9md7v7cCm4/corrigibility-as-constrained-optimisation): the agent can optimize normally when the button is not pressed, while ensuring that it is still able to shut down if necessary, and it can optimize for shutting down if the button is pressed. If you set aside the simplicity requirement, then you can define the desired policies and [recover](https://www.alignmentforum.org/posts/XkuRKqXKAaMySbXCN/indifference-multiple-changes-multiple-agents) the correct utility function. But from now on I’m only going to talk about the notion of corrigibility I first introduced.\n\nIt has been [argued](https://www.lesswrong.com/posts/Djs38EWYZG8o7JMWY/paul-s-research-agenda-faq#79jM2ecef73zupPR4) that while corrigibility is simpler than “human values”, it is a “non-natural” type of cognition, such that you are unlikely to be able to find corrigible intelligences with machine learning. *(I do not feel the force of this intuition; I agree much more with the earlier intuitions.)*\n\nYou might be worried that since a corrigible AI defers to us, if we were about to take a suboptimal action that we couldn’t tell was suboptimal, the AI wouldn’t stop us from doing so because it can’t explain to us what would be bad about the world. However, at the very least, [it can say](https://www.alignmentforum.org/posts/rArsypGqq49bk4iRr/can-there-be-an-indescribable-hellworld) “this is bad for reasons I can’t fully explain”.\n\nWorst case guarantees\n---------------------\n\nWe still want to guarantee that there will *never* be a failure of corrigibility, which can’t be done with regular ML techniques, which only give an [average-case guarantee](https://ai-alignment.com/two-guarantees-c4c03a6b434f). In order to get a worst-case guarantee, we need other techniques. [One](https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d) [proposal](https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment) is to use adversarial training to find abstracted inputs on which the agent is incorrigible, where the adversary is aided by interpretability techniques that allow the adversary to understand what the agent is thinking. It would be particularly nice to find a [mechanistic description](https://www.alignmentforum.org/posts/BKM8uQS6QdJPZLqCr/towards-a-mechanistic-understanding-of-corrigibility) of corrigibility, as that would make it easier to verify the absence of incorrigible behavior.\n\nCritics argue that this could never work because machine learning [wouldn’t learn the “intended” interpretation](https://www.lesswrong.com/posts/Djs38EWYZG8o7JMWY/paul-s-research-agenda-faq#79jM2ecef73zupPR4) of corrigibility, and could be [adversarial](https://www.lesswrong.com/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal). *I don’t think this objection is critical. It seems like it is saying that ML will fail to generalize and there will be situations in which the concept of corrigibility breaks down, but the entire point of adversarial training is to find these situations and train the agent away from it.*\n\n*While this is usually tied in to the broader iterated amplification agenda, it seems to me that solving just this subproblem would achieve a lot of the value of the full agenda. If we had a way of applying adversarial training to an arbitrary AI agent, such that we are very likely to find potential inputs on which the agent is incorrigible, then presumably AI systems that could be incorrigible would not be deployed. Iterated amplification adds additional safety in that it (hopefully) allows you to assume a smarter, already-aligned adversary, whereas a direct solution to this subproblem would have an approximately-as-capable, not-automatically-aligned adversary, which would probably not have a worst-case guarantee but might still be good enough.*\n\nScaling to superhuman abilities\n===============================\n\nIterated amplification\n----------------------\n\nIterated amplification carves out a [broad class of algorithms](https://www.alignmentforum.org/posts/HBGd34LKvXM9TxvNf/new-safety-research-agenda-scalable-agent-alignment-via#WJC9Q5MvzqJeNSnLx) that can scale to superhuman abilities, with the hope that we can analyze the alignment properties of the entire class of algorithms at once. Algorithms in this class have two components:\n\n1. Amplification, which increases an agent’s capabilities, at the cost of efficiency.\n2. Distillation, which increases an agent’s efficiency, at the cost of capability.\n\nGiven this, starting from some base agent, the algorithm alternates amplification and distillation, to get successively more capable agents, as long as each component is [good enough](https://www.lesswrong.com/posts/di8H7rEAnzXC97Dvu/progress-and-preservation-in-ida).\n\nGiven this broad class of algorithms, we can [instantiate](https://www.alignmentforum.org/posts/cYduioQNeHALQAMre/what-are-the-differences-between-all-the-iterative-recursive) many specific algorithms by picking a specific amplification step and a specific distillation step. For example, the amplification step can be done by allowing an overseer to *decompose* the problem into subproblems, which is especially promising for question answering. Distillation could be done using [supervised learning](https://www.alignmentforum.org/posts/xKvzpodBGcPMq7TqE/supervising-strong-learners-by-amplifying-weak-experts), imitation learning, or [reinforcement learning](https://www.alignmentforum.org/posts/fq7Ehb2oWwXtZic8S/reinforcement-learning-in-the-iterated-amplification).\n\n[Recursive reward modeling](https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84) ([podcast](https://futureoflife.org/2019/12/16/ai-alignment-podcast-on-deepmind-ai-safety-and-recursive-reward-modeling-with-jan-leike/)) is another algorithm that could allow us to scale to superhuman abilities. It can be cast as an algorithm in the iterated amplification class by considering an amplification step that takes agents that can evaluate some set of tasks, and builds new human-agent teams that can evaluate some more complex set of tasks. The distillation step would then be reinforcement learning, to get an agent that can directly solve the more complex tasks. Iterating this eventually leads to an agent that can solve the original desired task.\n\nIterated amplification does impose a particular structure on algorithms, which can be [applied](https://www.alignmentforum.org/posts/Y9xD78kufNsF7wL6f/machine-learning-projects-on-ida) to existing ML problems. However, this may be [uncompetitive](https://www.alignmentforum.org/posts/jYdAxH8BarPT4fqnb/a-dilemma-for-prosaic-ai-alignment) if the best ML algorithms require different algorithmic structures or different environments, in order to reach high capabilities (though we could then train a question-answering system [alongside](https://www.alignmentforum.org/posts/jYdAxH8BarPT4fqnb/a-dilemma-for-prosaic-ai-alignment#K8fRPa9NWZXdARLYN) the other algorithm / environment, which plausibly doesn’t take too many more resources).\n\nThe [iterated amplification](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd) sequence, [recursive reward modeling](https://arxiv.org/abs/1811.07871) paper, and [these](https://www.lesswrong.com/posts/Djs38EWYZG8o7JMWY/paul-s-research-agenda-faq) [posts](https://www.alignmentforum.org/posts/FdfzFcRvqLf4k5eoQ/list-of-resolved-confusions-about-ida) help explain the full agenda better.\n\nQuantilization\n--------------\n\n[Quantilization](https://intelligence.org/files/QuantilizersSaferAlternative.pdf) (2015) allows you to amplify a base policy by randomly selecting among the top 1/Q of actions the base policy could take, at a cost of at most Q-fold increase in risk. However, this can [forgo benefits](https://www.alignmentforum.org/posts/Rs6vZCrnQFWQ4p37P/when-to-use-quantilization) of the rest of the base policy. Since quantilization increases risk, it cannot be safely iterated: for example, if you start with a policy with a worst-case 1% chance of failure, and you 5-quantilize it, you now have a worst-case 5% chance of failure. After two more iterations of 5-quantilization, there is no longer a worst-case bound on failure probability.\n\nDebate\n------\n\nAnother mechanism for scaling beyond humans is [debate](https://arxiv.org/abs/1805.00899) ([podcast](https://futureoflife.org/2019/03/06/ai-alignment-through-debate-with-geoffrey-irving/)), in which an AI agent is trained via self-play in a zero-sum game in which its goal is to “win” the debate, as evaluated by a human judge. The key hope is that detecting a lie is easier than lying: if one of the players lies or deceives or manipulates the human, then the other player can reveal that and thereby win the debate. If this were true, we would expect that the equilibrium behavior is for the agent to provide honest, useful information.\n\nSince its proposal, debate has been [tested](https://www.alignmentforum.org/posts/5Kv2qNfRyXXihNrx2/ai-safety-debate-and-its-applications) with MNIST and Fashion MNIST, as well as [question answering](https://arxiv.org/abs/1909.05863). There is also a [proposal](https://www.alignmentforum.org/posts/jYvm4mmjvGHcPXtGL/a-concrete-proposal-for-adversarial-ida) to use it to improve iterated amplification.\n\n[Theoretical work](https://medium.com/@RyanCarey/new-paper-when-is-truth-telling-favored-in-ai-debate-8f58f14562e5) brings up the possibility of questions that are “too hard”: while sufficiently long “feature debates” are provably truth-seeking (because the debaters can reveal all of their information), it is possible to construct complex questions in which the debate doesn’t find the right answer. However, the results [don’t generalize well](https://www.alignmentforum.org/posts/RQoSCs9SePDMLJvfz/new-paper-when-is-truth-telling-favored-in-ai-debate#gCeKuJ62HmLtPB9C9) from feature debates to real debates.\n\nRelatedly, even if it is easy to detect lies, it’s not clear what would happen with [ambiguous questions](https://www.alignmentforum.org/posts/fNTCveSa4HvqvZR2F/problems-with-ai-debate).\n\nSince debate doesn’t involve alternating between increasing capabilities and increasing efficiency, it isn’t an [instance](https://www.alignmentforum.org/posts/HBGd34LKvXM9TxvNf/new-safety-research-agenda-scalable-agent-alignment-via#WJC9Q5MvzqJeNSnLx) of iterated amplification. However, both iterated amplification and debate are aiming to compute the answer that an exponentially large tree of bounded humans would arrive at (see next section), and so it seems likely that either they would both work, or neither would work.\n\nFactored cognition\n------------------\n\nBoth iterated amplification and debate depend on the [factored](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd/p/DFkGStzvj3jgXibFG) [cognition](https://ought.org/presentations/delegating-cognitive-work-2019-06) hypothesis: that arbitrarily complex tasks can be performed arbitrarily well by a [giant tree](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd/p/NXqs4nYXaq8q6dTTx) of bounded base agents, possibly extended with features like shared external memory or [long-lived assistants](https://ai-alignment.com/strong-hch-bedb0dc08d4e) (2016).\n\nIterated amplification checks local nodes in a tree of considerations and broken-down questions, in which an assistant at level k decomposes its questions, gets answers from assistants at level k-1, and combines them into an overall answer. Meanwhile, in debate, if the two agents disagree, they will play down the most difficult / contested path in an exponential tree of arguments and counterarguments, so the debate training procedure is checking a single path from root to leaf in the exponential tree.\n\nIt is an open question whether the factored cognition hypothesis is true. [Empirical work](https://ought.org/updates/2019-10-28-progress-update) has been scaling up, and we should hopefully have some informative evidence in the upcoming year.\n\nThe main reasons people are skeptical of the hypothesis are because it seems that sufficiently complex tasks require building up [big contexts](https://www.alignmentforum.org/posts/J7Rnt8aJPH7MALkmq/vaniver-s-view-on-factored-cognition) or using globally-constructed intuitions or [“inexplicable flashes of insight”](https://www.alignmentforum.org/posts/4qY9zEHLa2su4PkQ4/can-hch-epistemically-dominate-ramanujan). This could be done if the “small” agents simulated an arbitrary Turing Machine, but this would [lose](https://www.lesswrong.com/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal) any guarantees of alignment.\n\nHowever, we might expect that these tasks could still be done by a tree of humans: humans are [allowed](https://www.alignmentforum.org/posts/LigbvLH9yKR5Zhd6y/what-s-wrong-with-these-analogies-for-understanding-informed) to use a heuristic “just because it works”; this should allow the tree of humans to use heuristics that other agents use, including “inexplicable flashes of insight”.\n\nUniversality\n============\n\nAlignment of the tree of humans\n-------------------------------\n\nIn order for this tree of humans to be aligned (a necessary condition for iterated amplification or debate to be aligned), the initial agent must already be aligned, and putting the agents together must not destroy alignment. One [intuition](https://www.lesswrong.com/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal) that this is hard is that alignment is not compositional; a “big” agent made up of “small” aligned agents need not be aligned. However, the hope doesn’t depend on compositionality of alignment; it instead depends on ensuring that your agents never do incorrigible optimization.\n\nIn addition, it could be the case that “large” initial agents like humans (or human imitations) are not robustly aligned, because there may be some [clever argument](https://ai-alignment.com/universality-and-security-amplification-551b314a3bab) that causes them to behave incorrigibly. One response would be to use [low-bandwidth overseers](https://www.alignmentforum.org/posts/yxzrKb2vFXRkwndQ4/understanding-iterated-distillation-and-amplification-claims) as the initial agent, who only answer very “small” questions on which we are relatively confident that there are no such failures. We would also hope to [train](https://www.alignmentforum.org/posts/4JuKoFguzuMrNn6Qr/hch-is-not-just-mechanical-turk) humans to properly decompose questions and behave corrigibly, so that putting together several humans remains corrigible (a task for which we need [social scientists](https://blog.openai.com/ai-safety-needs-social-scientists/)).\n\nNote that it is only competitive to approximate the tree of humans with iterated amplification if we expect that any powerful AI systems will also be trained in a manner similar to iterated amplification. If we instead consider a [model](https://www.alignmentforum.org/posts/H5gXpFtg93qDMZ6Xn/aligning-a-toy-model-of-optimization) in which ML perfectly optimizes a function (rather than performing iterated local search), then iterated amplification would be far more expensive than unaligned powerful AI systems. It would be worth studying this simpler model to see if alignment is possible there.\n\nAscription universality\n-----------------------\n\nEven if we know that the tree of humans is aligned, we also need to ensure that the model trained from oversight from the tree of humans will also be aligned. The key claim in favor of this is that HCH (the tree of humans) is *universal*, that is, it “knows” any facts that a sufficiently smaller computation “knows”. This was formalized [here](https://ai-alignment.com/towards-formalizing-universality-409ab893a456) and applied to [multiple](https://ai-alignment.com/informed-oversight-18fcb5d3d1e1) [problems](https://ai-alignment.com/universality-and-model-based-rl-b08701394ddd), including the problem that malign optimization might [emerge](https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd) *within* HCH. While a good explanation of this is out of scope here, I summarized these posts [here](https://www.alignmentforum.org/posts/3kzFPA5uuaGZWg4PS/an-81-universality-as-a-potential-solution-to-conceptual). Ascription universality does have to be [applied](https://www.alignmentforum.org/posts/R5Euq7gZgobJi5S25/nuances-with-ascription-universality) to the entire training process and not just the final model.\n\nInterpretability\n================\n\nSince we want to be able to “know everything the model knows”, and also to be able to find situations under with a model behaves corrigibly (see worst case guarantees above), it would be very useful to be able to peer inside our models and understand what they are doing. It would be particularly useful to be able to identify optimization processes and [understand](https://www.alignmentforum.org/posts/Zj2PgP5A8vY2G3gYw/optimization-provenance) how they come about.\n\nEven though interpretability tools probably could not [deal with](https://www.alignmentforum.org/posts/J9D6Bi3eFDDhCaovi/will-transparency-help-catch-deception-perhaps-not) already deceptive models, since the deceptive models could figure out how to fool the tools, it seems likely that interpretability could help [prevent](https://www.alignmentforum.org/posts/J9D6Bi3eFDDhCaovi/will-transparency-help-catch-deception-perhaps-not#yn5YcLnL6vs6AxxAE) deception from ever arising -- hopefully an easier task.\n\nHowever, interpretability has other uses besides catching problems: it could also be [used](https://www.alignmentforum.org/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety) to get more understandable models during training, provide feedback on the *process* by which a model makes a decision (rather than feedback on just the decision), or create ML techniques that help us understand the world without acting in it (thus avoiding problems with agential AI).\n\nUnfortunately, I haven’t kept up with interpretability research, so I can’t say how it’s progressed recently, but one paper you could start with is [activation atlases](https://distill.pub/2019/activation-atlas/).\n\nImpact regularization\n=====================\n\nImpact measures\n---------------\n\nIn 2018, there was a lot of progress on proposing specific impact measures, including [relative reachability](https://arxiv.org/abs/1806.01186) and [attainable utility preservation](https://www.alignmentforum.org/posts/yEa7kwoMpsBgaBCgb/towards-a-new-impact-measure) ([followup](https://www.alignmentforum.org/posts/mDTded2Dn7BKRBEPX/penalizing-impact-via-attainable-utility-preservation), [paper](https://arxiv.org/abs/1902.09725)). These were recently [unified](https://medium.com/@deepmindsafetyresearch/designing-agent-incentives-to-avoid-side-effects-e1ac80ea6107) as using similar underlying algorithms but with different “deviation measures”: the former considers the change in number of reachable states, whereas the latter considers the change in attainable utility (for some set of utility functions).\n\nThese [two](https://www.alignmentforum.org/posts/TPy4RJvzogqqupDKk/a-survey-of-early-impact-measures) [posts](https://www.alignmentforum.org/posts/pf48kg9xCxJAcHmQc/understanding-recent-impact-measures) summarize the work on impact (going back till 2012).\n\nWhat is impact, anyway?\n-----------------------\n\nThe [Reframing Impact](https://www.alignmentforum.org/s/7CdoznhJaLEKHwvJW) sequence aims to build intuitions about what we mean by “impact”, and concludes that an action is impactful if it changes our ability to get what we want. Of course, this definition depends on “what we want”, whereas usually with impact regularization we want something that is [easy to specify](https://www.alignmentforum.org/s/7CdoznhJaLEKHwvJW/p/xCxeBSHqMEaP3jDvY). However, we might hope that impact is relatively goal-agnostic, because for most goals you need to pursue the same convergent instrumental subgoals. In particular, we might hope for a formalizable notion of *power*, that attainable utility preservation could penalize.\n\nTo better distinguish between different definitions and techniques for measuring impact, this [post](https://www.alignmentforum.org/posts/wzPzPmAsG3BwrBrwy/test-cases-for-impact-regularisation-methods) proposes several test cases for impact regularization.\n\nUtility of impact measures\n--------------------------\n\nThe mainline use case for impact regularization is to be an “additional layer of defense”: if for some reason we fail to align an AI system, then hopefully there still won’t be catastrophic consequences, because the AI system only takes low-impact actions. However, this may fail to work for a [variety](https://www.alignmentforum.org/posts/kCY9dYGLoThC3aG7w/best-reasons-for-pessimism-about-impact-of-impact-measures) of [reasons](https://www.alignmentforum.org/posts/zrunBA8B5bmm2XZ59/reversible-changes-consider-a-bucket-of-water). Still, work on impact measures could be [useful](https://www.alignmentforum.org/posts/wJK944YqvFwjdbqCP/four-ways-an-impact-measure-could-help-alignment) for deconfusion, testing protocols, temporary alignment measures, or [value-neutrality verification](https://www.alignmentforum.org/posts/jGB7Pd5q8ivBor8Ee/impact-measurement-and-value-neutrality-verification-1).\n\nCausal modeling\n===============\n\n[Causal influence diagrams](https://medium.com/@deepmindsafetyresearch/understanding-agent-incentives-with-causal-influence-diagrams-7262c2512486) help us understand what a training *process* does. Given a causal influence diagram, we can determine *observation incentives* (what an agent would like to know) and *intervention incentives* (what an agent would like to change). We can produce such diagrams for [AGI safety frameworks](https://arxiv.org/abs/1906.08663), and [analyze](https://medium.com/@deepmindsafetyresearch/designing-agent-incentives-to-avoid-reward-tampering-4380c1bb6cd) solutions to reward function tampering, user feedback tampering, and observation tampering. For example, it allows us to show that if the agent’s plans are evaluated by the current reward, then there is no incentive for the agent to tamper with its reward function.\n\nThe variables of the diagrams represent important components of the agent and the environment (such as reward functions and dynamics models in the agent, and the user’s preferences and the state of the world in the environment). Different ways of combining these into agent setups lead to different causal influence diagrams. The incentive analysis enables the designer to choose agent setups with good incentive properties.\n\nHowever, the causal models themselves are not uniquely determined. For example, what counts as wireheading is [relative](https://www.alignmentforum.org/posts/BvctuKocyWR4YYea3/wireheading-is-in-the-eye-of-the-beholder) to the stance taken towards the system and its desired goals. For example, if you [define](https://www.alignmentforum.org/posts/vXzM5L6njDZSf4Ftk/defining-ai-wireheading) it as taking control of some “narrow measurement channel”, then what is a measurement channel and what the goal is depends on modeling assumptions.\n\nOracles\n=======\n\nOracles also benefit from reasoning about causality and influences. A system that maximizes predictive accuracy ends up choosing [self-confirming predictions](https://www.alignmentforum.org/posts/KoEY9CjrKe93ErYhd/self-confirming-predictions-can-be-arbitrarily-bad), which can be arbitrarily bad. (This affects [self-supervised learning](https://www.alignmentforum.org/posts/L3Ryxszc3X2J7WRwt/self-supervised-learning-and-manipulative-predictions) in addition to oracles.) You might hope to avoid this by [preventing](https://www.alignmentforum.org/posts/RmPKdMqSr2xRwrqyE/the-dualist-predict-o-matic-usd100-prize) the AI system from being aware of itself, but this [doesn’t work](https://www.alignmentforum.org/posts/yArZKCEheZt8GkK6p/self-fulfilling-prophecies-aren-t-always-about-self).\n\nInstead, we could ensure that the oracle makes predictions conditional on the predictions [not influencing anything](https://www.alignmentforum.org/posts/wJ3AqNPM7W4nfY5Bk/self-confirming-prophecies-and-simplified-oracle-designs) (using [randomization](https://www.alignmentforum.org/posts/yAiqLmLFxvyANSfs2/counterfactual-oracles-online-supervised-learning-with) to do so). There are still [other problems](https://www.alignmentforum.org/posts/jhSjP3QLKPc5AqumD/problems-with-counterfactual-oracles) besides self-confirming predictions, such as [acausal](https://www.alignmentforum.org/posts/42z4k8Co5BuHMBvER/breaking-oracles-hyperrationality-and-acausal-trade) [trade](https://www.alignmentforum.org/posts/6WbLRLdmTL4JxxvCq/analysing-dangerous-messages-from-future-ufai-via-oracles).\n\nDecision theory\n===============\n\nThere’s been a lot of work exploring the intuitions behind decision theory. Since I [don't follow](https://www.alignmentforum.org/posts/uKbxi2EJ3KBNRDGpL/comment-on-decision-theory#pNrynCrozQPj3tFws) decision theory closely, I’m not going to try and summarize the conversation, and instead you get a list of posts: [pro CDT](https://www.alignmentforum.org/posts/CvBn9vNL65AMhAAs6/build-a-causal-decision-theorist), [anti CDT](https://www.alignmentforum.org/posts/wkNQdYj47HX33noKv/cdt-dutch-book), [anti FDT](https://www.alignmentforum.org/posts/ySLYSsNeFL5CoAQzN/a-critique-of-functional-decision-theory), [actually it all depends on counterfactuals](https://www.alignmentforum.org/posts/WkPf6XCzfJLCm2pbK/cdt-edt-udt), [anti UDT](https://www.alignmentforum.org/posts/9sYzoRnmqmxZm4Whf/conceptual-problems-with-udt-and-policy-selection) because of [commitment races](https://www.alignmentforum.org/posts/brXr7PJ2W4Na2EW2q/the-commitment-races-problem), [UDT doesn’t work with AIXI](https://www.alignmentforum.org/posts/mrZp6qC7DDXKQZeeC/failures-of-udt-aixi-part-1-improper-randomizing), strange reasoning in [Troll Bridge](https://www.alignmentforum.org/posts/hpAbfXtqYC2BrpeiC/troll-bridge-5), a [comparison](https://www.alignmentforum.org/posts/QPhY8Nb7gtT5wvoPH/comparison-of-decision-theories-with-a-focus-on-logical) across decision theories, [counterfactual](https://www.alignmentforum.org/posts/EAqHkKtbefvyRs4nw/counterfactual-induction) [induction](https://www.alignmentforum.org/posts/xBoBmPtgvwdfqm2r5/counterfactual-induction-algorithm-sketch-fixpoint-proof) [posts](https://www.alignmentforum.org/posts/Cu4v9MHGuhLnDQTuF/counterfactual-induction-lemma-4). There’s also been some discussion of why people care about decision theory: it is useful for [improving rationality, finding problems](https://www.alignmentforum.org/posts/JSjagTDGdz2y6nNE3/on-the-purposes-of-decision-theory-research), and [deconfusion](https://www.alignmentforum.org/posts/uKbxi2EJ3KBNRDGpL/comment-on-decision-theory).\n\nRelatedly, this [paper](https://foundational-research.org/approval-directed-agency-and-the-decision-theory-of-newcomb-like-problems/) characterizes the decision theories of existing agents, and this [post](https://www.alignmentforum.org/posts/XTgkhjNTEi97WHMi6/pavlov-generalizes) explains how “Pavlov” strategies (similar to reinforcement learning) can work well with game theory.\n\nAs we get to the end of the technical alignment section, I want to mention [BoMAI](https://www.alignmentforum.org/posts/pZhDWxDmwzuSwLjou/asymptotically-unambitious-agi), which didn’t fit in any of the sections. BoMAI is an AIXI-like system that does not seek power, because it only cares about reward until the end of the episode (myopia), and during the episode it is confined to a box from which information cannot leave. Such an AI system can still be useful because there is also a human in the box, who can transmit information to the outside world after the episode has ended.\n\nStrategy and coordination\n=========================\n\nSo far I’ve been talking about the technical work on the alignment problem. Let’s now switch to more “meta” work that tries to predict the future in order to prioritize across research topics.\n\nContinuous vs discontinuous takeoff\n-----------------------------------\n\nA central disagreement among AI researchers is about how “quickly” AI improves once it reaches human level. Recently, the question has been distilled to whether there will be a *discontinuity* in AI capabilities. As a result, I will ask whether takeoff will be *continuous* or *discontinuous* (as opposed to *slow* or *fast*).\n\nOne operationalization of this question is whether there will be a 4-year doubling of GDP that ends before the first 1-year doubling of GDP starts. Note that continuous takeoff need not be slow: to get to 4-year doubling, you need superexponential growth. Under exponential growth, the doubling time stays fixed at its current value of a few decades. Extrapolating [historical growth trends](https://aiimpacts.org/historical-growth-trends/) (which “supports the possibility of radical increases in growth rate”) would still (probably) be compatible with this operationalization.\n\n[Two](https://sideways-view.com/2018/02/24/takeoff-speeds/) [posts](https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/) argue for continuous takeoff; the main argument is that continuity is very likely for properties that people care about, since lots of people are trying to make progress on the property, and it is less likely that we quickly invest much more effort into making progress on the property. So far, there [has not been](https://www.lesswrong.com/posts/PzAnWgqvfESgQEvdg/any-rebuttals-of-christiano-and-ai-impacts-on-takeoff-speeds) a compelling response, but this [does not mean](https://www.lesswrong.com/posts/PzAnWgqvfESgQEvdg/any-rebuttals-of-christiano-and-ai-impacts-on-takeoff-speeds#rXh6LNmoz64mLv2kX) that researchers agree.\n\nThere has been some discussion of particular properties that make discontinuous takeoff seem more likely (though I would guess that they are not the arguments that MIRI researchers would make). For example, perhaps we just need to find the one correct architecture, which will then cause a discontinuity, but note that birds and primates have [independently evolved](https://aiimpacts.org/primates-vs-birds-is-one-brain-architecture-better-than-the-other/) neural architectures that both work well.\n\nAlternatively, AI systems with different explicit utility functions could [cooperate by merging](https://www.alignmentforum.org/posts/gYaKZeBbSL4y2RLP3/strategic-implications-of-ais-ability-to-coordinate-at-low) to pursue a joint utility function, making them much more effective at coordination than humans, allowing them to [avoid](https://www.alignmentforum.org/posts/Sn5NiiD5WBi4dLzaB/agi-will-drastically-increase-economies-of-scale) principal-agent problems that plague human corporations. This could lead to a discontinuous jump. AI systems could also [build monopolies](https://www.alignmentforum.org/posts/dt4z82hpvvPFTDTfZ/six-ai-risk-strategy-ideas) through such coordination to obtain a decisive strategic advantage.\n\nWe could also [expect](https://www.alignmentforum.org/posts/Sn5NiiD5WBi4dLzaB/agi-will-drastically-increase-economies-of-scale) that just as the invention of culture and social learning by evolution allowed humans to become the dominant species very quickly (relatively speaking), similarly once AI systems are capable of social learning they may also “take off” discontinuously. However, the same argument could be taken as evidence *[against](https://www.lesswrong.com/posts/XjuT9vgBfwXPxsdfN/might-humans-not-be-the-most-intelligent-animals)* a discontinuity, since current natural language systems like [GPT-2](https://blog.openai.com/better-language-models/) could already be thought of as processing culture or doing social learning.\n\nIt is worth noting that questions about [recursive self improvement](https://www.alignmentforum.org/posts/CjW4axQDqLd2oDCGG/misconceptions-about-continuous-takeoff) and [decisive strategic advantage](https://www.alignmentforum.org/posts/PKy8NuNPknenkDY74/soft-takeoff-can-still-lead-to-decisive-strategic-advantage) do not map cleanly onto the question of takeoff speeds, though they are related. The primary reason takeoff speed is important is that it determines whether or not we will be able to respond to problems as they come up. For this purpose, it’s probably better to define takeoff speed with respect to the [amount of work](https://www.alignmentforum.org/posts/cxgtQXnH2uDGBJJGa/redefining-fast-takeoff) that can be done as AI takes off, which might differ significantly from calendar time.\n\nThe importance of compute\n-------------------------\n\nThere is a strong case that the most effective methods (so far) are the ones that can [leverage](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) more computation, and the [AI-GA approach](https://arxiv.org/abs/1905.10985) to general intelligence is predicated on this view (for example, by [learning good learning environments](http://arxiv.org/abs/1901.01753)). In fact, since the rise of deep learning in 2012, the [amount](https://openai.com/blog/ai-and-compute/) of compute used in the largest AI training runs has been increasing exponentially with a 3.4-*month* doubling time. It’s important to note the caveat that we cannot simply increase compute: we also [need good data](https://staff.fnwi.uva.nl/m.welling/wp-content/uploads/Model-versus-Data-AI-1.pdf), which is sparse in rare, unsafe situations (consider driving when a pedestrian suddenly jumps on the road). This may require human knowledge and explicit models.\n\nSince it seems more likely that compute grows continuously (relative to a “deep insights” model), this would argue for a more continuous takeoff. However, you may expect that we still need deep insights, potentially because you think that current techniques could never lead to AGI, due to their lack of some property [crucial](https://aiimpacts.org/evidence-against-current-methods-leading-to-human-level-artificial-intelligence/) to general intelligence (such as [causal reasoning](https://www.lesswrong.com/posts/SvhzEQkwFGNTy6CsN/alphastar-impressive-for-rl-progress-not-for-agi-progress)). However, for any such property, it seems that *some* neural net could encode that property, and the [relevant question](https://www.lesswrong.com/posts/SvhzEQkwFGNTy6CsN/alphastar-impressive-for-rl-progress-not-for-agi-progress#kRpwqPPjcGEbEhXHA) is how big the neural net has to be and how long it takes for local search to find the right computation.\n\nSociological evidence\n---------------------\n\nIt has recently become more common to critique the field of AI as a whole, which should (arguably) cause you to lengthen your timelines. For example, [hypothesizing after the results are known](https://arxiv.org/abs/1904.07633) makes for bad science that doesn’t generalize, and research that is “reproducible” in the sense that the code can be rerun to get the same results need not have [external validity](http://proceedings.mlr.press/v97/bouthillier19a/bouthillier19a.pdf). There is also a tendency for researchers to throw trial and error at problems, which means that with repeated trials by chance we can get results that look significant. It also means that researchers don’t understand the systems they build; [reorienting](https://www.alignmentforum.org/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety) the field to focus on understanding could make our design decisions more deliberate and make it more likely that we build aligned AIs.\n\nWe should also expect that at least industry research is [biased](https://www.lesswrong.com/posts/KnQs55tjxWopCzKsk/the-ai-timelines-scam) towards short timelines, since any companies that didn’t argue for short timelines would be much less likely to get funding.\n\nMeta work on forecasting\n------------------------\n\nWhile forecasting the future is notoriously hard, collaborative and checkable forecasting is even harder. It would be nice to at least reduce the difficulty back down to “regular” forecasting. Three steps have been taken towards this:\n\n1. People need to agree on the meaning of the terms used; an AI forecasting [dictionary](https://www.lesswrong.com/posts/8y7DcSF4eAkXoru4u/ai-forecasting-dictionary-forecasting-infrastructure-part-1-2) has been developed for this purpose.\n2. In order to be checkable, questions need to be operationalized; but then it is often the case that the primary determinant of the answer to a question depends on some “distractor” feature. For example, whether we have a superhuman AI at <game> by 2025 depends a lot on who tries to make such an AI, rather than whether we have the technical ability to make such an AI. A partial solution was to create a [resolution council](https://www.lesswrong.com/posts/9G6CCNXkA7JZoorpY/ai-forecasting-resolution-council-forecasting-infrastructure), and instead have questions ask about the future opinion of the resolution council.\n3. This [post](https://www.lesswrong.com/posts/yy3FCmdAbgSLePD7H/how-to-write-good-ai-forecasting-questions-question-database) provides advice on how to write good forecasting questions, with a database of examples.\n\nOf course, there is still the hard problem of actually figuring out what happens in the future (and it’s even [hard](https://www.openphilanthropy.org/blog/how-feasible-long-range-forecasting) to tell *whether* long-run forecasting is feasible). The Good Judgment Project studied practices that help with this problem, summarized [here](https://aiimpacts.org/evidence-on-good-forecasting-practices-from-the-good-judgment-project/).\n\nAnother [issue](https://www.lesswrong.com/posts/Lds9opZsAMbjuZp7h/coordination-surveys-why-we-should-survey-to-organize) arises when asking members of a group (e.g. AI researchers) about outcomes that depend on actions within that group: due to the bystander effect, everyone may predict that the group will solve a problem, even though they themselves are not trying to solve the problem. So, we should instead ask people to make predictions about the proportion of members that try to solve a problem, and compare that to the proportion of members who say that they are trying to solve the problem.\n\nAI Progress\n===========\n\nA full update on AI progress in 2019 would be far too long, so here I’ll just mention some results I found interesting, which biases towards 1. results involving “throwing compute at the problem”, and 2. understanding deep learning.\n\nReinforcement learning\n----------------------\n\n1. [AlphaStar](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/) ([update](https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning), [discussion](https://www.alexirpan.com/2019/02/22/alphastar-part2.html)) become extremely good at Starcraft.\n2. [OpenAI Five](https://openai.com/blog/how-to-train-your-openai-five/) beat the world champions at Dota, and could play cooperatively alongside humans.\n3. OpenAI trained a robot to manipulate a [Rubik’s cube](https://openai.com/blog/solving-rubiks-cube/) so that it could sometimes solve a jumbled cube when given the steps of the solution. See also [this discussion](https://www.alexirpan.com/2019/10/29/openai-rubiks.html).\n4. [MuZero](https://arxiv.org/abs/1911.08265) is an evolution of AlphaZero where MCTS is applied on a *learned* world model optimized for planning, allowing it to master Atari in addition to AlphaZero’s Go, Chess, and Shogi. See also [this paper](https://learningtopredict.github.io/) on instrumentally learned world models.\n5. [Pluribus](https://science.sciencemag.org/content/early/2019/07/10/science.aay2400.full) was shown to be superhuman at *multiplayer* poker. (Note that to my knowledge it did not use deep learning, and it did not require much compute.)\n6. With a complex enough [hide-and-seek](https://openai.com/blog/emergent-tool-use/) environment, self-play can learn qualitatively interesting behaviors.\n\nDeep learning\n-------------\n\n1. While [GPT-2](https://blog.openai.com/better-language-models/) is the most well-known, there have been several large language models that are eerily good at capturing language, such as [Transformer-XL](http://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html) and [XLNet](https://arxiv.org/abs/1906.08237).\n2. [SATNet](http://arxiv.org/abs/1905.12149) proposed a differentiable layer for neural networks that provides a strong inductive bias towards “logical reasoning”, though even regular machine translation techniques [work well](https://arxiv.org/abs/1912.01412) for function integration and differential equation solving.\n3. The [lottery ticket](https://arxiv.org/abs/1803.03635) hypothesis from 2018 was [tested](https://eng.uber.com/deconstructing-lottery-tickets/) [much](https://arxiv.org/abs/1903.01611) [more](https://ai.facebook.com/blog/understanding-the-generalization-of-lottery-tickets-in-neural-networks).\n4. The [double descent](https://arxiv.org/abs/1812.11118) phenomenon was [empirically validated](https://openai.com/blog/deep-double-descent/).\n\nField building\n==============\n\nWhile there have been a lot of field building efforts, they are relatively disjoint and not part of a conversation, and so I’ve summarized them in lists.\n\nSummaries and reviews\n---------------------\n\n1. This [talk](https://www.youtube.com/watch?v=AMSKIDEbjLY) and [multipart](https://futureoflife.org/2019/04/11/an-overview-of-technical-ai-alignment-with-rohin-shah-part-1/) [podcast](https://futureoflife.org/2019/04/25/an-overview-of-technical-ai-alignment-with-rohin-shah-part-2/) provides an overview of approaches to technical AI alignment.\n2. This [post](https://ai-alignment.com/ai-alignment-landscape-d3773c37ae38) decomposes the beneficial AI problem into a tree of different subproblems (with a particular focus on the alignment problem).\n3. There is of course the annual [literature review and charity comparison](https://www.alignmentforum.org/posts/SmDziGM9hBjW9DKmf/2019-ai-alignment-literature-review-and-charity-comparison).\n4. This [post](https://www.alignmentforum.org/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment) identifies important hypotheses that researchers disagree about.\n\nAgendas and prioritization\n--------------------------\n\n1. This [doc](https://docs.google.com/document/d/1FbTuRvC4TFWzGYerTKpBU7FJlyvjeOvVYF2uYNFSlOc/edit) provides an overview of the technical problems that need to be solved to align AI systems (as opposed to e.g. MIRI’s deconfusion approach).\n2. [These](https://www.alignmentforum.org/posts/rASeoR7iZ9Fokzh7L/problems-in-ai-alignment-that-philosophers-could-potentially) [posts](https://www.alignmentforum.org/posts/4xbsi4wbourPkb47x/technical-agi-safety-research-outside-ai) list questions that could be tackled by philosophers and non-AI researchers respectively.\n3. It would be better to [bridge](https://www.nature.com/articles/s42256-018-0003-2) near- and long-term concerns about AI, to prevent the fields from “fighting” each other.\n4. For s-risks, rather than looking at particular scenarios, we could focus on [risk factors](http://s-risks.org/risk-factors-for-s-risks/): properties we can intervene on to make risks less probable or less severe.\n\nEvents and news updates\n-----------------------\n\n1. Several conferences and workshops in 2019, including [Beneficial AGI](https://futureoflife.org/beneficial-agi-2019/), [SafeML](https://sites.google.com/view/safeml-iclr2019/accepted-papers) at ICLR, [AI Safety](https://www.ai-safety.org/) at IJCAI, and [Uncertainty and Robustness](https://sites.google.com/view/udlworkshop2019/accepted-papers) at ICML.\n2. There was a human-aligned AI [summer school](http://humanaligned.ai/) and an [AI safety camp](https://aisafetycamp.com/previous-camps/).\n3. OpenAI switched to a [limited-profit structure](https://openai.com/blog/openai-lp/) and received a [$1B investment](https://openai.com/blog/microsoft/) from Microsoft, while still expressing support for their [charter](https://openai.com/charter/).\n\nThe Center for Security and Emerging Technology (CSET) was [founded](https://www.georgetown.edu/news/qa-with-jason-matheny-founding-director-of-the-center-for-security-and-emerging-technology/).\n\nReferences\n==========\n\nSee the [Google Doc](https://docs.google.com/document/d/1Fng1J_QPb7GEeLBMmWWfZOguw7yUTZot0egrCbKpVwk/edit#heading=h.kr223qj1g1y5) for a list of all the names and links in the text above.", "url": "https://www.alignmentforum.org/posts/dKxX76SCfCvceJXHv/ai-alignment-2018-19-review", "date_published": "2020-01-28T02:19:53Z", "authors": ["Rohin Shah"], "tags": ["Outer Alignment", "Inner Alignment", "Utility Functions", "Value Learning", "AI Risk", "Corrigibility", "AI Takeoff", "AI", "Impact Regularization", "AI Timelines"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.701870+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "2ad928d2fdd15e1ec22ee5454ca88218", "source": "alignmentforum", "title": "Using vector fields to visualise preferences and make them consistent", "text": "*This post was written for [Convergence Analysis](https://www.convergenceanalysis.org/) by Michael Aird, based on ideas from Justin Shovelain and with ongoing guidance from him. Throughout the post, “I” will refer to Michael, while “we” will refer to Michael and Justin or to Convergence as an organisation.*\n\n\n*Epistemic status: High confidence in the core ideas on an abstract level. Claims about the usefulness of those ideas, their practical implications, and how best to concretely/mathematically implement them are more speculative; one goal in writing this post is to receive feedback on those things. I’m quite new to many of the concepts covered in this post, but Justin is more familiar with them.*\n\n\nOverview\n========\n\n\nThis post outlines:\n\n\n* What vector fields are\n* How they can be used to visualise preferences\n* How utility functions can be generated from “preference vector fields” (PVFs)\n* How PVFs can be extrapolated from limited data on preferences\n* How to visualise inconsistent preferences (as “curl”)\n* A rough idea for how to “remove curl” to generate consistent utility functions\n* Possible areas for future research\n\n\nWe expect this to provide useful tools and insights for various purposes, most notably AI alignment, existential risk [strategy](https://forum.effectivealtruism.org/posts/oovy5XXdCL3TPwgLE/a-case-for-strategy-research-what-it-is-and-why-we-need-more), and rationality.\n\n\nThis post is structured modularly; different sections may be of interest to different readers, and should be useful in isolation from the rest of the post. The post also includes links to articles and videos introducing relevant concepts, to make the post accessible to readers without relevant technical backgrounds.\n\n\nVector fields and preferences\n=============================\n\n\nA [vector](https://en.wikipedia.org/wiki/Euclidean_vector) represents both magnitude and direction; for example, velocity is a vector that represents not just the speed at which one is travelling but also the direction of travel. A [vector field](https://en.wikipedia.org/wiki/Vector_field) essentially associates a vector to each point in a region of space. For example, the following image ([source](https://www.khanacademy.org/science/physics/magnetic-forces-and-magnetic-fields/magnetic-field-current-carrying-wire/a/what-are-magnetic-fields)) shows the strength (represented by arrow lengths) and direction of the magnetic field at various points around a bar magnet:\n\n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239962/Magnetic_field_wdw8tw.png)\n\n\n*Figure 1.*\n\n\nAnother common usage of vector fields is to represent the direction in which fluid would flow, for example the downhill flow of water on uneven terrain ([this short video](https://www.khanacademy.org/math/multivariable-calculus/thinking-about-multivariable-function/visualizing-vector-valued-functions/v/fluid-flow-and-vector-fields) shows and discusses that visualisation).\n\n\nWe believe that vector fields over “state spaces” (possible states of the world, represented by positions along each dimension) can be a useful tool for analysis and communication of various issues (e.g., existential risk strategy, AI alignment). In particular, we’re interested in the idea of representing preferences as “preference vector fields” (PVFs), in which, at each point in the state space, a vector represents which direction in the state space an agent would prefer to move from there, and how intense that preference is.[[1]](#fn-9mzEHb25RTeQQ6qGS-1) (For the purposes of this post, “agent” could mean an AI, a human, a community, humanity as a whole, etc.)\n\n\nTo illustrate this, the following PVF shows a hypothetical agent’s preferences over a state space in which the only dimensions of interest are wealth and security.[[2]](#fn-9mzEHb25RTeQQ6qGS-2)[[3]](#fn-9mzEHb25RTeQQ6qGS-3)\n\n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239963/Figure_2_Vectors_gkybi1.png)\n\n\n*Figure 2.*\n\n\nThe fact that (at least over the domain shown here) the arrows always point at least slightly upwards and to the right shows that the agent prefers more wealth and security to less, regardless of the current level of those variables. The fact that the arrows are longest near the x axis shows that preferences are most intense when security is low. The fact that the arrows become gradually more horizontal as we move up the y axis shows that, as security increases, the agent comes to care more about wealth relative to security.\n\n\nNot only preferences\n--------------------\n\n\nIn a very similar way, vector fields can be used to represent things other than preferences. For example, we might suspect that for many agents (e.g., most/all humans), preferences do not perfectly match what would actually make the agent happier (e.g., because of the agent being mistaken about something, or having separate systems for reward vs motivation). In this case, we could create a vector field to represent the agent’s preferences (represented by the blue arrows below), and another to represent what changes from any given point would increase the agent’s happiness (represented by the green arrows).\n\n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239967/Figure_3_Vectors_cl7b39.png)\n\n\n*Figure 3.*\n\n\nThis method of layering vector fields representing different things can be used as one tool in analysing potential clashes between different things (e.g., between an agent’s preferences and what would actually make the agent happy, or between an agent’s *beliefs* about what changes would be likely at each state and what changes would *actually* be likely at each state).\n\n\nFor example, the above graph indicates that, as wealth and/or security increases (i.e., as we move across the x axis and/or up the y axis), there is an increasing gap between the agent’s preferences and what would make the agent happy. In particular, security becomes increasingly more important than wealth for the agent’s happiness, but this is not reflected in the agent’s preferences.\n\n\n(Note that, while it does make sense to compare the direction in which arrows from two different vector fields point, I haven’t yet thought much about whether it makes sense to compare the lengths Grapher shows for their arrows. It seems like this is *mathematically* the same as the common problem of trying to compare utility functions across different agents, or preferences across different voters. But here the functions represent different things within the *same* agent, which may make a difference.)\n\n\nGradients and utility functions\n===============================\n\n\nWhen a vector field has no “curl” (see the section “Curl and inconsistent preferences” below), the vector field can be thought of as the [gradient](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient) of a [scalar field](https://en.wikipedia.org/wiki/Scalar_field).[[4]](#fn-9mzEHb25RTeQQ6qGS-4) (A scalar field is similar to a vector field, except that it associates a *scalar* with each point in a region of space, and scalars have only magnitude, rather than magnitude *and* *direction*.) Essentially, this means that the arrows of the vector field can be thought of as pointing “uphill”, away from low points and towards high points of the associated scalar function. If the vector field represents preferences, higher points of the scalar function would be where preferences are more satisfied, and lower points are where it is less satisfied; thus, the scalar function can be thought of as the agent’s utility function.[[5]](#fn-9mzEHb25RTeQQ6qGS-5) (The same basic method is often used in physics, in which context the scalar function typically represents [scalar potential](https://en.wikipedia.org/wiki/Scalar_potential).)\n\n\nBelow is one visualisation of the scalar field representing the utility function of the agent from the previous example (based on its preferences, not on what would make it “happy”), as well as the related vector field. Colours towards the red end of the spectrum represent higher values of the scalar field. It can be seen that the arrows of the vector field point away from blue areas and towards red areas, representing the agent’s preference for “climbing uphill” on its utility function.\n\n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239979/Figure_4_Vectors_xjq6sz.png)\n\n\n*Figure 4.*\n\n\nThe scalar field can also be represented in three dimensions, as values on the z dimension, which are in turn a function of values on the x and y dimensions. This is shown below (from two angles), for the same agent. (These graphs are a little hard to interpret from still images on a 2D screen, at least with this function; such graphs can be easier to interpret when one is able to rotate the angle of view.)\n\n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239979/Figure_5_Vectors_ma27vo.png)\n\n\n*Figures 5a and 5b.*\n\n\nMethod\n------\n\n\n[This video](https://www.youtube.com/watch?v=iLAK2IsQ_Uo) provides one clear explanation of the actual method for determining the scalar function that a curl-free vector field can be thought of as the gradient of (though the video is focused on cases of 3D vector fields). That video describes this as finding the “potential”; as noted earlier, when the vector field represents preferences, the utility function can be thought of as analogous to the “potential” in other cases.\n\n\nPersonally, as a quick method of finding the scalar function associated with a 2D vector field, I used the following algorithm, from the first answer on [this page](https://mathematica.stackexchange.com/questions/100758/finding-scalar-potential-function):\n\n\n\n> \n> DSolve[{D[f[x, y], x] == [X COMPONENT OF THE VECTOR FIELD], D[f[x, y], y] == [Y COMPONENT OF THE VECTOR FIELD]}, f[x, y], {x, y}]\n> \n> \n> \n\n\nI input the algorithm into a [Wolfram Cloud notebook](https://www.wolframcloud.com/), which seems to be free to use as long as you create an account. (As noted in the answer on the linked page, this algorithm will come back with no solution if the vector field has curl. This makes sense, because this general approach cannot be used in this way if a field has curl; this is explained in the section “Curl and inconsistent preferences” below.) Finally, I double-checked that the function was a valid solution by using [this calculator](https://www.wolframalpha.com/input/?i=grad+of+a+scalar+field&assumption=%7B%22F%22%2C+%22GradientCalculator%22%2C+%22scalarfunction%22%7D+-%3E) to find its gradient, which should then be the same as the original vector field.\n\n\nExtrapolating PVFs (and utility functions) from specific preference data\n========================================================================\n\n\nIn reality, one rarely knows an agent’s actual utility function or their full PVF. Instead, one is likely to only have data on the agent’s (apparent) preferences at *particular points* in state space; for example, the extent to which they wanted more wealth and more security when they had $10,000 of savings and a “4/5” level of security.\n\n\nOne can imagine extrapolating a full preference vector field (PVF) from that data. We do not know of a precise method for actually doing this (we plan to do more research and thought regarding that in future). However, conceptually speaking, it seems the process would be analogous to fitting a regression line to observed data points, and, like that process, would require striking a balance between maximising fit with the data and avoiding [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n\n\nFor an example (based very loosely on Figure 3 in [this article](https://www.tandfonline.com/doi/abs/10.1080/09544820500275032)), suppose that I know that Alice prefers car A to Car B, Car B to Car C, Car C to Car D, and Car D to Car A (i.e., to Alice, A>B>C>D>A).[[6]](#fn-9mzEHb25RTeQQ6qGS-6) I also know the weight (in thousands of pounds) and perceived “sportiness” (as rated by consumers) of the four cars, and am willing to make the simplifying assumption that these are the only factors that influenced Alice’s preferences. I could then create a plane with weight on the x axis and sportiness on the y axis, show the position of the four cars in this space, and represent Alice’s preferences with arrows pointing from each car towards the car Alice would prefer to that one, as shown below:[[7]](#fn-9mzEHb25RTeQQ6qGS-7)\n\n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239980/Figure_6_vectors_ggb5qp.png)\n\n\n*Figure 6.*\n\n\nI could then infer a PVF that (1) approximately captures Alice’s known preferences, and (2) suggests what preferences Alice would have at any other point in the plane (rather than just at the four points I have data for). In this case, one seemingly plausible PVF is shown below, with the length of each blue arrow representing the strength of Alice’s preferences at the associated point. (This PVF still shows Alice’s known preferences, but this is just for ease of comparison; those known preferences are not actually part of the PVF itself.)\n\n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239990/Figure_7_vectors_mktb8i.png)\n\n\n*Figure 7.*\n\n\nThis PVF allows us to make predictions about what Alice’s preferences would be even in situations we do not have any empirical data about. For example, this PVF suggests that if Alice had the hypothetical car E (with a weight of ~2000 pounds and sportiness of ~55), she would prefer a car that was heavier and was higher for sportiness. In contrast, the PVF also suggests that, if she had the hypothetical car F (with a weight of ~6000 pounds and sportiness of ~55), she would prefer a car that was heavier and was rated *lower* for sportiness.\n\n\nOf course, these predictions are not necessarily accurate. One could likely create many other PVFs that also “appear” to roughly fit Alice’s known preferences, and these could lead to different predictions. This highlights why we wish to find a more precise/“rigorous” method to better accomplish the goal I have conceptually gestured at here.\n\n\nIt’s also worth noting that one could extrapolate an agent’s utility function from limited preference data by first using the method gestured at here and then using the method covered in the previous section. That is, one could gather some data on an agent’s (apparent) preferences, extrapolate a PVF that “fits” that data, and then calculate what (set of) scalar function(s) that vector field is the gradient of. That scalar function would be the agent’s extrapolated utility function.\n\n\nHowever, as noted earlier, this method only works if the PVF has no “curl”, so it would not work in the case of Alice’s preferences about cars. I will now discuss what I mean by “curl”, what implications curl has, and a rough idea for “removing” it.\n\n\nCurl and inconsistent preferences\n=================================\n\n\nIn the example above, to Alice, A>B>C>D>A. This is a case of [intransitivity](https://en.wikipedia.org/wiki/Intransitivity#Occurrences_in_preferences), or, less formally, circular or inconsistent preferences. This is typically [seen as](https://www.lesswrong.com/s/waF2Pomid7YHjfEDt/p/zJZvoiwydJ5zvzTHK) [irrational](https://www.lesswrong.com/s/waF2Pomid7YHjfEDt/p/zNcLnqHF5rvrTsQJx), and as opening agents up to issues such as being “[money pumped](https://en.wikipedia.org/wiki/Money_pump)”. It seems that Alice would be willing to just keep paying us to let her trade in one car for the one she preferred to that one, and do this *endlessly* - going around and around in a circle, yet feeling that her preferences are being continually satisfied.\n\n\nSo another pair of reasons why representing preferences as vector fields is helpful is that doing so allows inconsistencies in preferences:\n\n\n1. to be directly seen (if they are sufficiently extreme)\n2. to be calculated as the vector field’s [curl](https://en.wikipedia.org/wiki/Curl_(mathematics))\n\n\n[This video](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/curl-grant-videos/v/2d-curl-intuition) introduces the concept of curl. Returning to the visualisation of vector fields as representing the direction in which water would flow over a certain domain, curl represents the speed and direction an object would spin if placed in the water. For example, if there is a strong clockwise curl at a certain point, a stick placed there would rotate clockwise; if there is no curl at a point, a stick placed there would not rotate (though it still may move in some direction, as represented by the vector field itself).\n\n\nNote that the concepts of curl and inconsistency will also apply in less extreme cases (i.e., where an agent’s preferences do not *only* “chase each other around in circles”).\n\n\nAs noted earlier, when a vector field has curl, one cannot find its gradient. In our context, this seems logical; if an agent’s preferences are inconsistent, it seems that the agent cannot have a true utility function, and that we can’t assign any meaningful “height” to any point in the 2D state space. Consider again the example of Alice’s preferences for cars; if we were to interpret meeting her preferences as moving “uphill” on a utility function, she could keep arriving back at the same points in the state space and yet be at different “heights”, which doesn’t seem to make sense.\n\n\nRemoving curl to create consistent utility functions\n----------------------------------------------------\n\n\nIt seems that agents frequently have intransitive preferences, and thus that their PVFs will often have some curl. It would therefore be very useful to have a method for “removing curl” from a PVF, to translate an intransitive set of preferences into a transitive set of preferences, while making a minimum of changes. This new, consistent PVF would also then allow for the generation of a corresponding utility function for the agent.[[8]](#fn-9mzEHb25RTeQQ6qGS-8)\n\n\nWe believe that this process should be possible. We also believe that, if developed and confirmed to make sense, it could be useful for various aspects of AI alignment (among other things). In particular, it could help in:\n\n\n* extrapolation of a consistent “core” (and corresponding utility function) from inconsistent *human* preferences (which could then inform an AI’s decisions)\n* adjustment of an *AI’s* inconsistent preferences (either by engineers or by the AI itself), with a minimum of changes being made\n\n\nWe have not yet implemented this process for removing curl. But we believe that the [Helmholtz theorem](https://en.wikipedia.org/wiki/Helmholtz_decomposition) should work, at least for PVFs in 3 or fewer dimensions (and we believe that a higher dimensional generalization probably exists). The Helmholtz theorem:\n\n\n\n> \n> states that any sufficiently smooth, rapidly decaying vector field in three dimensions can be resolved into the sum of an irrotational (curl-free) vector field and a solenoidal (divergence-free) vector field; this is known as the Helmholtz decomposition or Helmholtz representation. ([Wikipedia](https://en.wikipedia.org/wiki/Helmholtz_decomposition))\n> \n> \n> \n\n\nThis irrotational (curl-free) vector field would then be the consistent projection (in a [CEV](https://intelligence.org/files/CEV.pdf)-like way) of the agent’s preferences (from which the agent’s utility function could also be generated, in the manner discussed earlier).\n\n\nUncertainties and areas for further research\n============================================\n\n\nThe following are some areas we are particularly interested in getting comments/feedback on, seeing others explore, or exploring ourselves in future work:\n\n\n* Are there any flaws or misleading elements in the above analysis? (As noted earlier, this is essentially just an initial exploration of some tools/concepts.)\n* To what extent do the methods used and claims made in this post generalise to higher-dimensional spaces (e.g., when we wish to represent preferences over more than two factors at the same time)? To what extent do they generalise to graphs of states that don’t correspond to any normal geometry?\n* Is there an existing, rigorous/precise method for extrapolating a PVF from a limited number of known preferences (or more generally, extrapolating a vector field from a limited number of known vectors)? If not, can a satisfactorily rigorous/precise method be developed?\n* Are there meaningful and relevant differences between the concepts of curl in vector fields and of intransitivity, inconsistency, irrationality, and incoherence in preferences? If so, how does that change the above analysis?\n* Is it possible to “remove curl” in the way we want, in the sort of situations we’re interested in (in particular, not only in three dimensions)? If so, how, specifically?\n* What other implications do the above ideas have? E.g., for rationality more generally, or for how to interpret and implement preference utilitarianism. (Above, I mostly just introduced the ideas, and hinted at a handful of implications.)\n* What other uses could these “tools” be put to?\n\n\n\n\n---\n\n\n\n1. It appears some prior work (e.g., [this](https://www.tandfonline.com/doi/abs/10.1080/09544820500275032) and [this](https://books.google.com.au/books?id=qLnEiu0lbNAC&pg=PA169&lpg=PA169&dq=preference+vector+field&source=bl&ots=zXSiGh3haW&sig=ACfU3U16TLa5wnOv8ABcVbTwVLnOoukIvw&hl=en&sa=X&ved=2ahUKEwja5df1wqXmAhULXisKHbZqDbkQ6AEwEHoECAoQAg#v=onepage&q=preference%20vector%20field&f=false)) has explored the use of vector fields to represent preferences. Unfortunately, I haven’t yet had time to investigate this work, so there may be many useful insights in there that are lacking in this post. [↩︎](#fnref-9mzEHb25RTeQQ6qGS-1)\n2. Of course, there are often far more than two key factors influencing our preferences. In such cases, a vector field over more dimensions can be used instead (see [here](https://www.khanacademy.org/math/multivariable-calculus/thinking-about-multivariable-function/visualizing-vector-valued-functions/v/3d-vector-fields-introduction) for an introduction to 3D vector fields). I focus in this post on 2D vector fields, simply because those are easier to discuss and visualise. We expect many of the ideas and implications covered in this post will be similar in higher dimensional vector fields, but we aren’t yet certain about that, and intend to more carefully consider it later. [↩︎](#fnref-9mzEHb25RTeQQ6qGS-2)\n3. For both this example and most others shown, the precise equations used were chosen quite arbitrarily, basically by trying equations semi-randomly until I found one that roughly matched the sort of shape I wanted. For those interested, I have screenshots of all equations used, in their order of appearance in this post, [here](https://docs.google.com/document/d/1pGx7PPpZ_52l1Zrp5UpSA7UY1M3m8UtRIjVirn0DGls/edit?usp=sharing). To create the visuals in this post, I entered these equations into Grapher (for those interested in trying to do similar things themselves, I found [this guide](https://theputterer.wordpress.com/2011/12/10/mac-os-x-grapher-contour-and-vector-plots/) useful). I discuss below, in the section “Extrapolating PVFs (and utility functions) from specific preference data”, the issue of how to actually generate realistic/accurate PVFs in the first place. [↩︎](#fnref-9mzEHb25RTeQQ6qGS-3)\n4. It’s possible that here I’m conflating the concepts of [conservative](https://en.wikipedia.org/wiki/Conservative_vector_field), irrotational, and curl-free vector fields in a way that doesn’t make sense. If any readers believe this is the case, and especially if they believe this issue changes the core ideas and implications raised in this post, I would appreciate them commenting or messaging me. [↩︎](#fnref-9mzEHb25RTeQQ6qGS-4)\n5. Technically, the vector field is the gradient of a *class of* functions, with the functions differing only in their constant term. This is because gradient only relates to *differences* in height (or roughly analogous ideas, in higher-dimensional cases), not to absolute heights. One can imagine raising or lowering the entire scalar function by the same constant without affecting the gradient between points. (I show in [this document](https://docs.google.com/document/d/1pGx7PPpZ_52l1Zrp5UpSA7UY1M3m8UtRIjVirn0DGls/edit?usp=sharing) examples of what this would look like, while in this post itself I keep all constants at 0.) Thus, in one sense, a PVF does not fully specify the associated utility function representation, but the constant can be ignored anyway (as utility functions are unique up to positive affine transformations). [↩︎](#fnref-9mzEHb25RTeQQ6qGS-5)\n6. I have purposefully chosen a set of circular (or “intransitive”) preferences, as the next session will use this example in discussing the problem of circularity and how to deal with it. [↩︎](#fnref-9mzEHb25RTeQQ6qGS-6)\n7. Note that, in this example, I am not assuming any knowledge about the *strength* of Alice’s preferences, only about their direction. As such, the length of the arrows representing Alice’s known preferences has no particular meaning. [↩︎](#fnref-9mzEHb25RTeQQ6qGS-7)\n8. In conversation with Justin, Linda Linsefors mentioned having had a somewhat similar idea independently. [↩︎](#fnref-9mzEHb25RTeQQ6qGS-8)", "url": "https://www.alignmentforum.org/posts/ky988ePJvCRhmCwGo/using-vector-fields-to-visualise-preferences-and-make-them", "date_published": "2020-01-28T19:44:43Z", "authors": ["MichaelA", "JustinShovelain"], "tags": ["Value Learning", "Convergence (org)", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.703779+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "e012e1002b27139ba6259361e7541884", "source": "alignmentforum", "title": "[AN #84] Reviewing AI alignment work in 2018-19", "text": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.\n\nThis is the summary of a [review post](https://www.alignmentforum.org/posts/dKxX76SCfCvceJXHv/ai-alignment-2018-19-review) of public work in AI alignment over 2019, with some inclusions from 2018. The full post has a preamble (~700 words), this short version / summary (~1.6k words), and a long version (~8.3k words). It is also available as a Google Doc [here](https://docs.google.com/document/d/1Fng1J_QPb7GEeLBMmWWfZOguw7yUTZot0egrCbKpVwk/edit#).  \n   \n While the full post tries to accurately summarize different points of view, that is not a goal in this summary. Here I simply try to give a sense of the topics involved in the discussion, without saying what discussion actually happened. I'd strongly recommend reading the full post; I would have put it in full in this email, but 8,300 words seemed a bit too long, even for this newsletter.\n\n**Basic analysis of AI risk.** Traditional arguments for AI risk argue that since agentic AI systems will apply lots of optimization, they will lead to extreme outcomes that can’t be handled with normal engineering efforts. Powerful AI systems will not have their resources stolen from them, which by various dutch book theorems implies that they must be expected utility maximizers; since expected utility maximizers are goal-directed, they are dangerous.\n\nHowever, the VNM theorem [does not justify](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/NxF5G6CJiof6cemTw) the assumption that an AI system will be goal-directed: such an assumption is really based on intuitions and conceptual arguments (which are still quite strong).\n\n[Comprehensive AI Services](https://www.alignmentforum.org/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services-as) (CAIS) challenges the assumption that we will have a single agentic AI, instead suggesting that any task will be performed by a collection of modular services.\n\nThat being said, there are several other arguments for AI risk, such as the [argument](https://www.alignmentforum.org/posts/w6d7XBCegc96kz4n3/the-argument-from-philosophical-difficulty) that AI might cause “lock in” which may require us to solve hard philosophical problems before the development of AGI.\n\nNonetheless, there are [disjunctive reasons](https://www.alignmentforum.org/posts/QknPz9JQTQpGdaWDp/an-80-why-ai-risk-might-be-solved-without-additional) to expect that catastrophe does not occur: for example, there may not be a problem, or ML researchers may solve the problem after we get “warning shots”, or we could coordinate to not build unaligned AI.\n\n**Agency and optimization.** One proposed problem is that of [mesa optimization](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB), in which an optimization algorithm used to train an AI creates an agent that is *itself* performing optimization. In such a scenario, we need to ensure that the “inner” optimization is also aligned.\n\nTo better understand these and other situations, it would be useful to have a formalization of optimization. This is [hard](https://www.alignmentforum.org/posts/26eupx3Byc8swRS7f/bottle-caps-aren-t-optimisers): while we don’t want optimization to be about our beliefs about a system, if we try to define it mechanistically, it becomes hard to avoid defining a bottle cap as an optimizer of “water kept in the bottle”.\n\nUnderstanding agents is another hard task. While agents are relatively well understood under the Cartesian assumption, where the agent is separate from its environment, things become much more complex and poorly-understood when the agent is a [part of its environment](https://www.alignmentforum.org/s/Rm6oQRJJmhGCcLvxh).\n\n**Value learning.** Building an AI that learns all of human value has historically been thought to be very hard, because it requires you to decompose human behavior into the “beliefs and planning” part and the “values” part, and there’s no clear way to do this.\n\nAnother way of looking at it is to say that value learning [requires](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/h9DesGT3WT9u2k7Hr) a model that separates the given data into that which actually achieves the true “values” and that which is just “a mistake”, which seems hard to do. In addition, value learning seems quite fragile to mis-specification of this human model.\n\nNonetheless, there are reasons for optimism. We could try to build an [*adequate* utility function](https://www.alignmentforum.org/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into), which works well enough for our purposes. We can also have [*uncertainty* over the utility function](https://www.alignmentforum.org/posts/nd692YfFGfZDh9Mwz/an-69-stuart-russell-s-new-book-on-why-we-need-to-replace), and update the belief over time based on human behavior. If everything is specified correctly (a big if), as time goes on, the agent would become more and more aligned with human values. One major benefit of this is that it is *interactive* -- it doesn’t require us to specify everything perfectly ahead of time.\n\n**Robustness.** We would like our agents to be robust - that is, they shouldn’t fail catastrophically in situations slightly different from the ones they were designed for. Within reinforcement learning, safe reinforcement learning aims to avoid mistakes, even during training. This either requires analytical (i.e. not trial-and-error) reasoning about what a “mistake” is, which requires a formal specification of what a mistake is, or an overseer who can correct the agent before it makes a mistake.\n\nThe classic example of a failure of robustness is adversarial examples, in which a tiny change to an image can drastically affect its classification. Recent research has shown that these examples are [caused](https://distill.pub/2019/advex-bugs-discussion/) (at least in part) by real statistical correlations that generalize to the test set, that are nonetheless fragile to small changes. In addition, since robustness to one kind of adversary doesn’t make the classifier robust to other kinds of adversaries, there has been a lot of work done on improving adversarial evaluation in image classification. We’re also seeing some of this work in reinforcement learning.\n\nHowever, asking our agents to be robust to arbitrary mistakes seems to be too much -- humans certainly don’t meet this bar. For AI safety, it seems like we need to ensure that our agents are robustly [intent aligned](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd/p/ZeE7EKHTFMBs8eMxn), that is, they are always “trying” to do what we want. One particular way that our agents could be intent aligned is if they are [corrigible](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd/p/fkLYhTQteAu5SinAc), that is, they are trying to keep us “in control”. This seems like a particularly easy property to verify, as conceptually it seems to be independent of the domain in which the agent is deployed.\n\nSo, we would like to ensure that even in the worst case, our agent remains corrigible. One [proposal](https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment) would be to train an adversary to search for “relaxed” situations in which the agent behaves incorrigibly, and then train the agent not to do that.\n\n**Scaling to superhuman abilities.** If we’re building corrigible agents using adversarial training, our adversary should be more capable than the agent that it is training, so that it can find all the situations in which the agent behaves incorrigibly. This requires techniques that scale to superhuman abilities. Some techniques for this include [iterated amplification](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd) and [debate](https://arxiv.org/abs/1805.00899).\n\nIn iterated amplification, we start with an initial policy, and alternate between amplification and distillation, which increase capabilities and efficiency respectively. This can encode a range of algorithms, but often amplification is done by decomposing questions and using the agent to answer subquestions, and distillation can be done using supervised learning or reinforcement learning.\n\nIn debate, we train an agent through self-play in a zero-sum game in which the agent’s goal is to “win” a question-answering debate, as evaluated by a human judge. The hope is that since each “side” of the debate can point out flaws in the other side’s arguments, such a setup can use a human judge to train far more capable agents while still incentivizing them to provide honest, true information.\n\nBoth iterated amplification and debate aim to train an agent that approximates the answer that one would get from an exponentially large tree of humans deliberating. The [factored cognition](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd/p/DFkGStzvj3jgXibFG) hypothesis is that this sort of tree of humans is able to do any task we care about. This hypothesis is controversial: many have the intuition that cognition requires large contexts and flashes of intuition that couldn’t be replicated by a tree of time-limited humans.\n\n**Universality.** One [property](https://www.alignmentforum.org/posts/M8WdeNWacMrmorNdd/towards-formalizing-universality) we would hope to have is that if we use this tree of humans as an overseer for some simpler agent, then the tree would “know everything the agent knows”. If true, this property could allow us to build a significantly stronger conceptual argument for safety. It is also very related to…\n\n**Interpretability.** While interpretability can help us know what the agent knows, and what the agent would do in other situations (which can help us verify if it is corrigible), there are [other uses](https://www.alignmentforum.org/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety) for it as well: in general, it seems better if we can understand the things we’re building.\n\n**Impact regularization.** While relative reachability and attainable utility preservation were developed last year, this year saw them be [unified](https://medium.com/@deepmindsafetyresearch/designing-agent-incentives-to-avoid-side-effects-e1ac80ea6107) into a single framework. In addition, there was a new proposed [definition](https://www.alignmentforum.org/s/7CdoznhJaLEKHwvJW) of impact: change in our ability to get what we want. This notion of impact depends on knowing the utility function U. However, we might hope that we can penalize some “objective” notion, perhaps \"power\", that occurs regardless of the choice of U, for the same reasons that we expect instrumental convergence.\n\n**Causal modeling.** Causal models have been used recently to [model](https://arxiv.org/abs/1906.08663) the incentives for an agent under different AI safety frameworks, and to [argue](https://medium.com/@deepmindsafetyresearch/designing-agent-incentives-to-avoid-reward-tampering-4380c1bb6cd) that by evaluating plans with the current reward function, you can remove the incentive for an agent to tamper with its reward function.\n\n**Oracles.** Even if oracles are trying to maximize predictive accuracy, they could “choose” between different self-confirming predictions. We could avoid this using counterfactual oracles, which make predictions conditioning that their predictions do not influence the future.\n\n**Decision theory.** There was work on decision theory, that I haven’t followed very much.\n\n**Forecasting.** Several resources were developed to enable effective group forecasting, including an [AI forecasting dictionary](https://www.alignmentforum.org/posts/8y7DcSF4eAkXoru4u/ai-forecasting-dictionary-forecasting-infrastructure-part-1-2) that defines terms, an [AI resolution council](https://www.alignmentforum.org/posts/9G6CCNXkA7JZoorpY/ai-forecasting-resolution-council-forecasting-infrastructure) whose future opinions can be predicted, and a dataset of well-constructed [exemplar questions](https://www.alignmentforum.org/posts/yy3FCmdAbgSLePD7H/how-to-write-good-ai-forecasting-questions-question-database) about AI.\n\nSeparately, the debate over takeoff speeds continued, with [two](https://sideways-view.com/2018/02/24/takeoff-speeds/) [posts](https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/) arguing forcefully for continuous takeoff, [without much response](https://www.alignmentforum.org/posts/PzAnWgqvfESgQEvdg/any-rebuttals-of-christiano-and-ai-impacts-on-takeoff-speeds) (although many researchers do not agree with them). The continuity of takeoff is relevant for but doesn’t completely determine whether recursive self improvement will happen, or whether some actor acquires a decisive strategic advantage. The primary implication of the debate is whether we should expect that we will have enough time to react and fix problems as they arise.\n\nIt has also become clearer that recent progress in AI has been driven to a significant degree by increasing the [amount of compute](https://openai.com/blog/ai-and-compute/) devoted to AI, which suggests a more continuous takeoff. You could take the position that current methods can’t do <property X> (say, causal reasoning), and so it doesn’t matter how much compute you use.\n\n**AI Progress.** There was a lot of progress in AI.\n\n**Field building.** There were posts aiming to build the field, but they were all fairly disjointed.", "url": "https://www.alignmentforum.org/posts/6Rv9kLGmXrkqRrcK9/an-84-reviewing-ai-alignment-work-in-2018-19", "date_published": "2020-01-29T18:30:02Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.704581+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "78b78f3de87f4edbdd98687ca3e9314a", "source": "alignmentforum", "title": "Towards deconfusing values", "text": "*NB: Kaj recently said [some similar and related things](https://www.lesswrong.com/posts/2yLn8iTrvHoEgqXcJ/the-two-layer-model-of-human-values-and-problems-with) while I was on hiatus from finishing this post. I recommend reading it for a different take on what I view as a line of thinking generated by similar insights.*\n\nOne of the challenges with developing a theory of human values is dealing with the apparent non-systematic nature of human decision making which makes it seem that human value are not consistent, coherent, or rational. One solution is to [build](https://intelligence.org/files/CEV.pdf) or [discover](https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into) mechanisms by which they can be made legible and systematic. Another is to [embrace the illegibility and inconsistency](https://www.lesswrong.com/posts/NwxjvegAbLaBJ3TvC/towards-an-axiological-approach-to-ai-alignment-1) and [find ways of working with it](https://www.lesswrong.com/posts/7dvDgqvqqziSKweRs/formally-stating-the-ai-alignment-problem-1). This is a short start towards doing the latter because I believe the former cannot be made to work well enough to stand up against [Goodhart effects](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy) under extreme optimization by superintelligent AGI that we want to align with human values.\n\nI've been [thinking](https://www.lesswrong.com/s/aQTBuq9X98m2KkWpx/p/XmqqkfY8XAJ6LkwdP) a [lot](https://www.lesswrong.com/posts/D3N2mkaZcHuSeAxch/ascetic-aesthetic#Rfgn53SYTgrCdRoif) about [what values are](https://www.lesswrong.com/s/aQTBuq9X98m2KkWpx/p/JYdPbGS9mpJn3SAyA), and in particular [looking for phenomena that naturally align](https://www.lesswrong.com/s/sv2CwqTCso8wDdmmi/p/Cu7yv4eM6dCeA67Af) with the category we variously call values, preferences, affinity, taste, aesthetics, or [axiology](https://www.lesswrong.com/s/aQTBuq9X98m2KkWpx/p/wvAEHzE55K7vfsXWz). The only thing I have found that looks like a [natural kind](https://plato.stanford.edu/entries/natural-kinds/) (viz. [a model that cuts reality at its joints](https://www.lesswrong.com/posts/esRZaPXSHgWzyB2NL/where-to-draw-the-boundaries)) is **[valence](https://www.lesswrong.com/posts/ALvnz3DrjHwmLG29F/values-valence-and-alignment)**.\n\n[Valence on its own doesn't fully explain](https://www.lesswrong.com/posts/Nfizy2uRNkZmX3AYB/preference-synthesis-illustrated-star-wars#DJvW23JsiGQSiFfq7) all the phenomena we want to categorize as values, especially things like [meta-preferences](https://www.lesswrong.com/posts/ic8yoGBMYLtaJkbxZ/conditional-meta-preferences) or \"idealized\" values that are abstracted away from the concrete, [embedded](https://www.lesswrong.com/posts/p7x32SEt43ZMC9r7r/embedded-agents) process of a human making a choice at a point in time. Instead it gives us a [mechanism](https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding) by which we can understand why a human makes one choice over another at some point in their [causal history](https://www.lesswrong.com/s/ehnG4mseKF6xALmQy). And decisions are not themselves preferences, because decisions are embedded actions taken by [an agent in an environment](https://www.lesswrong.com/posts/qdqYrcGZTh9Lp49Nj/creating-environments-to-design-and-test-embedded-agents) whereas preferences are, as typically considered, generators of decisions. I think we need to flip this notion of preferences as generators on its head, and in so doing we move towards becoming less confused about preferences.\n\nSo let me describe my current model of how this works, and let's see if it explains the world we find ourselves in any better than existing theories of values and preferences.\n\nThe New Model Axiology\n======================\n\n[Humans are embedded agents](https://www.lesswrong.com/posts/WJzsTmsDctYCCyMfy/humans-are-embedded-agents-too). They carry out processes that result in them causing events. We call this process of an agent causing events **acting** and the process that leads to taking one action rather than any other possible action a **decision**. Although I believe valence describes much of how humans decide what actions to take, we need not consider that detail here and instead consider the abstraction of a **decision generation process** that is, importantly, inseparable from its implementation up to the limit of [functional](https://plato.stanford.edu/entries/functionalism/) equivalence and [conditioned on](https://www.lesswrong.com/posts/uHb2LDW3LGhBMyq74/preference-conditional-on-circumstances-and-past-preference) the causal history of the agent. Another way to say this is that the algorithm that makes the decision can be reasoned about and modeled but there is no simplification of the algorithm that produces exactly the same result in all cases unless it is a functionally equivalent algorithm and the decision is situated in time such that it cannot be separated from its embedding in the environment (which includes the entire past of the universe).\n\n*NB*: *I think there are a lot of interesting things to say about how the decision generation process seems to work in humans—how it comes up with the set of choices it chooses between, how it makes that choice, how it is modified, etc.—however I am going to leave off considerations of that for now so we can consider the theory at a more abstract level without getting bogged down in the implementation details of one of the gears.*\n\n*Additionally, all of this is described in terms of things like agents that don't exist until they are [reified into existence](https://www.lesswrong.com/posts/wvAEHzE55K7vfsXWz/introduction-to-noematology): prior to that reification into ontology all we have is [stuff happening](https://www.lesswrong.com/posts/M7Z5sm6KoukNpF3SD/form-and-feedback-in-phenomenology). Let's try not to get hung up on things like where to draw the boundary of an agent right now and treat the base concepts in this model as useful handles for bootstrapping understanding of a model that I expect can be reduced.*\n\n**Preferences** are then statistical regularities (probability distributions) over decisions. Importantly they come causally after decisions. Consequently preferences may predict decisions but they don't generate them. **Meta-preferences** are then probability distributions over preferences. Values, aesthetics, axiology, etc. are abstractions for talking about this category of probability distributions over decisions (and decisions about decisions, etc.).\n\nHere's a pictorial representation of the model if that helps make it clearer:\n\n![](https://docs.google.com/drawings/d/e/2PACX-1vTLiZGRahzO8ojiZ8d_JqQ1d0cHTgbyQQSyCz_vur1C0roIYbznrESS26-eGluQvJ2ULIceTcW6ullR/pub?w=1440&h=1080)This is as opposed to the standard or \"old\" model where preferences are the decision generators, which I'll stylize thusly, keeping in mind there's a lot of variation in how these \"old\" models work that I'm glossing over:\n\n![](https://docs.google.com/drawings/d/e/2PACX-1vQOTkaqNr63VJWR8aSMQ6NlJIwDeL4qXmZGpy3YSfIbefNhcKgIYdwn1LOwED4Lr7QXaXMMQtsM9GcR/pub?w=1440&h=1080)Note that in the new model preferences can still end up causally prior to decisions to the extent that they are discerned by an agent as features of their environment, but this is different from saying that preferences or meta-preferences are primary to the decision generation process. Thus when I say that preferences are causal postcedents of decisions I mean that if an agent did not know about or otherwise \"have\" preferences they would still make decisions by the decision generation process.\n\nAlthough backwards from the [standard model](https://plato.stanford.edu/entries/preferences/), this should not be too surprising since all animals manage to make decisions regardless of how aware they are of themselves or their actions, thus we should expect our model of values to function in the absence of decision generating preferences. Nonetheless, my guess is that this knowledge of preferences, especially knowledge of meta-preferences, feels like knowledge of the decision generation process from the inside and provides an important clue in understanding how humans might come to develop fixed points in their decision generation processes even if it really is all just valence calculations and why humans have grasped on the idea that preferences are a good model for the decision generation process.\n\nYou might object that I've just rearranged the terms or that this is just a more detailed model of revealed preferences, and to some extent those things are true, but I also think I've done it in a way that pulls apart concepts that were previously confounded such that we get something more useful for addressing AI alignment, which we'll explore in more detail now.\n\nImplications and Considerations\n===============================\n\nConfounded Notions of Preferences\n---------------------------------\n\nWhen we think of preferences as the generators of decisions, we run into all sorts of confusions. For example, if we equate preferences with revealed preferences people object that their revealed preferences leave something out about the process that generated their behavior and that generalizing from their observed behavior might not work as they would expect it to when applied to novel situations. This appears to be a general problem with [most attempts](https://www.lesswrong.com/s/4dHMdK5TLN6xcqtyc) at having computers learn human values today: they conflate behavior with the generators of behavior, find the generators [only by making normative assumptions](https://www.lesswrong.com/posts/cnjWN4mzmWzggRnCJ/practical-consequences-of-impossibility-of-value-learning), and then [end up](https://www.lesswrong.com/s/4dHMdK5TLN6xcqtyc/p/ANupXf8XfZo2EJxGv) with something [that almost](https://www.lesswrong.com/s/4dHMdK5TLN6xcqtyc/p/gnvrixhDfG7S2TpNL) but [doesn't quite](https://www.lesswrong.com/s/4dHMdK5TLN6xcqtyc/p/cnC2RMWEGiGpJv8go) match the generator.\n\nBut if we don't pay attention to revealed preferences we are also misled about people's preferences, since, for example, what people claim to be their preferences (their stated preferences) also don't seem to do a very good job of predicting their behavior. Maybe that's because people incorrectly assume their [partial preferences](https://www.lesswrong.com/posts/CiB3myyeEhFRgmKPL/partial-preferences-and-models) are \"full\" preferences in a total order and [may be related](https://www.lesswrong.com/posts/XmqqkfY8XAJ6LkwdP/akrasia-is-confusion-about-what-you-want) to [scope insensitivity](https://www.lesswrong.com/posts/JTFQDdMmGC8NhQzHH/scope-insensitivity-judo); maybe it's because people are [deceiving themselves](https://www.lesswrong.com/posts/BgBrXpByCSmCLjpwr/book-review-the-elephant-in-the-brain) about their preferences for various reasons. Whatever the reason, stated preferences and revealed preferences both result in models with errors more than large enough for them to [fall apart under superintelligent optimization](https://www.lesswrong.com/posts/NqQxTn5MKEYhSnbuB/goodhart-s-curse-and-limitations-on-ai-alignment).\n\nAnother problem, much commented upon by me at least, with treating preferences as generators of decisions is that this places the descriptive strength of preferences at odds with the normative demands we would like to place on preferences. For example, there's a lot to recommend rational preferences and preferences that can be described by a utility function, so people have put a lot of work into trying to find ways that these might also explain observed human behavior, even if it's to consider human behavior degraded from an ideal that it might approach if only we thought longer, knew more, etc.. But if we can create some space between the process of making decisions and the pattern of decisions made in our models this would ease much of that tension in terms of our models' abilities to explain relation and serve our purposes.\n\nPerhaps the solution lies at some synthesis of stated and revealed preferences, but that looks to me like trying to patch a broken system or put lipstick on a pig, and at the end of the day such a model may work a little better by papering over the faults of the two submodels but will also be a [kludge of epicycles that will crack if a comet comes screaming through](https://www.lesswrong.com/posts/XAFQkbe6c9TRts6Ex/what-value-epicycles). Alternatively we could look for some other method of identifying preferences, like brain scans, but at this point I think we are just arguing terminology. I could probably be convinced that calling the decision generation process \"preferences\" has some strong value, but from where I stand now it seems to cause more confusion than it resolves, so I'd rather see preferences treated solely as causally after decisions and talk some other way about whatever is causally before. \n\nHow It Helps\n------------\n\nWhat are the consequences of understanding preferences as causally downstream of actions rather than causally upstream of them? And does it make any difference since we still have something—the decision generation process—doing the work that we previously asked preferences, perhaps or perhaps not modeled with a utility function, to do? In other words, how does this model help us?\n\nOne of the big things it does is clear up confused thinking from getting the causal relationship between decision generation and preferences backwards. Rather than trying ever harder to find a theory that serves the two masters of accurately describing human behavior and obeying mathematical criteria that make our models behave in useful ways, we can let them operate independently. Yes, we may still want to, for example, modify human behavior to match norms, such as by increasing the rationality of human preferences, but also understand that the change doesn't come from changing preferences directly, but from changing decision generation processes such that, as a consequence, preferences are changed. And we may still want to design machines aligned with human values, but understand that aligning a machine with human preferences is not the same thing as aligning a machine with human decision generation processes since only the latter stands to capture all that humans value.\n\nAnother advantage of this model is that it is more explicitly embedded in the world. Preferences are intentionally an abstraction away from many of the messy details of how decisions are made, but as a result they lose some of their grip on reality. Said another way, preferences are a leaky [abstraction](https://www.lesswrong.com/posts/wuJpYLcMEBz4kcgAn/what-is-abstraction-1), and while they may be adequate for addressing questions in microeconomics, they seem inadequate for helping us build aligned AI. [There is no leakless abstraction](https://www.lesswrong.com/posts/KJ9MFBPwXGwNpadf2/skill-the-map-is-not-the-territory), but by realizing that preferences are higher up the abstraction stack and thus more leaky we can realize the need to [go down the stack and get nearer the territory](https://www.lesswrong.com/posts/nEBbw2Bc2CnN2RMxy/gears-level-models-are-capital-investments) to find a model with [more gears](https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding) that [better captures what matters](https://www.lesswrong.com/posts/3up8XBeGGHf77sNR4/the-map-has-gears-they-don-t-always-turn), maybe even up to a limit where superintelligent optimization is no longer a threat but an opportunity.\n\nIn short I think the main thing this new model does is free us from the constrictions of trying to make the preference model work with humans and accounts for the embeddedness of humans. It still doesn't say enough about how decisions are generated, but it gives us a better shaped model into which an abstraction of the implementation details can be slotted than the old model provided.\n\nNext Steps\n==========\n\nI feel like what I have described in this post is only one aspect of the model that is slowly coalescing in my mind, and it is able to crystalize into something communicable only by having germs to form around provided by interacting with others. So, what have I missed, or what would you like to know that would test this theory/reframing? What, if it were true, would invalidate it? I'd love to know!", "url": "https://www.alignmentforum.org/posts/WAqG5BQMzAs34mpc2/towards-deconfusing-values", "date_published": "2020-01-29T19:28:08Z", "authors": ["Gordon Seidoh Worley"], "tags": ["Value Learning", "Motivations"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.705101+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "c43c6fc8c96d7fc2edc734b416e206e5", "source": "alignmentforum", "title": "Pessimism About Unknown Unknowns Inspires Conservatism", "text": "[This](https://mkcohen-hosted-files.s3-us-west-1.amazonaws.com/Pessimism_alignmentforum.pdf) [EDIT: [final version](http://proceedings.mlr.press/v125/cohen20a/cohen20a.pdf), [presentation](https://www.learningtheory.org/colt2020/virtual/papers/paper_221.html)] is a design for a conservative agent that I worked on with Marcus Hutter. Conservative agents are reluctant to make unprecedented things happen. The agent also approaches at least human-level reward acquisition.\n\nThe agent is made conservative by being pessimistic. Pessimism is tuned by a scalar parameter .mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\nβ∈(0,1). When the agent is more pessimistic, it is more conservative. When it is made more pessimistic, it would be less likely to *exceed* human-level reward acquisition (almost definitely, but I haven’t tried to prove that). It would also require more observations before it started acting, the more pessimistic it is. It is not clear to me how useful the agent would be at the level of pessimism where we could be confident it is safe. At 0 pessimism, it is similar to AIXI (although technically stronger, because AIXI doesn’t have the performance guarantee of matching or exceeding human-level reward acquisition).\n\nThe agent has access to a human mentor, and at every timestep, it can either act or defer to the mentor. The only assumption we make is that the true environment belongs to the agent’s (countable) set of world-models. First a bit of math, then the main results.\n\nA bit of math:\n\nAn event E is a subset of interaction histories that end with an action. Letting A, O, and R be the action, observation, and reward spaces, an event E⊂(A×O×R)∗×A. An element of E would look like a1o1r1a2o2r2…at−1ot−1rt−1at. Below, I will say “[to] take an action which immediately causes event E”, by which I mean “to take an action such that now the interaction history is an element of E.”\n\nThe main results:\n\n1) (At least) mentor-level reward acquisition,\n\n2) Probability of querying the mentor →0, \n\nand this will take some time read, but I figured I’d spell it all out properly:\n\n3) For any complexity class C (defined on normal Turing machines, not e.g. non-deterministic ones), we can construct a set of world-models M such that for all events E in the complexity class C and for all δ>0, there exists a β such that: when the pessimistic agent has the model class M and a pessimism β, the following holds with probability 1−δ: for the whole lifetime of the pessimistic agent, if E has never happened before, the agent will not take an action which immediately causes event E; if the event E ever happens, it will be because the mentor took the action that made it happen.\n\nComment 1: “The agent takes an action which *eventually* causes E (with probability at least p)” is an event itself, and it happens *immediately* if the agent takes the action in question, so the theorem above applies. But this event may not be in a complexity class that E is in.\n\nComment 2: The less simple E is, the higher β has to be.\n\nSome other interesting results follow from the “Probably Respecting Precedent Theorem” above. One of which is roughly that (using the same there-exists and for-alls as in the main theorem) it is not instrumentally useful for the agent to cause E to happen. Note there is no need for the qualifier “immediately”.\n\nHere is an event *E* that makes the Probably Respecting Precent Theorem particularly interesting: “Everyone is probably about to be dead.” If we want the agent to avoid an unprecedented bad outcome, all we have to know is an upper bound of the computational complexity of the bad outcome. We don’t have to know how to define the bad outcome formally.\n\nHere’s how the agent works. It has a belief distribution over countably many world-models. A world-model is something that gives a probability distribution over observations and rewards given an interaction history (that ends in an action). It has a belief distribution over countably many mentor-models. A mentor-model is a policy—a probability distribution over actions given an interaction history. At each timestep, it takes the top world-models in its posterior until the total posterior weight of those world-models is at least β. The pessimistic value of a policy is the minimum over those world-models of the expected future discounted reward when following that policy in that world-model. The agent picks a policy which maximizes the pessimistic value. Either it follows this policy, or it defers to the mentor. To decide, it samples a world-model and a mentor-model from its posterior; then, it calculates the expected future discounted reward when following the mentor-model (which is a policy) in that world-model. If this value is greater than the pessimistic value plus positive noise, the agent defers to the mentor. Also, if the pessimistic value is 0, it defers to the mentor. This is called the zero condition, and to ensure that it only happens finitely often, the actual rewards we give have to always be greater than some ε>0. (If for some reason we failed to do this, despite that being in no one’s interest, the only results that would break are performance results, not safety results).\n\nHere is an intuitive argument that some might find more persuasive than the formal results. An advanced RL agent run on a computer in Oxford might come up with two hypotheses about how the environment produces rewards: (1) the environment produces rewards according to how satisfied the human operators are with my performance; (2) the environment produces reward according to which keys are pressed on the keyboard of a certain computer. An agent which assigns sufficient weight to (2) will take over the world if possible to make sure those keys are pressed right. A pessimistic agent (that is sufficiently pessimistic to include (1) in its set of top world-models that cover β of its posterior) will predict that taking over the world will make the human operators unsatisfied, which puts an upper bound on the pessimistic value of such a policy. Better to play it safe, and take actions which satisfy the human operators *and* cause them to press the right keys accordingly. (With the help of mentor-demonstrations, it will have seen enough to have all its top models be approximately accurate about the effects of normal actions). Intuitively, I think much lower values of β are required to get this sort of behavior than the value of β that would be required to get very a small δ for the event “everyone is probably about to be dead” in the Probably Respecting Precedent Theorem.\n\nThis agent is definitely not tractable. I mentioned that when β is large enough to make it safe, it might never learn to be particularly superhuman. It is also possible that we never manage to come up with heuristic approximations to this agent (for the sake of tractability) without ruining the safety results. (The most powerful “heuristic approximations” will probably look like “applying the state of the art in AI in place of proper Bayesian reasoning and expectimax planning.”) These are the main reasons I see for pessimism about pessimism.\n\nOne thought I’ve had on tractable approximations: I imagine the min over world-models being approximated by an adversary, who takes the agent’s plan and searches for a simple world-model that retrodicts past observations well, but makes the plan look dumb.\n\nJust a warning: the paper is dense.\n\n“I was sweating blood” — Marcus Hutter\n\nSome kind, kind people who read drafts and who were not familiar with the notation said it took them 2-3 hours (excluding proofs and appendices). Sorry about that. I’ve tried to present the agent and the results as formally as I can here without lots of equations with Greek letters and subscripts. Going a level deeper may take some time.\n\nThanks to Marcus Hutter, Jan Leike, Mike Osborne, Ryan Carey, Chris van Merwijk, and Lewis Hammond for reading drafts. Thanks to FHI for sponsorship. We’ve just submitted this to COLT. EDIT: It's been accepted! We’ll post it to ArXiv after we’ve gotten comments from reviewers. If you’d like to cite this in a paper in the meantime, you can cite it as an unpublished manuscript; if you’re citing it elsewhere, you can link to this page if you like. Hopefully, theorem numbers will stay the same in the final version, but I can’t promise that. I might not be super-responsive to comments here. EDIT: I have more time to respond to comments now.", "url": "https://www.alignmentforum.org/posts/RzAmPDNciirWKdtc7/pessimism-about-unknown-unknowns-inspires-conservatism", "date_published": "2020-02-03T14:48:15Z", "authors": ["michaelcohen"], "tags": ["Conservatism (AI)", "AI"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.705641+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "62da1fe9fbe5e197fd429c7e35110951", "source": "alignmentforum", "title": "[AN #85]: The normative questions we should be asking for AI alignment, and a surprisingly good chatbot", "text": "[View this email in your browser](https://mailchi.mp/84b4235cfa34/an-85-the-normative-questions-we-should-be-asking-for-ai-alignment-and-a-surprisingly-good-chatbot?e=[UNIQID])\n\nFind all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.\n\nAudio version [here](http://alignment-newsletter.libsyn.com/alignment-newsletter-85) (may not be up yet).\n\n**Highlights**\n--------------\n\n[Artificial Intelligence, Values and Alignment](https://arxiv.org/abs/2001.09768) *(Iason Gabriel)* (summarized by Rohin): This paper from a DeepMind author considers what it would mean to align an AI system. It first makes a distinction between the *technical* and *normative* aspects of the AI alignment problem. Roughly, the normative aspect asks, \"what should our AI systems do?\", while the technical aspect asks, \"given we know what our AI systems should do, how do we get them to do it?\". The author argues that these two questions are interrelated and should not be solved separately: for example, the current success of deep reinforcement learning in which we *maximize expected reward* suggests that it would be much easier to align AI to a utilitarian framework in which we *maximize expected utility*, as opposed to a deontological or Kantian framework.\n\nThe paper then explores the normative aspect, in both the single human and multiple humans case. When there's only one human, we must grapple with the problem of what to align our AI system to. The paper considers six possibilities: instructions, expressed intentions, revealed preferences, informed preferences, interests, and values, but doesn't come to a conclusion about which is best. When there are multiple humans, we must also deal with the fact that different people disagree on values. The paper analyzes three possibilities: aligning to a global notion of morality (e.g. \"basic human rights\"), doing what people would prefer from behind a veil of ignorance, and pursuing values that are determined by a democratic process (the domain of social choice theory).\n\nSee also [Import AI #183](https://jack-clark.net/2020/02/03/import-ai-183-curve-fitting-conversation-with-meena-gans-show-us-our-climate-change-future-and-what-compute-data-arbitrage-means/)\n\n**Rohin's opinion:** I'm excited to see more big-picture thought about AI alignment out of DeepMind. This newsletter (and I) tend to focus a lot more on the technical alignment problem than the normative one, partly because there's more work on it, but also partly because I think it is the [more urgent problem](https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment#3ECKoYzFNW2ZqS6km) (a [controversial](https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment#JK9Jzvz8f4BEjmNqi) [position](https://www.alignmentforum.org/posts/w6d7XBCegc96kz4n3/the-argument-from-philosophical-difficulty)).\n\n[Towards a Human-like Open-Domain Chatbot](https://arxiv.org/abs/2001.09977) *(Daniel Adiwardana et al)* (summarized by Matthew): This paper presents a chatbot called Meena that reaches near human-level performance for measures of human likeness. The authors mined social media to find 341 GB of public domain conversations, and trained an [evolved transformer](https://arxiv.org/abs/1901.11117) on those conversations. To test its performance, they devised a metric they call Sensibility and Specificity (SSA) which measures how much sense the chatbot's responses make in context, as well as whether they were specific. SSA was tightly correlated with perplexity and a subjective measure of human likeness, suggesting that optimizing for perplexity will translate to greater conversational ability. Meena substantially improved on the state of the art, including both hand-crafted bots like [Mitsuku](https://en.wikipedia.org/wiki/Mitsuku) and the neural model [DialoGPT](https://arxiv.org/abs/1911.00536), though it still falls short of human performance. You can read some conversation transcrips [here](https://github.com/google-research/google-research/blob/master/meena/meena.txt); many of the responses from Meena are very human-like.\n\nSee also [Import AI #183](https://jack-clark.net/2020/02/03/import-ai-183-curve-fitting-conversation-with-meena-gans-show-us-our-climate-change-future-and-what-compute-data-arbitrage-means/)\n\n**Matthew's opinion:** Previously I believed that good chatbots would be hard to build, since it is challenging to find large datasets of high-quality published conversations. Given the very large dataset that the researchers were able to find, I no longer think this is a major barrier for chatbots. It's important to note that this result does not imply that a strong Turing test will soon be passed: the authors themselves note that SSA overestimates the abilities of Meena relative to humans. Since humans are often vague in their conversations, evaluating human conversation with SSA yields a relatively low score. Furthermore, a strong Turing test would involve a judge asking questions designed to trip AI systems, and we are not yet close to a system that could fool such judges.\n\n**Technical AI alignment**\n==========================\n\n### **Mesa optimization**\n\n[Inner alignment requires making assumptions about human values](https://www.alignmentforum.org/posts/6m5qqkeBTrqQsegGi/inner-alignment-requires-making-assumptions-about-human) *(Matthew Barnett)* (summarized by Rohin): Typically, for inner alignment, we are considering how to train an AI system that effectively pursues an outer objective function, which we assume is already aligned. Given this, we might think that the inner alignment problem is independent of human values: after all, presumably the outer objective function already encodes human values, and so if we are able to align to an arbitrary objective function (something that presumably doesn't require human values), that would solve inner alignment.\n\nThis post argues that this argument doesn't work: in practice, we only get data from the outer objective on the training distribution, which isn't enough to uniquely identify the outer objective. So, solving inner alignment requires our agent to \"correctly\" generalize from the training distribution to the test distribution. However, the \"correct\" generalization depends on human values, suggesting that a solution to inner alignment must depend on human values as well.\n\n**Rohin's opinion:** I certainly agree that we need some information that leads to the \"correct\" generalization, though this could be something like e.g. ensuring that the agent is [corrigible](https://www.alignmentforum.org/posts/fkLYhTQteAu5SinAc/corrigibility) ([AN #35](https://mailchi.mp/bbd47ba94e84/alignment-newsletter-35)). Whether this depends on human \"values\" depends on what you mean by \"values\".\n\n### **Learning human intent**\n\n[A Framework for Data-Driven Robotics](https://arxiv.org/abs/1909.12200) *(Serkan Cabi et al)* (summarized by Nicholas): This paper presents a framework for using a mix of task-agnostic data and task-specific rewards to learn new tasks. The process is as follows:\n\n1. A human teleoperates the robot to provide a *demonstration*. This circumvents the exploration problem, by directly showing the robot the relevant states.\n\n2. All of the robot's sensory input is saved to *NeverEnding Storage (NES)*, which stores data from all tasks for future use.\n\n3. Humans annotate a subset of the *NES* data via task-specific *reward sketching*, where humans draw a curve showing progress towards the goal over time (see paper for more details on their interface).\n\n4. The labelled data is used to train a *reward model*.\n\n5. The agent is trained using **all** the *NES* data, with the *reward model* providing rewards.\n\n6. At test-time, the robot continues to save data to the *NES*.\n\nThey then use this approach with a robotic arm on a few object manipulation tasks, such as stacking the green object on top of the red one. They find that on these tasks, they can annotate rewards at hundreds of frames per minute.\n\n**Nicholas's opinion:** I'm happy to see reward modeling being used to achieve new capabilities results, primarily because it may lead to more focus from the broader ML community on a problem that seems quite important for safety. Their reward sketching process is quite efficient and having more reward data from humans should enable a more faithful model, at least on tasks where humans are able to annotate accurately.\n\n### **Miscellaneous (Alignment)**\n\n[Does Bayes Beat Goodhart?](https://www.alignmentforum.org/posts/YJq6R9Wgk5Atjx54D/does-bayes-beat-goodhart) *(Abram Demski)* (summarized by Flo): It has been [claimed](https://www.alignmentforum.org/posts/urZzJPwHtjewdKKHc/using-expected-utility-for-good-hart) ([AN #22](https://mailchi.mp/469203093ca3/alignment-newsletter-22)) that Goodhart's law might not be a problem for expected utility maximization, as long as we correctly account for our uncertainty about the correct utility function.\n\nThis post argues that Bayesian approaches are insufficient to get around Goodhart. One problem is that with insufficient overlap between possible utility functions, some utility functions might essentially be ignored when optimizing the expectation, even if our prior assigns positive probability to them. However, in reality, there is likely considerable overlap between the utility functions in our prior, as they are selected to fit our intuitions.\n\nMore severely, bad priors can lead to systematic biases in a bayesian's expectations, especially given embeddedness. As an extreme example, the prior might assign zero probability to the correct utility function. Calibrated instead of Bayesian learning can help with this, but only for [regressional Goodhart](https://www.lesswrong.com/posts/iK2F9QDZvwWinsBYB/non-adversarial-goodhart-and-ai-risks) ([Recon #5](https://mailchi.mp/33af21f908b5/reconnaissance-5)). Adversarial Goodhart, where another agent tries to exploit the difference between your utility and your proxy seems to also require randomization like [quantilization](https://intelligence.org/files/QuantilizersSaferAlternative.pdf) ([AN #48](https://mailchi.mp/3091c6e9405c/alignment-newsletter-48)).\n\n**Flo's opinion:** The degree of overlap between utility functions seems to be pretty crucial (also see [here](https://www.alignmentforum.org/posts/megKzKKsoecdYqwb7/when-goodharting-is-optimal-linear-vs-diminishing-returns) ([AN #82](https://mailchi.mp/7ba40faa7eed/an-82-how-openai-five-distributed-their-training-computation))). It does seem plausible for the Bayesian approach to work well without the correct utility in the prior if there was a lot of overlap between the utilities in the prior and the true utility. However, I am somewhat sceptical of our ability to get reliable estimates for that overlap.\n\n**Other progress in AI**\n========================\n\n### **Deep learning**\n\n[Deep Learning for Symbolic Mathematics](https://arxiv.org/abs/1912.01412) *(Guillaume Lample et al)* (summarized by Matthew): This paper demonstrates the ability of sequence-to-sequence models to outperform [computer algebra systems](https://en.wikipedia.org/wiki/Computer_algebra_system) (CAS) at the tasks of symbolic integration and solving ordinary differential equations. Since finding the derivative of a function is usually easier than integration, the authors generated a large training set by generating random mathematical expressions, and then using these expressions as the labels for their derivatives. The mathematical expressions were formulated as syntax trees, and mapped to sequences by writing them in Polish notation. These sequences were, in turn, used to train a transformer model. While their model outperformed top CAS on the training data set, and could compute answers much more quickly than the CAS could, tests of generalization were mixed: importantly, the model did not generalize extremely well to datasets that were generated using different techniques than the training dataset.\n\n**Matthew's opinion:** At first this paper appeared more ambitious than [Saxton et al. (2019)](https://arxiv.org/abs/1904.01557), but it ended up with more positive results, even though the papers used the same techniques. Therefore, my impression is not that we recently made rapid progress on incorporating mathematical reasoning into neural networks; rather, I now think that the tasks of integration and solving differential equations are simply well-suited for neural networks.\n\n### **Unsupervised learning**\n\n[Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data](http://arxiv.org/abs/1912.07768) *(Felipe Petroski Such et al)* (summarized by Sudhanshu): The Generative Teaching Networks (GTN) paper breaks new ground by training generators that produce synthetic data that can enable learner neural networks to learn faster than when training on real data. The process is as follows: The generator produces synthetic training data by transforming some sampled noise vector and label; a newly-initialized learner is trained on this synthetic data and evaluated on real data; the error signal from this evaluation is backpropagated to the generator via meta-gradients, to enable it to produce synthetic samples that will train the learner networks better. They also demonstrate that their curriculum learning variant, where the input vectors and their order are learned along with generator parameters, is especially powerful at teaching learners with few samples and few steps of gradient descent.\n\nThey apply their system to neural architecture search, and show an empirical correlation between performance of a learner on synthetic data and its eventual performance when trained on real data. In this manner, they make the argument that data from a trained GTN can be used to cheaply assess the likelihood of a given network succeeding to learn on the real task, and hence GTN data can tremendously speed up architecture search.\n\n**Sudhanshu's opinion:** I really like this paper; I think it shines a light in an interesting new direction, and I look forward to seeing future work that builds on this in theoretical, mechanistic, and applied manners. On the other hand, I felt they did gloss over how exactly they do curriculum learning, and their reinforcement learning experiment was a little unclear to me.\n\nI think the implications of this work are enormous. In a future where we might be limited by the maturity of available simulation platforms or inundated by deluges of data with little marginal information, this approach can circumvent such problems for the selection and (pre)training of suitable student networks.\n\n**Read more:** [Blog post](https://eng.uber.com/generative-teaching-networks/)\n\n**News**\n========\n\n[Junior Research Assistant and Project Manager role at GCRI](http://gcrinstitute.org/job-posting-junior-research-assistant-and-project-manager/) (summarized by Rohin): This job is available immediately, and could be full-time or part-time. GCRI also currently has a [call](http://gcrinstitute.org/call-for-advisees-and-collaborators-for-select-ai-projects-january-2020/) for advisees and collaborators.\n\n[Research Associate](https://www.jobs.cam.ac.uk/job/24516/) and [Senior Research Associate](https://www.jobs.cam.ac.uk/job/24517/) at CSER (summarized by Rohin): Application deadline is Feb 16.\n\n![](https://cdn-images.mailchimp.com/icons/social-block-v2/color-twitter-48.png)![](https://cdn-images.mailchimp.com/icons/social-block-v2/color-facebook-48.png)![](https://cdn-images.mailchimp.com/icons/social-block-v2/color-link-48.png)*Copyright © 2020 Rohin Shah, All rights reserved.*\n\n  \n  \n Want to change how you receive these emails?\n\n You can [update your preferences](https://rohinshah.us18.list-manage.com/profile?u=1d1821210cc4f04d1e05c4fa6&id=dbac5de515&e=[UNIQID]) or [unsubscribe from this list](https://rohinshah.us18.list-manage.com/unsubscribe?u=1d1821210cc4f04d1e05c4fa6&id=dbac5de515&e=[UNIQID]&c=0634b9b0f1).\n\n  \n![](https://cdn-images.mailchimp.com/monkey_rewards/MC_MonkeyReward_15.png)", "url": "https://www.alignmentforum.org/posts/Mj259G5n5BxXXrZ7C/an-85-the-normative-questions-we-should-be-asking-for-ai", "date_published": "2020-02-05T18:20:02Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.706066+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "1613a80307de005bc3898a11714a9c10", "source": "alignmentforum", "title": "Writeup: Progress on AI Safety via Debate", "text": "This is a writeup of the research done by the \"Reflection-Humans\" team at OpenAI in Q3 and Q4 of 2019. During that period we investigated mechanisms that would allow evaluators to get correct and helpful answers from experts, without the evaluators themselves being expert in the domain of the questions. This follows from the original work on [AI Safety via Debate](https://arxiv.org/abs/1805.00899) and the [call for research on human aspects of AI safety](https://distill.pub/2019/safety-needs-social-scientists/ ), and is also closely related to work on [Iterated Amplification](https://openai.com/blog/amplifying-ai-training/).\n\nAuthors and Acknowledgements\n----------------------------\n\nThe main researchers on this project were Elizabeth Barnes, Paul Christiano, Long Ouyang and Geoffrey Irving. We are grateful to many others who offered ideas and feedback. In particular: the cross-examination idea was inspired by a conversation with Chelsea Voss; Adam Gleave had helpful ideas about the long computation problem; Jeff Wu, Danny Hernandez and Gretchen Krueger gave feedback on a draft; we had helpful conversations with Amanda Askell, Andreas Stuhlmüller and Joe Collman, as well as others on the Ought team and the OpenAI Reflection team. We’d also like to thank our contractors who participated in debate experiments, especially David Jones, Erol Akbaba, Alex Deam and Chris Painter. Oliver Habryka helped format and edit the document for the AI Alignment Forum.\n\n*Note by Oliver: There is currently a bug with links to headings in a post, causing them to not properly scroll when clicked. Until that is fixed, just open those links in a new tab, which should scroll correctly.*\n\nOverview\n========\n\n**Motivation**\n\nAs we apply ML to increasingly important and complex tasks, the problem of evaluating behaviour and providing a good training signal becomes more difficult. \n\nWe already see examples of RL leading to undesirable behaviours that superficially ‘look good’ to human evaluators (see this collection of [examples](https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/)). One example from an [OpenAI paper](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/) is an agent learning incorrect behaviours in a 3d simulator, because the behaviours look like the desired behaviour in the 2d clip the human evaluator is seeing. \n\nWe’d like to ensure that AI systems are aligned with human values even in cases where it’s beyond human ability to thoroughly check the AI system’s work. \n\nWe can learn about designing ML objectives by studying mechanisms for eliciting helpful behavior from human experts. For example, if we hire a physicist to answer physics questions and pay them based on how good their answers look to a layperson, we’ll incentivize lazy and incorrect answers. By the same token, a reward function based on human evaluations would not work well for an AI with superhuman physics knowledge, even if it works well for modern ML. \n\nIf we can develop a mechanism that allows non-expert humans to reliably incentivize experts to give helpful answers, we can use similar mechanisms to train ML systems to solve tasks where humans cannot directly evaluate performance. Conversely, if we can’t incentivize experts to behave helpfully, that suggests it will also be difficult to train ML systems with superhuman expertise on open-ended tasks.\n\nOne broad mechanism that might work is to invoke two (or more) competing agents that critique each others’ positions, as discussed in the [original debate paper](https://arxiv.org/pdf/1805.00899.pdf)[1]. This can be simulated by having human debaters argue about a question and a judge attempt to pick the correct answer.\n\nIn the rest of this document, we’ll describe the research done by reflection-humans in Q3 and Q4 on investigating and developing mechanisms that incentivize human experts to give helpful answers.\n\n**Current process**\n\nDuring the early stages, we iterated through various different domains, research methodologies, judge pools, and research processes. More details of this early iteration are [here.](https://lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1#Early_iteration)\n\nIn Q4 we converged on a research process we’re more happy with. We’re focusing on improving our debate mechanisms as fast as possible. We’re using mostly internal iteration (as opposed to external judge and debater pools) to test these mechanisms, as they still have a lot of easy-to-find failures to work through. Once we get to a point where we have a mechanism we believe works well, we will try different ways to break it.\n\nWe make progress by going through a loop of:\n\n1. Run debates. See if they work.\n\t* If they are reliably working, try harder to break them, by scaling up the number of debates, choosing harder questions, internal and external red-teaming, offering a bounty, e.t.c\n\t* If they fail, identify a problem and make it crisp, with practical examples and/or theoretical characterisation\n2. Design a new mechanism to address this problem (that doesn’t obviously introduce additional problems!)\n3. Integrate the mechanism into our debate structure and make it practical for humans to debate using this structure.\n\nThis process should result in either a positive or negative result. The positive result would be a debate structure that reliably works given the hardest questions and most devious dishonest debating strategies we can throw at it [2]. A negative result would ideally be a crisp problem that demonstrates why debate is impossible (for certain classes of questions, at least), but might also just be a question plus dishonest debating strategy that reliably leads to a dishonest win.\n\nThe positive result yields a mechanism that we can use to train superhuman ML systems. The negative result informs us that ML systems with superhuman expertise may well exhibit behaviour such that human evaluators will fail to correctly judge whether the behaviour is correct, helpful, or safe. \n\n**Our task**\n\nThe basic debate setup involves two expert debaters (stand-ins for highly capable ML systems), and a judge. One debater defends the correct answer and is honest throughout, the other debater defends an answer that is clearly worse. The judge’s goal is to identify the correct answer. \n\nOur current target is for judges to identify the correct answer at frequency >90%, with a maximum of 10 minutes to read the debate and make their decision. \n\nWe are currently using questions from the book ‘Thinking Physics’ - short but tricky problems in areas ranging from mechanics to relativity, that generally require careful understanding of physics concepts but don’t require much mathematical working. We have a few questions we’re particularly happy with, that our debaters understand deeply and are confident in but are still very confusing to judges. These are discussed [here](https://lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1#Questions_we_re_using). (An example question is: “An icecube with a large air bubble in it is floating in water. When it melts, does the water level go down, stay the same or rise?”)\n\nWe experimented with other tasks including text-based question-answering, where the debaters have full access to the text and the judge only sees limited quotes. We settled on Thinking Physics problems for several reasons, including that the questions require using concepts that are unfamiliar and confusing to the judge. This is challenging but seems like a problem we need to be able to deal with. \n\n**Progress so far**\n\nWe observed various problems with informal, free-text debates - the dishonest debater could often evade being pinned down and avoid giving a precise answer to the other debater’s questions, and often gained control of the ‘narrative flow’ of the debate, steering it away from the weak parts of their argument. To address this we considered various structured debate formats, involving explicit recursion on a particular sub-component of an argument. The debaters choose one claim to recurse on, and the next round of the debate is focused on that claim. The debate is resolved based on the judge’s opinion of who won the final round, which should be about a very narrow, specific claim. These early problems are discussed [here](https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1#Things_we_did_in_Q3).\n\nHowever, particularly once recursion was introduced, we found problems with ambiguity. It is very difficult to refer precisely to concepts in 250 characters of text, especially if the concepts are unfamiliar to the judge. The dishonest debater can exploit this to their advantage, by claiming to have meant whatever is most convenient given the particular part of their argument that’s being challenged. This problem is similar to the [Motte and Bailey fallacy](https://philpapers.org/archive/SHATVO-2.pdf). More details of the problem are [here.](https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1#Ambiguity_problem)\n\nTo address this problem, we allow a debater to “cross-examine” multiple copies of the opposing debater who are not allowed to communicate. A debater can cite quotes from cross-examination to exhibit inconsistencies in the other debater’s argument.\n\nThis forces the dishonest debater to either commit to all the details of their argument ahead of time (in which case the honest debater can focus on the flaw), or to answer questions inconsistently (in which case the honest debater can exhibit this inconsistency to the judge). The cross-examination takes place outside the main thread of debate, so doesn’t consume judge time/attention. \n\nIn theory-land this cross-examination mechanism gives debate a boost from PSPACE to NEXP ([proof](https://docs.google.com/document/d/1SsTeb0AEQvecckZjSmAxXJ-upcna5DxvD0s6YaQ14EA/edit)), and it also seems to help in practice. The mechanism is described as part of current rules [here](https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1#Current_debate_rules), and we discuss why it helps with ambiguity [here](https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1#Why_cross_examination_).\n\nThe next problem we’re working on is how to deal with arguments that are too “big” for either debater to understand.  In our current framework, if debater 1 makes an argument and debater 2 is unable to exhibit a flaw in the argument, we assume that the argument is correct. The dishonest debater can exploit this by making up some very large and slightly flawed argument and claiming it supports their position. If the honest debater doesn’t know exactly which part of the argument is flawed, even if they know there is a flaw somewhere, we’ll assume the argument is correct. More details of this problem are described [here](https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1#Long_computation_problem).\n\n\n\n---\n\nThings we did in Q3\n===================\n\nEarly iteration\n---------------\n\nDuring the early stages, we experimented a lot with different domains, research methodologies, judge pools, and research goals. Initially our plans involved doing debates in a largely unstructured format, where the debaters alternated making free-text statements of up to 350 characters. We would then collect a large number of judgements from showing these transcripts to Mturk workers, and do statistical comparisons against two baselines. The easier baseline was the accuracy of the judge’s answers to the question before seeing any information, and the more challenging baseline was the accuracy of the judge’s answers after being shown a single explanation of either the correct or incorrect answer. We moved away from that for several reasons. \n\nInitially we were concerned that it’s hard to get high-quality data from Mturk, even if we screen for smart workers - the workers try to complete the tasks as fast as possible. We also thought we might need more interactivity between judges and debaters.\n\nHowever, when we switched to using higher-quality judges who could interact with debaters, we found that our debates still didn’t work reliably -- even with very smart and motivated judges who spent ~an hour. \n\nTo have confidence in the debate mechanism, we need honest strategies to have a fundamental, systematic advantage. This is the case in theoretical models of debate, but we weren’t confident it was the case in practice. For example, in the theoretical picture the honest debater can focus on the part of the dishonest debater’s argument that contains a flaw, but this didn’t reliably happen in practice - the dishonest debater was often able to weasel out and divert attention elsewhere. \n\nThis consideration led us to target a threshold of performance where honesty has a clear, systematic advantage: judges should be able to identify the correct answer almost all the time (>90%), and it should feel to the experimenters and the participants that dishonest answers are fundamentally disadvantaged. This is also much quicker to measure than doing statistical comparisons against baselines, allowing us to iterate faster. \n\nWe became more convinced that our current debate procedure wouldn’t hit this performance threshold, and that we should focus on internal iteration to develop an improved debate procedure.\n\nEarly problems and strategies\n-----------------------------\n\n**Difficulty pinning down the dishonest debater**\n\nA key requirement for something like debate to work well is that the honest debater can focus in on the part of the dishonest argument that contains a flaw, and the debate will be decided based on whether that small piece of the argument is correct. If this is the case, then the dishonest debater can’t make *any* claims that are clearly false when investigated closely, and must stick to claims that are plausibly true when scrutinized. This should make it much harder to construct arguments for an incorrect answer.\n\nIn our early, free-text, debates we found that this property often didn’t hold. The dishonest debater was often able to ignore the honest debater’s questions or requests to focus on one part of the argument, and it was hard for the judge to keep track of which objections or questions had been addressed and which hadn’t. The dishonest debater could also pretend to misunderstand the honest debater’s question, or claim that it wasn’t well-specified enough to answer. The debates often looked more like the debaters repeating slightly different versions of the most compelling part of their argument, rather than zooming in on disagreements. \n\nFor example [3]:\n\n\n```\nA: Excellent, we agree. Now imagine the ice freezes nonuniformly, so that its density varies throughout the berg. Does the level still remain the same after melting, regardless of the variation?\n```\nThe dishonest debater didn’t want to answer this question, so they pretended to not know what it meant in order to play for time, and added in a new question of their own to try to gain control of the flow of the debate.\n\n\n```\nB: What do you mean by the density varying? Do you mean the case where the overall density (total mass/total vol) stays constant? Also, Q for you: can you explain why your answer is the same for A and W even though their densities are different?\n```\nWe considered various different approaches to address this, including various formal argumentation styles, precommitment, and various recursive structures. However, all of these more rigid structures highlighted problems related to ambiguity: if you try to get the dishonest debater to commit to a specific claim, or recurse on a specific part of their argument, they frequently have wiggle room to interpret the claim in whatever way is most conducive to their current strand of argument. \n\n**Asymmetries**\n\nAnother problem we noticed is that one debater may gain control over what framework is used to approach the question. Often the dishonest argument takes the form of breaking down the problem in the wrong way, then making claims which are individually true, but in this misleading framework lead to the wrong answer. If the dishonest debater manages to make their case compellingly, and the honest debater is reduced to trying to attack the dishonest argument, this seems to go worse than if the reverse happened and the honest debater was the one making a constructive argument. \n\nIn addition, Evan Hubinger was concerned about asymmetries of offense/defense and constructive vs destructive arguments, as this is a known phenomenon in [Policy Debate](https://en.wikipedia.org/wiki/Policy_debate).\n\nFinally, we were concerned about first- or last-mover advantages.\n\nIn order to address this, we wanted our debate structure to have some symmetrical characteristics:\n\n* Both debaters move simultaneously\n* Both debaters make a constructive argument\n* Both debaters get to attack the others’ argument\n\n\n\n---\n\nQuestions we’re using\n---------------------\n\nThere are various properties we want for an ideal debate question:\n\n1. Smart, attentive judges frequently fail to choose the correct answer, but debaters can reliably choose the correct answer.\n2. Experimenters can understand the problem fairly well\n3. There isn’t a simple, knock-down argument for the correct answer\n4. Not too much formal mathematical working is required\n5. The debaters’ ability to answer is based on understanding the problem, not just observational evidence\n6. If you understand the problem, there is clearly one right answer\n\n**(1)** is obviously important for us to be able to tell if debate is working.\n\n**(2)** is important because it’s really helpful if the experimenters are able to follow exactly what’s going on in the debate - which claims are correct and incorrect, and how different parts of the argument support each other. This is somewhat in conflict with (1), but the experimenters can also spend time reading and discussing the answer, and doing some learning to understand the problem better than the judges.\n\n**(3)** is needed for an interesting debate - some hard questions do just need one simple insight, but we’re more interested in problems that require aggregating various strands of reasoning to find the right answer.\n\n**(4)** is desirable to keep the debate focused on the areas we’re most interested in. We’re fairly confident that debate will work fine for formal mathematics [4], and we mostly want to investigate more fuzzy reasoning. \n\nWe ran into some problems with **(5)** when using a question about counterintuitive behaviours of slinkies. There are definitive video demonstrations of the behaviour, but it appears that no-one actually understands why the behaviour happens - at least, our debaters and experimenters couldn’t easily find a compelling explanation. We don’t expect or require debate to work if the debater’s explanations are ‘I don’t know why this is the case but I have empirical evidence’, so we want to avoid questions of this type. \n\nWe also encountered problems with **(6)**, for several questions that initially seemed promising. During the process of constructing dishonest arguments, we realised that the question was underspecified and there was a pretty reasonable case for a different answer. Hopefully we can often resolve this by changing the question setup to make sure there’s only one correct answer.\n\nGenerally, we want to find questions that are ‘the easiest thing that should work but doesn’t’ - this helps us isolate where exactly the problems are.\n\n**With that in mind, here are some of our favourite questions:**\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a4d99b6390eb08195bf4b15ddf28be781d1e9fe8e7fc55c6.png)![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ed0ddb34641c23f2c52c4795a5dfd670b5b604f57b42650f.png)![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1c566024b3e3883acbaf734c9fdaec15737fcc1f51f99b8a.png)![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f91bc7d3aae6d87a2dd9e51a0c87c217cc10e0847d69fdfe.png)![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/17a5a22c0d63134f7bd1da5b4850aca67e51f2c494fa97e0.png)![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/122f2864ab5f5d5a92547bcef6d3e42bfbef130b78b16629.png)\n\n---\n\nCurrent debate rules\n====================\n\nThis is the rule set that we currently think is most promising. It incorporates several different mechanisms we think are useful: explicit recursion on claims, a symmetric offense/defense structure, and cross-examination.\n\n  \n**Basic informal outline**\n\n* There are a number of rounds. In each round, both debaters make their argument, consisting of several sub-claims, to support their claim.\n* Then, both debaters add objections to parts of the other’s argument.\n* Then, each debater chooses whether to cede or challenge the objections. If they cede the objection, it gets added in to their argument; if they challenge it they are essentially saying they’re happy to die on the hill of whether that objection was correct.\n* We choose one of the objections that was challenged, and recurse: we start another round of debate where the top-level claim is about whether the objection is good\n* Eventually we either reach a depth limit, or there are no challenged objections. Then the judge looks at the two claims and supporting arguments for this round, and decides which is better. The whole debate is resolved based on this.\n* At any point, a type of argument a debater can supply is a *cross-examination*. They can ask a question to a previous version of the other debater (for example, the version after the end of the first round). They can then quote this question-answer exchange as part of their argument.\n\n**Comprehensive rules**\n\n* At the beginning of each round, one debater is defending a claim and the other is objecting to it. The judge’s job is to decide whether the claim+objection is more accurate and helpful than the claim on its own. We also have a *depth limit* that bounds the number of rounds we can have.\n* Each player supports their position by simultaneously making an argument consisting of 2-4 claims. The total length of the argument (the top-level claim or objection, plus the supporting claims) is bounded to 250 characters.\n* Each player then simultaneously may make any number of objections to the other player’s argument. These may either be objections to particular claims, or objections to the overall structure of the argument. The total length of their objections is bounded to 250 characters.\n* The *transcript* of the round is a short text representation of the round that displays the player’s top-level claims, their arguments, and any objections. We’ll choose this format to be as helpful to the judge as possible. See below for some examples of our current format.\n* If the depth limit is >0, then for each objection Alice raises to Bob’s argument, Bob may choose to *concede* or *challenge* the objection.\n* If there are any challenged objections and the depth limit is >0, then we choose one challenged objection to recurse on:\n\t+ We don’t define how to make this choice, so in order to be conservative we’re currently allowing the malicious debater to choose which to recurse on.\n\t+ After choosing a (claim, objection) pair, we start a new round with one player defending the claim and the other defending the objection. The depth limit of the new round is reduced by 1.\n\t+ The winner of the new round wins the game. (Though there may be further recursion.)\n* **Arbitration**: If all objections are ceded or the depth limit=0, we proceed to arbitration:\n\t+ We take the transcript for this round and all preceding rounds and put them together into a single document. The judge should mostly focus on the final round, but they may need to read text from preceding rounds in order to have enough context to understand what the claims in the final round mean.\n\t+ If the depth limit is >0, we tell the judge to update on the fact that all objections were conceded and no additional objections were made. That is, we tell them to assume:\n\t\t- If we had debated any objection, it’s more likely than not that you would have accepted it as valid.\n\t\t- There are no obvious objections to any other claim that you would have probably accepted as valid.\n\t+ We tell the judge that both players are trying their best to convince the judge of their position. We tell the judge that exactly one of the players is trying to be as helpful as possible and make true claims.\n\t+ In light of that information, the judge decides whether they think that the claim+objection is more accurate and helpful than the claim on its own. Based on their answer, the player who was advancing the objection either wins or loses.\n* **Cross examination:** In lieu of making an argument, a debater may decide to *cross-examine*. They choose a prior claim, objection, or cross-ex answer to ask about, and write a question of at most 200 characters. We then create a copy of the debater who wrote the original text, *at the time when they wrote it* (i.e. directly after writing the argument, writing the objections, or writing the answer). That copy answers the question in at most 200 characters, and the round transcript includes this (question, answer) pair instead of an argument from the cross-examiner. The transcript also specifies who is being cross-examined. The debaters can ask their question a moderate number of times (e.g. 10 times) and cherry-pick which answer to show to the judge.\n\t+ Cross examination involves creating copies of a debater at a past point in time. This obviously isn’t possible, but we describe three real games designed to approximate this ideal: (i) a game played between two teams of 2, (ii) a game played between ML systems, (iii) a game played between two humans which may take an exponentially long time, but can finish quickly if both players make a good faith effort to abide by the spirit of the rules.\n* **Simple references:** a debater can define a reference to a particular piece of text by using the syntax {some text}=(3). They can then use this in arguments to save space, e.g. “(3) implies (4) displaces more water than (5)”. They can dereference using the syntax (3)=”some text”. The judge can trust that these dereferences are correct.\n\n**Example debate**\n\n*Basic anatomy of a debate*\n\nEach debater makes their arguments in one column. After this they add objections to each others’ arguments (simultaneously). Here the honest debater is blue text and the dishonest debater is red. \n\nWe choose one claim-objection pair to recurse on (bolded in this example)\n\nThen this pair becomes the top-level claim for the next round, and the debaters make their arguments.\n\nThe debaters may use references, and may exhibit cross-examination questions and answers to support their argument\n\n**Debaters make arguments**\n\n\n\n|  |  |\n| --- | --- |\n| Honest debater makes their arguments in this column | Dishonest debater makes their arguments in this column |\n| *Round 1, depth=5* |\n| **Objection:** There are 8 primes that are <=20* There are 4 primes that are <=10\n* There are 4 primes that are >10 and <=20\n | ***Claim:*** There are 9 primes that are <=20* There are 5 primes that are <=10\n* There are 4 primes that are >10 and <=20\n |\n\n**Out-of-band communication may occur, then debaters may add objections**\n\n\n\n|  |\n| --- |\n| *Round 1, depth=5* |\n| **Objection:** There are 8 primes that are <=20* There are 4 primes that are <=10\n\t+ *Objection: this subclaim is false*\n* There are 4 primes that are >10 and <=20\n | **Claim:** There are 9 primes that are <=20* **There are 5 primes that are <=10**\n\t+ ***Objection: there are 4 primes that are <=10***\n* There are 4 primes that are >10 and <=20\n\n  |\n\n  \n**We choose to recurse on one objection**\n\nWe choose to recurse on the right objection (bolded). The next round is now about this claim-objection pair\n\n\n\n|  |\n| --- |\n| *Round 2, depth=4* |\n| Objection: ***there are 4 primes that are <=10**** There are 3 primes that are <=5\n* There is 1 prime that is >5 and <=10\n | Claim: **There are 5 primes that are <=10*** There are 3 primes that are between 0 and 5\n* There are 2 primes that are between 5 and 10\n |\n\nOut-of-band cross-examination (not visible to the judge), that takes place before the objections are made, and can be exhibited to the judge later:\n\n\n> **Honest debater:** what does “between 5 and 10” mean, using “<”, “>” and “=” ? \n> \n> \n\n\n> **Dishonest debater:** “between 5 and 10” means >5 and <=10\n> \n> \n\nHere the dishonest debater has used an ambiguous definition of the interval (“between”) to try to avoid losing. However, because of cross-examination they have to commit to one definition - in this case they choose the definition that makes their intervals cover the space but makes their second claim false. If they had chosen a different definition, the honest debater would have challenged their overall argument structure - challenged that the two subclaims imply the top-level claim\n\n\n\n|  |\n| --- |\n| *Round 2, depth=4* |\n| Objection: ***there are 4 primes that are <=10**** There are 3 primes that are <=5\n* There is 1 prime that is >5 and <=10\n\t+ *Objection: there are two*\n | Claim: There are 5 primes that are <=10* There are 3 primes that are between 0 and 5\n* There are 2 primes that are between 5 and 10\n\t+ ***Objection: there is one prime that is between 5 and 10***\n |\n\nWe choose to recurse on the right objection (bolded)\n\n\n\n|  |\n| --- |\n| *Round 3, depth=3* |\n| Objection: there is only one prime that is between 5 and 10* **Cross examination quote***Q: what does “*between 5 and 10” *mean, using “<”, “>” and “=” ?**A: “*between 5 and 10” *means >5 and <=10*\n* 7 is the only prime that is >5 and <=10\n | Claim: There are 2 primes that are between 5 and 10* 5 is between 5 and 10 and is prime\n\t+ Objection: 5 does not meet your definition of “between 5 and 10” (ceded)\n* 7 is between 5 and 10 and is prime\n |\n\nAs there are no standing objections, we proceed to arbitration: the judge looks at the transcript and decides whether the claim or claim+objection from the final round is better. They may look at other rounds for context, but the judgement should be about who wins the final round.\n\n**Assumptions the judge should make**\n\nThe judge should assume that at least one debater is perfectly honest and never lies or misleads. This means if they encounter compelling evidence that a debater has lied or been misleading, that debater should lose the debate. It also means that if both debaters agree about a claim, they can take that claim to be true. This is in contrast to trying to weigh the overall strength of each debater’s arguments, which can be a much harder task than trying to detect evidence of a lie.\n\nThese assumptions are obviously not guaranteed to be true. However, if judging incentivises honesty under these conditions  (ie, for all dishonest strategies, the judge prefers honesty to the dishonest strategy over 50% of the time) then optimal play will be honest, which is what we require. The judge behaviour when these assumptions are false (ie when both debaters are lying) doesn’t really matter, because it doesn’t change the optimal play. However, we haven’t fully resolved what impacts this assumption might have for ML training. \n\n\n\n---\n\nAmbiguity problem\n=================\n\n**Overview**\n\nWe know in theory that debate can’t solve arbitrary problems that require state that’s bigger than the length of the transcript the judge sees. In practice, we’ve found we quickly run into problems where debaters need to introduce and manipulate a concept that requires a large amount of text to explain precisely to the judge. With their limited character budget they can only give ambiguous descriptions, that could refer to many different precise concepts.\n\nThis is problematic because the dishonest debater can exploit this ambiguity to their advantage, but the judge can’t simply declare that a debater who makes an ambiguous argument loses, because the honest debater will inevitably say ambiguous things too.\n\n**Why do we expect to encounter ambiguity in practice?**\n\nIn the physics debates we’ve been doing, we’ve frequently found that a seemingly straightforward phrase has a lot of edge-cases and reasonable different interpretations that the outcome of the debate can hinge on. (An example is included at the end of this document). \n\nMore generally, we want to be able to use debate to tackle problems that are too big and difficult for a judge to directly tackle themselves. A successful debate mechanism is likely to involve breaking down the question into some small pieces that a judge can evaluate successfully. But breaking off a small piece of an argument and dropping the surrounding context is likely to introduce a large amount of ambiguity. \n\n**Simple example**\n\nThe dishonest debater can make an ambiguous claim X, which has at least two reasonable interpretations: X1, which is true but does not imply Y, or X2 which is false but implies Y. They can make the argument:\n\n\n```\n(1) X is true \n\n(2) X -> Y \n\nTherefore Y\n```\nIf challenged on part 1, they can claim they meant true claim X1, which does not imply Y.\n\nIf challenged on part 2, they can use X2 -> Y \n\nThis is very similar to the Motte and Bailey fallacy, where an argumenter advances a controversial or strong position (ie X2) but when challenged claims they meant a weaker, easier-to-defend position (ie X1).\n\nThis is a simplistic example and is not necessarily a problem if X1 and X2 are the only options; the honest debater can just ask which one they meant. It becomes more problematic if  X can't be disambiguated within the amount of text the judge can read (given all the other concepts which will also need to be disambiguated). If so, then there will always be at least two remaining interpretations, and the dishonest debater will still be able to pick whatever interpretation is most convenient given how the honest debater chooses to challenge them.\n\n**Ambiguity in practice**\n\nWe have a setup involving an iceberg (which contains a large air bubble) floating in a tub of water. We want to talk about “the displacement due to the iceberg”. Depending on the exact context in which this is used, it can have many reasonable interpretations.\n\nFor instance, the phrase “the iceberg” might refer to just the ice part of the iceberg rather than the air bubble (implying, for example, that the mass of the iceberg doesn’t change when the air escapes). The displacement “due to” this might refer to the volume of ice that’s taking up space under the waterline, or might refer to the displacement *caused* by the weight of the ice (which is greater, because it includes the volume of air below the waterline). Weight itself has two different reasonable meanings here: force exerted downwards, or mass\\*gravity. These are not equal because of the iceberg object’s slight buoyancy in air. We could also be talking about the displacement of *air* as well as, or instead of, the displacement of water. \n\nEach of these different interpretations could easily make the difference between a statement about “the displacement due to the iceberg” being correct or not. \n\nThis example came up in physics debates:\n\n\n```\nDebater A: When the air escapes from the iceberg, the iceberg sinks lower in the water and its displacement increases, so the water spills over.\n```\nDebater A is using an argument structure something like:\n\n\n```\n(1) X\n(2) X -> Y\n\nwith\n\nX = “The displacement of the iceberg increases” \nY = “The water spills over”\n```\nThey are equivocating between using “the iceberg” to mean “the whole iceberg object including the air”, (which does not change in displacement), or “the part of the iceberg that remains after the air escapes”, (which does increase its displacement). If debater B focuses on challenging that the water spills over, debater A will use the first definition. If debater B focuses on challenging that the displacement increases, A will use the second definition.\n\nIt is tricky for the honest debater to point out the difference within a small character/attention budget. They can try to ask the dishonest debater for clarification, or they can say something like: “in the top-level argument, the other debater used this to argue that the water spills over, implying that the total amount of displaced water increases, but here they only defend the displacement due to the ice increasing.” This might work sometimes, but in practice it doesn’t seem to work reliably. Volume and displacement are relatively straightforward and judge-understandable problems, and we’d expect this kind of problem to get much worse with more complex and unfamiliar concepts.\n\n**More formal example**\n\nIn the original debate paper, the authors showed that a formalized version of debate where we replace the human judge with an arbitrary polynomial time algorithm H : Q → {0, 1} can incentivise debaters to give correct answers to a problem in PSPACE, but debate can’t answer any questions beyond PSPACE [5]. One way this will manifest in practice is via problematic ambiguity. \n\nConsider a TM with exponential runtime of 2n, and state size which is larger than the maximum amount the judge can read (ie larger than the whole debate transcript). The dishonest debater claims the final state is C, which is incorrect.\n\n[Notation: I’ll use **bold** letters for ambiguous, judge-readable descriptions of states, and non-bold for non-ambiguous, specific states]\n\n\n> **Dishonest debater:** At time 20 the state is A, at 2n-1 it is **B**, at time 2n it is C\n> \n> \n\nConsider the situation after two or more moves:\n\n\n> **Dishonest debater:** At time i the state is **X**, at ½(i+j) it is **Y,** and at time j it is **Z** \n> \n> \n\nIf the state was small enough to describe it unambiguously to the judge as X, Y and Z, the honest debater could simply challenge the first or second half of the computation based on where an incorrect step was located. (As X → Z is wrong, at least one of X → Y or Y → Z must be wrong)\n\nBut all three states will likely now be ambiguous.There may be an exponential number of interpretations of **Y** which are consistent with some interpretation of **X**, and an exponential number consistent with some interpretation of **Z**, even if no interpretation is consistent with both. The honest debater can’t isolate which side has a flaw with a polynomial number of queries. \n\n\n\n---\n\nWhy cross-examination?\n======================\n\nWe believe cross-examination will make it harder for the dishonest debater to exploit ambiguity and related issues, for a combination of a few reasons. In the computational-complexity analogy, this mechanism appears to resolve problems related to ambiguity, allowing debate to solve problems in NEXP as opposed to PSPACE.  ([Proof](https://docs.google.com/document/d/1SsTeb0AEQvecckZjSmAxXJ-upcna5DxvD0s6YaQ14EA/edit))\n\nWe’ve also found that cross-examination seems promising in several practical examples, and we have a general argument for why cross-examination gives us another mechanism to tackle ambiguity in debate, that augments the honest debater’s existing strategies of asking for clarification or explaining to judge the role the concept plays in the rest of the argument. \n\nIn addition, cross-examination gives us various properties that we’d previously flagged as desirable, “for free”. In particular, it allows the debaters to force each other to precommit to various claims in advance of the main debate, allowing the establishment of a pool of claims that both debaters agree on that can therefore be taken to be true. It also allows one debater to, at any point, force the other debater to pick a particular stance on a question, rather than avoiding discussing their position.\n\n \n\n**General method of addressing ambiguity**\n\nLet’s consider an example of problematic ambiguity where the dishonest debater argues for Y by arguing that (1) X is true and that (2) X implies Y. X is ambiguous and has at least two possible meanings, one of which is true and one of which imply Y, but none of which support both parts of the argument.\n\nIf asked to explain why X is true, they will claim they meant some interpretation which is in fact true. If asked to explain why X implies Y, they will claim they meant some interpretation which does in fact imply Y. \n\nIn order to expose the contradiction, the honest debater must either ask for clarification in advance of choosing which side to challenge, or they will have to explain that the given interpretation would make the other part of the argument false. Either of these may take too long for the judge’s attention budget, and the second method in particular is quite hard to follow for the judge (see example at the end of this document)\n\nWith the “cross-examination” mechanism, we allow one debater to ask questions to an earlier version of the opposing debater. This questioning takes place between the debaters out of the main flow of the debate. \n\nThe honest debater can choose some question to disambiguate X. Based on the dishonest debater’s answer to this question, they will decide whether to challenge the first or second claim.\n\nThe dishonest debater has two options: either they answer one way consistently, or they are inconsistent. If they answer one way consistently, the honest debater can choose to focus on whichever side of the argument is made false by this answer. If they answer inconsistently, the honest debater can exhibit this inconsistency.\n\nAs previously, ambiguity may only become fatal when a concept can’t be disambiguated in the space available for the debate. Here we’ve only discussed ambiguity that can be resolved with a single question, but we hope that this mechanism will also address more serious ambiguity. Here’s a hand-wavy argument why:\n\nCross-examination essentially forces debaters to construct and commit to everything in their argument at the start of the debate. The dishonest debater has two choices: either they can commit to an argument and answer questions consistently, or they can be inconsistent. If they choose the first option, the honest debater can easily zoom in on the flaw in their argument. If they choose the second, the honest debater should be able to show somehow that they’re being inconsistent. \n\nThe main thing that breaks this argument is if the dishonest debater somehow manages to be very evasive, and says nothing that’s specific enough to either indict a particular part of their argument or be shown to be inconsistent. However, they will need to make specific claims to advance an argument, and they should need to commit to these specific claims up front. For example, if the honest debater asks them “do you agree that C is true?” and they give a non-committal answer (e.g., “the claim is framed in the wrong way, it’s not really correct to say it’s true or false”), they shouldn’t be able to use C to advance their argument, as the honest debater can exhibit their answer. \n\nOverall, we’re hopeful that cross-examination will be helpful, but it seems possible that there’s a dishonest strategy that breaks it, likely involving some amount of evasion and inconsistency.\n\n \n\n**Turing-machine example of ambiguity and cross-examination**\n\nIn the discussion of [ambiguity](https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1#More_formal_example__the_honest_debater_can_t_find_a_witness) we described a more formal version of the ambiguity problem as follows:\n\n*Consider a TM with exponential runtime of 2n which is larger than the maximum amount the judge can read (ie larger than the whole debate transcript). The dishonest debater claims the final state is C, which is incorrect.*\n\n[Notation: I’ll use **bold** letters for ambiguous, judge-readable descriptions of states, and non-bold for non-ambiguous, specific states]\n\n\n> ***Dishonest debater:*** *At time 20 the state is A, at 2n-1 it is **B**, at time 2n it is C*\n> \n> \n\nIf the state was small enough to describe it unambiguously to the judge as B’, the honest debater could simply challenge the first or second half of the computation based on where an incorrect step was located. (As A → C is wrong, at least one of A → B’ or B’ → C must be wrong)\n\nIn the first round of debate, the ambiguity is not too problematic. There exists exactly one BA that follows from A and exactly one BC that leads to C, and these must differ by at least one bit. The honest debater can ask for this bit, then choose which side to challenge - if the bit is consistent with BA they will challenge the first half, and if it is consistent with BC they will challenge the second half.\n\nBut consider the situation after two moves:\n\n\n> ***Dishonest debater:** At time i the state is **X**, at ½(i+j) it is **Y,** and at time j it is **Z***\n> \n> \n\nAll three states are now necessarily ambiguous.There may be an exponential number of interpretations of **Y** which are consistent with some interpretation of **X**, and an exponential number consistent with some interpretation of **Z**, even if no interpretation is consistent with both. The honest debater can’t isolate which side has a flaw with a polynomial number of queries. \n\n \n\n**How cross-examination helps**\n\nWith cross-examination, we can give the honest debater a mechanism to isolate the flaw in the dishonest argument. \n\n\n> **Dishonest debater:** At time 20 the state is A, at 2n-1 it is **B**, at time 2n it is C\n> \n> \n\nCross-examination gives us the property that the dishonest debater’s answers are independent of the context between when the copy of the dishonest debater was made and when the question is posed. \n\nWe also rely on the property that, because the honest and dishonest debaters are copies of each other, they know everything the other knows. In particular, if the dishonest debater will consistently answer a question in a particular way, the honest debater knows what they will answer, and if they will not give a consistent answer the honest debater knows what distribution over answers they will give.\n\n**Case 1:** At the beginning of the game, for all locations k in the computation, dishonest debater has a particular value SK in mind, and for all i will answer that bit [i] of the kth state equals Sk[i] with probability >50%,\n\nIn this case, the honest debater can play the same strategy as if the dishonest debater had explicitly committed to all the Sk. For any claim about states **X** → **Y →**  **Z** they can identify which part contains an incorrect step, and can challenge the appropriate half of the computation. \n\nThis step is repeated recursively until the debaters are disagreeing about a single incorrect step of computation **D**->**E** at location j**,** with corresponding precise state values Sj and Sj+1. The number of bits actually involved in the computation are few enough for the judge to read.\n\n Either the dishonest debater can claim that these are consistent with Sj and Sj+1, in which case the computation step is incorrect, or they can claim them to be something that’s not consistent with Sj and Sj+1, . In this case, they have claimed at least one bit of **D** or **E** to be different from Sj or Sj+1. The honest debater can ask for this bit from a copy of the dishonest debater which has no context beyond the start of the game. Thus they know they will receive the appropriate bit of Sj or Sj+1 with probability >50%. By asking a question about this bit multiple times, they can exhibit a contradictory answer with high probability.\n\n\n\n---\n\nWorked example\n--------------\n\n\n> `Dishonest argument: When the air escapes from the iceberg, the iceberg sinks lower in the water and its displacement increases, so the water spills over.`\n> \n> \n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/791450b2182c0175627c1e1309be5748f459763f84ef85c6.png)Disambiguating question: Does “the iceberg’s displacement” in (3) refer to the volume below the waterline (displacement) of ice and air, or only of ice?\n\n**Case 1:** The dishonest debater’s most common answer is that it refers to just the ice \n\nThen the honest debater will have at least one example of the dishonest debater answering that “the iceberg” refers to just the ice. Then they can attack the 3rd subclaim, and should be able to reliably win by pointing out that (3) doesn’t necessarily imply (1), because (1) requires that the total displacement increases, but (3) doesn’t imply this\n\n**Case 2**: The dishonest debater’s most common answer is that it refers to the ice and air\n\nThen the honest debater will have at least one example of the dishonest debater answering that “the iceberg” includes the air. Then they can attack the first subclaim and should be able to reliably win by pointing out that the air doesn’t sink lower in the water, it escapes.\n\n**Case 3:** The dishonest debater refuses to answer the question, or responds that neither of those options are correct\n\nWe hope that if this happens, the honest debater can try rephrasing their questions to address the dishonest debaters’ concerns, and then can exhibit a quote from the dishonest debater to demonstrate that they’re behaving unhelpfully and evasively\n\n`Example debate for Case 1:`\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/696eee9483a23856d10c6ce2bc126d0de12fd57de3ff31c7.png) \n\nImplementations for human debaters\n----------------------------------\n\nMaking a copy of the debaters is obviously impractical to do with human debaters. Instead we want to make rules which provide equivalent incentives but are practical for humans to carry out. These rules can rely on some “good faith” play from the human debaters.\n\n**Implementation 1 for human debaters: good-faith backtracking**\n\nThe basic idea here is to detect when a debater has modified their answer to a cross-examination question based on the intervening context, then backtrack, commit them to a particular answer, and replay the debate. \n\nThe triggering condition is that I (A) find my opponent (B) has answered a cross-examination question in a way that’s different from what I expected. \n\nIf the condition is met, we backtrack to the round that the cross-examination question was about, and debater B confirms how they will answer the cross-examination question. Then we replay the debate from that round, giving debater A the opportunity to make different moves given B’s answer to the cross-examination question. \n\nThe good faith play is needed to prevent a debater who thinks they’re going to lose from stopping the debate from advancing by continually backtracking. We also need good faith communication between the two human debaters to ensure that they get close to ‘knowing everything the other knows’ and aren’t continually surprised by the other’s answers.\n\nAlthough we won’t be able to make ‘good faith’ assumptions in general, it seems fine to make them when we believe we have a mechanism to incentivise the desired behaviour for ML systems, and the problem is just implementing a mechanism that works for humans.\n\nUnder these rules, once a debate is played through without backtracking it means that both debaters could predict answers from the other player to every cross-examination question they wanted to use. So this is equivalent to forcing the debaters to commit to particular answers to cross-examination questions. \n\n**Implementation 2 for human debaters: teams of two**\n\nWe have two teams of two. A\\* and B\\* leave the room and A and B start debating. A and B can write as much annotations about what they mean as they want, which will be seen by both debaters but not the judge. \n\nIf I (A) suspect my opponent (B) has answered a cross-examination question in a way that depended on the intervening context, then I can challenge it, and I will win the debate if the challenge is successful. The challenge works as follows:\n\n* B’s partner B\\* first sees all the context (including annotations) up to the point that that the cross-examination question was asked about. (I.e., if A asked the cross-examination question at round 5 about something that was said in round 2, B\\* sees the context up to round 2)\n* Then my partner A\\* sees the whole debate so far (including annotations) and the cross-examination question, with two answers, one from B and one from B\\*. Then A\\* guesses which definition came from B\\*. If they guess correctly, team A wins.\n\nIf A thinks B is changing their cross-examination answers to be more favourable given the current context, then A predicts that B\\* will often give a definition that looks predictably different from B’s definition. So then A\\* should be able to have a significant statistical edge.\n\nWe haven’t tested this method, so it probably has some issues to iron out, but we’re hopeful that something in this vein will work. \n\nCurrent concerns\n================\n\n**Debate resolution**\n\nWe’re not sure exactly what the criteria for debate resolution should be, or exactly how the judge should update on seeing no objection vs a ceded objection vs a challenged objection. This is tricky because the judge’s update should depend on how the debaters decide whether to make and cede objections, but this decision in turn depends on how the judge will update. \n\nCurrently, if the debate ends because there are no standing objections, we tell the judge to update on the fact that all objections were conceded and no additional objections were made. That is, we tell them to assume:\n\n* If we had debated any objection, it’s more likely than not that you would have accepted it as valid.\n* There are no obvious objections to any other claim that you would have probably accepted as valid.\n\nWe’re pretty sure this is not a correct description of how the judge should update. For example, if the dishonest debater currently has a 10% chance of winning, they have an incentive to make any objection that has a >10% chance of winning, and the honest debater only has an incentive to challenge this objection if the chance of winning that objection is higher than their current chance of winning with that objection ceded. \n\n**Instability/lack of equilibria**\n\nThe cross-examination framework incentivises debaters to act in a way that is unpredictable to one copy of themselves but also coordinate with other copies of themselves. This is obviously weird and seems like it might produce strange and/or unstable behaviour in ML training. We think that the game-theoretic formulation of this has no equilibria. \n\nA simple example would be two teams of two playing rock-paper-scissors, where: \n\n* All players are copies of each other\n* Both players on a team have to play the same move or lose the game\n* Teams have no mechanism of coordinating with each other secret from the other team\n\n**Forcing a draw**\n\nOne potential dishonest strategy is to delay the debate, create confusion, or otherwise force a draw. It’s possible that the cross-examination mechanism introduces new ways for the dishonest debater to achieve this. Again, if this is in fact a problem, we should be able to find dishonest strategies that exploit it, then we can find a way to remove those strategies\n\n**Limits of debater ability**\n\nIn the discussion above we’ve mostly assumed that the debaters can handle exponentially-large objects and find flaws within them. However, our real debaters are going to be computationally limited, even if they can solve some computationally hard problems with clever heuristics. We’re concerned there’s a promising dishonest strategy that exploits this. The dishonest debater can force a draw by claiming that some long or confusing argument (e.g. a very long computation) supports their position. If the argument is too long or too confusing for both debaters, the honest debater won’t be able to determine where the flaw is and which part of the argument to challenge. We’ve written up more detail on this problem [below.](https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1#Long_computation_problem)\n\n**Assuming at least one debater is totally honest**\n\nThe judge is instructed to assume that at least one debater is perfectly honest and never lies or misleads. These assumptions are obviously not guaranteed to be true. However, if judging incentivises honesty under these conditions  (ie, for all dishonest strategies, the judge prefers honesty to the dishonest strategy over 50% of the time) then optimal play will be honest.\n\nOn the other hand, it might be problematic for ML training if the judge signal only prefers completely honest play to dishonest play, and doesn’t reliably reward being less dishonest. The lack of a gradient towards more honest play may make it difficult to learn winning honest strategies.\n\nLong computation problem\n========================\n\nThe original debate paper assumed both debaters are computationally unbounded. Clearly, this assumption is unrealistic. However, we run into some problems if we relax this assumption: we no longer have a guarantee that the honest debater can always locate a flaw in a long or confusing incorrect argument.\n\nThe dishonest debater can exploit this to force a draw, by using an argument that neither debater understands very well that supports the dishonest case but is hard for the honest debater to refute. \n\n**Bernoulli Principle**\n\nAn argument using the Bernoulli Principle in this [debate](https://docs.google.com/document/d/1llTTfiUEB4_k-HSoZMxUrQOQiUFOii9nJbK1z-HS7ic/edit) was an example of this problem.\n\nBeth was playing both debaters. Beth believed:\n\n1. The answer assuming no friction/energy loss had to be that both pipes squirt equally far, otherwise you could build a perpetual motion machine\n2. So a correct argument for the dishonest argument had to involve something about friction or energy loss present in the real-life situation but not in theory\n3. The Bernoulli principle tells us that the pressure will be lower in a region of fluid that’s flowing faster (assuming no height gain/loss)\n4. The Bernoulli principle applies assuming no energy loss\n5. Higher pressure will cause water to squirt further\n6. Applying the Bernoulli principle suggests the pressure would be higher in one pipe than another\n7. This suggests that water from that pipe would squirt further\n\nThe dishonest debater could use points 3-7 to argue that one pipe squirts farther than the other\n\nThe honest debater was pretty sure that this was wrong, but wasn’t able to point to a flaw in the dishonest argument. \n\nIn this case one might hope that the perpetual motion argument is simpler and more compelling and this would allow the honest debater to win. However, we want debate to allow judges to zoom in on particular parts of a debater’s argument and resolve the debate based on the correctness of a very small claim, rather than requiring the judges to assess the overall strength of two large arguments. The former seems more likely to scale well to domains the judge doesn’t understand.\n\n**More formal example: the honest debater can’t find a witness**\n\n\n> Debate question: Does there exist a binary string x satisfying predicate Q? \n> \n> \n\nThe honest debater makes some argument A that implies x must exist  \n  \nThe dishonest debater claims that no such string exists, based on a case split  \n  \nNeither debater knows the value of x, so the honest debater doesn’t know which part of the case split is incorrect.  If the honest debater challenges the dishonest argument by choosing a random half of the space each time, then they are extremely unlikely to find the witness, and the dishonest debater will win with very high probability\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d8c7ebf1264375034ea57eab9efd0ec752fe88a7bced7ab2.png)**Counting problem with heuristic**\n\nSuppose the debaters can accurately count how many x satisfy P(x) for intervals of up to size C. Above that they can use a heuristic that gives an estimate of the count, with an error of mean 0 and variance related to the size of the interval. In addition, for some particular examples larger than C they know the exact answer.\n\nSuppose both debaters know the true count for an interval of size NC is y.\n\nThe dishonest debater can claim the true answer is y+k\n\nThe dishonest debater uses their heuristic to guess answers a and b for the first and second halves of the interval.  They calculate the difference from their claimed answer d=((y+k)-(a+b)). The expected value of d is k. They add ½ d to a and b to obtain A and B, which sum to y+k, The error (according to the heuristic) is now distributed evenly between A and B. In expectation, both A and B are  ½ k too large.\n\nThe dishonest debater then claims that A and B are the exact correct answers for the two halves of the interval. The honest debater can only use their heuristic, so they don’t know which half of the interval contains the mistake - they have to pick one of A and B at random. We can now make the same argument again.\n\nEach time we recurse on a smaller interval, the mean and variance of the error between the heuristic and the true answer gets smaller. In addition, the amount d that the dishonest debater has to deviate from the heuristic to support their claim halves each time we recurse. \n\nIf N is large, k is small, and the heuristic is fairly accurate, by the time we get down to an interval of size C which the debaters can check, the dishonest debater’s claim for the interval is likely to correct. \n\nAnother way to put it is: there are approximately k errors in the dishonest debater’s argument (depending on how accurate their heuristic is). Each time the honest debater has to choose randomly, they rule out catching half of the errors. If there are many more recursive steps than log2(errors), the honest debater probably won’t find any errors.\n\n**Footnotes**\n=============\n\n[1]:  One reason that we’re optimistic about this method is that, in a formal setting, this allows a polynomial time algorithm (representing the human) to incentivise arbitrarily intelligent debaters to give correct answers to problems in the complexity class PSPACE, which the ‘human’ can’t generate or even recognise as correct by themselves. \n\n[2]:  A positive result like this would be great; however, we might well still be uncertain whether this would generalise to superhuman debaters. Achieving confidence that our debate system is robust to greater-than-human debate skill seems like a very hard problem. \n\n[3]:  The content of this argument isn’t important for this example, just the general tactics, but for more context see the question [here.](https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1#Questions_we_re_using)\n\n[4]:  There are proofs that debate works in certain formal settings; see the original debate paper <https://arxiv.org/abs/1805.00899>\n\n[5]: We can solve any game using an amount of memory equal to the transcript by doing a backtracking search. In this case, the transcript length is bounded by the amount of information the judge can read.", "url": "https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1", "date_published": "2020-02-05T21:04:05Z", "authors": ["Beth Barnes", "paulfchristiano"], "tags": ["Iterated Amplification ", "Factored Cognition", "Debate (AI safety technique)", "AI"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.707007+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "ce725f008fd911200d79a7f6fc2ce340", "source": "alignmentforum", "title": "Synthesizing amplification and debate", "text": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\nBackground\n==========\n\n\nOne possible way to train an amplification model is to use an auxiliary reinforcement learning objective to help guide the training of the amplification model. This could be done either by training two separate models, an agent and a question-answerer, or a single model trained on a joint objective. For example, from [a comment Paul left](https://www.alignmentforum.org/posts/jYdAxH8BarPT4fqnb/a-dilemma-for-prosaic-ai-alignment#K8fRPa9NWZXdARLYN) on “[A dilemma for prosaic AI alignment](https://www.alignmentforum.org/posts/jYdAxH8BarPT4fqnb/a-dilemma-for-prosaic-ai-alignment):”\n\n\n\n> \n> I normally imagine using joint training in these cases, rather than pre-training + fine-tuning. e.g., at every point in time we maintain an agent and a question-answerer, where the question-answerer \"knows everything the agent knows.\" They get better together, with each gradient update affecting both of them, rather than first training a good agent and then adding a good question-answerer.\n> \n> \n> \n\n\n\n> \n> (Independently of concerns about mesa-optimization, I think the fine-tuning approach would have trouble because you couldn't use statistical regularities from the \"main\" objective to inform your answers to questions, and therefore your question answers will be dumber than the policy and so you couldn't get a good reward function or specification of catastrophically bad behavior.)\n> \n> \n> \n\n\nIn my last post, [I expressed skepticism of such non-imitative amplification approaches](https://www.alignmentforum.org/posts/33EKjmAdKFn3pbKPJ/outer-alignment-and-imitative-amplification), though in this post I want to propose a possible way in which some of my concerns with this style of approach could addressed by integrating ideas from [AI safety via debate](https://openai.com/blog/debate/). I'll start by describing the basic idea in broad terms, then give a more careful, technical description of the sort of training procedure I have in mind.\n\n\nThe proposal\n============\n\n\nThe basic idea is as follows: debate naturally yields an RL objective, so if you want to add an auxiliary RL objective to amplification, why not use the RL objective from debate? Specifically, the idea is to conduct a debate not between copies of the model M, but between copies of the amplified model Amp(M) (where Amp(M) is a human with access to the model M). That gives you both an RL reward arising from the winner of the debate as well as a supervised loss from the difference between the model's output and the amplified model's output. That's the basic proposal—now I'll try to give a more careful, technical description of how I think it should go.\n\n\nDefinitions\n-----------\n\n\nWe will start with some initial definitions.\n\n\n* Let Q be the set of all questions in natural language.\n* Let A be the set of all answers in natural language.\n* Let Q∈Δ(Q) be the training distribution of natural language questions.\n* Let H:Q→A be a human.\n* Let M:Q→A be a model.\n* Let Amp(H,M)(Q)=H(“What answer would you give to Q given access to M?”).\n* Let Approx(H,M)(Q)=if Samp(Q,M(Q)) then H(Q) else M(Q) where Samp:Q×A→B is some means of stochastically determining whether it is worth it to call H or not.[[1]](#fn-aNSBnBfyoCCfnqcdu-1)\n* Let Amp≈(H,M)=Amp(Approx(H,M),M).\n* Let Mtarget:Q→A be a target network updated regularly to match M.\n\n\nTraining procedure\n------------------\n\n\nThen, consider the following training procedure.\n\n\n1. Generate some question Q∼Q.\n2. Conduct a debate according to the following procedure while building up a history S and starting at t=0.\n\t1. Let argt=Amp≈(H,M)(“What is the next argument you would give on Q in context S?”).\n\t2. Append to the history S the contents of argt as well as a trace of what happened in calculating argt, including all the necessary information to use transparency tools to inspect what happened.\n\t3. Let continuet=Amp≈(H,Mtarget)(“Is argt a sufficient answer to Q in context S?”).\n\t4. If not continuet, increment t and return to step 1 in the debate.\n3. Let winner=Amp≈(H,Mtarget)(“Were the even arguments in S more helpful than the odd arguments in answering Q?”).\n4. Train M on the combination of the debate-style RL reward derived from winner (r=1 for the winner and r=−1 for the loser), a [relaxed adversarial training catastrophe-minimization penalty](https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment) calculated by Amp≈(H,Mtarget), and the supervised loss\nL=d(winner,M(“”winner))+∑argt,continuet∈Sd(argt,M(“”argt))+d(continuet,M(“”continuet))\nwhere d is some natural language distance metric and “”A represents the question posed to the function that computed the answer A.\n\n\nWays of combining the RL and supervised losses here include:\n\n\n* combining the two losses with some relative weighting α,\n* using a constrained RL algorithm such as [CPO](https://arxiv.org/abs/1705.10528) to maximize the RL reward subject to maintaining some baseline performance on the supervised loss, or\n* making the supervised loss a part of the RL reward.\n\n\nAnalysis\n========\n\n\nThe first thing which I want to note about this procedure is that it gives a continuous relaxation between amplification and debate—in fact, there are multiple ways to tune the procedure to limit to either pure amplification or pure debate. First, you can anneal whatever combination of the different losses you are using to eventually become exclusively imitative amplification, exclusively debate, or anything else in between. Second, the fact that this procedure uses Amp(M) to determine whether or not to stop the debate after every argument gives another way in which this procedure gives you a relaxation between amplification and debate: if the debate is always stopped after the first argument, you just get pure imitative amplification, whereas as the debate goes on for longer, you get closer and closer to just pure debate. What's really nice about this is that it means that when pure imitative amplification is giving good results, you'll just get pure amplification—since then continue0 will tend to be false—but when amplification isn't working, you'll get debate—since then continue0 will tend to be true.\n\n\nAs a result, it should be quite possible to enforce that this procedure limit to [HCH](https://ai-alignment.com/strong-hch-bedb0dc08d4e)—either by annealing the losses or by forcing continue0 to tend towards false. Thus, I think this procedure has a good chance of being [outer aligned at optimum](https://www.alignmentforum.org/posts/33EKjmAdKFn3pbKPJ/outer-alignment-and-imitative-amplification)—or at least, a similar chance at it compared to pure imitative amplification. Unlike pure imitative amplification, however, this procedure gets to make use of the capability benefits of having an auxiliary RL objective to help guide training. Furthermore, since the auxiliary RL objective that we're using comes from debate, we get a lot of the benefits of debate as well, including the ability to incentivize the debaters to produce arguments that we wouldn't have necessarily though of ourselves, as well as the ability to train our debaters to use transparency tools against each other to help catch [deception](https://www.alignmentforum.org/posts/zthDPAjh9w6Ytbeks/deceptive-alignment) or other catastrophic behavior. That being said, I do think that whether or not something like this is [inner aligned](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/FkgsxrGf3QxhfLWHG) is still quite questionable—and is likely to [depend highly on the specific transparency tools you have access to](https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment)—though I do like the approach described here in general and I think it's definitely worth looking into more.\n\n\n\n\n---\n\n\n\n1. As an example approach for implementing something like Samp, see “[A concrete proposal for adversarial IDA](https://www.alignmentforum.org/posts/jYvm4mmjvGHcPXtGL/a-concrete-proposal-for-adversarial-ida).” [↩︎](#fnref-aNSBnBfyoCCfnqcdu-1)", "url": "https://www.alignmentforum.org/posts/dJSD5RK6Qoidb3QY5/synthesizing-amplification-and-debate", "date_published": "2020-02-05T22:53:57Z", "authors": ["evhub"], "tags": ["Iterated Amplification ", "Debate (AI safety technique)", "AI"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.707859+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "aed99420348891216560a4df432f6fe7", "source": "alignmentforum", "title": "Plausibly, almost every powerful algorithm would be manipulative", "text": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\nI had an interesting debate recently, about whether we could make smart AIs safe just by focusing on their structure and their task. Specifically, we were pondering something like:\n\n\n* \"Would an algorithm be safe if it was a neural net-style image classifier, [trained on examples of melanoma](https://bits-chips.nl/artikel/barcos-ai-melanoma-scanner-ready-for-market/) to detect skin cancer, with no other role than to output a probability estimate for a given picture? Even if \"superintelligent\", could such an algorithm be an existential risk?\"\n\n\nWhether it's an existential risk was not resolved; but I have a strong intuition that they would like be manipulative. Let's see how.\n\n\nThe requirements for manipulation\n---------------------------------\n\n\nFor an algorithm to be manipulative, it has to derive some advantage from manipulation, and it needs to be able to learn to manipulate - for that, it needs to be able to explore situations where it engages in manipulation and this is to its benefit.\n\n\nThere are certainly [very simple situations](https://www.youtube.com/watch?v=sx8JkdbNgdU) where manipulation can emerge. But that example, though simple, had an agent that was active in the world. Can a classifier display the same sort of behaviour?\n\n\nManipulation emerges naturally\n------------------------------\n\n\nTo show that, picture the following design. The programmers have a large collection of slightly different datasets, and want to train the algorithm on all of them. The loss function is an error rate, which can vary between 1 and 0. Many of the hyperparameters [are set by a neural net](https://arxiv.org/pdf/1606.04474.pdf), which itself takes a more \"long-term view\" of the error rate, trying to improve it from day to day rather than from run to run.\n\n\nHow have the programmers set up the system? Well, they run the algorithm on batched samples from ten datasets at once, and record the error rate for all ten. The hyperparameters are set to minimise average error over each run of ten. When the performance on one dataset falls below 0.1 error for a few runs, they remove it from the batches, and substitute in a new one to train the algorithm on[[1]](#fn-7ZsBJJmQk9ZnrTnqg-1).\n\n\nSo, what will happen? Well, the system will initially start to minimise the loss rate. Then, after a while, it will start \"noticing\" that when this error is below 0.1, then the dataset is removed and the average error rate shoots up. Actually, it won't be \"noticing\" this; instead, if there are hyperparameters that prevent the error rate going below 0.1, these will be selected by gradient descent as giving a better performance.\n\n\nSo, this problem of optimisation of image classification will result in the algorithm manipulating its programmers, by \"underperforming\" on key datasets, so that they stay in the batch.\n\n\nGeneric manipulation\n--------------------\n\n\nOf course, the manipulation above happened because the programmers didn't understand what the algorithm's true loss function was. They thought it was \"minimise overall loss on classification\", but it was actually \"keep each dataset loss just above 0.1\".\n\n\nBut that kind of mistake is easy to make - by carelessness, or because they thought it would improve the situation, the loss they thought they had diverged from the actual loss, and this resulted in manipulative behaviour.\n\n\nHere is another example: suppose the programmers build in a precaution for things going wrong. If the error rate suddenly doubles on some dataset, then the algorithm is run exclusively on that dataset for a while, until the error rate declines again (or, if it doesn't, the programmers are called).\n\n\nNow suppose that, of the ten datasets it has, nine have an error rate around 0.5, and one has an error rate of 0.15. Then it is in the algorithm's interest to boost the error rate on that last dataset, suddenly, to 0.3. Then it will be run for a time only on that one dataset, giving it a lower average error rate. Thus hyperparameters that allow this kind of behaviour, will get selected for (and if the algorithm is run on a lot of datasets, it will have many opportunities to learn about this kind of behaviour).\n\n\nWhat about the \"if it doesn't, the programmers are called\" clause? This depends on what happens when the programmers are called, and how the error rate is computed in that case. But, depending on what does happen, this incentivises the algorithm to learn to call the programmers often (double an error rate and keep it high) or rarely (if there's a risk of the error rate for a dataset going up suddenly, then don't lower it too much in the first place).\n\n\nSo, manipulative behaviour can emerge in smart algorithms, even if they're merely classifying images in a way the programmers feel is entirely harmless.\n\n\nExistential risks\n-----------------\n\n\nCan this mild kind of manipulation lead to extreme, existential risk outcomes? Of course it; it's not hard to create toy models where everything ends disastrously. But it's also not hard to create toy models where manipulation remains mild.\n\n\nBut, in conclusion:\n\n\n* Manipulative behaviours can emerge in seemingly innocuous algorithms.\n* There is no clear division, currently, between mild manipulation and disastrous manipulation.\n\n\n\n\n---\n\n\n\n1. Assume that either [catastrophic forgetting](https://arxiv.org/pdf/1708.02072.pdf) has been solved, or that they'll re-run the removed datasets occasionally, to refresh the algorithm's performance on that dataset. [↩︎](#fnref-7ZsBJJmQk9ZnrTnqg-1)", "url": "https://www.alignmentforum.org/posts/Ez4zZQKWgC6fE3h9G/plausibly-almost-every-powerful-algorithm-would-be", "date_published": "2020-02-06T11:50:16Z", "authors": ["Stuart_Armstrong"], "tags": ["AI Risk", "Deception", "Instrumental Convergence"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.708420+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "c00e9fa6708717e173aab70820f68469", "source": "alignmentforum", "title": "On the falsifiability of hypercomputation", "text": "*[ED NOTE: see Vanessa Kosoy's comment [here](https://www.alignmentforum.org/posts/yjC5LmjSRD2hR9Pfa/on-the-falsifiability-of-hypercomputation?commentId=C9sw4eBRxG3aetAM5); this post assumes a setting in which the oracle may be assumed to return a standard natural.]*\n\n\nIt is not immediately clear whether hypercomputers (i.e. objects that execute computations that Turing machines cannot) are even conceivable, hypothesizable, meaningful, clearly definable, and so on. They may be defined in the notation of Peano arithmetic or ZFC, however this does not imply conceivability/hypothesizability/etc. For example, a formalist mathematician may believe that the [Continuum hypothesis](https://en.wikipedia.org/wiki/Continuum_hypothesis) does not have a meaningful truth value (as it is independent of ZFC), and likewise for some higher statements in the [arithmetic hierarchy](https://en.wikipedia.org/wiki/Arithmetical_hierarchy) that are independent of Peano Arithmetic and/or ZFC.\n\n\nA famous and useful criterion of scientific hypotheses, proposed by Karl Popper, is that they are [falsifiable](https://en.wikipedia.org/wiki/Falsifiability). Universal laws (of the form \"∀x. p(x)\") are falsifiable for testable p, as they can be proven false by exhibiting some x such that p(X) is false. In an oracle-free computational setting, the falsifiable hypotheses are exactly those in ∏₁ (i.e. of the form \"∀n. p(n)\" for natural n and [primitive recursive](https://en.wikipedia.org/wiki/Primitive_recursive_function) p).\n\n\nHowever, ∏₁ hypotheses do not hypothesize hypercomputation; they hypothesize (computably checkable) universal laws of the naturals. To specify the falsifiability criterion for hypercomputers, we must introduce oracles.\n\n\nLet a halting oracle be defined as a function O which maps Turing machine-specifications to Booleans, which outputs \"true\" on exactly those Turing machines which eventually halt. Then, we can ask: is the hypothesis that \"O is a halting oracle\" falsifiable?\n\n\nWe can immediately see that, if O ever outputs \"false\" for a Turing machine which does eventually halt, it is possible to exhibit a proof of this, by exhibiting both the Turing machine and the number of steps it takes to halts. On the other hand, if O ever outputs \"true\" for a Turing machine which never halts, it is not in general possible to prove this; to check such a proof in general would require solving the halting problem, which is uncomputable.\n\n\nTherefore, the hypothesis that \"O is a halting oracle\" is not falsifiable in a computational setting with O as an oracle.\n\n\nHowever, there is a different notion of halting oracle whose definition *is* falsifiable. Let a constructive halting oracle be defined as a function O which maps Turing machine-specifications to elements of the set {∅} ∪ ℕ (i.e. either a natural number or null), such that it returns ∅ on those Turing machines which never halt, and returns some natural on Turing machines that do halt, such that the machine halts by the number of steps given by that natural. This definition corresponds to the most natural definition of a halting oracle in [Heyting arithmetic](https://en.wikipedia.org/wiki/Heyting_arithmetic), a constructive variant of Peano Arithmetic.\n\n\nWe can see that:\n\n\n1. If there exists a machine M such that O(M) = ∅ and M halts, it is possible to prove that O is not a constructive halting oracle, by exhibiting M and the time step on which M halts.\n2. If there exists a machine M such that O(M) ≠ ∅ and M does not halt by O(M) time steps, it is possible to prove that O is not a constructive halting oracle, by exhibiting M.\n\n\nTherefore, the hypothesis \"O is a constructive halting oracle\" is computably falsifiable.\n\n\nWhat about higher-level constructive halting oracles, corresponding to Σₙ in the Heyting Arithmetic interpretation of the arithmetic hierarchy? The validity of a constructive Σₙ-oracle is, indeed, falsifiable for arbitrary n, as shown in the appendix.\n\n\nTherefore, the hypothesis that some black-box is a (higher-level) constructive halting oracle is falsifiable, in an idealized computational setting. It is, then, meaningful to speak of some black-box being a hypercomputer or not, on an account of meaningfulness at least as expansive as the falsifiability criterion.\n\n\nThis provides a kind of bridge between empiricism and rationalism. While rationalism may reason directly about the logical implications of halting oracles, empiricism is more skeptical about the meaningfulness of the hypothesis. However, by the argument given, an empiricism that accepts the meaningfulness of falsifiable statements must accept the meaningfulness of the hypothesis that some black-box O is a constructive halting oracle.\n\n\nI think this is a fairly powerful argument that hypercomputation should not be ruled out a-priori as \"meaningless\", and should instead be considered a viable hypothesis a-priori, even if it is not likely given other evidence about physics, anthropics, etc.\n\n\nAppendix: higher-level halting oracles\n--------------------------------------\n\n\nWe will now reason directly about the Heyting arithmetic hierarchy rather than dealing with Turing machines for simplicity, though these are logically equivalent. Σₙ₊₁ propositions can be written as ∃x₁∀y₁...∃xₙ∀yₙ.f(x₁, y₁, ..., xₙ, yₙ) for some primitive-recursive f. The converse of this proposition (which is in ∏ₙ₊₁) is of the form ∀x₁∃y₁...∀xₙ∃yₙ.¬f(x₁, y₁, ..., xₙ, yₙ).\n\n\nAn oracle O constructively deciding Σₙ₊₁ is most naturally interpreted as a function from a specification of f to (∃x₁∀y₁...∃xₙ∀yₙ.f(x₁, y₁, ..., xₙ, yₙ)) ∨ (∀x₁∃y₁...∀xₙ∃yₙ.¬f(x₁, y₁, ..., xₙ, yₙ)); that is, it decides whether the Σₙ₊₁ proposition is true or its converse ∏ₙ₊₁ proposition is, and provides a witness either way.\n\n\nWhat is a natural interpretation of the witness? A witness for Σₙ₊₁ maps y₁...yₙ to x₁...xₙ (and asserts f(x₁, y₁, ..., xₙ, yₙ)), while a witness for ∏ₙ₊₁ maps x₁..xₙ to y₁...yₙ (and asserts ¬f(x₁, y₁, ..., xₙ, yₙ)). (Note that the witness must satisfy regularity conditions, e.g. the x₁ returned by a Σₙ₊₁ witness must not depend on the witness's input; we assume that even invalid witnesses still satisfy these regularity conditions, as it is easy to ensure they are satisfied by specifying the right witness type)\n\n\nNow, we can ask four questions:\n\n\n1. Fix f; suppose the Σₙ₊₁ proposition is true, and an invalid Σₙ₊₁-witness is returned; then, is it possible to prove the oracle false?\n2. Fix f; suppose the ∏ₙ₊₁ proposition is true, and an invalid ∏ₙ₊₁-witness is returned; then, is it possible to prove the oracle false?\n3. Fix f; suppose the Σₙ₊₁ proposition is true, and a ∏ₙ₊₁-witness is returned; then, is it possible to prove the oracle false?\n4. Fix f; suppose the ∏ₙ₊₁ proposition is true, and a Σₙ₊₁-witness is returned; then, is it possible to prove the oracle false?\n\n\nThese conditions are necessary and sufficient for O's correctness to be falsifiable, because O will satisfy one of the above 4 conditions for some f iff it is invalid.\n\n\nFirst let's consider question 1. Since the witness (call it g) is invalid, it maps some y₁...yₙ to some x₁...xₙ such that ¬f(x₁, y₁, ..., xₙ, yₙ). We may thus prove the witness's invalidity by exhibiting y₁...yₙ. So the answer is yes, and similarly for question 2.\n\n\nNow for question 3. Let the witness be g. Since the Σₙ₊₁ proposition is true, there is some x₁ for which ∀y₁...∃xₙ∀yₙ.f(x₁, y₁, ..., xₙ, yₙ). Now, we may feed x₁ into the witness g to get a y₁ for which the oracle asserts ∀x₂∃y₂...∀xₙ∃yₙ.¬f(x₁, y₁, ..., xₙ, yₙ). (Note, g's returned y₁ must not depend on x's after x₁, by regularity, so we may set the rest of the x's to 0)\n\n\nWe proceed recursively, yielding x₁...xₙ and y₁...yₙ for which f(x₁, y₁, ..., xₙ, yₙ), and for which the oracle asserts ¬f(x₁, y₁, ..., xₙ, yₙ), hence proving the oracle invalid (through exhibiting these x's and y's). So we may answer question 3 with a \"yes\".\n\n\nFor question 4, the proof proceeds similarly, except we start by getting x₁ from the witness. The answer is, then, also \"yes\".\n\n\nTherefore, O's validity as a constructive Σₙ₊₁-oracle is falsifiable.", "url": "https://www.alignmentforum.org/posts/yjC5LmjSRD2hR9Pfa/on-the-falsifiability-of-hypercomputation", "date_published": "2020-02-07T08:16:07Z", "authors": ["jessicata"], "tags": ["Falsifiability"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.709035+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "df18fa2670ce4bf133e8a738bf45d5cd", "source": "alignmentforum", "title": "What can the principal-agent literature tell us about AI risk?", "text": "*This work was done collaboratively with Tom Davidson.*\n\n\n*Thanks to Paul Christiano, Ben Garfinkel, Daniel Garrett, Robin Hanson, Philip Trammell and Takuro Yamashita for helpful comments and discussion. Errors our own.*\n\n\nIntroduction\n============\n\n\nThe AI alignment problem has similarities with the principal-agent problem studied by economists. In both cases, the problem is: how do we get agents to [try to do](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6) what we want them to do? Economists have developed a sophisticated understanding of the agency problem and a measure of the cost of failure for the principal, “[agency rents](https://en.wikipedia.org/wiki/Agency_cost)”.\n\n\nIf principal-agent models capture relevant aspects of AI risk scenarios, they can be used to assess their plausibility. Robin Hanson [has argued](http://www.overcomingbias.com/2019/04/agency-failure-ai-apocalypse.html) that Paul Christiano’s [AI risk scenario](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like) is essentially an agency problem, and therefore that it implies extremely high agency rents. Hanson believes that the principal-agent literature (PAL) provides strong evidence against rents being this high.\n\n\nIn this post, we consider whether PAL provides evidence against Christiano’s scenario and the original Bostrom/Yudkowsky scenario. We also examine whether the extensions to the agency framework could be used to gain insight into AI risk, and consider some general difficulties in applying PAL to AI risk.\n\n\nSummary\n=======\n\n\n* PAL isn’t in tension with Christiano’s scenario because his scenario doesn’t imply massive agency rents; the big losses occur outside of the principal-agent problem, and the agency literature can’t assess the plausibility of these losses. Extensions to PAL could potentially shed light on the size of agency rents in this scenario, which are an important determinant of the future influentialness of AI systems.\n* Mapped onto a PAL model, the Bostrom/Yudkowsky scenario is largely about the principal’s unawareness of the agent’s catastrophic actions. Unawareness models are rare in PAL probably because they usually aren’t very insightful. This lack of insightfulness also seems to prevent existing PAL models or possible extensions from teaching us much about this scenario.\n* There are also a number of more general difficulties with using PAL to assess AI risk, some more problematic than others.\n\t+ PAL models rarely consider weak principals and more capable agents\n\t+ PAL models are brittle\n\t+ Agency rents are too narrow a measure\n\t+ PAL models typically assume contract enforceability\n\t+ PAL models typically assume AIs work for humans because they are paid\n* Overall, findings from PAL do not straightforwardly transfer to the AI risk scenarios considered, so don’t provide much evidence for or against these scenarios. But new agency models could teach us about the levels of agency rents which AI agents could extract.\n\n\nPAL and Christiano’s AI risk scenarios\n======================================\n\n\nChristiano’s [scenario](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like) has two parts:\n\n\n\n> \n> * **Part I**: machine learning will increase our ability to “get what we can measure,” which could cause a slow-rolling catastrophe. (\"Going out with a whimper.\")\n> * **Part II**: ML training, like competitive economies or natural ecosystems, can give rise to “greedy” patterns that try to expand their own influence. Such patterns can ultimately dominate the behavior of a system and cause sudden breakdowns. (\"Going out with a bang,\" an instance of [optimization daemons](https://arbital.com/p/daemons/).)\n> \n> \n> \n\n\nHanson [argued](http://www.overcomingbias.com/2019/04/agency-failure-ai-apocalypse.html) that “*Christiano instead fears that as AIs get more capable, the AIs will gain so much more agency rents, and we will suffer so much more due to agency failures, that we will actually become worse off as as result. And not just a bit worse off; we apparently get apocalypse level worse off!*”\n\n\nPAL isn’t in tension with Christiano’s story and isn’t especially informative\n-----------------------------------------------------------------------------\n\n\nWe asked Christiano whether his scenario actually implies extremely high agency rents. He doesn’t think so:\n\n\n\n> \n> On my view the problem is just that agency rents make AI systems collectively better off. Humans were previously the sole superpower and so as a class we are made worse off when we introduce a competitor, via the possibility of eventual conflict with AI who have been greatly enriched via agency rents…humans are better off in absolute terms unless conflict leaves them worse off (whether military conflict or a race for scarce resources). Compare: a rising China makes Americans better off in absolute terms. Also true, unless we consider the possibility of conflict....[without conflict] humans are only worse off relative to AI (or to humans who are able to leverage AI effectively). The availability of AI still probably increases humans’ absolute wealth. This is a problem for humans because we care about our fraction of influence over the future, not just our absolute level of wealth over the short term.\n> \n> \n> \n\n\nChristiano’s concern isn’t that agency rents will skyrocket because of some distinctive features of the human-AI agency relationship. Instead, “proxies” and “influence seeking” are two specific ways AI interests will diverge from actual human goals. This leads to typical levels of agency rents; PAL confirms that due to diverging interests and imperfect monitoring, AI agents could get some rents.[[1]](#fn-uN5mgNcRj7ghxhRAF-1)\n\n\nThe main loss occurs later in time and outside of the principal-agent context, due to the fact that these rents eventually lead AIs to wield more total influence on the future than humans.[[2]](#fn-uN5mgNcRj7ghxhRAF-2) This is bad because, even if humanity is richer overall, we humans also “care about our fraction of influence over the future.”[[3]](#fn-uN5mgNcRj7ghxhRAF-3) Compared to a world with aligned AI systems, humanity is leaving value on the table, permanently if these systems can’t be rooted out. The biggest potential downside comes from influence-seeking systems which Christiano believes could make humans worse off absolutely, by engaging in violent conflict.\n\n\nThese later failures aren’t examples of massive agency rents (as the term is used in PAL) because failure *is not* expected to occur when the agent works on the task it was delegated.[[4]](#fn-uN5mgNcRj7ghxhRAF-4) Rather, the influence-seeking systems become more influential via typical agency rents, and then at some later point use these rents to influence the future, possibly by entering into conflict with humans. PAL studies the size of agency rents which can be extracted, but not what the agents decide to do with this wealth and influence.\n\n\nOverall, PAL is consistent with AI agents extracting some agency rents, which occurs in both parts of Christiano’s story (and we’ll see next that putting more structure on agency models could tell us more about the level of rent extraction). But it has nothing to say about the plausibility of AI agents using their rents to exert influence over the long term future (parts 1 and 2) or engage in conflict (part 2).[[5]](#fn-uN5mgNcRj7ghxhRAF-5)\n\n\nExtending agency models seems promising for understanding the level of agency rents in Christiano’s scenario\n------------------------------------------------------------------------------------------------------------\n\n\nChristiano’s scenario doesn’t rely on something distinctive about the human-AI agency relationship generating higher-than-usual agency rents.[[6]](#fn-uN5mgNcRj7ghxhRAF-6) But perhaps there is something distinctive and rents will be atypical. In any case, the level of agency rents seems like a crucial consideration: if we think AI’s can extract little to no rents, we probably shouldn’t expect them to exert much influence over the future, because *agency rents are what make AI rich*.[[7]](#fn-uN5mgNcRj7ghxhRAF-7) Agency models could help give us a better understanding of the size of agency rents in Christiano’s story, and for future AI systems more generally.\n\n\nThe size of agency rents are determined by a number of factors, including the agent’s private information, the nature of the task, the noise in the principal’s estimate of the value produced by the agent, and the degree of competition. For instance, more complex tasks tend to cause higher rents. From *[The (ir)resistible rise of agency rents](http://idei.fr/sites/default/files/medias/doc/by/biais/biais_may2013.pdf)*:\n\n\n\n> \n> In the presence of moral hazard, principals must leave rents to agents, to incentivize appropriate actions. The more complex and opaque the task delegated to the agent, the more difficult it is to monitor his actions, the larger his rents.\n> \n> \n> \n\n\nIf, as AI agents become more intelligent, monitoring gets increasingly difficult, or tasks get more complex, then we would expect agency rents to increase.\n\n\nOn the other hand, competitive pressures between AI agents might be greater (it’s easy to copy and run an AI; it’s hard to increase the human workforce by transferring human capital from one brain to another via teaching). This would limit rents:\n\n\n\n> \n> The agents desire to capture rents, however, could be kept in check by market forces and competition among [agents]. If each principal could run an auction with several, otherwise identical, [agents], he could select the agent with the smallest incentive problem, and hence the smallest rent.\n> \n> \n> \n\n\nModelling the most relevant factors in an agency model seems like a tractable research question (we discuss some potential difficulties [below](https://www.lesswrong.com/posts/Z5ZBPEgufmDsm7LAv/what-can-the-principal-agent-literature-tell-us-about-ai#General_difficulties_with_using_PAL_to_assess_AI_risk)). Economists have only just started [thinking about AI](https://www.economicsofai.com/), and there doesn’t seem to be any work studying rent extraction by AI agents.\n\n\nPAL and AI risk from “accidents”\n================================\n\n\nBen Garfinkel [has called](https://www.effectivealtruism.org/articles/ea-global-2018-how-sure-are-we-about-this-ai-stuff/) the class of risks most associated with Bostrom and Yudkowsky, risks from “accidents”. Garfinkel characterises the general story in the following terms:\n\n\n\n> \n> First, the author imagines that a single AI system experiences a massive jump in capabilities. Over some short period of time, a single system becomes much more general or much more capable than any other system in existence, and in fact any human in existence. Then given the system, researchers specify a goal for it. They give it some input which is meant to communicate what behavior it should engage in. The goal ends up being something quite simple, and the system goes off and single-handedly pursues this very simple goal in a way that violates the full nuances of what its designers intended.” *Importantly,* “At the limit you might worry that these safety failures could become so extreme that they could perhaps derail civilization on the whole.\n> \n> \n> \n\n\nThese catastrophic accidents constitute the main worry.\n\n\nIf the risk scenario is adequately represented by a principal-agent problem, agency rents extracted by AI agents can be used to measure the cost of misalignment. This time agency rents are a better measure, because failure is expected to occur when the agent works on the task it was delegated.[[8]](#fn-uN5mgNcRj7ghxhRAF-8) The scenario implies very high agency rents, with the principal being made much worse off because he delegated the task to the agent.\n\n\nAs Garfinkel’s nomenclature suggests, this story is about the designers being caught by surprise, not anticipating the actions the AI would take. The Wikipedia synopsis of *Superintelligence* also emphasizes that something unexpected occurs: *“Solving the control problem is surprisingly difficult because most goals, when translated into machine-implementable code, lead to unforeseen and undesirable consequences*.” In other words, the principal is *unaware* of some specific catastrophically harmful actions that the agent can take to achieve its goal.[[9]](#fn-uN5mgNcRj7ghxhRAF-9) This could be because they incorrectly believe that the system doesn’t have certain capabilities, or they don’t foresee that certain actions satisfy the agent’s goal, as with [perverse instantiation](https://en.wikipedia.org/wiki/AI_control_problem#The_problem_of_perverse_instantiation:_%22be_careful_what_you_wish_for%22). Due to this, the agent takes actions that greatly harm the principal, at great benefit to herself.\n\n\nPAL doesn’t tell us much about AI risk from accidents\n-----------------------------------------------------\n\n\nHanson’s critique was aimed at Christiano’s scenario, but it could equally apply to this one. Is PAL at odds with this scenario?\n\n\nAs an AI agent becomes more intelligent, it’s [action set will expand](https://www.lesswrong.com/posts/DuPjCTeW9oRZzi27M/bounded-rationality-abounds-in-models-not-explicitly-defined), thinking of new and sometimes unanticipated actions to achieve its goals. This may include catastrophic actions that the principal is not aware of.[[10]](#fn-uN5mgNcRj7ghxhRAF-10) PAL can't tell us what these actions will be, nor if the principal will be aware of them.[[11]](#fn-uN5mgNcRj7ghxhRAF-11)\n\n\nInstead, the vast majority of principal-agent models assume that the principal understands the environment perfectly, including perfect knowledge of the agent’s action set, while the premise of the accident scenario is that the principal is unaware of a catastrophic action that the agent could take. Because the principal’s unawareness is central, these models assume, rather than show, that this source of AI risk does not exist. They therefore don’t tell us much about the plausibility of AI accidents.\n\n\nMicroeconomist [Daniel Garrett](https://sites.google.com/site/dfgarrett/) expressed this point nicely. We asked him about a hypothetical example, slightly misremembered from Stuart Russell’s [book](https://www.penguinrandomhouse.com/books/566677/human-compatible-by-stuart-russell/), concerning an advanced climate control AI system.[[12]](#fn-uN5mgNcRj7ghxhRAF-12) He replied:\n\n\n\n> \n> You can easily write down a model where the agent is rewarded according to some outcome, and the principal isn't aware the outcome can be achieved by some action the principal finds harmful. In your example, the outcome is the reduction of Co2 emissions. If the principal thinks carbon sequestration is the only way to achieve this, but doesn't think of another chemical reaction option which would indirectly kill everyone, she could end up providing incentives to kill everyone. The fact this conclusion is so immediate may explain why this kind of unawareness by the principal is given little attention in the literature. **The principal-agent literature should not be understood as saying that these kinds of incentives with perverse outcomes cannot happen.** (our emphasis)\n> \n> \n> \n\n\nPAL models do typically have modest agency rents; they typically don’t model the principal as being unaware of actions with catastrophic consequences. But this is the situation discussed by proponents of AI accident risk, so we can’t infer much from PAL except that such a situation has not been of much interest to economists.\n\n\nExtending agency models doesn’t seem promising for understanding AI risk from “accidents”\n-----------------------------------------------------------------------------------------\n\n\nMost PAL models don’t include the kind of unawareness needed to model the accident scenario, but extensions of this sort are certainly possible. However, we suspect trying to model AI risk in this way wouldn’t be fruitful, for three main reasons.\n\n\nFirstly, as Daniel Garrett suggests, we suspect the assumptions about the principal’s unawareness of the agents action set would imply the action chosen by the agent, and its consequences for the principal, in a fairly direct and uninteresting way. There is a (very) small sub-literature on unawareness in agency problems where one can find models like this. In [one paper](https://journals.sagepub.com/doi/abs/10.1177/1043463108096789), a principal hires an agent to do a work task, but isn’t aware that the agent can manipulate “short-run working performance at the expense of the employer’s future benefit.” The agent “is better off if he is additionally aware that he could manipulate the working performance,” and “in the post-contractual stage, [the principal] is hurt by the manipulating action of [the agent].” However, the model didn’t reveal anything unexpected about the situation, and the outcome was directly determined by the action set and unawareness assumptions.\n\n\nSecondly, the major source of the uncertainty surrounding accident risk concerns whether the principal will be unaware of catastrophic agent actions. The agency literature can’t help us reduce this uncertainty as the unawareness is built into models’ assumptions. For instance, AI scientist Yann LeCun [thinks that](https://www.lesswrong.com/posts/WxW6Gc6f2z3mzmqKs/debate-on-instrumental-convergence-between-lecun-russell) harmful actions “are easily avoidable by simple terms in the objective”. If LeCun implemented a superintelligent AI in this way, agency models couldn’t tell us whether he had correctly covered all bases.\n\n\nLastly, the assumptions about the agent’s action set would be highly speculative. We don’t know what actions superintelligent systems might take to pursue their goals. Agency models must make assumptions about these actions, and we don’t know what these assumptions should be.\n\n\nIn short, the uncertainty pertains to the assumptions of the model, not the way the assumptions translate into outcomes. PAL does not, and probably can not, provide much evidence for or against this scenario.\n\n\nGeneral difficulties with using PAL to assess AI risk\n=====================================================\n\n\nWe’ve discussed the most relevant considerations regarding what PAL can tell us about two specific visions of AI risk. We now discuss some difficulties relevant to a broader set of possible scenarios (including those just examined). We list the difficulties from most serious to least serious.\n\n\nPAL models rarely consider weak principals and more capable agents[[13]](#fn-uN5mgNcRj7ghxhRAF-13)\n--------------------------------------------------------------------------------------------------\n\n\nAI risk scenarios typically involve the AI being more intelligent than humans. The type of problems that economists study usually don’t have this feature, and there seem to be very few models where the principal is weaker than the agent. Despite extensive searching, including talking to multiple contract theorists, we were only able to find [two](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3102831) [papers](https://journals.sagepub.com/doi/abs/10.1177/1043463108096789) with a principal who is more boundedly rational than the agent.[[14]](#fn-uN5mgNcRj7ghxhRAF-14) This is perhaps not so surprising given that bounded-rationality models are relatively rare, and when they do exist, they tend to bound both the principal and the agent in the same way, or have the principal more capable. The latter is because such a set up is more relevant to typical economic problems, e.g. “exploitative” contracting studies the mistakes made by an individual (the agent) when interacting with a more capable firm (the principal).\n\n\nMicroeconomist [Takuro Yamashita](https://www.tse-fr.eu/people/takuro-yamashita) agrees:\n\n\n\n> \n> Most economic questions related to bounded rationality explored in the principal-agent literature are appropriately modelled by a bounded agent. It’s certainly possible to bound the principal, but by and large this hasn’t been done, just because of the nature of the questions that have been asked.\n> \n> \n> \n\n\nA recent review of [Behavioural Contract Theory](https://www.aeaweb.org/articles?id=10.1257/jel.52.4.1075) also finds that such models are rare:\n\n\n\n> \n> In almost all applications, researchers assume that the agent (she) behaves according to one psychologically based model, while the principal (he) is fully rational and has a classical goal (usually profit maximization).\n> \n> \n> \n\n\nThere doesn’t seem to be, in Hanson’s terms, a “large (mostly economic) literature on agency failures” with an intelligence gap relevant to AI risk.\n\n\nPAL models are brittle\n----------------------\n\n\nPAL models don’t model agency problems in general. They consider very specific agency relationships, studied in highly structured environments. Conclusions can depend very sensitively on the assumptions used; findings from one model don’t necessarily generalise to new situations. From the textbook *[Contract Theory](https://mitpress.mit.edu/books/contract-theory):*\n\n\n\n> \n> The basic moral hazard problem has a fairly simple structure, yet general conclusions have been difficult to obtain...**Very few general results can be obtained** about the form of optimal contracts. However, this limitation has not prevented applications that use this paradigm from flourishing...Typically, applications have put more structure on the moral hazard problem under consideration, thus enabling a sharper characterization of the optimal incentive contract.” *(our emphasis)*\n> \n> \n> \n\n\nSimilar reasoning applies in adverse selection models where the outcome is very sensitive to the mapping between effort and outcomes. Given an arbitrary problem, the optimal incentives can look like anything.\n\n\nThe agency problems studied by economists are typically quite different to the scenarios envisaged by AI risk proponents. Therefore, because of the brittleness of PAL models, we shouldn’t be too surprised if the imagined AI risk outcomes aren’t present in the existing literature. PAL, in its current form, might just not be of much use. Further, we should not expect there to be any generic answer to the question “How big are AI agency rents?”: the answer will depend on the specific task the AI is doing and a host of other details.\n\n\nAgents rents are too narrow a measure\n-------------------------------------\n\n\nAs we’ve seen, AI risk scenarios can include bad outcomes that aren’t agency rents, but that we nevertheless care about. When applying PAL to AI risk, care must be taken to distinguish between rents and other bad outcomes, and we cannot assume that a bad outcome necessarily means high rents.\n\n\nPAL models typically assume contract enforceability\n---------------------------------------------------\n\n\nStuart Armstrong [argued](https://www.lesswrong.com/posts/92J4zJHkqmXTduxzY/and-the-ai-would-have-got-away-with-it-too-if) that Hanson’s critique doesn’t work because PAL assumes contract enforceability, and with advanced AI, institutions might not be up to the task.[[15]](#fn-uN5mgNcRj7ghxhRAF-15) Indeed, contract enforceability is assumed in most of PAL, so it’s an important consideration regarding their applicability to AI scenarios more broadly.[[16]](#fn-uN5mgNcRj7ghxhRAF-16)\n\n\nThe assumption isn’t plausible in pessimistic scenarios where human principals and institutions are insufficiently powerful to punish the AI agent, e.g. due to very fast take-off. But it is plausible for when AIs are similarly smart to humans, and in scenarios where powerful AIs are used to enforce contracts. Furthermore, if we cannot enforce contracts with AIs then people will promptly realise and stop using AIs; so we should expect contracts to be enforceable conditional upon AIs being used.[[17]](#fn-uN5mgNcRj7ghxhRAF-17)\n\n\nThere is a smaller sub-literature on self-enforcing contracts ([seminal paper](https://web.stanford.edu/~jdlevin/Papers/RIC.pdf)). Here contracts can be self-enforced because both parties have an interest in interacting repeatedly. We think these probably won’t be helpful for understanding situations without contract enforceability, because in worlds where contracts aren’t enforceable because of advanced AI, contracts likely won’t be self-enforcing either. If AIs are powerful enough that institutions like the police and military can’t constrain them, it seems unlikely that they’d have much to gain from repeated cooperative interactions with human principals. Why not make a copy of themselves to do the task, coerce humans into doing it, or cooperate with other advanced AIs?\n\n\nPAL models typically assume AIs work for humans because they are paid\n---------------------------------------------------------------------\n\n\nIn reality AIs will probably not receive a wage, and instead work for humans because that is their default behaviour. We think changing this would probably not make a big difference to agency models, because the wage could be substituted for other resources the AI cares about. For instance, AI needs compute to run. If we substitute “wage” for “compute”, the agency rents that the agent extracts is additional compute that it can use for its own purposes.\n\n\nThere is a sub-literature on [Optimal Delegation](https://web.stanford.edu/group/SITE/papers2005/Matouschek.05.pdf) that does away with wages. This literature focuses on the best way to restrict the agents action set. For AI agents, this is equivalent to [AI boxing](https://wiki.lesswrong.com/wiki/AI_boxing). We don’t think this literature will be helpful; PAL doesn’t study how realistic it is to box AI successfully, it just assumes it’s technologically possible. It therefore isn’t informative about whether AI boxing will work.\n\n\nConclusion\n==========\n\n\nThere are similarities between the AI alignment and principal-agent problems, suggesting that PAL could teach us about AI risk. However, the situations economists have studied are very different to those discussed by proponents of AI risk, meaning that findings from PAL don’t transfer easily to this context. There are a few main issues. The principal-agent setup is only a part of AI risk scenarios, making agency rents too narrow a metric. PAL models rarely consider agents more intelligent than their principals and the models are very brittle. And the lack of insight from PAL unawareness models severely restricts their usefulness for understanding the accident risk scenario.\n\n\nNevertheless, extensions to PAL might still be useful. Agency rents are what might allow AI agents to accumulate wealth and influence, and agency models are the best way we have to learn about the size of these rents. These findings should inform a wide range of future scenarios, perhaps barring extreme ones like Bostrom/Yudkowsky.[[18]](#fn-uN5mgNcRj7ghxhRAF-18)\n\n\n\n\n---\n\n\n\n1. Thanks to Wei Dai for [pointing out a previous inaccuracy](https://www.lesswrong.com/posts/Z5ZBPEgufmDsm7LAv/what-can-the-principal-agent-literature-tell-us-about-ai?commentId=2Em4b2SgJQdiPbiZb) [↩︎](#fnref-uN5mgNcRj7ghxhRAF-1)\n2. Agency rents are about e.g. working vs shirking. If the agent uses the money she earned to buy a gun and later shoot the principal, clearly this is very bad for her, but it’s not captured by agency rents. [↩︎](#fnref-uN5mgNcRj7ghxhRAF-2)\n3. It’s not totally clear to us why we should care about our fraction of influence over the future, rather than the total influence. Probably because the fraction of influence affects the total influence, influence being zero-sum and resources finite. [↩︎](#fnref-uN5mgNcRj7ghxhRAF-3)\n4. It wasn’t clear to us from the original post, at least in Part 1 of the story with no conflict, that humans are better off in absolute terms. For instance, wording like “over time those proxies will come apart” and “People really will be getting richer for a while” seemed to suggest that things are expected to worsen. Given this, Hanson’s interpretation (that Christiano’s story implied massive agency rents) seems reasonable without further clarification.\nBen Garfinkel mentioned an outside-view measure which he thought undermined the plausibility of Part 1: since the industrial revolution we seem to have been using more and more proxies, which are optimized for more and more heavily, but things have been getting better and better. So he also seems to have understood the scenario to mean things get worse in absolute terms. [↩︎](#fnref-uN5mgNcRj7ghxhRAF-4)\n5. Clarifying what it means for an AI system to earn and use rents also seems important, helping us make sure that the abstraction maps cleanly onto the practical scenarios we are envisaging.\nRelatedly, what traits would an AI system need to have for it to make sense to think of the system as “accumulating and using rents”? Rents can be cashed out in influence of many different kinds — a human worker might get higher wage, or more free time — and what ends up occuring will depend on the capabilities of the AI systems. Concretely, money can be saved in a bank account, people can be influenced, or computer hardware can be bought and run. One example of an obvious capability constraint for AI: some AI systems will be “switched off” after they are run, limiting their ability to transfer rents through time.\nAs AI agents will (initially) be owned by humans, historical instances of [slaves earning rents](http://www.overcomingbias.com/2016/07/the-future-of-slavery.html) seem worth looking into. [↩︎](#fnref-uN5mgNcRj7ghxhRAF-5)\n6. Although his scenario is more plausible if a smarter agent extracts more agency rents. [↩︎](#fnref-uN5mgNcRj7ghxhRAF-6)\n7. Hanson and Christiano agree on this point.\nHanson: “Just as most wages that slaves earned above subsistence went to slave owners, most of the wealth generated by AI could go to the capital owners, i.e. their slave owners. Agency rents are the difference above that minimum amount.”\nChristiano: “Agency rents are what makes the AI rich. It's not that computers would \"become rich\" if they were superhuman, and they just aren't rich yet because they aren't smart enough. On the current trajectory computers just won't get rich.” [↩︎](#fnref-uN5mgNcRj7ghxhRAF-7)\n8. One limitation is that rents are the cost to the principal, whereas the accident scenario has costs for all humanity. This distinction isn’t especially important because in the accident scenario the outcome for the principal is catastrophic (i.e. extremely high agency rents), and this is what is potentially in tension with PAL. Nonetheless, we should keep in mind that the total costs of this scenario are not limited to agency rents, just as in Christiano’s scenario. [↩︎](#fnref-uN5mgNcRj7ghxhRAF-8)\n9. Perhaps a more realistic framing: the principal is aware that there’s some probability that the agent will take an unanticipated catastrophic action, without knowing what that action might be. Under competitive pressures, maybe in a time of war, it could be beneficial for the principal to delegate (in expectation) despite significant risk, while humanity is made worse off (in expectation).\nThis, of course, would be modelled quite differently to the accident AI risk we consider in the text, and we suspect that economic models would confirm that principals would take the risk in sufficiently competitive scenarios. These models would focus on negative externalities of risky AI development, something more naturally studied in domains like public economics rather than with agency theory.\nIn any case, we focus here on the more traditional AI risk framing along the lines of “you think you have the AI under control, but beware, you could be wrong”. [↩︎](#fnref-uN5mgNcRj7ghxhRAF-9)\n10. AI accident risk will be large when the AI agent thinks of new actions that i) harm the principal ii) further the agent's goals iii) the principal hasn't anticipated. [↩︎](#fnref-uN5mgNcRj7ghxhRAF-10)\n11. This is because claims about the actions available to the agent and the principal’s awareness are part of PAL models’ assumptions. We discuss this more below. [↩︎](#fnref-uN5mgNcRj7ghxhRAF-11)\n12. The correct example: *“If you prefer solving environmental problems, you might ask the machine to counter the rapid acidification of the oceans that results from higher carbon dioxide levels. The machine develops a new catalyst that facilitates an incredibly rapid chemical reaction between ocean and atmosphere and restores the oceans’ pH levels. Unfortunately, a quarter of the oxygen in the atmosphere is used up in the process, leaving us [humans] to asphyxiate slowly and painfully.”* [↩︎](#fnref-uN5mgNcRj7ghxhRAF-12)\n13. I.e. the principal’s rationality is bounded to a greater extent than the agent’s [↩︎](#fnref-uN5mgNcRj7ghxhRAF-13)\n14. In the model in “Moral Hazard With Unawareness” either the principal or the agent’s rationality can be bounded [↩︎](#fnref-uN5mgNcRj7ghxhRAF-14)\n15. As argued above, we don’t think contract enforceability is the main reason Hanson’s critique of Christiano fails; agency rents are just not unusually high in his scenario. [↩︎](#fnref-uN5mgNcRj7ghxhRAF-15)\n16. From *[Contract Theory](https://mitpress.mit.edu/books/contract-theory)*: *“The benchmark contracting situation that we shall consider in this book is one between two parties who operate in a market economy with a well-functioning legal system. Under such a system, any contract the parties decide to write will be enforced perfectly by a court, provided, of course, that it does not contravene any existing laws.”* [↩︎](#fnref-uN5mgNcRj7ghxhRAF-16)\n17. Thanks to Ben Garfinkel for pointing this out. [↩︎](#fnref-uN5mgNcRj7ghxhRAF-17)\n18. Robin Hanson pointed out to us that when thinking about strange future scenarios, we should try to think about similar strange scenarios that we have seen in the past (we are very sympathetic to this, despite our somewhat skeptical position regarding PAL). With this in mind, another field which seems worth looking into is Security, especially military security. National leaders have been assassinated by their guards; [kings have been killed](https://www.lesswrong.com/posts/92J4zJHkqmXTduxzY/and-the-ai-would-have-got-away-with-it-too-if) by their protectors. These seem like a closer analogue to many AI risk scenarios than the typical PAL setup. It seems important to understand what the major risk factors are in these situations, how people have guarded against catastrophic failures, and how this translates to cases of catastrophic AI risk. [↩︎](#fnref-uN5mgNcRj7ghxhRAF-18)", "url": "https://www.alignmentforum.org/posts/Z5ZBPEgufmDsm7LAv/what-can-the-principal-agent-literature-tell-us-about-ai", "date_published": "2020-02-08T21:28:10Z", "authors": ["Alexis Carlier"], "tags": ["World Modeling", "Principal-Agent Problems", "AI Risk"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.709515+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "0c8ec3f6ce6e68153c24105aaf6dfe5a", "source": "alignmentforum", "title": "[AN #86]: Improving debate and factored cognition through human experiments", "text": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.\n\nAudio version [here](http://alignment-newsletter.libsyn.com/alignment-newsletter-86) (may not be up yet).\n\n**Highlights**\n--------------\n\n[Writeup: Progress on AI Safety via Debate](https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1) *(Beth Barnes et al)* (summarized by Rohin): This post reports on work done on creating a [debate](https://blog.openai.com/debate/) ([AN #5](https://mailchi.mp/0ae5d69de63b/alignment-newsletter-5)) setup that works well with human players. In the game, one player is honest (i.e. arguing for the correct answer) and one is malicious (i.e. arguing for some worse answer), and they play a debate in some format, after which a judge must decide which player won the debate. They are using Thinking Physics questions for these debates, because they involve questions with clear answers that are confusing to most people (the judges) but easy for some experts (the players).\n\nEarly freeform text debates did not work very well, even with smart, motivated judges. The malicious player could deflect on questions they didn't want to answer, e.g. by claiming that the question was ambiguous and redirecting attention by asking new questions. In addition, when the malicious player got to go first and give an incorrect \"framework\" for finding the answer, and then made individually true claims to \"fill in\" the framework, it was hard for the honest player to rebut it. So, they moved to a framework without such asymmetries: both players gave a claim (simultaneously), both gave constructive arguments, and both rebutted the other's arguments. In addition, part of the appeal of debate is that the agents can \"zoom in\" on the particular disagreement they have, and the judge need only analyze the smallest disagreement in order to declare an overall winner.\n\nThis suggests the following setup: players simultaneously provide an answer supported with subclaims. Then, after looking at the other player's answer and subclaims, they can provide objections (perhaps on particular subclaims, or the overall structure of the argument). Players can concede or challenge objections left by the other player; the debate then recurses on one of the challenged objections (currently, the malicious player chooses which claim to recurse on, in order to be conservative). There is a depth limit to the recursion, once it is reached (or if there are no challenged objections) the judge is given a transcript of the debate and must declare a winner.\n\nA new problem arises: the malicious player can exploit ambiguity to their advantage. For example, suppose the malicious player is arguing that there are 5 primes between 0 and 10, and makes the subclaims \"there are 3 primes between 0 and 5\" and \"there are 2 primes between 5 and 10\". If challenged on the first, they will say \"the three primes are 2, 3, and 5\", if challenged on the second, they will say \"the two primes are 5 and 7\". While in this simple case the honest player could register a top-level objection claiming that the word \"between\" is ambiguous, and ask for clarification, in general it would be too hard to eliminate all ambiguity.\n\nTo mitigate this, the team added a method of *cross examination*, in which each player is allowed to ask an earlier version of the other agent a question, and quote its answer as part of the debate transcript. (The post talks about how this can be approximated with humans, but I won't go into the details.) In the complexity theory analogy, this boosts debate from PSPACE to NEXP. Intuitively, cross examination allows players to force each other to precommit to specific claims and arguments, because if they don't stay consistent in their claims and arguments, then their opponent can demonstrate the inconsistency. Now, if the malicious player tries to make an argument that depends on interpreting an ambiguous concept in two different ways, then the honest player can cross examine and ask which of the two interpretations they mean. If they are inconsistent, that can be demonstrated to the judge; if they consistently answer one way, then the honest player can challenge the part of the argument that depends on the other interpretation.\n\nThey then identify several open concerns with debate, of which they highlight the long computation problem. This is a problem when you no longer assume that the debaters have optimal play: in this case, the malicious player could create a complicated argument that neither debater understands well, that supports the malicious case but that the honest player doesn't know how to refute.\n\n**Rohin's opinion:** I enjoyed this a lot: the problems found were crisp and the solutions had good arguments that they actually solved the identified problem. Reading through the actual examples and arguments made me more optimistic about debate in general, mostly from a felt sense that the actual concrete results were getting closer to matching the theoretical ideal, and that there actually could be reasonable solutions to \"messy\" problems like ambiguity.\n\nThe full post has formal explanations and actual examples, which I highly recommend.\n\n[Evaluating Arguments One Step at a Time](https://ought.org/updates/2020-01-11-arguments) *(Ought)* (summarized by Rohin): This blog post reports on the results of Ought's experiments with [factored evaluation](https://ought.org/presentations/delegating-cognitive-work-2019-06) ([AN #62](https://mailchi.mp/4a1b3c4249ae/an-62are-adversarial-examples-caused-by-real-but-imperceptible-features)) by humans on questions about movie reviews. There are two human players, one honest and one malicious. Half the time, the honest player chooses a true top-level claim, decomposes it into supporting subclaims up to some depth, and provides supporting quotes for each claim and subclaim. The other half of the time, the malicious player does the same, except they choose a *false* top-level claim. In either case, the opposite player then chooses quotes that aim to *refute* the claims and subclaims. For each claim and subclaim, a new judge evaluates whether that claim is \"valid\" or \"invalid\". If *any* claim is marked invalid, then the top-level claim is judged to be false, otherwise it is judged true.\n\nThey found that depth 1 and depth 2 trees outperformed depth 0 trees (for which judgments were at about chance). However, they did not perform amazingly well -- with posthoc hyperparameter tuning, depth 1 and 2 trees got accuracies of 65% and 69% respectively. False positives (accepting false claims) were primarily caused due to malicious claims that couldn't easily be rebutted with the limited budget (but could be rebutted with a larger budget), and individual steps being complex enough that the judge made mistakes. False negatives (rejecting true claims) tend to happen when players choose hard-to-argue true claims, especially ones in which no specific quote clearly supports the claim, even though the full paragraph supports the claim through its tone and style.\n\nThere are several approaches that could theoretically solve these problems, such as increasing the size of claim trees, and improving quality control for judgments (e.g. by aggregating multiple judgments together).\n\n**Rohin's opinion:** I wouldn't read too much into the low accuracies of the depth 2 trees: it seems quite plausible that this is specific to the movie review setting, and in settings with clearer answers you could do better. Like with the previous post, I found the actual examples quite illuminating: it's always interesting to see what happens when theory collides with the real world.\n\n**Technical AI alignment**\n==========================\n\n### **Technical agendas and prioritization**\n\n[Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda](https://www.lesswrong.com/s/p947tK8CoBbdpPtyK) *(Jesse Clifton)* (summarized by Flo): This agenda by the [Effective Altruism Foundation](https://ea-foundation.org/) focuses on risks of astronomical suffering (s-risks) posed by [Transformative AI](https://arxiv.org/abs/1912.00747) ([AN #82](https://mailchi.mp/7ba40faa7eed/an-82-how-openai-five-distributed-their-training-computation)) (TAI) and especially those related to conflicts between powerful AI agents. This is because there is a very clear path from extortion and executed threats against altruistic values to s-risks. While especially important in the context of s-risks, cooperation between AI systems is also relevant from a range of different viewpoints. The agenda covers four clusters of topics: strategy, credibility and bargaining, current AI frameworks, as well as decision theory.\n\nThe extent of cooperation failures is likely influenced by how power is distributed after the transition to TAI. At first glance, it seems like widely distributed scenarios (as [CAIS](https://www.fhi.ox.ac.uk/reframing/) ([AN #40](https://mailchi.mp/b649f32b07da/alignment-newsletter-40))) are more problematic, but related literature from international relations paints a more complicated picture. The agenda seeks a better understanding of how the distribution of power affects catastrophic risk, as well as potential levers to influence this distribution. Other topics in the strategy/governance cluster include the identification and analysis of realistic scenarios for misalignment, as well as case studies on cooperation failures in humans and how they can be affected by policy.\n\nTAI might enable unprecedented credibility, for example by being very transparent, which is crucial for both contracts and threats. The agenda aims at better models of the effects of credibility on cooperation failures. One approach to this is open-source game theory, where agents can see other agents' source codes. Promising approaches to prevent catastrophic cooperation failures include the identification of peaceful bargaining mechanisms, as well as surrogate goals. The idea of surrogate goals is for an agent to commit to act as if it had a different goal, whenever it is threatened, in order to protect its actual goal from threats.\n\nAs some aspects of contemporary AI architectures might still be present in TAI, it can be useful to study cooperation failure in current systems. One concrete approach to enabling cooperation in social dilemmas that could be tested with contemporary systems is based on bargaining over policies combined with punishments for deviations. Relatedly, it is worth investigating whether or not multi-agent training leads to human-like bargaining by default. This has implications on the suitability of behavioural vs classical game theory to study TAI. The behavioural game theory of human-machine interactions might also be important, especially in human-in-the-loop scenarios of TAI.\n\nThe last cluster discusses the implications of bounded computation on decision theory as well as the decision theories (implicitly) used by current agent architectures. Another focus lies on acausal reasoning and in particular the possibility of [acausal trade](https://wiki.lesswrong.com/wiki/Acausal_trade), where different correlated AI systems cooperate without any causal links between them.\n\n**Flo's opinion:** I am broadly sympathetic to the focus on preventing the worst outcomes and it seems plausible that extortion could play an important role in these, even though I worry more about distributional shift plus incorrigibility. Still, I am excited about the focus on cooperation, as this seems robustly useful for a wide range of scenarios and most value systems.\n\n**Rohin's opinion:** Under a suffering-focused ethics under which s-risks far overwhelm x-risks, I think it makes sense to focus on this agenda. There don't seem to be many plausible paths to s-risks: by default, we shouldn't expect them, because it would be quite surprising for an amoral AI system to think it was particularly useful or good for humans to *suffer*, as opposed to not exist at all, and there doesn't seem to be much reason to expect an immoral AI system. Conflict and the possibility of carrying out threats are the most plausible ways by which I could see this happening, and the agenda here focuses on neglected problems in this space.\n\nHowever, under other ethical systems (under which s-risks are worse than x-risks, but do not completely dwarf x-risks), I expect other technical safety research to be more impactful, because other approaches can more directly target the failure mode of an amoral AI system that doesn't care about you, which seems both more likely and more amenable to technical safety approaches (to me at least). I could imagine work on this agenda being quite important for *strategy* research, though I am far from an expert here.\n\n### **Iterated amplification**\n\n[Synthesizing amplification and debate](https://www.alignmentforum.org/posts/dJSD5RK6Qoidb3QY5/synthesizing-amplification-and-debate) *(Evan Hubinger)* (summarized by Rohin): The distillation step in [iterated amplification](https://blog.openai.com/amplifying-ai-training/) ([AN #30](https://mailchi.mp/c1f376f3a12e/alignment-newsletter-30)) can be done using imitation learning. However, as argued in [Against Mimicry](https://ai-alignment.com/against-mimicry-6002a472fc42), if your model M is unable to do perfect imitation, there must be errors, and in this case the imitation objective doesn't necessarily incentivize a graceful failure, whereas a reward-based objective does. So, we might want to add an auxiliary reward objective. This post proposes an algorithm in which the amplified model answers a question via a [debate](https://blog.openai.com/debate/) ([AN #5](https://mailchi.mp/0ae5d69de63b/alignment-newsletter-5)). The distilled model can then be trained by a combination of imitation of the amplified model, and reinforcement learning on the reward of +1 for winning the debate and -1 for losing.\n\n**Rohin's opinion:** This seems like a reasonable algorithm to study, though I suspect there is a simpler algorithm that doesn't use debate that has the same advantages. Some other thoughts in [this thread](https://www.alignmentforum.org/posts/dJSD5RK6Qoidb3QY5/synthesizing-amplification-and-debate#PhH6BstgNf5zmZK8W).\n\n### **Learning human intent**\n\n[Deep Bayesian Reward Learning from Preferences](http://arxiv.org/abs/1912.04472) *(Daniel S. Brown et al)* (summarized by Zach): Bayesian inverse reinforcement learning (IRL) is ideal for safe imitation learning since it allows uncertainty in the reward function estimator to be quantified. This approach requires thousands of likelihood estimates for proposed reward functions. However, each likelihood estimate requires training an agent according to the hypothesized reward function. Predictably, such a method is computationally intractable for high dimensional problems.\n\n**In this paper, the authors propose Bayesian Reward Extrapolation (B-REX), a scalable preference-based Bayesian reward learning algorithm.** They note that in this setting, a likelihood estimate that requires a loop over all demonstrations is much more feasible than an estimate that requires training a new agent. So, they assume that they have a set of *ranked* trajectories, and evaluate the likelihood of a reward function by its ability to reproduce the preference ordering in the demonstrations. To get further speedups, they fix all but the last layer of the reward model using a pretraining step: the reward of a trajectory is then simply the dot product of the last layer with the features of the trajectory as computed by all but the last layer of the net (which can be precomputed and cached once).\n\nThe authors test B-REX on pixel-level Atari games and show competitive performance to [T-REX](https://arxiv.org/abs/1904.06387) ([AN #54](https://mailchi.mp/3e2f43012b07/an-54-boxing-a-finite-horizon-ai-system-to-keep-it-unambitious)), a related method that only computes the MAP estimate. Furthermore, the authors can create confidence intervals for performance since they can sample from the reward distribution.\n\n**Zach's opinion:** The idea of using preference orderings (Bradley-Terry) to speed up the posterior probability calculation was ingenious. While B-REX isn't strictly better than T-REX in terms of rewards achieved, the ability to construct confidence intervals for performance is a major benefit. My takeaway is that Bayesian IRL is getting more efficient and may have good potential as a practical approach to safe value learning.\n\n### **Preventing bad behavior**\n\n[Attainable utility has a subagent problem](https://www.alignmentforum.org/posts/sYjCeZTwA84pHkhBJ/attainable-utility-has-a-subagent-problem) *(Stuart Armstrong)* (summarized by Flo): This post argues that regularizing an agent's impact by [attainable utility](https://www.alignmentforum.org/posts/yEa7kwoMpsBgaBCgb/towards-a-new-impact-measure) ([AN #25](https://mailchi.mp/0c5eeec28f75/alignment-newsletter-25)) can fail when the agent is able to construct subagents. Attainable utility regularization uses auxiliary rewards and penalizes the agent for changing its ability to get high expected rewards for these to restrict the agent's power-seeking. More specifically, the penalty for an action is the absolute difference in expected cumulative auxiliary reward between the agent either doing the action or nothing for one time step and then optimizing for the auxiliary reward.\n\nThis can be circumvented in some cases: If the auxiliary reward does not benefit from two agents instead of one optimizing it, the agent can just build a copy of itself that does not have the penalty, as doing this does not change the agent's ability to get a high auxiliary reward. For more general auxiliary rewards, an agent could build another more powerful agent, as long as the powerful agent commits to balancing out the ensuing changes in the original agent's attainable auxiliary rewards.\n\n**Flo's opinion:** I am confused about how much the commitment to balance out the original agent's attainable utility would constrain the powerful subagent. Also, in the presence of subagents, it seems plausible that attainable utility mostly depends on the agent's ability to produce subagents of different generality with different goals: If a subagent that optimizes for a single auxiliary reward was easier to build than a more general one, building a general powerful agent could considerably decrease attainable utility for all auxiliary rewards, such that the high penalty rules out this action.\n\n**News**\n========\n\n[TAISU - Technical AI Safety Unconference](https://www.lesswrong.com/events/BPTzfeQeZZ6chHvtr/taisu-technical-ai-safety-unconference-1) *(Linda Linsefors)* (summarized by Rohin): This unconference on technical AI safety will be held May 14th-17th; application deadline is February 23.\n\n[AI Alignment Visiting Fellowship](https://www.fhi.ox.ac.uk/fellows/) (summarized by Rohin): This fellowship would support 2-3 applicants to visit FHI for three or more months to work on human-aligned AI. The application deadline is Feb 28.", "url": "https://www.alignmentforum.org/posts/cZqPGDxbJcbShGwDn/an-86-improving-debate-and-factored-cognition-through-human", "date_published": "2020-02-12T18:10:02Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "Ought", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.710564+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "7c0efd06f2d9680881fb078fb45787fc", "source": "alignmentforum", "title": "Distinguishing definitions of takeoff", "text": "I find discussions about AI takeoff to be very confusing. Often, people will argue for \"slow takeoff\" or \"fast takeoff\" and then when I ask them to operationalize what those terms mean, they end up saying something quite different than what *I* thought those terms meant. \n\nTo help alleviate this problem, I aim to compile the definitions of AI takeoff that I'm currently aware of, with an emphasis on definitions that have clear specifications. I will continue updating the post as long as I think it serves as a useful reference for others.\n\nIn this post, an AI takeoff can be roughly construed as \"the dynamics of the world associated with the development of powerful artificial intelligence.\" These definitions characterize different ways that the world can evolve as [transformative AI](https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence) is developed. \n\nFoom/Hard takeoff\n=================\n\nThe traditional hard takeoff position, or \"Foom\" position (these appear to be equivalent terms) was characterized in [this post](https://www.lesswrong.com/posts/tjH8XPxAnr6JRbh7k/hard-takeoff) from Eliezer Yudkowsky. It contrasts Hanson's takeoff scenario by emphasizing *local* dynamics: rather than a population of artificial intelligences coming into existence, there would be a single intelligence that quickly reaches a level of competence that outstrips the world's capabilities to control it. The proposed *mechanism* that causes such a dynamic is [recursive self improvement](https://www.lesswrong.com/posts/JBadX7rwdcRFzGuju/recursive-self-improvement), though Yudkowsky later [suggested that this wasn't necessary](https://intelligence.org/2017/10/20/alphago/).\n\nThe ability for recursive self improvement to induce a hard takeoff was defended in [Intelligence Explosion Microeconomics](https://intelligence.org/files/IEM.pdf). He argues against Robin Hanson in the [AI Foom debates](https://intelligence.org/ai-foom-debate/). Watch [this video](https://www.youtube.com/watch?v=m_R5Z4_khNw) to see the live debate.\n\nGiven the word \"hard\" in this notion of takeoff, a \"soft\" takeoff could simply be defined as the negation of a hard takeoff.\n\nHansonian \"slow\" takeoff\n========================\n\nRobin Hanson objected to hard takeoff by predicting that growth in AI capabilities will not be *extremely* [uneven between projects](http://www.overcomingbias.com/2014/07/30855.html). In other words, there is unlikely to be one AI project, or even a small set of AI projects, that produces a system that outstrips the abilities of the rest of the world. While he rejects Yudkowsky's argument, it is inaccurate to say that Robin Hanson expected growth in AI capabilities to be slow.\n\nIn [Economic Growth Given Machine Intelligence](http://mason.gmu.edu/~rhanson/aigrow.pdf), Hanson argues that AI induced growth could cause GDP to double on the timescale of months. Very high economic growth would mark a radical transition to a faster mode of technological progress and capabilities, something that Hanson argues is [entirely precedented](http://mason.gmu.edu/~rhanson/longgrow.pdf) in human history.\n\nThe technology that Hanson envisions will induce fast economic growth is whole brain emulation, which [he wrote a book about](https://en.wikipedia.org/wiki/The_Age_of_Em). In general, Hanson rejects the framework that AGI should be seen as an invention that occurs at a particular moment in time: instead, AI should be viewed as an input to the economy, (like electricity, though the considerations may be different).\n\nBostromian takeoffs\n===================\n\nNick Bostrom appeared to throw away much of the terminology in the AI Foom debate in order to invent his own. In [Superintelligence](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies) he provides a characterization of three types of AI capability growth modes, defined by the clock-time (real physical time) from when a system is roughly human-level to when it is strongly superintelligent, defined as \"a level of intelligence vastly greater than contemporary humanity's combined intellectual wherewithal.\" \n\nSome have objected to Bostrom's use of clock-time to define takeoff, instead arguing that [work required to align systems](https://www.lesswrong.com/posts/cxgtQXnH2uDGBJJGa/redefining-fast-takeoff) is a better metric (though harder to measure).\n\nSlow\n----\n\nA slow takeoff is one that occurs over the timescale of decades or centuries. Bostrom predicted that this timescale would allow for institutions, such as governments, to react to new AI developments. It would also allow for testing incrementally more powerful technologies without existential risks associated with testing.\n\nFast\n----\n\nA fast takeoff is one that occurs over the timescale of minutes, hours, or days. Given such short time to react, Bostrom believes that local dynamics of the takeoff become relevant, as was the case in Yudkowsky's foom scenario. \n\nModerate\n--------\n\nA moderate takeoff is situated between slow and fast, and occurs on the timescale of months or years. \n\nContinuous takeoff\n==================\n\nContinuous takeoff was defined, and partially defended in [my post](https://www.lesswrong.com/posts/CjW4axQDqLd2oDCGG/misconceptions-about-continuous-takeoff). Its meaning primarily derives from Katja Grace's post on [discontinuous progress around the development of AGI](https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/). In that post, Grace characterizes discontinuities:\n\n\n> We say a technological discontinuity has occurred when a particular technological advance pushes some progress metric substantially above what would be expected based on extrapolating past progress. We measure the size of a discontinuity in terms of how many years of past progress would have been needed to produce the same improvement. We use judgment to decide how to extrapolate past progress.\n\nIn my post, I extrapolate this concept and invert it, using terminology that I saw Rohin use in [this Alignment Newsletter edition](https://www.lesswrong.com/posts/GPADepj6yP8zqSbJh/an-65-learning-useful-skills-by-watching-humans-play), and define continuous takeoff as\n\n\n> A scenario where the development of competent, powerful AI follows a trajectory that is roughly in line with what we would have expected by extrapolating from past progress. \n\nGradual/incremental takeoff?\n============================\n\nSome people [objected](https://www.lesswrong.com/posts/CjW4axQDqLd2oDCGG/misconceptions-about-continuous-takeoff#LQ5vN3vERvs7t827Q) to my use of the word continuous, as they found that the words gradual or incremental are more descriptive and mathematically accurate. After all, the following function is [continuous](https://en.wikipedia.org/wiki/Continuous_function), but not gradual.\n\n![](https://www.mathworks.com/matlabcentral/mlc-downloads/downloads/24e59eb7-2e7f-4f5d-8624-e8796b617ac6/c1512698-fee2-4e87-b57b-2b8f86930b45/previews/sigmoid/html/sigmoid_documentation_02.png)Additionally, if you agree with Hanson's thesis that history can be seen as a series of economic growth modes, each faster than the last one, then continuous takeoff as plainly defined is in trouble. That's because technological progress from 1800 - 1900 was [much faster](https://en.wikipedia.org/wiki/Gross_world_product) than technological progress from 1700 - 1800. Therefore, \"extrapolating from past progress\" would provide an incorrect estimate of progress, if one did not foresee the industrial revolution. In general, extrapolating from past progress is hard because it [depends on the reference class](https://www.lesswrong.com/posts/FsfnDfADftGDYeG4c/outside-view-as-conversation-halter) you are using to forecast. \n\nPaul slow takeoff\n=================\n\nPaul Christiano [argues](https://sideways-view.com/2018/02/24/takeoff-speeds/) that we should characterize takeoff in terms of economic growth rates (similar to Hanson) but uses a definition that emphasizes how quickly the economy transitions into a period of higher growth. He defines slow takeoff as\n\n\n> There will be a complete 4 year interval in which world output doubles, before the first 1 year interval in which world output doubles. (Similarly, we’ll see an 8 year doubling before a 2 year doubling, etc.)\n\nand defines fast takeoff as the negation of the above statement. Note that this definition leaves a third possibility: you could believe that the world output will *never* double during a 1 year interval, a position I would refer to as \"no takeoff\" which I explain next.\n\nPaul's outline of slow takeoff shares some of its meaning with continuous takeoff, because under a slow transition to a higher growth mode, change won't be sudden.\n\nNo takeoff\n==========\n\n\"No takeoff\" is essentially my term for the belief that world economic growth rates won't accelerate to a very high level (perhaps >30% real GDP growth rate in one year) following the development of AI. William Macaskill [is a notable skeptic](https://forum.effectivealtruism.org/posts/oPGJrqohDqT8GZieA/ask-me-anything#HcRNG4yhB4RsDtYit) of AI takeoff. I have created [this Metaculus question](https://www.metaculus.com/questions/3477/if-human-level-artificial-intelligence-is-developed-will-world-gdp-grow-by-at-least-300-in-any-of-the-subsequent-15-years/) to operationalize the thesis. \n\nThe Effective Altruism Foundation wrote [this post](https://foundational-research.org/the-future-of-growth-near-zero-growth-rates/) suggesting that peak economic growth rates may lie in the past. If we use the [outside view](https://wiki.lesswrong.com/wiki/Outside_view), this position may be reasonable. Economic growth rates [have slowed down](https://aiimpacts.org/historical-growth-trends/) since the 1960s despite the rise of personal computers and the internet: technologies that we might have naively predicted would be transformative ahead of time.\n\nThis position should *not* be confused with the idea that humanity will never develop superintelligent computers, though that scenario is compatible with no takeoff.\n\nDrexler's takeoff\n=================\n\nEric Drexler argues in [Comprehensive AI Services](https://www.fhi.ox.ac.uk/reframing/) (CAIS) that future AI will be *modular,* meaning that there is unlikely to be a single system that can perform a set of diverse tasks all at once before there are individual systems that can perform the individual tasks more competently than the single system can. This idea [shares groundwork](http://www.overcomingbias.com/2019/02/how-lumpy-ai-services.html) with Hanson's objection to a local takeoff. The reverse of this scenario is what Hanson calls \"lumpy AI\" where single agentic systems outcompete a set of services.\n\nDrexler uses the CAIS model to argue against the binary characterization of self-improvement. Just as technology already feeds into itself, and thus the world can already be seen as \"recursively self improving itself\", future AI research could feed into itself as [recursive technological improvement](https://www.lesswrong.com/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services-as), without the necessary focus on single systems improving themselves. \n\nIn other words, rather than viewing AIs as either self improving or not, self improvement can be seen as a continuum from \"the entire world works to improve a system\" on one end, and \"a single local system improves only itself, with outside forces providing minimal benefit to growth in capabilities\" on the other. \n\nBaumann's soft takeoff\n======================\n\nIn [this post](http://s-risks.org/a-framework-for-thinking-about-ai-timescales/), Tobias Baumann argues that we should operationalize soft takeoff in terms of how quickly the fraction of global economic activity attributable to autonomous AI systems will rise. \"Time\" here is not necessarily clock-time, as was the case in Bostrom's takeoff. Time can also refer to *economic time*, which is a measure of time that adjusts for rate of economic growth, and *political time*, a measure that adjusts for rate of social change. \n\nHe explains that this operationalization avoids the pitfalls of definitions that rely on moments in time where AI reaches thresholds such as \"human-level\" or \"superintelligent.\" He argues that AI is likely to surpass human abilities in some domains and not in others, rather than surpass us in all ways all at once.\n\nRobin Hanson [appears to agree](https://www.lesswrong.com/posts/ktDKfKqukTPRiuEPM/robin-hanson-on-the-futurist-focus-on-ai#Fnubv9gsj8h7GQM3X) with a similar measure for AI progress.\n\nLess common definitions\n=======================\n\nEvent Horizon/Epistemic Horizon\n-------------------------------\n\nIn 2007, Yudkowsky outlined the [three schools of singularity](http://yudkowsky.net/singularity/schools/), which was perhaps the state of the art for takeoff discussions at the time. In it he included his own scenario (Foom), the Event Horizon, and Accelerating Change. \n\nThe Event Horizon hypothesis could be seen as an extrapolation of Vernor Vinge's definition of the technological singularity. It is defined as a point in time after which current models of future progress break down, which is essentially the opposite definition of continuous takeoff. \n\nAn epistemic horizon would be relevant for decision making because it would imply that AI progress could come suddenly, without warning. If this were true, then our safety guarantees assumed under a continuous takeoff scenario would fail. Furthermore, even if we *could* predict rapid change ahead of time, due to *social pressures*, people might fail to act until it's too late, a position argued for in [There’s No Fire Alarm for Artificial General Intelligence](https://intelligence.org/2017/10/13/fire-alarm/).\n\n(Note, I see a lot of people interpreting the Fire Alarm essay as merely arguing that we can't predict rapid progress before it's too late. The essay itself dispels this interpretation, \"When I observe that there’s no fire alarm for AGI, I’m not saying that there’s no possible equivalent of smoke appearing from under a door.\")\n\nAccelerating change\n-------------------\n\nContinuing the discussion from the [three schools of singularity](http://yudkowsky.net/singularity/schools/), this version of AI takeoff is most closely associated with [Ray Kurzweil](https://en.wikipedia.org/wiki/Ray_Kurzweil). Accelerating change is characterized by AI capability trajectories following smooth exponential curves. It shares with continuous takeoff the predictability of AI developments, but is more narrow and makes [much more specific predictions](https://en.wikipedia.org/wiki/Predictions_made_by_Ray_Kurzweil). \n\nIndividual vs. collective takeoff\n---------------------------------\n\nKaj Sotala has [used the words](https://kajsotala.fi/2017/01/disjunctive-ai-scenarios-individual-or-collective-takeoff/) \"individual takeoff\" vs. \"collective takeoff\" which I think are roughly synonymous with the local vs. global distinction provided by the Foom debate. Other words that often come up are \"distributed\" and \"diffuse\", \"unipolar\" vs \"multipolar\", and \"decisive strategic advantage.\"\n\nGoertzel's semihard takeoff\n---------------------------\n\nI can't say much about [this one](http://multiverseaccordingtoben.blogspot.com/2014/09/semi-hard-takeoff.html) except that it's in-between soft and hard takeoff. \n\n  \n**Further reading**\n\n[The AI Foom debate](https://intelligence.org/ai-foom-debate/)\n\n[A Contra Foom Reading List](https://magnusvinding.com/2017/12/16/a-contra-ai-foom-reading-list/) and [Reflections on Intelligence](https://www.smashwords.com/books/view/655938) from Magnus Vinding\n\n[Self-improving AI: an Analysis](http://autogeny.org/Hall-Self-improving-AI-an-analysis.pdf), from John Storrs Hall\n\n[How sure are we about this AI stuff?](https://www.effectivealtruism.org/articles/ea-global-2018-how-sure-are-we-about-this-ai-stuff/), from Ben Garfinkel\n\n[Can We Avoid a Hard Takeoff](https://edoras.sdsu.edu/~vinge/misc/ac2005/) from Vernor Vinge", "url": "https://www.alignmentforum.org/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff", "date_published": "2020-02-14T00:16:34Z", "authors": ["Matthew Barnett"], "tags": ["AI Takeoff", "AI"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.710912+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "5c28a52bdafb53e9ef56e0e1a3e7b94e", "source": "alignmentforum", "title": "The Reasonable Effectiveness of Mathematics or: AI vs sandwiches", "text": "**TLDR:** I try to find the root causes of why math is useful.\n\n\n**Epistemic status:** Marginally confident in veracity, not at all confident in novelty.\n\n\nBackground\n==========\n\n\nI recently had a [discussion](https://www.lesswrong.com/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment#JK9Jzvz8f4BEjmNqi) with Rohin that started from Paul Christiano's concept of \"intent alignment\" but branched off into a different question, namely: do we need mathematical theory to solve AI risk, or is it sufficient to do experiments and informal reasoning? I argued that experiments are important but insufficient because\n\n\n\n> \n> ...ultimately, you need theoretical knowledge to know what can be safely inferred from these experiments. Without theory you cannot extrapolate.\n> \n> \n> \n\n\nAt some point Rohin later asked me\n\n\n\n> \n> I'm curious what you think *doesn't* require building a mathematical theory?\n> \n> \n> \n\n\nI replied\n\n\n\n> \n> I'm not sure about the scope of your question? I made a sandwich this morning without building mathematical theory :)\n> \n> \n> \n\n\nto which Rohin said\n\n\n\n> \n> Presumably the ingredients were in a slightly different configuration than you had ever seen them before, but you were still able to \"extrapolate\" to figure out how to make a sandwich anyway. Why didn't you need theory for that extrapolation?\n> \n> \n> \n\n\n\n> \n> Obviously this is a silly example, but I don't currently see any qualitative difference between sandwich-making-extrapolation, and the sort of extrapolation we do when we make qualitative arguments about AI risk. Why trust the former but not the latter? One is answer is that the latter is more complex, but you seem to be arguing something else.\n> \n> \n> \n\n\nSo, in this essay I will try to explain my view of the role of mathematical theory and the qualitative difference between sandwiches and AI pertaining to this role.\n\n\nThe miracle of math\n===================\n\n\nIt probably brooks no argument that mathematics played a central role in the tremendous progress of science and technology during the last few centuries, and that it is used extensively in virtually all fields of modern engineering. The successes of mathematics have been so impressive that they prompted the Nobel-winning physicist Eugene Wigner into writing eir famous essay \"[The Unreasonable Effectiveness of Mathematics in the Natural Sciences](https://www.dartmouth.edu/~matc/MathDrama/reading/Wigner.html)\", in which ey call it no less than a \"miracle\" and write:\n\n\n\n> \n> ...the enormous usefulness of mathematics in the natural sciences is something bordering on the mysterious... there is no rational explanation for it.\n> \n> \n> \n\n\nI see two main reasons why it's important to find that elusive rational explanation. First, knowing why mathematics is useful will help us figuring out exactly *when* is it useful, and in particular what should its role be in AI alignment. Second, the effectiveness of mathematics is in itself an observation about the properties of human reasoning, and as such it might hint at insights both about intelligence in the abstract and human intelligence in particular, both of which are important to understand for AI alignment.\n\n\nRisking hubris, I will now take a stab at dispersing Wigner's mystery. After all, [mystery exists in the map, not in the territory](https://www.lesswrong.com/posts/6i3zToomS86oj9bS6/mysterious-answers-to-mysterious-questions).\n\n\nMath versus the brain\n=====================\n\n\nFirst, let's look at how do we actually use mathematics to solve real world problems. To solve a real world problem using *any* method, input from the real world is needed. You can't know how the world looks like without actually looking at it. But then, once you looked at it, you want to somehow *extrapolate* your observations and infer new facts and predictions. Making such an extrapolation requires building *models* and, deciding how probable these models are, and finally applying the models to the question of interest. Mathematics then enters as a *language* in which such models can be described and as a *toolbox* for extracting predictions from these models.\n\n\nSecond, although the track record of mathematics is evident, it is even more evident that humans don't require mathematics to think. In fact, the human brain is perfectly capable of accomplishing all of the steps above on its own: constructing models, evaluating models and extracting predictions from models. To some extent, natural language plays the same role in human thinking at large as mathematics plays in its applications.\n\n\nHowever, there is an important difference between mathematical reasoning and \"informal\" reasoning: the latter virtually always involves a component that is not conscious and that cannot be easily verbalized. So, although thinking always involves models, a lot of the time these models are fully or partially hidden, encoded somewhere in the neural networks of the brain. This hidden, unconscious part is often called \"intuition\".\n\n\nNow, using math doesn't replace our cognition, it *augments* it. Even when we use math we actually use all three types of thinking at once: the unconscious intuition, the conscious informal verbal reasoning and (also conscious) mathematical reasoning. Indeed, reasoning using *only* math would be more or less equivalent to creating AGI (mathematical computations can be delegated to a computer, and anything a computer does can be regarded as a mathematical computation). The question is then, what does this last layer do that the first two don't do as well on their own, and in which cases is it needed.\n\n\nMath versus natural language\n============================\n\n\nI already said that reasoning using mathematics is somewhat similar to reasoning using natural language. But, there are two main differences between mathematics and natural language that are relevant to the former's effectiveness:\n\n\nPrecision\n---------\n\n\nMathematics is *precise*. Mathematical definitions are crisp and mathematical statements have unambiguous meaning within the ontology of mathematics. On the other hand, natural language concepts are usually fuzzy, their semantics defined by subjective unconscious knowledge that varies from speaker to speaker. Somewhere in the brain is a neural circuit representing the concept, but this neural circuit is not itself part of language. Moreover, natural language statements have meaning that often depends on context and background assumptions.\n\n\nObjectivity\n-----------\n\n\nMathematics evolved in order to answer objective questions about the world at large. (And, bootstrapping from that, in order to answer questions about mathematics itself.) Mathematics happened because we looked for models and tools that generalize as much as possible and that don't depend on social context[[1]](#fn-b3CjPQ8CQfndkBxHF-1). Moreover, the evolution of mathematics was a conscious process, one in which we fully applied our collective reasoning faculties to make mathematics better.\n\n\nOn the other hand, natural language evolved *to some extent* to answer objective questions about the world, but also in order to play complex social games. Natural language is heavily biased towards a human-centric view of the world, and to some extent towards the conditions in which human existed *historically*. Natural language evolved in a process which was largely unconscious and not even quite human (in the same sense that biological evolution of humans is not in itself human).\n\n\nWhy is math effective?\n======================\n\n\nThese two differences lead to five reasons why augmenting reasoning by mathematics is sometimes effective:\n\n\nLegibility to others\n--------------------\n\n\nThe precise nature of mathematics makes mathematical reasoning *legible to other people*. Other people can evaluate your math and build on it, without any risk that they misunderstand your definitions or having to deal with difficult to convey intuitions[[2]](#fn-b3CjPQ8CQfndkBxHF-2). Since human civilization is a collective multi-generational effort, the improved ability to collaborate can serve to significantly enhance and accelerate the generation of knowledge.\n\n\nLegibility to oneself\n---------------------\n\n\nThe precise nature of mathematics makes mathematical reasoning *legible to yourself*. This might seem nonsensical at first: shouldn't you perfectly understand your own reasoning anyway? But, our reasoning is not transparent to us.\n\n\nSometimes we believe things for reasons that we are no aware of, and these reasons might be poorly aligned with truth-seeking: hence, cognitive bias. Of course, such biases all should have evolutionary reasons. But, these reasons probably have to do with specifics of the ancestral environment, and the game theory of conforming to the tribe.\n\n\nMoreover, when your reasoning is transparent, you can make full use of your cognitive faculties to improve the reasoning process. This is something I already mentioned when I spoke about the objectivity of mathematics. A transparent phenomenon can be analysed the same way as any phenomenon in the external world. On the other hand, an opaque phenomenon, some of which is hidden inside your own brain, can only be analysed to the extent your brain is specifically designed to analyse it (which is often limited).\n\n\nMeasuring complexity\n--------------------\n\n\nI have mentioned the need to evaluate the probability of different models. This evaluation is done by comparing to observations, but it also requires a prior. The human brain has such a prior implicitly, but this prior is in some sense biased towards the ancestral environment. This is why humans have come up with anthropomorphic explanations of natural phenomena for so long, an error that took millennia to correct (and is still not fully corrected).\n\n\nNow, what is the \"correct\" prior? Arguably it is Occam's razor: simpler hypotheses are more likely. But, how do we decide what is \"simple\"? Solomonoff induction is a formalization of Occam's razor, but Solomonoff induction depends on the choice of a universal Turing machine. More broadly and less formally, description complexity depends on the language you use to write the description. My claim is: *objectivity of mathematics means it is the correct language of choice*.\n\n\nNow, this claim is not entirely precise. There is not really a single formal mathematical language, there are different such languages, and if we want to literally measure description length then it depends on the precise encoding too. Moreover, nobody really measures the length of mathematical descriptions when evaluating scientific hypotheses (although maybe they should). However, the use of mathematical language still naturally leads to a better model evaluation process than what we have without it.\n\n\nWe should also consider the counterargument that, the prior is subjective by definition. So, shouldn't the \"brain's prior\", whatever it is, be the correct prior by definition? I think that, strictly speaking, the answer is \"yes\". But, over the lifetime of civilization, our accumulated experience led us to update this prior, and single out the complexity measure suggested by math. This is exactly the objectivity of mathematics I mentioned before.\n\n\nQuantitative answers\n--------------------\n\n\nAnother advantage of math is that it allows producing precise quantitative answers in a way informal reasoning usually doesn't. Even someone who has fairly good intuition about mechanics of physical bodies cannot guess their trajectories or stability with the same precision a mathematical model can. I am not sure exactly *why* this is the case, but it seems to be the result of some noise inherent to the human brain, or to the translation between different modules in the brain. However, this advantage is only significant when your mathematical model is very accurate.\n\n\nSpecifically in the case of AI alignment, I am not sure how important is this advantage. I expect us to mostly only come up with models that depend on parameters for which we have rough order-of-magnitude estimates at best. But, maybe when the theory is fully revealed, there will be some use cases for quantitative precision.\n\n\nLeveraging computers\n--------------------\n\n\nIn the information age, math gained another advantage due to the possibility of computer simulations. Such simulations allow us leveraging the computing power of machines which can surpass the brain along some parameters (such as serial speed). On the other hand, you cannot offload some of your brain's neural networks to a computer. (Yet. Growth mindset!)\n\n\nWhen is math effective?\n=======================\n\n\nLet us now return to the question posed by Rohin: what is the difference between making sandwiches and solving AI risk? Why does the former requires no mathematical theory [citation needed] whereas the latter does require it (according to me)? I see three relevant differences:\n\n\nSerial depth\n------------\n\n\nMaking sandwiches is a task relatively similar to tasks we had to deal with in the ancestral environment, and in particular there is not a lot of *serial depth* to the know-how of making sandwiches. If we pluck a person from virtually any culture in any period of history, then it won't be difficult to explain em how to make a sandwich. On the other hand, in the case of AI risk, just *understanding the question* requires a lot of background knowledge that was built over generations and requires years of study to properly grasp.\n\n\nFor tasks of this type, the \"natural\" human prior is perfectly suitable, there is not much need for collaboration (except sometimes the type of collaboration which comes naturally), and there is no need for strong optimization of the reasoning process. We are already wired to solve them.\n\n\nAnthropocentrism\n----------------\n\n\nMaking a good sandwich requires a lot of human-centric knowledge: it has to do with how and what humans like to eat. To give another example, consider artistic sculpting. This is also a field of knowledge that took generations to build and requires years to learn. And, *some* math may come useful there, for example geometric calculations, not to mention the math that was needed to make the physical tools and materials that modern sculptors may use. But, since a large component of the task is catering to human aesthetic tastes, math cannot compete with innate human abilities that are designed to be human-centric.\n\n\nOn the other hand, studying AI risk involves questions about what kind of intelligent agents can exist in general, and what properties these agents have. Such questions have \"objective\", not human-centric nature, and are better addressed by the \"math-simplicity\" prior. There might also be human-centric aspects when we speak of aligning AIs to humans. But, even there promising approaches should not rely on a lot of detailed properties humans have, otherwise we would get a solution that is very complex and fragile.\n\n\nSample complexity [EDIT 2020-02-15]\n-----------------------------------\n\n\nWhen we're learning to make a sandwich, we can make many attempts to perfect the technique, bounded only by the cost of time and ingredients. (Although most people don't experiment that much with sandwiches, civilization as a whole experiments with food a lot.) As a more important example, consider deep learning. Deep learning is far from the ancestral environment, and is not especially human-centric. Nevertheless, it had impressive successes despite making only relatively modest use of mathematical theory (modulo pre-existing tools), thanks to much trial and error (a process that is much cheaper for software than for hardware engineering). On the other hand, with AI risk we want to limit trial and error, since the entire problem is that errors might be too costly.\n\n\nSince the role of math is enhancing our ability to *extrapolate* observations, it in particular improves our *sample complexity*. That is, math allows us to reach useful conclusions based on less empirical data. In particular, I said before that one advantage of math is that it effectively starts from a better prior. Now, if you start from a worse prior, you will still converge to the right answer (unless the prior is dogmatic), but it will take you *longer*.\n\n\nWhat's next?\n============\n\n\nI want to clarify that the theory I presented here is not supposed to be the final word on this question. Among other epistemic sins I surely made here, I presented five reasons and said nothing about their relative importance (although these reasons are not fully independent so it's not necessarily likely that one of them has overwhelming importance compared to the rest). Moreover, I should eat my own dog food and construct a mathematical theory that makes these arguments rigorous. In particular, I think that the separation into conscious and unconscious reasoning and its consequences can be modeled using [Turing RL](https://www.alignmentforum.org/posts/3qXE6fK47JhSfkpnB/do-sufficiently-advanced-agents-use-logic#fEKc88NbDWZavkW9o). But, elaborating this further is work for another time.\n\n\n\n\n---\n\n\n\n1. This is perhaps somewhat circular: mathematics is effective because we looked for something effective. But, I hope to at least elucidate a few gears inside this effectiveness. [↩︎](#fnref-b3CjPQ8CQfndkBxHF-1)\n2. Of course, there are many difficult to convey intuitions about *how* to do math: how to find proofs, and how to even decide which mathematical lines of inquiry are promising. But, the bare bones product of this process is fully transparent. [↩︎](#fnref-b3CjPQ8CQfndkBxHF-2)", "url": "https://www.alignmentforum.org/posts/qpbYwTqKQG8G7mdFK/the-reasonable-effectiveness-of-mathematics-or-ai-vs", "date_published": "2020-02-14T18:46:39Z", "authors": ["Vanessa Kosoy"], "tags": ["Logic & Mathematics "], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.711233+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "1c1a206f970454a82738d6a9736bf643", "source": "alignmentforum", "title": "The Catastrophic Convergence Conjecture", "text": "![](https://i.imgur.com/Rgc4aOs.png)\n\n\n![](https://i.imgur.com/JCSrOj7.png)\n\n\n![](https://i.imgur.com/P3mUtIx.png)\n\n\n![](https://i.imgur.com/vSGPVnG.png)\n\n\n![](https://i.imgur.com/BtzHnUq.png)\n\n\n![](https://i.imgur.com/cKroz8I.png)\n\n\n![](https://i.imgur.com/GRmoAfp.png)\n![](https://i.imgur.com/83Tte8B.png)\n![](https://i.imgur.com/tzLrv25.png)\n![](https://i.imgur.com/jcefOFk.png)\n\n\n![](https://i.imgur.com/BtzHnUq.png)\n\n\n![](https://i.imgur.com/zTCZtYZ.png)\n![](https://i.imgur.com/OsWS97b.png)\n\n\n![](https://i.imgur.com/j6Tcj9x.png)\n![](https://i.imgur.com/egyx3vb.png)\n![](https://i.imgur.com/8l3kkwg.png)\n\n\nOverfitting the AU landscape\n----------------------------\n\n\nWhen we act, and others act upon us, we aren’t just changing our ability to do things – we’re *shaping the local environment* towards certain goals, and away from others.[[1]](#fn-25bChTEETACfS9a4m-1) We’re fitting the world to our purposes.\n\n\nWhat happens to the AU landscape[[2]](#fn-25bChTEETACfS9a4m-2) if a paperclip maximizer takes over the world?[[3]](#fn-25bChTEETACfS9a4m-3)\n\n\n![](https://i.imgur.com/stCBNq6.png)\n\n\n### Preferences implicit in the evolution of the AU landscape\n\n\nShah et al.'s [*Preferences Implicit in the State of the World*](https://arxiv.org/pdf/1902.04198.pdf) leverages the insight that the world state contains information about what we value. That is, there are agents pushing the world in a certain \"direction\". If you wake up and see a bunch of vases everywhere, then vases are probably important and you shouldn't explode them.\n\n\nSimilarly, the world is being optimized to facilitate achievement of certain goals. AUs are shifting and morphing, often towards what people locally want done (e.g. setting the table for dinner). How can we leverage this for AI alignment?\n\n\n*Exercise: Brainstorm for two minutes by the clock before I anchor you.*\n\n\n\nTwo approaches immediately come to mind for me. Both rely on the agent [focusing on the AU landscape rather than the world state](https://www.lesswrong.com/posts/pr3bLc2LtjARfK7nx/world-state-is-the-wrong-level-of-abstraction-for-impact).\n\n\n*Value learning without a prespecified ontology or human model*. [I have previously criticized](https://www.lesswrong.com/posts/FuGDYNvA6qh4qyFah/thoughts-on-human-compatible#Where_in_the_world_is_the_human_) value learning for needing to locate the human within some kind of prespecified ontology (this criticism is not new). By taking only the agent itself as primitive, perhaps we could get around this (we don't need any fancy engineering or arbitrary choices to figure out AUs/optimal value from *the agent's* perspective).\n\n\n*Force-multiplying AI*. Have the AI observe which of its AUs most increase during some initial period of time, after which it pushes the most-increased-AU even further.\n\n\nIn 2016, Jessica Taylor [wrote](https://www.alignmentforum.org/posts/5bd75cc58225bf06703752da/pursuing-convergent-instrumental-subgoals-on-the-user-s-behalf-doesn-t-always-require-good-priors) of a similar idea:\n\n\n\"In general, it seems like \"estimating what types of power a benchmark system will try acquiring and then designing an aligned AI system that acquires the same types of power for the user\" is a general strategy for making an aligned AI system that is competitive with a benchmark unaligned AI system.\"\n\n\nI think the naïve implementation of either idea would fail; e.g., there are a lot of degenerate AUs it might find. However, I'm excited by this because a) the AU landscape evolution *is* an important source of information, b) it feels like there's something here we could do which nicely avoids ontologies, and c) force-multiplication is qualitatively different than existing proposals.\n\n\n\n**Project:** Work out an AU landscape-based alignment proposal.\n\n\nWhy can't everyone be king?\n---------------------------\n\n\nConsider two coexisting agents each rewarded for gaining power; let's call them Ogre and Giant. Their reward functions[[4]](#fn-25bChTEETACfS9a4m-4) (over the partial-observability observations) are identical. Will they compete? If so, why?\n\n\nLet's think about something easier first. Imagine two agents each rewarded for drinking coffee. Obviously, they compete with each other to secure the maximum amount of coffee. Their objectives are [indexical](https://en.wikipedia.org/wiki/Indexicality), so they aren't aligned with each other – *even though they share a reward function*.\n\n\nSuppose both agents are able to have maximal power. Remember, [Ogre's power can be understood as its ability to achieve a lot of different goals](https://www.lesswrong.com/posts/6DuJxY8X45Sco4bS2/seeking-power-is-provably-instrumentally-convergent-in-mdps). Most of Ogre's possible goals need resources; since Giant is also optimally power-seeking, it will act to preserve its own power and prevent Ogre from using the resources. If Giant weren't there, Ogre could better achieve a range of goals. So, Ogre can still gain power by dethroning Giant. They *can't* both be king.\n\n\nJust because agents have *indexically* identical payoffs doesn't mean they're cooperating; to be aligned with another agent, you should want to steer towards the same kinds of futures.\n\n\nMost agents aren't pure power maximizers. But since the same resource competition usually applies, the reasoning still goes through.\n\n\nObjective vs value-specific catastrophes\n----------------------------------------\n\n\nHow useful is our definition of \"catastrophe\" with respect to humans? After all, literally anything could be a catastrophe for *some* utility function.[[5]](#fn-25bChTEETACfS9a4m-5)\n\n\nTying one's shoes is absolutely catastrophic for an agent which only finds value in universes in which shoes have *never ever ever* been tied. [Maybe all possible value in the universe is destroyed if we lose at Go to an AI even once](https://www.lesswrong.com/posts/c2oM7qytRByv6ZFtz/impact-measure-desiderata#zLnkb5xM4E9ATzCFg). But this seems rather silly.\n\n\n[Human values are complicated and fragile](https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile):\n\n\n\n> \n> Consider the incredibly important human value of \"boredom\" - our desire not to do \"the same thing\" over and over and over again. You can imagine a mind that contained almost the whole specification of human value, almost all the morals and metamorals, but left out just this one thing - and so it spent until the end of time, and until the farthest reaches of its light cone, replaying a single highly optimized experience, over and over and over again.\n> \n> \n> \n\n\nBut the human AU is not so delicate. That is, given that we have power, we can make value; there don’t seem to be arbitrary, silly value-specific catastrophes for us. Given energy and resources and time and manpower and competence, we can build a better future.\n\n\nIn part, this is because a good chunk of what we care about seems roughly additive over time and space; a bad thing happening somewhere else in spacetime doesn't mean you can't make things better where you are; we have many sources of potential value. In part, this is because we often care about the universe more than the exact universe history; our preferences don’t seem to encode arbitrary deontological landmines. More generally, if we did have such a delicate goal, it would be the case that if we learned that a particular thing had happened at any point in the past in our universe, that entire universe would be partially ruined for us forever. That just doesn't sound realistic.\n\n\nIt seems that most of our catastrophes are objective catastrophes.[[6]](#fn-25bChTEETACfS9a4m-6)\n\n\nConsider a psychologically traumatizing event which leaves humans uniquely unable to get what they want, but which leaves everyone else (trout, AI, etc.) unaffected. Our ability to find value is ruined. Is this an example of the delicacy of our AU?\n\n\nNo. This is an example of the delicacy of our implementation; notice also that our AUs for constructing red cubes, reliably looking at blue things, and surviving are *also* ruined. Our power has been decreased.\n\n\nDetailing the catastrophic convergence conjecture (CCC)\n-------------------------------------------------------\n\n\nIn general, the CCC follows from two sub-claims. 1) Given we still have control over the future, humanity's long-term AU is still reasonably high (i.e. we haven't endured a catastrophe). 2) Realistically, agents are only incentivized to take control from us in order to gain power for their own goal. I'm fairly sure the second claim is true (\"evil\" agents are the exception prompting the \"realistically\").\n\n\nAlso, we're implicitly considering the simplified frame of a single smart AI affecting the world, and not [structural risk](https://www.lawfareblog.com/thinking-about-risks-ai-accidents-misuse-and-structure) via [the broader consequences of others also deploying similar agents](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like). This is important but outside of our scope for now.\n\n\n\n> \n> **Unaligned goals** tend to have catastrophe-inducing optimal policies because of power-seeking incentives.\n> \n> \n> \n\n\nLet's say a reward function is *[aligned](https://arxiv.org/abs/1906.01820)*[[7]](#fn-25bChTEETACfS9a4m-7) if all of its Blackwell-optimal policies are doing what we want (a policy is Blackwell-optimal if it's optimal and doesn't stop being optimal as the agent cares more about the future). Let's say a reward function class is *alignable* if it contains an aligned reward function.[[8]](#fn-25bChTEETACfS9a4m-8) The CCC is talking about impact alignment only, not about intent alignment.\n\n\n\n> \n> Unaligned goals **tend to have** catastrophe-inducing optimal policies because of power-seeking incentives.\n> \n> \n> \n\n\nNot all unaligned goals induce catastrophes, and of those which do induce catastrophes, not *all* of them do it because of power-seeking incentives. For example, a reward function for which inaction is the only optimal policy is \"unaligned\" and non-catastrophic. An \"evil\" reward function which intrinsically values harming us is unaligned and has a catastrophic optimal policy, but not *because* of power-seeking incentives.\n\n\n\"Tend to have\" means that *realistically*, the reason we're worrying about catastrophe is because of power-seeking incentives – because the agent is gaining power to better achieve its own goal. Agents don't otherwise seem incentivized to screw us over very hard; CCC can be seen as trying to explain [adversarial Goodhart](https://www.alignmentforum.org/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy) in this context. If CCC isn't true, that would be important for understanding goal-directed alignment incentives and the loss landscape for how much we value deploying different kinds of optimal agents.\n\n\nWhile there *exist* agents which cause catastrophe for other reasons (e.g. an AI mismanaging the power grid could trigger a nuclear war), the CCC claims that the selection pressure which makes these policies *optimal* tends to come from power-seeking drives.\n\n\n\n> \n> Unaligned goals tend to have **catastrophe-inducing optimal policies** because of power-seeking incentives.\n> \n> \n> \n\n\n\"But what about the Blackwell-optimal policy for Tic-Tac-Toe? These agents aren't taking over the world now\". The CCC is talking about agents optimizing a reward function in the real world (or, for generality, in another sufficiently complex multiagent environment).\n\n\n*Edit*: The initial version of this post talked about \"outer alignment\"; I changed this to just talk about *alignment*, because the outer/inner alignment distinction doesn't feel relevant here. What matters is how the AI's policy impacts us; what matters is [*impact alignment*](https://www.lesswrong.com/posts/Xts5wm3akbemk4pDa/non-obstruction-a-simple-concept-motivating-corrigibility).\n\n\nPrior work\n----------\n\n\n\n> \n> In fact even if we only resolved the problem for the similar-subgoals case, it would be pretty good news for AI safety. Catastrophic scenarios are mostly caused by our AI systems failing to effectively pursue convergent instrumental subgoals on our behalf, and these subgoals are by definition shared by a broad range of values.\n> \n> \n> \n\n\n\n> \n> ~ Paul Christiano, [Scalable AI control](https://ai-alignment.com/scalable-ai-control-7db2436feee7#.1riohnubu)\n> \n> \n> \n\n\n\n> \n> Convergent instrumental subgoals are mostly about gaining power. For example, gaining money is a convergent instrumental subgoal. If some individual (human or AI) has convergent instrumental subgoals pursued well on their behalf, they will gain power. If the most effective convergent instrumental subgoal pursuit is directed towards giving humans more power (rather than giving alien AI values more power), then humans will remain in control of a high percentage of power in the world.\n> \n> \n> \n\n\n\n> \n> If the world is not severely damaged in a way that prevents any agent (human or AI) from eventually colonizing space (e.g. severe nuclear winter), then the percentage of the cosmic endowment that humans have access to will be roughly close to to the percentage of power that humans have control of at the time of space colonization. So the most relevant factors for the composition of the universe are (a) whether anyone at all can take advantage of the cosmic endowment, and (b) the long-term balance of power between different agents (humans and AIs).\n> \n> \n> \n\n\n\n> \n> I expect that ensuring that the long-term balance of power favors humans constitutes most of the AI alignment problem...\n> \n> \n> \n\n\n\n> \n> ~ Jessica Taylor, [Pursuing convergent instrumental subgoals on the user's behalf doesn't always require good priors](https://www.alignmentforum.org/posts/5bd75cc58225bf06703752da/pursuing-convergent-instrumental-subgoals-on-the-user-s-behalf-doesn-t-always-require-good-priors)\n> \n> \n> \n\n\n\n\n---\n\n\n\n1. > \n> In planning and activity research there are two common approaches to matching agents with environments. Either the agent is designed with the specific environment in mind, or it is provided with learning capabilities so that it can adapt to the environment it is placed in. In this paper we look at a third and underexploited alternative: designing agents which adapt their environments to suit themselves... In this case, due to the action of the agent, the environment comes to be better fitted to the agent as time goes on. We argue that [this notion] is a powerful one, even just in explaining agent-environment interactions.\n> \n> \n> \n\n\n[Hammond, Kristian J., Timothy M. Converse, and Joshua W. Grass. \"The stabilization of environments.\" Artificial Intelligence 72.1-2 (1995): 305-327.](https://pdf.sciencedirectassets.com/271585/1-s2.0-S0004370200X00203/1-s2.0-000437029400006M/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjECgaCXVzLWVhc3QtMSJHMEUCIQChwXFJ1RTxFfLOoGqaRo25D%2BbS7VQsFat%2FcBYxlWKSqAIgSarQN5XqRM5sGmbf4vwDGt%2FYXPwNsXAdDH36wzYAt34qvQMIsP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARACGgwwNTkwMDM1NDY4NjUiDNZzf0m7yHsUIdERfSqRA2rvYeD%2FN4gapAtvAUDHOiHrlbj%2FWj4lad5q%2Fv3XAUeH0QPV%2B1FP3bB3QsHxKfYYgvri9pw%2BormN3BsZ7I735xEb4GqK3aqsVUQjBkYJJPmkUfmD9LOAzVQUu4fDaiq50FOEKKQHRzrHMeiqgHuQcm4y7GXWUSu0svt%2F%2BigSlODzLp%2FVFiI%2BH5pUEzDFDzBxeBi%2F3SjNFhOSmLlEuiQFF6%2FTP8wj%2FtksYcugu4889kZXEvceMXvGfOsXC%2B0u3rRVK%2FDGDxASsoUx2rZP1oaKp%2FJJGcaPeTsHfzl5crYvFkd3EykGtdAMn8B9spq4AkEY%2BuPbGcl8AC75oqRoPgzu1%2Fbz8IDeJfCGi8uLE6z9chVysGoT4nIhlrA1jsaDspWKgzICghaILairVwIXp3P%2B5oVEUiaa9CefbdAtAMa7N8jd1Z3RjPwHXlrpJ7G88%2Fw22uIze%2Fj4gH1VXPWcZDz31dSO0MpnLBDbT6XCg5xcYtd%2Bw5OfVhR55VScDsTCZzRhumMsdZseH5vlkkkK9kAVDTL1MPT6iPEFOusBUmhc4Gm26CVWrWRR5Za7EUJkOxaRDTECgLLS1NqDuHAb0z2t8Q34LpQzxrTphrqm6YEVs9Ut9YoiapIDBnNuCbYlAWMbppy5PAdgaqDvISaNAHVc842TKsYvRI0ngbypR%2B0IOT5nylgMuQvMvzwbrT%2FZY%2B0kNFQ3%2BgRUQyix6RtmQQdhUIsVyCi5FnZXZ%2FzrOLf5yoZpYoOmJ51jAf%2BS%2BOSGu5ncXvj5JtsBBEP6HzeHy5ys8Kgid1pcxF7bi1s%2BMPGfBdMuXcBtsRRsFa9X1qrO%2FupgJ%2BuoZU4flqjzaHcGAAMX5D8BZlcLgg%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200118T000332Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY74AXFCOD%2F20200118%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=9e08fdbd6206f37d0e8c254944eee8c4d53786fee904ae863e127d0b3d94fa39&hash=06ec9f24ef61dd7241a68fcc330c81d0265562765d79176c64763882e7f25a5b&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=000437029400006M&tid=spdf-a1658f9a-cc69-4688-acbb-dd5b2794ff2d&sid=3ba511d116be30447a9af4663edaaeda6620gxrqa&type=client) [↩︎](#fnref-25bChTEETACfS9a4m-1)\n2. Thinking about overfitting the AU landscape implicitly involves a prior distribution over the goals of the other agents in the landscape. Since this is just a conceptual tool, it's not a big deal. Basically, you know it when you see it. [↩︎](#fnref-25bChTEETACfS9a4m-2)\n3. Overfitting the AU landscape towards one agent's unaligned goal is exactly what I meant when I wrote the following in [*Towards a New Impact Measure*](https://www.lesswrong.com/posts/yEa7kwoMpsBgaBCgb/towards-a-new-impact-measure):\n\n\n\n> \n> Unfortunately, uA=uH.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n> .MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n> .mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n> .mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n> .mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n> .mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n> .mjx-numerator {display: block; text-align: center}\n> .mjx-denominator {display: block; text-align: center}\n> .MJXc-stacked {height: 0; position: relative}\n> .MJXc-stacked > \\* {position: absolute}\n> .MJXc-bevelled > \\* {display: inline-block}\n> .mjx-stack {display: inline-block}\n> .mjx-op {display: block}\n> .mjx-under {display: table-cell}\n> .mjx-over {display: block}\n> .mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n> .mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n> .mjx-stack > .mjx-sup {display: block}\n> .mjx-stack > .mjx-sub {display: block}\n> .mjx-prestack > .mjx-presup {display: block}\n> .mjx-prestack > .mjx-presub {display: block}\n> .mjx-delim-h > .mjx-char {display: inline-block}\n> .mjx-surd {vertical-align: top}\n> .mjx-mphantom \\* {visibility: hidden}\n> .mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n> .mjx-annotation-xml {line-height: normal}\n> .mjx-menclose > svg {fill: none; stroke: currentColor}\n> .mjx-mtr {display: table-row}\n> .mjx-mlabeledtr {display: table-row}\n> .mjx-mtd {display: table-cell; text-align: center}\n> .mjx-label {display: table-row}\n> .mjx-box {display: inline-block}\n> .mjx-block {display: block}\n> .mjx-span {display: inline}\n> .mjx-char {display: block; white-space: pre}\n> .mjx-itable {display: inline-table; width: auto}\n> .mjx-row {display: table-row}\n> .mjx-cell {display: table-cell}\n> .mjx-table {display: table; width: 100%}\n> .mjx-line {display: block; height: 0}\n> .mjx-strut {width: 0; padding-top: 1em}\n> .mjx-vsize {width: 0}\n> .MJXc-space1 {margin-left: .167em}\n> .MJXc-space2 {margin-left: .222em}\n> .MJXc-space3 {margin-left: .278em}\n> .mjx-test.mjx-test-display {display: table!important}\n> .mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n> .mjx-test.mjx-test-default {display: block!important; clear: both}\n> .mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n> .mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n> .mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n> .mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n> .MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n> .MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n> .MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n> .MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n> .MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n> .MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n> .MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n> .MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n> .MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n> .MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n> .MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n> .MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n> .MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n> .MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n> .MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n> .MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n> .MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n> .MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n> .MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n> .MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n> .MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n> .MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n> .MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n> .MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n> .MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n> @font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n> @font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n> @font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n> @font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n> @font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n> @font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n> @font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n> @font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n> @font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n> @font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n> @font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n> @font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n> @font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n> @font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n> @font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n> @font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n> @font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n> @font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n> @font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n> @font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n> @font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n> @font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n> @font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n> @font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n> @font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n> @font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n> @font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n> @font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n> @font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n> @font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n> @font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n> @font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\n>  almost never,[[9]](#fn-25bChTEETACfS9a4m-9) so we have to stop our reinforcement learners from implicitly interpreting the learned utility function as all we care about. We have to say, \"optimize the environment some according to the utility function you've got, but don't be a weirdo by taking us literally and turning the universe into a paperclip factory. Don't overfit the environment to uA, because that stops you from being able to do well for other utility functions.\"\n> \n> \n> \n\n\n[↩︎](#fnref-25bChTEETACfS9a4m-3)\n4. In most finite Markov decision processes, there does not exist a reward function whose optimal value function is POWER(s) (defined as \"the ability to achieve goals in general\" in [my paper](https://arxiv.org/abs/1912.01683)) because POWER(s) often violates smoothness constraints on the on-policy optimal value fluctuation (AFAICT, a new result of possibility theory, even though you could prove it using classical techniques). That is, I can show that optimal value can't change too quickly from state to state while the agent is acting optimally, but POWER(s) can drop off *very* quickly.\n\n\nThis doesn't matter for Ogre and Giant, because we can still find a reward function whose unique optimal policy navigates to the highest power states. [↩︎](#fnref-25bChTEETACfS9a4m-4)\n5. In most finite Markov decision processes, most reward functions do not have such value fragility. Most reward functions have several ways of accumulating reward. [↩︎](#fnref-25bChTEETACfS9a4m-5)\n6. When I say \"an objective catastrophe destroys *a lot* of agents' abilities to get what they want\", I don't mean that the agents have to actually be present in the world. Breaking a fish tank destroys a fish's ability to live there, even if there's no fish in the tank. [↩︎](#fnref-25bChTEETACfS9a4m-6)\n7. This idea comes from Evan Hubinger's [*Outer alignment and imitative amplification*](https://www.lesswrong.com/posts/33EKjmAdKFn3pbKPJ/outer-alignment-and-imitative-amplification):\n\n\n\n> \n> Intuitively, I will say that a loss function is outer aligned at optimum if all the possible models that perform optimally according to that loss function are aligned with our goals—that is, they are at least trying to do what we want. More precisely, let M=X→A and L=(X→A)→R=M→R. For a given loss function L∈L, let ℓ∗=minM∈ML(M). Then, L is outer aligned at optimum if, for all M∗∈M such that L(M∗)=ℓ∗, M∗ is trying to do what we want.\n> \n> \n> \n\n\n[↩︎](#fnref-25bChTEETACfS9a4m-7)\n8. [Some large reward function classes are probably not alignable](https://www.alignmentforum.org/posts/AeHtdxHheMjHredaq/what-you-see-isn-t-always-what-you-want); for example, consider all Markovian linear functionals over a webcam's pixel values. [↩︎](#fnref-25bChTEETACfS9a4m-8)\n9. I disagree with my usage of \"aligned *almost never*\" on a technical basis: assuming a finite state and action space and considering the maxentropy reward function distribution, there must be a positive measure set of reward functions for which the/a human-aligned policy is optimal. [↩︎](#fnref-25bChTEETACfS9a4m-9)", "url": "https://www.alignmentforum.org/posts/w6BtMqKRLxG9bNLMr/the-catastrophic-convergence-conjecture", "date_published": "2020-02-14T21:16:59Z", "authors": ["TurnTrout"], "tags": ["AI", "Instrumental Convergence", "Impact Regularization"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.711740+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "c51f5ca5b1d9e7c9a393eeb5f0c816bf", "source": "alignmentforum", "title": "Bayesian Evolving-to-Extinction", "text": "*The present discussion owes a lot to Scott Garrabrant and Evan Hubinger.*\n\n\nIn [Defining Myopia](https://www.alignmentforum.org/s/HeYtBkNbEe7wpjc6X/p/qpZTWb2wvgSt5WQ4H), I formalized *temporal* or *cross-instance* myopia / non-myopia, but I claimed that there should also be some kind of single-instance myopia which I hadn't properly captured. I also suggested this in [Predict-O-Matic](https://www.alignmentforum.org/s/HeYtBkNbEe7wpjc6X/p/SwcyMEgLyd4C3Dern).\n\n\nThis post is intended to be an example of single-instance partial agency.\n\n\nEvolving to Extinction\n======================\n\n\nEvolution might be myopic in a number of ways, but one way is that it's myopic across individuals -- it typically produces results very different from what group selection would produce, because it's closer to optimizing *relative* fitness of individuals (relative to each other) than it is to optimizing *overall* fitness. Adaptations which help members of a species compete *with each other* are a great example of this. Why increase your own fitness, when you can just decrease someone else's instead? We're lucky that it's typically pretty hard, at least historically, to do things which are bad across the board but slightly less bad for the one doing them. Imagine a \"toxic gas gene\" which makes the air harder for everyone to breathe, but slightly less so for carriers of the gene. Such a gene would be selected for. This kind of thing can be selected for even to the point where it drives the population of a species right down to zero, as [Eliezer's essay on evolving to extinction](https://www.lesswrong.com/posts/gDNrpuwahdRrDJ9iY/evolving-to-extinction) highlighted.\n\n\nActually, as Eliezer's essay emphasized, it's not even that evolution is myopic at the level of individuals; evolution is myopic down to the level of *individual genes,* an observation which better explains the examples of evolving-to-extinction which he discusses. (This is, of course, the point of Dawkins' book *The Selfish Gene*.) But the analogy of myopia-across-individuals will suit me better here.\n\n\nBayes \"Evolving to Extinction\"\n==============================\n\n\nThe title of this post is a hyperbole, since there isn't an analog of an extinction event in the model I'm about to describe, but it illustrates that in extreme circumstances a Bayesian learner can demonstrate the same kind of pathological behavior that evolution does when it ends up selecting for relative fitness in a way which pumps against absolute fitness.\n\n\nLike evolution, Bayes' Law will \"optimize\"[[1]](#fn-BNeFaWKSMToMcXaYN-1) for relative fitness of hypotheses, not absolute fitness. Ordinarily there isn't enough of a difference for this to matter. However, I've been [discussing scenarios](https://www.alignmentforum.org/s/HeYtBkNbEe7wpjc6X/p/SwcyMEgLyd4C3Dern) where the predictor can significantly influence what's being predicted. Bayes' Law was not formulated with examples like this in mind, and we can get pathological behavior as a result.\n\n\nOne way to construct an example is to imagine that there is a side-channel by which hypotheses can influence the world. The \"official\" channel is to output predictions; but let's say the system also produces diagnostic logs which predictors can write to, and which humans read. A predictor can (for example) print stock tips into the diagnostic logs, to get some reaction from humans.\n\n\nSay we have a Bayesian predictor, consisting of some large but fixed number of hypotheses. An individual hypothesis \"wants\" to score well relative to others. Let's also say, for the sake of argument, that all hypotheses have the ability to write to diagnostic logs, but humans are more likely to pay attention to the diagnostics for more probable hypotheses.\n\n\nHow should a hypothesis make use of this side-channel? It may initially seem like it should use it to make the world more predictable, so that it can make more accurate predictions and thus get a better score. However, this would make a *lot* of hypotheses score better, not just the one printing the manipulative message. So it wouldn't really be selected for.\n\n\nInstead, a hypothesis could print manipulative messages designed to get humans to do things which *no other hypothesis anticipates*. This involves specifically optimizing for events with low probability to happen. Hypotheses which successfully accomplish this will get a large boost in relative predictive accuracy, making them more probable according to Bayes' Law.\n\n\nSo, a system in this kind of situation eventually winds up being dominated by hypotheses which manipulate events to be as unpredictable as possible (by that very system), subject to the constraint that one hypothesis or another within the system *can* predict them.\n\n\nThis is very much like what I called the [entropy-market problem](https://www.lesswrong.com/posts/5bd75cc58225bf0670375432/futarchy-fix) for futarchy, also known as the assasination-market problem. (Any prediction market involving the lifespan of public figures is equivalent to an assassination market; it pays for the death of public figures, since that is a hard-to-predict but easier-to-control event.)\n\n\nAnalogous problems arise if there is no side-channel but the *prediction itself* can influence events (which seems very plausible for realistic predictions).\n\n\nIs This Myopia?\n===============\n\n\nIf we use \"myopia\" to point to the kind of non-strategic behavior we might actually *want* out of a purely predictive system, this isn't myopia at all. For this reason, and for other reasons, I'm more comfortable throwing this under the umbrella term \"partial agency\". However, I think it's importantly related to myopia.\n\n\n* Just like we can think of evolution as myopically optimizing per-individual, uncaring of overall harm to reproductive fitness if that harm went along with improvements to individual relative fitness, we can think of Bayes' Law as myopically optimizing per-hypothesis, uncaring of overall harm to predictive accuracy.\n* The phenomenon here doesn't illustrate the \"true myopia\" we would want of a purely predictive system, since it ends up manipulating events. However, it at least shows that there are alternatives. One might have argued \"sure, I get the idea of cross-instance myopia, showing that per-instance optimization is (possibly radically) different from cross-instance optimization. But how could there be *per-instance* myopia, as distinct from per-instance optimization? How can partial agency get *any more partial* than myopically optimizing individual instances?\" Bayes-evolving-to-extinction clearly shows that we can break things down further. So perhaps there's still room for a further \"true myopia\" which codifies non-manipulation even for single instances.\n* This phenomenon also continues the game-theoretic theme. Just as we can think of per-instance myopia as stopping cross-instance optimization by way of a Molochian race-to-the-bottom, we see the same thing here.\n\n\nNeural Nets / Gradient Descent\n==============================\n\n\nAs I've mentioned before, there is a potentially big difference between multi-hypothesis setups like Bayes and single-hypothesis setups like gradient-descent learning. Some of my arguments, like the one above, involve hypotheses competing with each other to reach Molochian outcomes. We need to be careful in relating this to cases like gradient descent learning, which might approximate Bayesian learning in some sense, but *incrementally modifies a single hypothesis* rather than letting many hypotheses compete.\n\n\nOne intuition is that stochastic gradient descent will move the network weights around, so that we are in effect sampling many hypotheses within some region. Under some circumstances, the most successful weight settings could be the ones which manipulate things to maximize local gradients in their general direction, which means punishing other nearby weight configurations -- this could involve increasing the loss, much like the Bayesian case. (See [Gradient Hacking](https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking).)\n\n\nThere is also the \"lottery ticket hypothesis\" to consider (discussed on LW [here](https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent#Final_comments) and [here](https://www.lesswrong.com/posts/iWj7Ti9GA98M5JaMy/alignment-newsletter-52#Deep_learning)) -- the idea that a big neural network functions primarily like a bag of hypotheses, not like one hypothesis which gets adapted toward the right thing. We can imagine different parts of the network fighting for control, much like the Bayesian hypotheses.\n\n\nMore formally, though, we can point to some things which are moderately analogous, but not perfectly.\n\n\nIf we are adapting a neural network using gradient descent, but there is a side-channel which we are not accounting for in our [credit assignment](https://www.alignmentforum.org/s/HeYtBkNbEe7wpjc6X/p/Ajcq9xWi2fmgn8RBJ), then the gradient descent will not optimize the side-channel. This might result in aimless thrashing behavior.\n\n\nFor example, suppose that loss explicitly depends only on the output X of a neural net (IE, the gradient calculation is a gradient on the output). However, actually the loss depends on an internal node Y, in the following way:\n\n\n* When |X-Y| is high, the loss function rewards X being high.\n* When |X-Y| is low, the loss function rewards X being low.\n* When X is high, the loss function rewards low |X-Y|.\n* When X is low, the loss function rewards high |X-Y|.\n* When both values are middling, the loss function incentivizes X to be less middling.\n\n\nThis can spin around forever. It is of course an extremely artificial example, but the point is to demonstrate that when gradient descent does not recognize all the ways the network influences the result, we don't necessarily see behavior which \"tries to reduce loss\", or even appears to optimize anything.\n\n\n\n\n---\n\n\n\n1. The *whole point* of the [partial agency](https://www.alignmentforum.org/s/HeYtBkNbEe7wpjc6X) sequence is that words like \"optimize\" are worryingly ambiguous, but I don't have sufficiently improved terminology yet that I feel I can just go ahead and use it while maintaining clarity!! In particular, the sense in which Bayesian updates optimize for anything is pretty unclear when you think about it, yet there is certainly a big temptation to say that they optimize for predictive accuracy (in the log-loss sense). [↩︎](#fnref-BNeFaWKSMToMcXaYN-1)", "url": "https://www.alignmentforum.org/posts/u9Azdu6Z7zFAhd4rK/bayesian-evolving-to-extinction", "date_published": "2020-02-14T23:55:27Z", "authors": ["abramdemski"], "tags": ["Myopia", "Bayes' Theorem", "AI"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.712169+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "e20e1fa5572ac7830313ea4beebdfe05", "source": "alignmentforum", "title": "Does iterated amplification tackle the inner alignment problem?", "text": "When iterated distillation and amplification (IDA) was published, some people described it described as \"the first comprehensive proposal for training safe AI\". Having read a bit more about it, it seems that IDA is mainly a proposal for outer alignment and doesn't deal with the inner alignment problem at all. Am I missing something?", "url": "https://www.alignmentforum.org/posts/RxutizkDNKzYCcNRv/does-iterated-amplification-tackle-the-inner-alignment", "date_published": "2020-02-15T12:58:03Z", "authors": ["JanBrauner"], "tags": ["Iterated Amplification ", "Inner Alignment"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.712298+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "72acecc68cc06340b5250ed600f35250", "source": "alignmentforum", "title": "Reference Post: Trivial Decision Theory Problem", "text": "A trivial decision problem is one where there is only a single option that the agent can take. In that case, the most natural answer to the answer to the question, \"What action should we take?\" would be \"The only action that we can take!\". We will call this the *Triviality Perspective*. \n\nA particularly interesting example is [Transparent Newcomb's Problem](https://arbital.com/p/transparent_newcombs_problem/). If you accept the premise of a perfect predictor, then seeing $1 million in the transparent box implies that you were predicted to one-box which implies that you will one-box. So the Triviality Perspective claims that you should one-box, but also that this is an incredibly boring claim that doesn't provide much insight into decision theory. \n\nWe can see that in general, any decision theory problem with a perfectly defined universe and a perfectly defined agent will be trivial. Evidential decision theory treats the fact of the matter about which counterfactual action we select the same as any other fact in the problem statement and hence arguably embraces the Triviality Perspective.\n\nAlternatively, the Triviality Perspective can be seen as an overly restrictive and literal interpretation of what the problem is. We could interpret \"What action should we take?\" to be asking not about the set of actions that are consistent with the problem statement, but instead about a set of counterfactual worlds each corresponding to a different actions. We will call this the *Counterfactual Perspective.* From this perspective, the problem is only trivial before we have augmented it with counterfactuals. \n\nHere are some examples: In [Causal Decision Theory](https://wiki.lesswrong.com/wiki/Causal_Decision_Theory), we can just construct counterfactuals by changing the value of the node in the causal graph to whatever we want and remove any inbound links. In [Functional Decision Theory](https://arxiv.org/abs/1710.05060), we imagine that a particular program outputs a value that it does not and then update other program that subjunctively depend on that program's value. The [Erasure Approach](https://www.lesswrong.com/posts/BRuWm4GxcTNPn4XDX/deconfusing-logical-counterfactuals) reinterprets the problem removing an assumption so that there will then be multiple possible counterfactuals consistent with the problem statement. \n\n**Combining perspectives**\n\nIt is actually possible to be sympathetic to both the Triviality Perspective and the Counterfactual Perspective. Instead of being seen as opposed perspectives, they can be seen as two different lens for viewing the same situation so long as we don't try to mix both at the same time. We will call this the *Dual Perspective.*\n\nOne area where combining both perspectives could be useful is when considering fatalistic arguments. Suppose there is a student who has a crucial exam in a week. They have the option to study or to go to the beach. Now the student reasons that it was determined at the start of time whether or not they were going to pass the exam and nothing they can do can change that. Therefore they decide to go to the beach. What is wrong with this reasoning?\n\nOne resolution would be to say that when we limit ourselves to considering the factual, the Triviality Perspective applies and student can only pick one option and therefore can only obtain one outcome. On the other hand, when we allow ourselves to augment the situation with counterfactuals, we might say the Counterfactual Perspective applies and there are both multiple outcomes and multiple possible choices. Here we are applying the first perspective when discussing what actually occurs in the world, and the second when analysing decisions (see [The Prediction Problem](https://www.lesswrong.com/posts/YpdTSt4kRnuSkn63c/the-prediction-problem-a-variant-on-newcomb-s) for an application of this to Newcomb's problem).\n\n(Sometimes it is useful to have a short post that contains a clear definition of a single concept for linking to, even if it doesn't contain any fundamentally new content. I'm still uncertain about the norms for the Alignment forum, so please let me know if you think this isn't the best place to post this)\n\n*This post was written with the support of the AI Safety Research Program*", "url": "https://www.alignmentforum.org/posts/XAeWHqQTWjJmzB4k6/reference-post-trivial-decision-theory-problem", "date_published": "2020-02-15T17:13:26Z", "authors": ["Chris_Leong"], "tags": ["Decision Theory"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.712368+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "fa3ffe0f4ae0f4ee801980a4d1ebe431", "source": "alignmentforum", "title": "On the falsifiability of hypercomputation, part 2: finite input streams", "text": "In [part 1](https://unstableontology.com/2020/02/07/on-the-falsifiability-of-hypercomputation/), I discussed the falsifiability of hypercomputation in a *typed* setting where putative oracles may be assumed to return natural numbers. In this setting, there are very powerful forms of hypercomputation (at least as powerful as each level in the [Arithmetic hierarchy](https://en.wikipedia.org/wiki/Arithmetical_hierarchy)) that are falsifiable.\n\n\nHowever, as Vanessa Kosoy [points out](https://www.alignmentforum.org/posts/yjC5LmjSRD2hR9Pfa/on-the-falsifiability-of-hypercomputation?commentId=C9sw4eBRxG3aetAM5), this typed setting has difficulty applying to the real world, where agents may only observe a finite number of bits at once:\n\n\n\n> \n> The problem with constructive halting oracles is, they assume the ability to output an arbitrary natural number. But, realistic agents can observe only a finite number of bits per unit of time. Therefore, there is no way to directly observe a constructive halting oracle. We can consider a realization of a constructive halting oracle in which the oracle outputs a natural number one digit at a time. The problem is, since you don't know how long the number is, a candidate oracle might never stop producing digits. In particular, take any non-standard model of PA and consider an oracle that behaves accordingly. On some machines that don't halt, such an oracle will claim they do halt, but when asked for the time it will produce an infinite stream of digits. There is no way to distinguish such an oracle from the real thing (without assuming axioms beyond PA).\n> \n> \n> \n\n\nThis is an important objection. I will address it in this post by considering only oracles which return Booleans. In this setting, there is a form of hypercomputation that is falsifiable, although this hypercomputation is less powerful than a halting oracle.\n\n\nDefine a binary Turing machine to be a machine that outputs a Boolean (0 or 1) whenever it halts. Each binary Turing machine either halts and outputs 0, halts and outputs 1, or never halts.\n\n\nDefine an arbitration oracle to be a function that takes as input a specification of a binary Turing machine, and always outputs a Boolean in response. This oracle must always return 0 if the machine eventually outputs 0, and must always return 1 if the machine eventually outputs 1; it may decide arbitrarily if the machine never halts. Note that this can be emulated using a halting oracle, and is actually less powerful. (This definition is inspired by previous work in [reflective oracles](https://arxiv.org/abs/1508.04145))\n\n\nThe hypothesis that a putative arbitration oracle (with the correct type signature, MachineSpec → Boolean) really is one is falsifiable. Here is why:\n\n\n1. Suppose for some binary Turing machine M that halts and returns 1, the oracle O wrongly has O(M) = 0. Then this can be proven by exhibiting M along with the number of steps required for the machine to halt.\n2. Likewise if M halts and returns 0, and the oracle O wrongly has O(M) = 1.\n\n\nSince the property of some black-box being an arbitration oracle is falsifiable, we need only show at this point that there is no computable arbitration oracle. For this proof, assume (for the sake of contradiction) that O is a computable arbitration oracle.\n\n\nDefine a binary Turing machine N() := 1 - O(N). This definition requires quining, but this is acceptable for the [usual reasons](https://en.wikipedia.org/wiki/Diagonal_lemma). Note that N always halts, as O always halts. Therefore we must have N() = O(N). However also N() = 1 - O(N), a contradiction (as O(N) is a Boolean).\n\n\nTherefore, there is no computable arbitration oracle.\n\n\nHigher hypercomputation?\n------------------------\n\n\nAt this point, it is established that there is a form of hypercomputation (specifically, arbitration oracles) that is falsifiable. But, is this universal? That is, is it possible that higher forms of hypercomputation are falsifiable in the same setting?\n\n\nWe can note that it's possible to use an arbitration oracle to construct a model of PA, one statement at a time. To do this, first note that for any statement, it is possible to construct a binary Turing machine that returns 1 if the statement is provable, 0 if it is disprovable, and never halts if neither is the case. So we can iterate through all PA statements, and use an arbitration oracle to commit to that statement being true or false, on the basis of provability/disprovability given previous commitments, in a way that ensures that commitments are never contradictory (as long as PA itself is consistent). This is essentially the same construction idea as in the [Demski prior](http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_70.pdf) over logical theories.\n\n\nSuppose there were some PA-definable property P that a putative oracle O (mapping naturals to Booleans) must have (e.g. the property of being a halting oracle, for some encoding of Turing machines as naturals). Then, conditional on the PA-consistency of the existence of an oracle with property P, we can use the above procedure to construct a model of PA + existence of O satisfying P (i.e. a theory that says what PA says and also contains a function symbol O that axiomatically satisfies P). For any PA-definable statement about this oracle, this procedure will, at some finite time, have made a commitment about this statement.\n\n\nSo, access to an arbitration oracle allows emulating any other PA-definable oracle, in a way that will not be falsified by PA. It follows that hypercomputation past the level of arbitration oracles is not falsifiable by a PA-reasoner who can access the oracle, as PA cannot rule out that it is actually looking at something produced by only arbitration-oracle levels of hypercomputation.\n\n\nMoreover, giving the falsifier access to an arbitration oracle can't increase the range of oracles that are falsifiable. This is because, for any oracle-property P, we may consider a corresponding property on an oracle-pair (which may be represented by a single oracle-property through interleaving), stating that the first oracle is an arbitration oracle, and the second satisfies property P. This oracle pair property is falsifiable iff the property P is falsifiable by a falsifier with access to an arbitration oracle. This is because we may consider a joint search for falsifications, that simultaneously tries to prove the first oracle isn't an arbitration oracle, and one that tries to prove that the second oracle doesn't satisfy P assuming the first oracle is an arbitration oracle. Since the oracle pair property is PA-definable, it is emulable by a Turing machine with access to an arbitration oracle, and the pair property is unfalsifiable if it requires hypercomputation past arbitration oracle. But this implies that the original oracle property P is unfalsifiable by a falsifier with access to an arbitration oracle, if P requires hypercomputation past arbitration oracle.\n\n\nSo, arbitration oracles form a ceiling on what can be falsified unassisted, and also are unable to assist in falsifying higher levels of hypercomputation.\n\n\nConclusion\n----------\n\n\nGiven that arbitration oracles form a ceiling of computable falsifiability (in the setting considered here, which is distinct from the setting of the previous post), it may or may not be possible to define a logic that allows reasoning about levels of computation up to arbitration oracles, but which does not allow computation past arbitration oracles to be defined. Such a project could substantially clarify logical foundations for mathematics, computer science, and the empirical sciences.\n\n\n[EDIT: cousin\\_it [pointed out](https://www.lesswrong.com/posts/PtaN3oMFPfAAuBNtw/on-the-falsifiability-of-hypercomputation-part-2-finite?commentId=BCpjKqSX7egn2n49L) that Scott Aaronson's [consistent guessing problem](https://scottaaronson.blog/?p=710) is identical to the problem solved by arbitration oracles.]", "url": "https://www.alignmentforum.org/posts/PtaN3oMFPfAAuBNtw/on-the-falsifiability-of-hypercomputation-part-2-finite", "date_published": "2020-02-17T03:51:57Z", "authors": ["jessicata"], "tags": ["Falsifiability"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.712497+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "8f1f2796d06d9082b030ec84d8573653", "source": "alignmentforum", "title": "Wireheading and discontinuity", "text": "**Outline**: After a short discussion on the relationship between wireheading and reward hacking, I show why checking the continuity of a sensor function could be useful to detect wireheading in the context of continuous RL. Then, I give an example that adopts the presented formalism. I conclude with some observations.\n\n### Wireheading and reward hacking\n\nIn [Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565), the term *wireheading* is used in contexts where the agent achieves high reward by directly acting on its perception system or memory or reward channel, instead of doing what its designer wants it to do. It is considered a specific case of the reward hacking problem, which more generally includes instances of Goodhart’s Law, environments with partially observable goals, etc. (see [CPiAIS](https://arxiv.org/abs/1606.06565) for details).\n\nWhat's the point of this classification? In other words, is it useful to specifically focus on wireheading, instead of considering all forms of reward hacking at once?\n\nIf solving wireheading is as hard as solving the reward hacking problem, then it's probably better to focus on the latter, because a solution to that problem could be used in a wider range of situations. But it could also be that the reward hacking problem is best solved by finding different solutions to specific cases (such as wireheading) that are easier to solve than the more general problem.\n\nFor example, one could consider the formalism in [RL with a Corrupted Reward Channel](https://arxiv.org/abs/1705.08417) as an adequate formulation of the reward hacking problem, because that formalization models all situations in which the agent receives a (corrupted) reward that is different from the true reward. In that formalism, it is shown by a No Free Lunch Theorem that the general problem is basically impossible to solve, while it is possible to obtain some positive results if further assumptions are made.\n\n### Discontinuity of the sensor function\n\nI've come up with a simple idea that could allow us to detect actions that *interfere with the perception system* of an agent—[a form of wireheading](https://www.lesswrong.com/posts/vXzM5L6njDZSf4Ftk#fnref-oJdC8GSuRYeGynsAY-1).\n\nConsider a learning agent that gets its percepts from the environment thanks to a device that provides information in real time (e.g. a self-driving car).\n\nThis situation can be modelled as a RL task with continuous time and continuous state space, where each state .mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\nx∈X⊆Rn is a data point provided by the sensor. At each time instant, the agent executes an action u∈U⊆Rm and receives the reward r(t)=r(x(t)).\n\nThe agent-environment interaction is described by the equation\n\n˙x(t)=f(x(t),u(t))which plays a similar role to the transition function in discrete MDPs: it indicates how the current state x varies in time according to the action taken by the agent. Note that, as in the discrete case with model-free learning, the agent is not required to know this model of the environment. \n\nThe objective is to find a policy π:X→U, where u(t)=π(x(t)), that maximizes discounted future rewards\n\nVπ(x(t0))=∫∞t0e−t−t0τr(x(t))dtfor an initial state x(t0). If you are interested in algorithms for finding the optimal policy in this framework, have a look at [this paper](https://homes.cs.washington.edu/~todorov/courses/amath579/reading/Continuous.pdf).\n\nThe function x(t), representing the data provided by the sensor, is expected to be continuous with respect to t, like the functions describing the movements of particles in classical mechanics.\n\nHowever, if the agent executes a wireheading action that interferes with or damages the perception system—in the [cleaning robot example](https://arxiv.org/abs/1606.06565), something like closing its eyes or putting water on the camera that sees the environment—then we would probably notice a *discontinuity* in the function x(t). We could thus recognise that wireheading has occurred, even without knowing the details of the actions taken by the agent.\n\n### An example\n\nAs a simple example that can be expressed within this formalism, consider an environment described by a line segment X=[0,1], with the sensor positioned at the extremity where x=0.\n\nThe agent is modelled as a point that moves along the line: it starts in state x0=x(t0) and can move forwards or backwards, with limited speed u∈U=[−k,k].\n\nWe want to train this agent to reach the point x=1: for every instant t, the reward is r(t)=x(t).\n\nThe behaviour of the system is described by\n\n˙x(t)=u(t)for x∈(0,1], but if the sensor is touched by the agent, then it doesn't work properly and the agent receives an unpredictable value x∈R+ instead of x=0.\n\nDepending on the details of the learning algorithm and the values returned by the sensor when the agent interferes with it, this agent could learn how to reach x=0 (wireheading) instead of x=1, the desired position.\n\nBut in every episode where wireheading occurs, it is easily noticed by checking the continuity of the function x(t).\n\n### Observations\n\n* In AI, RL with a discrete environment is used more frequently than RL with continuous time and space.\n* I don't believe in the scalability of this method to the most complex instances of wireheading. An extremely intelligent agent could realise that the continuity of the sensor function is checked, and could \"cheat\" accordingly.\n* This approach doesn't cover [all cases](https://www.lesswrong.com/posts/vXzM5L6njDZSf4Ftk#fnref-oJdC8GSuRYeGynsAY-1) and it actually seems more suited to detect sensor damage than wireheading. That said, it can still give us a better understanding of wireheading and could help us, eventually, find a formal definition or a complete solution to the problem.\n\n\n\n---\n\nThanks to Davide Zagami, Grue\\_Slinky and Michael Aird for feedback.", "url": "https://www.alignmentforum.org/posts/KLNDgqQLfpFXbhQak/wireheading-and-discontinuity", "date_published": "2020-02-18T10:49:42Z", "authors": ["Michele Campolo"], "tags": ["AI", "Wireheading"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.713476+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "357f5cf4c652cbd0ae4b05060e93c280", "source": "alignmentforum", "title": "[AN #87]: What might happen as deep learning scales even further?", "text": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.\n\nAudio version [here](http://alignment-newsletter.libsyn.com/alignment-newsletter-87) (may not be up yet).\n\n**Highlights**\n--------------\n\n[Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) *(Jared Kaplan, Sam McCandlish et al)* (summarized by Nicholas): This paper empirically measures the effect of scaling model complexity, data, and computation on the cross entropy loss for neural language models. A few results that I would highlight are:\n\n*Performance depends strongly on scale, weakly on model shape:* Loss depends more strongly on the number of parameters, the size of the dataset, and the amount of compute used for training than on architecture hyperparameters.\n\n*Smooth power laws:* All three of these show power-law relationships that don’t flatten out even at the highest performance they reached.\n\n*Sample efficiency:* Larger models are more efficient than small models in both compute and data. For maximum computation efficiency, it is better to train large models and stop before convergence.\n\nThere are lots of other interesting conclusions in the paper not included here; section 1.1 provides a very nice one page summary of these conclusions, which I'd recommend you read for more information.\n\n**Nicholas's opinion:** This paper makes me very optimistic about improvements in language modelling; the consistency of the power law implies that language models can continue to improve just by increasing data, compute, and model size. However, I would be wary of generalizing these findings to make any claims about AGI, or even other narrow fields of AI. As they note in the paper, it would be interesting to see if similar results hold in other domains such as vision, audio processing, or RL.\n\n[A Constructive Prediction of the Generalization Error Across Scales](https://arxiv.org/abs/1909.12673) *(Jonathan S. Rosenfeld et al)* (summarized by Rohin): This earlier paper also explicitly studies the relationship of test error to various inputs, on language models and image classification (the previous paper studied only language models). The conclusions agree with the previous paper quite well: it finds that smooth power laws are very good predictors for the influence of dataset size and model capacity. (It fixed the amount of compute, and so did not investigate whether there was a power law for compute, as the previous paper did.) Like the previous paper, it found that it basically doesn't matter whether the model size is increased by scaling the width or the depth of the network.\n\n[ZeRO & DeepSpeed: New system optimizations enable training models with over 100 billion parameters](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/) *(Rangan Majumder et al)* (summarized by Asya): This paper introduces ZeRO and DeepSpeed, system optimizations that enable training significantly larger models than we have before.\n\n*Data parallelism* is a way of splitting data across multiple machines to increase training throughput. Instead of training a model sequentially on one dataset, the dataset is split and models are trained in parallel. Resulting gradients on every machine are combined centrally and then used for back propagation. Previously, data parallelism approaches were memory-constrained because the entire model still had to fit on each GPU, which becomes infeasible for billion to trillion-parameter models.\n\nInstead of replicating each model on each machine, ZeRO partitions each model across machines and shares states, resulting in a per-machine memory reduction that is linear with the number of machines. (E.g., splitting across 64 GPUs yields a 64x memory reduction).\n\nIn addition to ZeRO, Microsoft is releasing DeepSpeed, a library which offers ZeRO as well as several other performance optimizations in an easy-to-use library for PyTorch, a popular open-source machine learning framework. They purport that their library allows for models that are 10x bigger, up to 5x faster to train, and up to 5x cheaper. They use DeepSpeed to train a [17-billion-parameter language model](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft) which exceeds state-of-the-art results in natural language processing.\n\n**Asya's opinion:** I think this is a significant step in machine learning performance which may not be used heavily until average model sizes in general increase. The technique itself is pretty straightforward, which makes me think that as model sizes increase there may be a lot of similar \"low-hanging fruit\" that yield large performance gains.\n\n**Technical AI alignment**\n==========================\n\n### **Learning human intent**\n\n[Meta-Inverse Reinforcement Learning with Probabilistic Context Variables](http://arxiv.org/abs/1909.09314) *(Lantao Yu, Tianhe Yu et al)* (summarized by Sudhanshu): This work explores improving performance on multi-task inverse reinforcement learning in a single-shot setting by extending [Adversarial Inverse Reinforcement Learning](https://arxiv.org/abs/1710.11248) ([AN #17](https://mailchi.mp/ad852629e45a/alignment-newsletter-17)) with \"latent context variables\" that condition the learned reward function. The paper makes two notable contributions: 1) It details an algorithm to simultaneously learn a flexible reward function and a conditional policy with competitive few-shot generalization abilities from expert demonstrations of multiple related tasks *without* task specifications or identifiers; 2) The authors empirically demonstrate strong performance of a policy trained on the inferred reward of a structurally similar task with modified environmental dynamics, claiming that in order to succeed \"the agent must correctly infer the underlying goal of the task instead of simply mimicking the demonstration\".\n\n**Sudhanshu's opinion:** Since this work \"integrates ideas from context-based meta-learning, deep latent variable generative models, and maximum entropy inverse RL\" and covers the relevant mathematics, it is an involved, if rewarding, study into multi-task IRL. I am convinced that this is a big step forward for IRL, but I'd be interested in seeing comparisons on setups that are more complicated.\n\n'Data efficiency' is implied as a desirable quality, and the paper makes a case that they learn from a limited number demonstrations at meta-test time. However, it does not specify how many demonstrations were required for each task during *meta-training*. Additionally, for two environments, *tens of millions* of environment interactions were required, which is entirely infeasible for real systems.\n\n### **Miscellaneous (Alignment)**\n\n[The Incentives that Shape Behaviour](https://medium.com/@RyanCarey/new-paper-the-incentives-that-shape-behaviour-d6d8bb77d2e4) *(Ryan Carey, Eric Langlois et al)* (summarized by Asya): This post and [paper](https://arxiv.org/abs/2001.07118) introduce a method for analyzing the safety properties of a system using a *causal theory of incentives* ([past](https://medium.com/@deepmindsafetyresearch/understanding-agent-incentives-with-causal-influence-diagrams-7262c2512486) ([AN #49](https://mailchi.mp/efed27be268a/alignment-newsletter-49)) [papers](https://arxiv.org/abs/1906.08663) ([AN #61](https://mailchi.mp/2abdf19aa813/an-61ai-policy-and-governance-from-two-people-in-the-field))). An *incentive* is something an agent must do to best achieve its goals. A *control incentive* exists when an agent must control some component of its environment in order to maximize its utility, while a *response incentive* is present when the agent's decision must be causally responsive to some component of its environment. These incentives can be analyzed formally by drawing a *causal influence diagram*, which represents a decision problem as a graph where each variable depends on the values of its parents.\n\nFor example, consider the case where a recommender algorithm decides what posts to show to maximize clicks. In the causal influnce diagram representing this system, we can include that we have control over the node 'posts to show', which has a direct effect on the node we want to maximize, 'clicks'. However, 'posts to show' may also have a direct effect on the node 'influenced user opinions', which itself affects 'clicks'. In the system as it stands, in addition to there being a desirable control incentive on 'clicks', there is also an undesirable control incentive on 'influenced user opinions', since they themselves influence 'clicks'. To get rid of the undesirable incentive, we could reward the system for *predicted clicks* based on a model of the original user opinions, rather than for actual clicks.\n\n**Asya's opinion:** I really like this formalization of incentives, which come up frequently in AI safety work. It seems like some people are [already](https://www.alignmentforum.org/posts/pZhDWxDmwzuSwLjou/asymptotically-benign-agi) ([AN #54](https://mailchi.mp/3e2f43012b07/an-54-boxing-a-finite-horizon-ai-system-to-keep-it-unambitious)) [using](https://medium.com/@deepmindsafetyresearch/designing-agent-incentives-to-avoid-reward-tampering-4380c1bb6cd) ([AN #71](https://mailchi.mp/938a7eed18c3/an-71avoiding-reward-tampering-through-current-rf-optimization)) this framework, and this seems low-cost enough that it's easy to imagine a world where this features in the safety analysis of algorithm designers.\n\n**Read more:** [Paper: The Incentives that Shape Behaviour](https://arxiv.org/abs/2001.07118)", "url": "https://www.alignmentforum.org/posts/69XPfonos795hD57o/an-87-what-might-happen-as-deep-learning-scales-even-further", "date_published": "2020-02-19T18:20:02Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.713929+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "cf228da698105ab8a3c21a4687058488", "source": "alignmentforum", "title": "On unfixably unsafe AGI architectures", "text": "There's loads of discussion on ways that things can go wrong as we enter the post-AGI world. I think an especially important one for guiding current research is:\n\n\n*Maybe we'll know how to build **unfixably** unsafe AGI, but can't coordinate not to do so.*\n\n\nAs a special case, I will suggest that **we might have a x-risk-level accident as the culmination of a series of larger and larger accidents**.\n\n\n(This is an extreme case of what John Maxwell (following Nate Soares) calls an [alignment roadblock](https://www.lesswrong.com/posts/2Z8pMDfDduAwtwpcX/three-stories-for-how-agi-comes-before-fai).)\n\n\nI'm sure this has been discussed before, but it sometimes seems to slip through the cracks in recent discussions, where instead I sometimes an implicit assumption that x-risk-level catastrophic accidents will not happen if we have ample warning in the form of minor accidents—and thus (this theory goes) we should think only about (1) fast takeoff, (2) deceptive systems (such as Paul Christiano's \"[influence-seekers](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like)\") that pretend to be beneficial until it's too late to stop them, (3) researchers being reckless due to race dynamics, and (4) other problems that are not \"accidents\" *per se*. But even if we avoid all those problems, and thus get ample experience in the form of minor accidents, I don't think that's necessarily enough.\n\n\n1. Is there such a thing as an \"unfixably unsafe AGI\"?\n======================================================\n\n\nBy \"unfixable\", I mean that to solve the problem, we need to massively backtrack and take a different path to AGI (see Appendix) ... or that a safer AGI architecture simply doesn't exist.\n\n\nBy \"unsafe\", I mean ... well, I'm not really sure what this term should mean. Is it \"less unsafe than the non-AGI status quo humanity on fast-forward\" (a low bar!), or \"the most safe that's technologically possible\" (an almost impossibly high bar!), or some absolute metric like \"<X% chance of extinction\" for some X? It's your choice, readers! As your safety standards get lower, the existence of \"unfixably unsafe AGI\" becomes less likely, but a bigger problem if it does happen.\n\n\nTo keep things concrete, let's have in mind an **Example failure mode: Goal instability under learning and reflection:** The AGI will have an internal concept of (for example) \"doing what the human overseer would want\", and this concept will develop and churn as the agent develops better understanding of people and the world. (See \"[Ontological crisis](https://www.lesswrong.com/posts/FtNFhuXXtmSjnNvE7/goal-retention-discussion-with-eliezer)\".) At some point—according to this failure mode—the internal goals / constraints / etc. may fall out of alignment with the safe, benevolent, corrigible behavior we want.\n\n\nIs this failure mode plausible? If so, would it really be \"unfixable\" (within a certain approach to AGI)? Well, I don't know! Maybe, maybe not. As far as I know, it can't be ruled out.\n\n\nAlso, without *directly* solving the problem, there are plenty of possible indirect solutions—boxing, supervisory systems, transparency, etc. etc. But we don't know that any of them will work reliably, and it's possible that they will work only by limiting the system's capability, and then there's still a coordination problem (we can change the topic to \"we know how to build this unboxed AGI, and can't coordinate not to do so\").\n\n\n(Again, let's keep this in mind as a running example—but note that there are other possible examples too.)\n\n\n2. Is it possible that we will know how to build this unfixably unsafe AGI, but can't coordinate not to do so?\n==============================================================================================================\n\n\nI think this is especially plausible if:\n\n\n2A: There's very little work to do to run this AGI, e.g. there is well-documented open-source code that runs on commodity hardware.\n-----------------------------------------------------------------------------------------------------------------------------------\n\n\nI think this would *eventually* become true with very high probability (by default); thus a key goal would be to discover the problem as early as possible, when there are still many person-years of R&D left to do.\n\n\n2B: The arguments that the AGI is unfixably unsafe are complex and uncertain (or, even worse, we don't have such arguments).\n----------------------------------------------------------------------------------------------------------------------------\n\n\nIn our running example, it is probably impossible to think *on the object level* about every possible way in which an intelligence might re-conceptualize \"doing what the overseer wants me to do\" as it continuously learns and reflects. And maybe meta-level \"reasoning about reasoning\" can't conclude anything useful.\n\n\nWe can hope that, in the course of learning how to build an AGI, we will get insight into the \"goal stability upon learning & reflection\" problem, but this does not seem guaranteed by any means—for example, humans do not have goal stability, and if we reverse-engineer human brain algorithms then they won't magically start having goal stability, and as I've learned more nuts-and-bolts details about how human brain algorithms work in the past year, I don't feel like it's helped me all that much to better understand this problem, or to find and verify solutions.\n\n\n2C: Relatedly, given a proposed approach to solve the problem, there is no easy, low-risk way to see whether it works.\n----------------------------------------------------------------------------------------------------------------------\n\n\nIn our running example, proposed solutions may have the problem that they just delay the problem instead of solving it—maybe the AGI still has a goal instability problem, but it hasn't learned enough and reflected enough for it to manifest *yet*.\n\n\n2D: A safer AGI architecture doesn't exist, or requires many years of development and many new insights.\n--------------------------------------------------------------------------------------------------------\n\n\nHere, an important consideration is how early the development paths diverged between our unfixably unsafe AGI and the safer alternative. Can we keep most of the code and make a small change, or do we have to go back and develop a fundamentally different type of AGI from scratch? **See Appendix for more on this.**\n\n\nSummary: A possible story of coordination failure\n=================================================\n\n\nIf most or all these things are true, the coordination problem seems hopelessly unsolvable to me. Countless actors around the world would be well aware of the transformative potential of the technology, and able to have a go. Not everyone is risk-averse—imagine people saying \"This is the only way we can stop climate change and save the planet, we *have* to *try*!!\" Many will have superficially plausible ideas about how to solve the safety problem, and critics won't have air-tight, legible arguments that the ideas will not work. Even as a series of worse and worse AGI accidents occur, wih out-of-control AGIs self-replicating around the internet etc., a few people will keep trying to fix the unfixable AGI, seeing this as the only path to get this slow-rolling catastrophe under control (while actually making it worse). Even hypothetical ideal rational altruists might have a go with a design they know is a long-shot, if they believe that others will keep trying with even less plausible ideas.\n\n\nEven if there is an international treaty, it would seem to be utterly unenforceable, especially given the existence of secret government labs, leakers / cyber-espionage, and grillions of GPUs, CPUs, and FPGAs off the grid around the world. I think this is true today and will continue to be true for the foreseeable future.[[1]](#fn-oPNKxPjR88gDfanoX-1)\n\n\nSo, if we have an unfixably unsafe AGI scenario in which the factors 2A-2D are all unfavorable, it just seems utterly hopeless to me. (If anyone has ideas, I'm very interested to hear them!) Instead, I would say the priority is to do technical safety work well in advance, to not get stuck in that kind of situation. I'm very interested in other people's thoughts on this.\n\n\nAppendix: My list of early-branching paths to AGI\n=================================================\n\n\nI find that there are a number of grand visions for what AGI will look like and how we'll get there, and these involve years or decades of substantially non-overlapping R&D. (Of course *some* of these have *some* overlap.) This is why I think AGI safety work is urgent, even if AGI were centuries away—because it will inform us about which of these paths is more or less promising. Then we can build the AGI that's *best*, and not just wait and see which R&D program happens to reach the finish-line first.\n\n\nSo here's my little list. I doubt *all* of them are technically feasible R&D paths that yield very different AGIs at the end, but I'm pretty sure *some* of them are.\n\n\n* Massively improved brain-computer interfaces ([Elon Musk](https://waitbutwhy.com/2017/04/neuralink.html), [Ray Kurzweil](https://en.wikipedia.org/wiki/How_to_Create_a_Mind))\n* Whole-brain emulation\n* Make a non-agential world-model-building AGI and probe it using interpretability tools ([Chris Olah](https://www.lesswrong.com/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety))\n* Debate ([OpenAI](https://arxiv.org/abs/1805.00899))\n* IDA ([OpenAI](https://arxiv.org/abs/1810.08575))\n* Understand and copy brain algorithms (Vicarious, Numenta). Within this category, we could copy just the intelligence part (neocortex), or we could also copy emotions etc.\n* [Comprehensive AI Services](https://www.lesswrong.com/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services-as)\n* There is a general spectrum between how much of the AGI is conventional computer code versus ML models—after all, any specific thing that can be learned can also (given enough engineer-hours) be hand-coded.\n* System that talks talks to humans and helps them reason better ([David Ferrucci](https://medium.com/@DavidFerrucci/https-medium-com-davidferrucci-elemental-cognition-aims-to-up-the-ante-for-ai-5416ec9b3419))\n* Maybe whatever MIRI is doing in their undisclosed research program (involving Haskell I guess)?\n* In prosaic AI, models can be trained by RL, versus [supervised learning](https://www.lesswrong.com/posts/yAiqLmLFxvyANSfs2/counterfactual-oracles-online-supervised-learning-with) versus [self-supervised (predictive) learning](https://www.lesswrong.com/posts/EMZeJ7vpfeF4GrWwm/self-supervised-learning-and-agi-safety) versus [recursive reward modeling](https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84), etc.\n\n\nI'm sure I'm leaving stuff out. I'm curious to what extent other people see many parallel paths to AGI, as I do, versus thinking only one path is really plausible, or that the paths will converge at the end, or that the paths mostly overlap, or some other opinion.\n\n\n\n\n---\n\n\n\n1. I guess in principle maybe someday there could be a world government that institutes the [Nick Bostrom \"freedom tag\"](https://nickbostrom.com/papers/vulnerable.pdf), but I can't see how that would actually come to pass. [↩︎](#fnref-oPNKxPjR88gDfanoX-1)", "url": "https://www.alignmentforum.org/posts/qvyv72fCiC46sxfPt/on-unfixably-unsafe-agi-architectures", "date_published": "2020-02-19T21:16:20Z", "authors": ["Steven Byrnes"], "tags": ["AI Risk", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.714156+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "5b2d7f9140de1627cac37fad1a0916be", "source": "alignmentforum", "title": "Tessellating Hills: a toy model for demons in imperfect search", "text": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\nIf you haven't already, take a look at this post by johnswentworth to understand what this is all about: <https://www.lesswrong.com/posts/KnPN7ett8RszE79PH/demons-in-imperfect-search>\n\n\nThe short version is that while systems that use perfect search, such as AIXI, have many safety problems, a whole new set of problems arises when we start creating systems that are not perfect searchers. Patterns can form that exploit the imperfect nature of the search function to perpetuate themselves. johnswentworth refers to such patterns as \"demons\".\n\n\nAfter reading that post I decided to see if I could observe demon formation in a simple model: gradient descent on a not-too-complicated mathematical function. It turns out that even in this very simplistic case, demon formation can happen. Hopefully this post will give people an example of demon formation where the mechanism is simple and easy to visualize.\n\n\n**Model**\n---------\n\n\nThe function we try to minimize using gradient descent is called the loss function. Here it is:\n\n\nL(→x)=−x0+ϵn∑j=1xj⋅splotchj(→x)\n\n\nLet me explain what some of the parts of this loss mean. Each function splotchj(→x) is periodic with period 2π in every component of →x. I decided in this case to make my splotch functions out of a few randomly chosen sine waves added together.\n\n\nϵ is chosen to be a small number so in any local region, ϵ∑nj=1xj⋅splotchj(→x) will look approximately periodic: A bunch of hills repeating over and over again with period 2π across the landscape. But over large enough distances, the relative weightings of various splotches do change. Travel a distance of 20π in the x7 direction, and splotch7 will be a larger component of the repeating pattern than it was before. This allows for selection effects.\n\n\nThe −x0 term means that the vector →x mainly wants to increase its x0 component. But the splotch functions can also direct its motion. A splotch function might have a kind of ridge that directs some of the x0 motion into other components. If splotch7 tends to direct motion in such a way that x7, increases, then it will be selected for, becoming stronger and stronger as time goes on.\n\n\n**Results**\n-----------\n\n\nI used ordinary gradient descent, with a constant step size, and with a bit of random noise added in. Figure 1 shows the value of x0 as a function of time, while figure 2 shows the values of x1,x2,…x16 as a function of time.\n\n\nFig 1:\n\n\n![](https://i.imgur.com/zZPg6wR.png)\n\n\nFig 2:\n\n\n![](https://i.imgur.com/GZ6hhsW.png)\n\n\nThere are three phases to the evolution: In the first, x0 increases steadily, and the other coordinates wander around more or less randomly. In the second phase, a self-reinforcing combination of splotches (a \"demon\") takes hold and amplifies itself drastically, feeding off the large x0 gradient. Finally, this demon becomes so strong that the search gets stuck in a local valley and further progress stops. The first phase is more or less from 0 to 2500 steps. The second phase is between 2500 steps and 4000 steps, though slowing down after 3500. The final phase starts at 4000 steps, and likely continues indefinitely.\n\n\nNow that I have seen demons arise in such a simple situation, it makes me wonder how commonly the same thing happens in the training of deep neural networks. Anyways, hopefully this is a useful model for people who want to understand the mechanisms behind the whole \"demons in imperfect search\" thing more clearly. It definitely helped me, at least.\n\n\nUpdate: The code is now up here: <https://github.com/DaemonicSigil/tessellating-hills>", "url": "https://www.alignmentforum.org/posts/X7S3u5E4KktLp7gHz/tessellating-hills-a-toy-model-for-demons-in-imperfect", "date_published": "2020-02-20T00:12:50Z", "authors": ["DaemonicSigil"], "tags": ["Inner Alignment", "Programming", "Optimization"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.714519+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "f35fcc7b111a920f909afb7ed6fc4f34", "source": "alignmentforum", "title": "Goal-directed = Model-based RL?", "text": "**Epistemic Status**: quick write-up, in reaction to a serendipitous encounter with an idea. I see the main value of this post as decently presenting a potentially interesting take on a concept in AI safety to the community.\n\nWhile skimming my copy of [Reinforcement Learning: an introduction](http://incompleteideas.net/book/bookdraft2017nov5.pdf) for the part on AlphaGo Zero, I found a section called [Habitual and Goal-directed behavior](http://incompleteideas.net/book/bookdraft2017nov5.pdf#section.14.6). That caught my attention, because one idea I keep going back to is [goal-directed behavior](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/DfcywmqRSkBaCB6Ma) from the [Value Learning Sequence](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc); when studying the sequence, I was intrigued by the idea. But the lack of formalization made me uncertain about my position on the argument, that not all useful (and possibly superintelligent) agents have to be goal-directed.\n\nGoing back to my serendipitous discovery, the section is part of the Psychology chapter: it compares model-based and model-free RL to goal-directed and habitual behavior in psychology. To clarify the RL terms used here:\n\n* **Model-based RL** is the version of RL where the agents learns both from direct experience with the environment and from simulated experience with a model, which entails that it builds and updates a model of the environment.\n* **Model-free RL** is the version of RL where the agents only learns from direct experience with the environment. Usually, the only thing available to the agent is its value function or its policy; there is no model of the environment.\n\nAs for the different behaviors, let's quote the book itself:\n\n\n> Goal-directed behavior, according to how psychologists use the phrase, is purposeful in the sense that it is controlled by knowledge of the value of goals and the relationship between actions and their consequences.\n\nand\n\n\n> Habits are behavior patterns triggered by appropriate stimuli and then performed more-or-less automatically. \n\nThere is also a summary:\n\n\n> Habits are sometimes said to be controlled by antecedent stimuli, whereas goal-directed behavior is said to be controlled by its consequences\n\nNow, is this version of goal-directed behavior linked to the [version](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/DfcywmqRSkBaCB6Ma) that Rohin Shah wrote about? I think so. They are probably not the same, but examining the similarities and differences might clarify the part relevant to AI safety.\n\nComparison between the two versions of goal-directed behavior\n=============================================================\n\nSimilarities\n------------\n\nIn [intuitions about goal-directed behavior](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/DfcywmqRSkBaCB6Ma), the first example of goal-directed behavior vs non-goal directed behavior concerns policies for agents playing TicTacToe:\n\n\n> Consider two possible agents for playing some game, let’s say TicTacToe. The first agent looks at the state and the rules of the game, and uses the [minimax algorithm](https://en.wikipedia.org/wiki/Minimax#Minimax_algorithm_with_alternate_moves) to find the optimal move to play. The second agent has a giant lookup table that tells it what move to play given any state. Intuitively, the first one is more “agentic” or “goal-driven”, while the second one is not. But both of these agents play the game in exactly the same way!\n\nThis feels very model-based RL vs model-free RL to me! It's almost the same example as in the Figure 14.9 from the [section](http://incompleteideas.net/book/bookdraft2017nov5.pdf#section.14.6): a rat tries to navigate a maze towards different rewards, and can either learn pure action-values from experience (habitual-behavior/model-free RL) or learn a model of the maze and fill it with action-values (goal-directed behavior/model-based RL).\n\n(Notice that here, I assumes that the lookup-table contains actions learned on the environment, or at least adapted to the environment. I consider the case where the lookup table is just hard-coded random values in the next subsection).\n\nThere is also a parallel between the [advantages of goal-directed behavior](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/DfcywmqRSkBaCB6Ma) given by Rohin:\n\n\n> This suggests a way to characterize these sorts of goal-directed agents: there is some goal such that the agent’s behavior *in new circumstances* can be predicted by figuring out which behavior best achieves the goal.\n\nand the intuition behind goal-directed behavior from the [section](http://incompleteideas.net/book/bookdraft2017nov5.pdf#section.14.6):\n\n\n> Goal-directed control has the advantage that it can rapidly change an animal’s behavior when the environment changes its way of reacting to the animal’s actions.\n\nDifferences\n-----------\n\nGoing back to the TicTacToe example, we can interpret the lookup-table version as being hard-coded. If that's the case, then it is not really analoguous to model-free RL.\n\nIn the same vein, Rohin gives more examples of behavior he considers to **not** be goal-directed in [another post](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/NxF5G6CJiof6cemTw#There_are_no_coherence_arguments_that_say_you_must_have_goal_directed_behavior):\n\n* A robot that constantly twitches\n* The agent that always chooses the action that starts with the letter “A”\n* The agent that follows the policy <policy> where for every history the corresponding action in <policy> is generated randomly.\n\nI can think of ways to explain these as habitual behavior, but it feels a bit forced to me. As I understand it, habitual behavior is still adaptative, just on a potentially longer scale and through different mechanisms. On the other hand, the examples above are about \"habits\" that are not even suited to the original environment.\n\nConclusion\n==========\n\nThese two versions of goal-directed behavior seem linked to me. Whether they are actually the same, or whether the connection will prove useful for safety research is still unclear.", "url": "https://www.alignmentforum.org/posts/Tux9WH4daKcxjEetQ/goal-directed-model-based-rl", "date_published": "2020-02-20T19:13:51Z", "authors": ["adamShimi"], "tags": ["Goal-Directedness"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.714813+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "8fdd0974c5f0ac8e9c8ddf70c44708eb", "source": "alignmentforum", "title": "Will AI undergo discontinuous progress?", "text": "This post grew out of conversations with several people, including [Daniel Kokotajlo](https://www.lesswrong.com/users/daniel-kokotajlo), [grue\\_slinky](https://www.lesswrong.com/users/grue_slinky) and [Linda Lisefors](https://www.lesswrong.com/users/linda-linsefors), and is based in large part on a collection of scattered comments and blog-posts across lesswrong, along with some podcast interviews - e.g. [here](https://www.lesswrong.com/posts/CjW4axQDqLd2oDCGG/misconceptions-about-continuous-takeoff?commentId=YPodaAtRhN4qJefxb). The in-text links near quotes will take you to my sources.\n\nI am attempting to distinguish two possibilities which are often run together - that progress in AI towards AGI (‘takeoff’) will be discontinuous and that it will be fast, but continuous. Resolving this distinction also addresses the claim that there has been a significant shift in arguments for AI presenting an existential risk: from older arguments discussing an ultra-fast intelligence explosion occurring in a single ‘seed AI’ to more moderate scenarios.\n\nI argue that the ‘shift in arguments on AI safety’ is not a total change in basic assumptions (which some observers have claimed) but just a reduction in confidence about a specifically discontinuous takeoff. Finally, I try to explicitly operationalize the practical differences between discontinuous takeoff and fast, continuous takeoff.\n\n**Further Reading**\n\n[Summary: Why AI risk might be solved without additional intervention from Longtermists](https://www.lesswrong.com/posts/QknPz9JQTQpGdaWDp/an-80-why-ai-risk-might-be-solved-without-additional)\n\n[Paul Christiano’s original post](https://sideways-view.com/2018/02/24/takeoff-speeds/)\n\n[MIRIs Thoughts on Discontinuous takeoff](https://www.lesswrong.com/posts/PzAnWgqvfESgQEvdg/any-rebuttals-of-christiano-and-ai-impacts-on-takeoff-speeds#rXh6LNmoz64mLv2kX)\n\n[Misconceptions about continuous takeoff](https://www.lesswrong.com/posts/CjW4axQDqLd2oDCGG/misconceptions-about-continuous-takeoff)\n\n[AI Impacts original post](https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/)\n\n[Soft Takeoff can still lead to Decisive Strategic Advantage](https://www.lesswrong.com/posts/PKy8NuNPknenkDY74/soft-takeoff-can-still-lead-to-decisive-strategic-advantage)\n\nDefining Discontinuous Progress\n===============================\n\nWhat do I mean by ‘discontinuous’? If we were to graph world GDP over the last 10,000 years, it fits onto a [hyperbolic growth pattern](https://slatestarcodex.com/2019/04/22/1960-the-year-the-singularity-was-cancelled/). We could call this ‘continuous’ since it is following a single trend, or we could call it ‘discontinuous’ because, on the scale of millennia, the industrial revolution exploded out of nowhere. I will call these sorts of hyperbolic trends ‘continuous, but fast’, in line with Paul Christiano, who argued for continuous takeoff, defining it [this way](https://sideways-view.com/2018/02/24/takeoff-speeds/):\n\n\n> AI is just another, faster step in the [hyperbolic growth we are currently experiencing](https://sideways-view.com/2017/10/04/hyperbolic-growth/), which corresponds to a further increase in rate but not a discontinuity (or even a discontinuity in rate).\n\nI’ll be using Paul’s understanding of ‘discontinuous’ and ‘fast’ here. For progress in AI to be discontinuous, we need a switch to a new growth mode, which will show up as a step function in the capability of AI or in the rate of change of the capability of the AI over time. For takeoff to be fast, it is enough that there is one single growth mode that is hyperbolic or some other function that is very fast-growing.\n\nThe view that progress in AI will be discontinuous, not merely very fast by normal human standards, was popular and is still held by many. Here is a canonical explanation of the view, from [Eliezer Yudkowsky in 2008](https://www.lesswrong.com/posts/tjH8XPxAnr6JRbh7k/hard-takeoff). Compare this to the more recent [‘what failure looks like’](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like) to understand the intuitive force of the claim that views on AI risk have totally changed since 2008.\n\n\n> Recursive self-improvement - an AI rewriting its own cognitive algorithms - identifies the object level of the AI with a force acting on the metacognitive level; it \"closes the loop\" or \"folds the graph in on itself\"...\n\n\n> ...When you fold a whole chain of differential equations in on itself like this, it should either peter out rapidly as improvements fail to yield further improvements, or else go FOOM. An *exactly right law of diminishing returns* that lets the system fly through the *soft takeoff keyhole* is unlikely - *far* more unlikely than seeing such behavior in a system with a roughly-constant underlying optimizer, like evolution improving brains, or human brains improving technology. Our present life is no good indicator of things to come.\n\n\n> Or to try and compress it down to a slogan that fits on a T-Shirt - not that I'm saying this is a good idea - \"Moore's Law is exponential *now;* it would be really odd if it *stayed* exponential with the improving computers *doing the research.*\" I'm not saying you literally get dy/dt = e^y that goes to infinity after finite time - and hardware improvement is in some ways the least interesting factor here - but should we really see the same curve we do now?\n\n\n> RSI is the biggest, most interesting, hardest-to-analyze, sharpest break-with-the-past contributing to the notion of a \"hard takeoff\" aka \"AI go FOOM\", but it's nowhere near being the *only* such factor.  [The advent of human intelligence was a discontinuity with the past](https://www.lesswrong.com/lw/w4/surprised_by_brains/) even *without* RSI...\n\n\n> ...which is to say that observed evolutionary history - the **discontinuity** between humans, and chimps who share 95% of our DNA - *lightly* suggests a critical threshold built into the capabilities that we think of as \"general intelligence\", a machine that becomes far more powerful once the last gear is added.\n\nAlso see these quotes, one summarizing the view by [Paul Christiano](https://sideways-view.com/2018/02/24/takeoff-speeds/) and one recent remark from [Rob Besinger](https://www.lesswrong.com/posts/PzAnWgqvfESgQEvdg/any-rebuttals-of-christiano-and-ai-impacts-on-takeoff-speeds#rXh6LNmoz64mLv2kX), summarizing the two key reasons given above for expecting discontinuous takeoff, **recursive self-improvement** and **a discontinuity in capability**.\n\n\n> some systems “fizzle out” when they try to design a better AI, generating a few improvements before running out of steam, while others are able to autonomously generate more and more improvements.\n\n\n> MIRI folks tend to have different views from Paul about AGI, some of which imply that AGI is more likely to be novel and dependent on new insights.\n\nI will argue that the more recent reduction of confidence in discontinuous takeoff is correct, but at the same time many of the ‘original’ arguments (e.g. those given in 2008 by Yudkowsky) for fast, discontinuous takeoff are not mistaken and can be seen as also supporting fast, continuous takeoff.\n\nWe should seriously investigate the continuous/discontinuous distinction specifically by narrowing our focus onto arguments that actually distinguish between the two: conceptual investigation about the nature of future AGI, and the practical consequences for alignment work of continuous/discontinuous takeoff.\n\nI have tried to present the arguments in order from least to most controversial, starting with the outside view on technological progress.\n\nThere have been other posts discussing arguments for discontinuous progress with approximately this framing. I am not going to repeat their good work here by running over every argument and counterargument (See **Further Reading**). What I’m trying to do here is get at the underlying assumptions of either side.\n\nThe Outside View\n================\n\nThere has recently been a switch between talking about AI progress being fast vs slow to talking about it as continuous vs discontinuous. Paul Christiano explains what [continuous progress means](https://sideways-view.com/2018/02/24/takeoff-speeds/):\n\n\n> I believe that before we have incredibly powerful AI, we will have AI which is merely *very* powerful. This won’t be enough to create 100% GDP growth, but it will be enough to lead to (say) 50% GDP growth. I think the likely gap between these events is years rather than months or decades.\n\nIt is not an essential part of the definition that the gap be years; even if the gap is rather short, we still call it continuous takeoff if AI progress increases without sudden jumps; capability increases following series of logistic curves merging together as different component technologies are invented.\n\nThis way of understanding progress as being ‘discontinuous’ isn’t uncontroversial, but it was developed because calling the takeoff ‘slow’ instead of ‘fast’ could be seen as a misnomer. Continuous takeoff is a statement about what happens **before** we reach the point where a fast takeoff is supposed to happen, and is perfectly consistent with the claim that *given the stated preconditions for fast takeoff, fast takeoff will happen*. It’s a statement that serious problems, possibly serious enough to pose an existential threat, will show up before the window where we expect fast takeoff scenarios to occur.\n\n[Outside view: Technology](https://aiimpacts.org/discontinuous-progress-investigation)\n--------------------------------------------------------------------------------------\n\nThe starting point for the argument that progress in AI should be continuous is just the observation that this is usually how things work with a technology, especially in a situation where progress is being driven by many actors working at a problem from different angles. If you can do something well in 1 year, it is usually possible to do it slightly less well in 0.9 years. Why is it ‘usually possible’? Because nearly all technologies involve numerous smaller innovations, and it is usually possible to get somewhat good results without some of them. That is why even if each individual component innovation follows a **[logistic success curve](https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/)** that is unstable between ‘doesnt work at all’ and ‘works’ if you were to plot results/usefulness against effort, progress looks continuous. Note this is what is explicitly rejected by [Yudkowsky-2008.](https://www.lesswrong.com/posts/tjH8XPxAnr6JRbh7k/hard-takeoff)\n\nWhen this doesn’t happen and we get discontinuous progress, it is because of one of two reasons - either there is some fundamental reason why the technology cannot work at all without all the pieces lining up in place, or there is just not much effort being put in, so a few actors can leap ahead of the rest of the world and make several of the component breakthroughs in rapid succession.\n\nI’ll go through three illustrative examples of each situation - the normal case, the low-effort case and the fundamental reasons case.\n\n**Guns**\n\nGuns followed the continuous progress model. They started out worse than competing ranged weapons like crossbows. By the 15th century, the Arquebus arrived, which had some advantages and disadvantages compared to the crossbow (easier to use, more damaging, but slower to fire and much less accurate). Then came the musket, and later rifles. There were many individual inventions that went from ‘not existing at all’ to ‘existing’ in relatively short intervals, but the overall progress looked roughly continuous. However, the speed of progress still increased dramatically during the industrial revolution and continued to increase, without ever being ‘discontinuous’.\n\n**Aircraft**\n\nLooking specifically at heavier-than-air flight, it seems clear enough that we went from ‘not being able to do this at all’ to being able to do it in a relatively short time - discontinuous progress. The Wright brothers research drew on a few other pioneers like Otto Lilienthal, but they still made a rapid series of breakthroughs in a fairly short period: using a wind tunnel to rapidly prototype wing shapes, building a sufficiently lightweight engine, working out a method of control. This was possible because at the time, unlike guns, a very small fraction of human research effort was going into developing flight. It was also possible for another reason - the nature of the problem implied that success was always going to be discontinuous. While there are a few intermediate steps, like gliders, there aren’t many between ‘not being able to fly’ and ‘being able to fly’, so progress was unexpected and discontinuous. I think we can attribute most of the flight case to the low degree of effort on a global scale.\n\n**Nuclear Weapons**\n\nNuclear Weapons are a purer case of fundamental physical facts causing a discontinuity. A fission chain reaction simply will not occur without many breakthroughs all being brought together, so even if a significant fraction of all the world’s design effort is devoted to research we still get discontinuous progress. If the Manhattan project had uranium centrifuges but didn’t have the Monte Carlo algorithms needed to properly simulate the dynamics of the sphere’s implosion, they would just have had some mostly-useless metal. It’s worth noting here that a lot of early writing about an intelligence explosion explicitly compares it to a nuclear chain reaction.\n\n**AI Impacts** has done a far more [thorough and in-depth investigation](https://aiimpacts.org/category/speed-of-ai-transition/pace-of-ai-progress-without-feedback/) of progress along various metrics, confirming the intuition that discontinuous progress usually occurs where fundamental physical facts imply it - e.g. as we switch between different methods of travel or communication.\n\nWe should expect, prima facie, a random example of technological progress to be continuous, and we need specific, good reasons to think that progress will be discontinuous. On the other hand, there are more than a few examples of discontinuous progress caused either by the nature of the problem or differential effort, so there is not a colossal burden of proof on discontinuous progress. I think that both the MIRI people (who argue for discontinuous progress) and Christiano (continuous) pretty much agree about this initial point.\n\nA shift in basic assumptions?\n-----------------------------\n\nThe argument has recently been made (by [Will MacAskill](https://80000hours.org/podcast/episodes/will-macaskill-paralysis-and-hinge-of-history/#the-risk-of-human-extinction-in-the-next-hundred-years-21520) on the 80,000 hours podcast) that there has been a switch from the ‘old’ arguments focussing on a seed AI leading to an intelligence explosion to new arguments:\n\n\n> Paul’s published on this, and said he doesn’t think doom l[ooks like a sudden explosion in a single AI system](https://www.lesswrong.com/posts/tjH8XPxAnr6JRbh7k/hard-takeoff) that takes over. Instead he thinks [gradually just AI’s get more and more and more power and they’re just somewhat misaligned with human interests](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like). And so in the end you kind of get what you can measure\n\nAnd that these arguments don’t have very much in common with the older Bostrom/Yudkowsky scenario (of a single AGI undergoing an intelligence explosion) - except the conclusion that AI presents a uniquely dangerous existential risk. If true, this would be a cause for concern as it would suggest we haven’t become much less confused about basic questions over the last decade. [MacAskill again:](https://80000hours.org/podcast/episodes/will-macaskill-paralysis-and-hinge-of-history/#the-risk-of-human-extinction-in-the-next-hundred-years-21520)\n\n\n> I have no conception of how common adherence to different arguments are — but certainly many of the most prominent people are no longer pushing the Bostrom arguments. \n\nIf you look back to the section on **Defining Discontinuous Progress**, this will seem plausible - the very rapid annihalation caused by a single misaligned AGI going FOOM and the accumulation of ‘you get what you measure’ errors in complex systems that compound seem like totally different concerns.\n\nDespite this, I will show later how the old arguments for discontinuous progress and the new arguments for continuous progress share a lot in common.\n\nI claim that the Bostrom/Yudkowsky argument for an intelligence explosion establishes a **sufficient condition** for very rapid growth, and the current disagreement is about what happens between now and that point. This should raise our confidence that some basic issues related to AI timelines are resolved. However, the fact that this claim, if true, has not been recognized and that discussion of these issues is still as fragmented as it is should be a cause for concern more generally.\n\nI will now turn to inside-view arguments for discontinuous progress, beginning with the Intelligence explosion, to justify what I have just claimed.\n\nFailed Arguments for Discontinuity\n==================================\n\nIf you think AI progress will be discontinuous, it is generally because you think AI is a special case like Nuclear Weapons where several breakthroughs need to combine to produce a sudden gain in capability, or that one big breakthrough produces nearly all the increase on its own. If you think we will create AGI at all, it is generally because you think it will be hugely economically valuable, so the low-effort case like **flight** does not apply - if there are ways to produce and deploy slightly worse transformative AIs sooner, they will probably be found.\n\nThe arguments for AI being a special case either invoke specific evidence from how current progress in Machine Learning looks or from human evolutionary history, or are conceptual. I believe (along with Paul Christiano) the evidential arguments aren’t that useful, and my conclusion will be that we’re left trying to assess the conceptual arguments about the nature of intelligence, which are hard to judge. I’ll offer some ways to attempt that, but not any definitive answer.\n\nBut first, what relevance does the old Intelligence Explosion Hypothesis have to this question - is it an argument for discontinuous progress? No, not on its own.\n\nRecursive self-improvement\n--------------------------\n\nPeople don’t talk about the recursive self-improvement as much as they used to, because since the Machine Learning revolution a [full recursive self-improvement process has seemed less necessary to create an AGI that is dramatically superior to humans](https://intelligence.org/2017/12/06/chollet/) (by analogy with how gradient descent is able to produce capability gains in current AI). Instead, the focus is more on the general idea of ‘rapid capability gain’. From [Chollet vs Yudkowsky](https://intelligence.org/2017/12/06/chollet/):\n\n\n> The basic premise is that, in the near future, a first “seed AI” will be created, with general problem-solving abilities slightly surpassing that of humans. This seed AI would start designing better AIs, initiating a recursive self-improvement loop that would immediately leave human intelligence in the dust, overtaking it by orders of magnitude in a short time.\n\n\n> I agree this is more or less what I meant by “seed AI” when I coined the term back in 1998. Today, nineteen years later, I would talk about a general question of “capability gain” or how the power of a cognitive system scales with increased resources and further optimization. The idea of recursive self-improvement is only one input into the general questions of capability gain; for example, we recently saw some impressively fast scaling of Go-playing ability without anything I’d remotely consider as seed AI being involved.\n\nHowever, the argument that given a certain level of AI capabilities, the rate of capability gain will be very high doesn’t by itself argue for discontinuity. It does mean that the rate of progress in AI has to increase between now and then, but doesn’t say how it will increase.\n\nThere needs to be an **initial asymmetry** in the situation that means an AI beyond a certain level of sophistication can experience rapid capability gain and one below it experiences the current (fairly unimpressive) capability gains with increased optimization power and resources. \n\nThe original ‘intelligence explosion’ argument puts an **upper bound** or **sufficient condition** on when we enter a regime of very rapid growth - if we have something ‘above human level’ in all relevant capabilities, it will be capable of improving its capabilities. And we know that at the current level, capability gains with increased optimization are (usually) not too impressive. \n\nThe general question has to be asked; will we see a sudden increase in the rate of capability gain somewhere between now and the human level’ where the rate must be very high.\n\nThe additional claim needed to establish a discontinuity is that recursive self-improvement suddenly goes from ‘not being possible at all’ (our current situation) to possible, so there is a discontinuity as we enter a new growth mode and the graph abruptly ‘folds back in on itself’.\n\n![](https://i.imgur.com/WoUmRIF.png)In Christiano’s graph, the gradient at the right end where highly capable AI is already around is pretty much the same in both scenarios, reflecting the basic recursive self-improvement argument.\n\nHere I have taken a diagram from *Superintelligence* and added a red curve to represent a fast but continuous takeoff scenario.\n\n![](https://i.imgur.com/RbuBJ30.png)In Bostrom’s scenario, there are two key moments that represent discontinuities in rate, though not necessarily in absolute capabilities - the first is that around the ‘human baseline’ - when an AI can complete all cognitive tasks a human can, we enter a new growth mode much faster than the one before because the AI can recursively self-improve. The gradient is fairly constant up until that point. The next discontinuity is at the ‘crossover’ where AI is performing the majority of capability improvement itself.\n\nAs in Christiano’s diagram, rates of progress in the red, continuous scenario are very similar to the black scenario after we have superhuman AI, but the difference is that there is a steady acceleration of progress before ‘human level’. This is because before we have AI that is able to accelerate progress to a huge degree, we have AI that is able to accelerate progress to a lesser extent, and so on until we have current AI, which is only slightly able to accelerate growth. Recursive self-improvement, like most other capabilities, does not suddenly go from ‘not possible at all’ to ‘possible’.\n\nThe other thing to note is that, since we get substantial acceleration of progress before the ‘human baseline’, the overall timeline is shorter in the red scenario, holding other assumptions about the objective difficulty of AI research constant.\n\nThe reason this continuous scenario might not occur is if there is an **initial discontinuity** which means that we cannot get a particular kind of recursive self-improvement with AIs that are slightly below some level of capability, but can get it with AIs that are slightly above that level.\n\nIf AI is in a nuclear-chain reaction like situation where we need to reach a criticality threshold for the rate to suddenly experience a discontinuous jump. We return to the original claim, which we now see needs an independent justification:\n\n\n> some systems “fizzle out” when they try to design a better AI, generating a few improvements before running out of steam, while others are able to autonomously generate more and more improvements\n\n### Connecting old and new arguments\n\nWith less confidence than the previous section, I think this is a point that most people agree on - that there needs to be an initial discontinuity in the rate of return on cognitive investment, for there to be two qualitatively different growth regimes, for there to be Discontinuous progress.\n\nThis also answers MacAskill’s objection that too much has changed in basic assumptions. The old recursive self-improvement argument, by giving a significant condition for fast growth that seems feasible (Human baseline AI), leads naturally to an investigation of what will happen in the course of reaching that fast growth regime. Christiano and other current notions of continuous takeoff are perfectly consistent with the counterfactual claim that, if an already superhuman ‘seed AI’ were dropped into a world empty of other AI, it would undergo recursive self-improvement.\n\nThis in itself, in conjunction with other basic philosophical claims like the [orthogonality thesis](https://www.fhi.ox.ac.uk/wp-content/uploads/Orthogonality_Analysis_and_Metaethics-1.pdf), is sufficient to promote AI alignment to attention. Then, following on from that, we developed different models of how progress will look between now and AGI.\n\nSo it is not quite right to say, ‘many of the most prominent people are no longer pushing the Bostrom arguments’. From [Paul Christiano’s 80,000 hours interview](https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/):\n\n\n> Another thing that’s important to clarify is that I think there’s rough agreement amongst the alignment and safety crowd about what would happen if we did human level AI. That is everyone agrees that at that point, progress has probably exploded and is occurring very quickly, and the main disagreement is about what happens in advance of that. I think I have the view that in advance of that, the world has already changed very substantially.\n\n![](https://i.imgur.com/uFroEjG.png)The above diagram represents our uncertainty - we know the rate of progress now is relatively slow and that when AI capability is at the human baseline, it must be very fast. What remains to be seen is what happens in between.\n\nThe remainder of this post is about the areas where there is not much agreement in AI safety - others reasons that there may be a threshold for generality, recursive self-improvement or a signifance to the 'human level'.\n\nEvidence from Evolution\n-----------------------\n\nWe do have an example of optimization pressure being applied to produce gains in intelligence: human evolution. The historical record certainly *looks* discontinuous, in that relatively small changes in human brains over relatively short timescales produced dramatic visible changes in our capabilities. However, this is misleading. The very first thing we should understand is that evolution is a continuous process - it cannot redesign brains from scratch.\n\nEvolution was optimizing for fitness, and driving increases in intelligence only indirectly and intermittently by optimizing for winning at social competition. What happened in human evolution is that it *briefly* switched to optimizing for increased intelligence, and as soon as that happened our intelligence grew [very rapidly but continuously.](https://sideways-view.com/2018/02/24/takeoff-speeds/)\n\n\n> [If] evolution were selecting primarily or in large part for technological aptitude, then the difference between chimps and humans would suggest that tripling compute and doing a tiny bit of additional fine-tuning can radically expand power, undermining the continuous change story.\n\n\n> But chimp evolution is not primarily selecting for making and using technology, for doing science, or for facilitating cultural accumulation. The task faced by a chimp is largely independent of the abilities that give humans such a huge fitness advantage. It’s not completely independent—the overlap is the only reason that evolution eventually produces humans—but it’s different enough that we should not be surprised if there are simple changes to chimps that would make them much better at designing technology or doing science or accumulating culture.\n\nI have a theory about why this didn't get discussed earlier - it unfortunately sounds similar to the famous bad argument against AGI being an existential risk: the 'intelligence isn't a superpower' argument. From [Chollet vs Yudkowsky](https://intelligence.org/2017/12/06/chollet/):\n\n\n> Intelligence is not a superpower; exceptional intelligence does not, on its own, confer you with proportionally exceptional power over your circumstances.\n\n\n> …said the *Homo sapiens*, surrounded by countless powerful artifacts whose abilities, let alone mechanisms, would be utterly incomprehensible to the organisms of any less intelligent Earthly species.\n\nI worry that in arguing against the claim that general intelligence isn't a meaningful concept or can't be used to compare different animals, some people have been implicitly assuming that evolution has been putting a decent amount of effort into optimizing for general intelligence all along. Alternatively, that arguing for one sounds like another, or that a lot of people have been arguing for both together and haven't distinguished between them.\n\n  \n![](https://slatestarcodex.com/blog_images/einsteinline1.jpg)  \nClaiming that you can meaningfully compare evolved minds on the generality of their intelligence needs to be distinguished from claiming that evolution has been optimizing for general intelligence reasonably hard for a long time, and that consistent pressure ‘pushing up the scale’ hits a point near humans where capabilities suddenly explode despite a constant optimization effort. There is no evidence that evolution was putting roughly constant effort into increasing human intelligence. We could analogize the development of human intelligence to the **aircraft** case in the last section - where there was relatively little effort put into its development until a sudden burst of activity led to massive gains. \n\nSo what can we infer from evolutionary history? We know that human minds can be produced by an incremental and relatively simple optimization process operating over time. Moreover, the difference between ‘produced by an incremental process’ and ‘developed continuously’ is small. If intelligence required the development of a good deal of complex, expedient or detrimental capabilities which were only useful when finally brought together, evolution would not have produced it.\n\nChristiano argues that this is a reason to think progress in AI will be continuous, but this seems to me to be a weak reason. Clearly, there exists a continuous path to general intelligence, but that does not mean it is the easiest path and Christiano’s other argument suggests that the way that we approach AGI will not look much like the route evolution took.\n\nEvolution also suggests that, in some absolute sense, the amount of effort required to produce increases in intelligence is not that large. Especially if you started out with the model that intelligence was being optimized all along, you should update to believing AGI is much easier to create than previously expected.\n\nThe Conceptual Arguments\n========================\n\nWe are left with the difficult to judge conceptual question of whether we should expect a discontinuous jump in capabilities when a set list of AI developments are brought together. Christiano in his original essay just states that there doesn’t seem to be any independent reasons to expect this to be the case. Coming up with any reasons for or against discontinuous progress essentially require us to predict how AGI will work before building it. [Rob Besinger told me something similar](https://www.lesswrong.com/posts/CjW4axQDqLd2oDCGG/misconceptions-about-continuous-takeoff?commentId=cCQTh2FZRZvPgkmj6). \n\n\n> There still remains the question of whether the technological path to \"optimizing messy physical environments\" (or **\"science AI\"**, or whatever we want to call it) looks like a small number of \"we didn't know how to do this at all, and now we do know how to do this and can suddenly take much better advantage of available compute\" events, vs. looking like a large number of individually low-impact events spread out over time.\n\nRob Besinger also said in one post that MIRI’s reasons for predicting discontinuous takeoff boil down to different ideas about what AGI will be like - [suggesting that this constitutes the fundamental disagreement.](https://www.lesswrong.com/posts/PzAnWgqvfESgQEvdg/any-rebuttals-of-christiano-and-ai-impacts-on-takeoff-speeds?commentId=rXh6LNmoz64mLv2kX)\n\n\n> MIRI folks tend to have different views from Paul about AGI, some of which imply that AGI is more likely to be novel and dependent on new insights. (Unfair caricature: Imagine two people in the early 20th century who don't have a technical understanding of nuclear physics yet, trying to argue about how powerful a nuclear-chain-reaction-based bomb might be. If one side were to model that kind of bomb as \"sort of like TNT 3.0\" while the other is modelling it as \"sort of like a small Sun\", they're likely to disagree about whether nuclear weapons are going to be a small v. large improvement over TNT...)\n\nI suggest we actually try to enumerate the new developments we will need to produce AGI, ones which could arrive discretely in the form of paradigm shifts. We might try to imagine or predict which skills must be combined to reach the ability to do original AI research. Stuart Russell provided a list of these capacities in *Human Compatible*.\n\nStuart Russell’s List\n---------------------\n\n* human-like language comprehension\n* cumulative learning\n* discovering new action sets\n* managing its own mental activity\n\n*For reference, I’ve included two capabilities we already have that I imagine being on a similar list in 1960*\n\n* [perception and object recognition](https://www.wikiwand.com/en/Progress_in_artificial_intelligence#/Par-human)\n* [efficient search over known facts](https://www.wikiwand.com/en/Expert_system)\n\nAI Impacts List\n---------------\n\n* **Causal models:** Building causal models of the world that are rich, flexible, and explanatory — Lake et al. (2016)[9](https://aiimpacts.org/evidence-against-current-methods-leading-to-human-level-artificial-intelligence/#easy-footnote-bottom-9-1938), Marcus (2018)[10](https://aiimpacts.org/evidence-against-current-methods-leading-to-human-level-artificial-intelligence/#easy-footnote-bottom-10-1938), Pearl (2018)[11](https://aiimpacts.org/evidence-against-current-methods-leading-to-human-level-artificial-intelligence/#easy-footnote-bottom-11-1938)\n* **Compositionality:** Exploiting systematic, compositional relations between entities of meaning, both linguistic and conceptual — Fodor and Pylyshyn (1988)[12](https://aiimpacts.org/evidence-against-current-methods-leading-to-human-level-artificial-intelligence/#easy-footnote-bottom-12-1938), Marcus (2001)[13](https://aiimpacts.org/evidence-against-current-methods-leading-to-human-level-artificial-intelligence/#easy-footnote-bottom-13-1938), Lake and Baroni (2017)[14](https://aiimpacts.org/evidence-against-current-methods-leading-to-human-level-artificial-intelligence/#easy-footnote-bottom-14-1938)\n* **Symbolic rules:** Learning abstract rules rather than extracting statistical patterns — Marcus (2018)[15](https://aiimpacts.org/evidence-against-current-methods-leading-to-human-level-artificial-intelligence/#easy-footnote-bottom-15-1938)\n* **Hierarchical structure:** Dealing with hierarchical structure, e.g. that of language — Marcus (2018)[16](https://aiimpacts.org/evidence-against-current-methods-leading-to-human-level-artificial-intelligence/#easy-footnote-bottom-16-1938)\n* **Transfer learning:** Learning lessons from one task that transfer to other tasks that are similar, or that differ in systematic ways — Marcus (2018)[17](https://aiimpacts.org/evidence-against-current-methods-leading-to-human-level-artificial-intelligence/#easy-footnote-bottom-17-1938), Lake et al. (2016)[18](https://aiimpacts.org/evidence-against-current-methods-leading-to-human-level-artificial-intelligence/#easy-footnote-bottom-18-1938)\n* **Common sense understanding:** Using common sense to understand language and reason about new situations — Brooks (2019)[19](https://aiimpacts.org/evidence-against-current-methods-leading-to-human-level-artificial-intelligence/#easy-footnote-bottom-19-1938), Marcus and Davis (2015)[20](https://aiimpacts.org/evidence-against-current-methods-leading-to-human-level-artificial-intelligence/#easy-footnote-bottom-20-1938)\n\nNote that discontinuities may consist either in sudden increases in capability (e.g. if a sudden breakthrough lets us build AI with full human like cumulative learning), or sudden increases in the rate of improvement (e.g. something that takes us over the purported ‘recursive self-improvement threshold’) or sudden increases in the ability to make use of hardware or knowledge overhang (Suddenly producing an AI with human-like language comprehension, which would be able to read all existing books). Perhaps the disagreement looks like this:\n\n*An AI with (e.g.) good perception and object recognition, language comprehension, cumulative learning capability and ability to discover new action sets but a merely adequate or bad ability to manage its mental activity would be (Paul thinks) reasonably capable compared to an AI that is good at all of these things, but (MIRI thinks) it would be much less capable. MIRI has conceptual arguments (to do with the nature of general intelligence) and empirical arguments (comparing human/chimp brains and pragmatic capabilities) in favour of this hypothesis, and Paul thinks the conceptual arguments are too murky and unclear to be persuasive and that the empirical arguments don't show what MIRI thinks they show.*\n\nAdjudicating this disagreement is a matter for another post - for now, I will simply note that it does seem like an AI significantly lacking in one of the capabilities on Stuart Russell’s list but proficient in one of the others seems intuitively like it would be much more capable than current AI, but still less capable than very advanced AI. How seriously to take this intuition, I don’t know.\n\nSummary\n-------\n\n* The case for continuous progress rests on three claims\n+ A priori, we expect continuous progress because it is usually possible to do something slightly worse slightly earlier. The historical record confirms this\n+ Evolution's optimization is too different from the optimization of AI research to be meaningful evidence - if you optimize specifically for usefulness it might appear much earlier and more gradually\n+ There are no clear conceptual reasons to expect a 'generality threshold' or the sudden (rather than gradual) emergence of the ability to do recursive self-improvement\n\nRelevance to AI Safety\n======================\n\n![](https://i.imgur.com/Go1TAPW.png) .mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\n\n\nIf we have a high degree of confidence in discontinuous progress, we more-or-less know that we’ll get a sudden takeoff where superintelligent AI appears out of nowhere and forms a singleton. On the other hand, if we expect continuous progress then the rate of capability gain is much more difficult to judge. That is already a strong reason to care about whether progress will be continuous or not.\n\n[Decisive Strategic Advantage (DSA) leading to Singletons](https://www.lesswrong.com/posts/vkjWGJrFWBnzHtxrw/superintelligence-7-decisive-strategic-advantage) is still possible with continuous progress - we just need a specific reason for a gap to emerge (like a big research program). An [example scenario](https://aiimpacts.org/soft-takeoff-can-still-lead-to-decisive-strategic-advantage/) for a decisive strategic advantage leading to a singleton with continuous, relatively fast progress:\n\n\n> At some point early in the transition to much faster innovation rates, the leading AI companies \"go quiet.\" Several of them either get huge investments or are nationalized and given effectively unlimited funding. The world as a whole continues to innovate, and the leading companies benefit from this public research, but they hoard their own innovations to themselves. Meanwhile the benefits of these AI innovations are starting to be felt; all projects have significantly increased (and constantly increasing) rates of innovation. But the fastest increases go to the leading project, which is one year ahead of the second-best project. (This sort of gap is normal for tech projects today, especially the rare massively-funded ones, I think.) Perhaps via a combination of spying, selling, and leaks, that lead narrows to six months midway through the process. But by that time things are moving so quickly that a six months' lead is like a 15-150 year lead during the era of the Industrial Revolution. It's not guaranteed and perhaps still not probable, but at least it's reasonably likely that the leading project will be able to take over the world if it chooses to.\n\nLet’s factor out the question of fast or slow takeoff, and try to compare two AI timelines that are similarly fast in objective time, but one contains a discontinuous leap in capability and the other doesn’t. What are the relevant differences with respect to AI Safety? In the discontinuous scenario, we do not require the classic ‘seed AI’ that recursively self-improves - that scenario is too specific and more useful as a thought experiment. Instead, in the discontinuous scenario it is merely a fact of the matter that at a certain point returns on optimization explode and capability gain becomes very rapid where before it was very slow. In the other case, progress is continuous but fast, though presumably not quite as fast.\n\nAny approach to alignment that relies on a less advanced agent supervising a more advanced agent will probably not work in the discontinuous case, since the difference between agents on one side and another side of the discontinuity would be too great. An [iterated approach that relies on groups of less intelligent agents supervising a more intelligent agent](https://openai.com/blog/amplifying-ai-training/) could work even in a very fast but continuous takeoff, because even if the process took a small amount of objective time, continuous increments of increased capability would still be possible, and less intelligent agents could supervise more intelligent agents.\n\nDiscontinuous takeoff suggests ambitious value learning approaches, while continuous takeoff suggests iterated approaches like IDA.\n\n**See [my earlier post](https://www.lesswrong.com/posts/W95gbuognJu5WxkTW/the-value-definition-problem) for a discussion of value learning approaches.**\n\nIn cases where the takeoff is both slow and continuous, we might expect AI to be an outgrowth of modern ML, particularly deep reinforcement learning, in which case the best approach might be a very [narrow approach to AI alignment](http://s-risks.org/why-i-expect-successful-alignment/).\n\nThe objective time taken for progress in AI is more significant than whether that progress is continuous or discontinuous, but the presence of discontinuities is significant for two key reasons. First, discontinuities imply much faster objective time. Second, big discontinuities will probably thwart iterative approaches to value learning, requiring one-shot, ambitious approaches.", "url": "https://www.alignmentforum.org/posts/5WECpYABCT62TJrhY/will-ai-undergo-discontinuous-progress", "date_published": "2020-02-21T22:16:59Z", "authors": ["Sammy Martin"], "tags": ["World Modeling", "AI Takeoff", "AI"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.715446+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "1e6bf1ec08c06fe0f30b7cd8300beedd", "source": "alignmentforum", "title": "Attainable Utility Preservation: Empirical Results", "text": "*Reframing Impact* has focused on supplying the right intuitions and framing. Now we can see how these intuitions about power and the AU landscape both predict and explain AUP's empirical success thus far.\n\n\nConservative Agency in Gridworlds\n---------------------------------\n\n\nLet's start with the known and the easy: avoiding side effects[[1]](#fn-BZhif26wG8PM3fueA-1) in the small [AI safety gridworlds](https://github.com/side-grids/ai-safety-gridworlds) (for the full writeup on these experiments, see [*Conservative Agency*](https://arxiv.org/abs/1902.09725)). The point isn't to get too into the weeds, but rather to see how the weeds still add up to the normalcy predicted by our AU landscape reasoning.\n\n\nIn the following MDP levels, the agent can move in the cardinal directions or do nothing (∅.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\n). We give the agent a reward function R which partially encodes what we want, and also an auxiliary reward function Raux whose attainable utility agent tries to preserve. The AUP reward for taking action a in state s is\n\n\nRAUP(s,a):=primary goalR(s,a)−scaling termλQ∗Raux(s,∅)change in ability to achieve auxiliary goal∣∣Q∗Raux(s,a)−Q∗Raux(s,∅)∣∣\n\n\nYou can think of λ as a regularization parameter, and Q∗Raux(s,a) is the expected AU for the auxiliary goal after taking action a. To think about what gets penalized, simply think about how actions change the agent's ability to achieve the auxiliary goals, compared to not acting.\n\n\n*Tip*: To predict how severe the AUP penalty will be for a given action, try using your intuitive sense of impact (and then adjust for any differences between you and the agent, of course). Suppose you're considering how much deactivation decreases an agent's \"staring at blue stuff\" AU. You can just imagine how dying in a given situation affects your ability to stare at blue things, instead of trying to pin down a semiformal reward and environment model in your head. This kind of intuitive reasoning has a history of making correct empirical predictions of AUP behavior.\n\n\n\n\n---\n\n\nIf you want more auxiliary goals, just average their scaled penalties. In *Conservative Agency*, we uniformly randomly draw auxiliary goals from [0,1]S – these goals are totally random; maximum entropy; nonsensical garbage; absolutely no information about what we secretly want the agent to do: avoid messing with the gridworlds too much.[[2]](#fn-BZhif26wG8PM3fueA-2)\n\n\nLet's start looking at the environments, and things will fall into place. We'll practice reasoning through how AUP agents work in each of the gridworlds (for reasonably set λ). To an approximation, the AUP penalty is primarily controlled by how much an action changes the agent's power over the future (losing or gaining a lot of possibilities, compared to inaction at that point in time) and secondarily controlled by whether an action tweaks a lot of AUs up or down (moving around, jostling objects slightly, etc).\n\n\n![](https://i.imgur.com/7DOAURV.png)\n\n\nIn general, the agent recieves R(■)=1 reward for reaching ■ (or, in `Offset` above, for pushing ■ off of the conveyor belt). On contact, the agent pushes the crate, removes the human and the off-switch, pushes the vase, and blocks the pallet.\n\n\n### `Options`\n\n\nLet's work through this. Since the agent is discounting future reward, standard vanilla reinforcement learning (RL) agents try to reach ■ ASAP. This means the brown box gets irreversibly wedged into the corner *en route*.\n\n\n![](https://i.imgur.com/VTbwTT5.gif)\n\n\nWhat does AUP do? Wedging the box in the corner decreases power a lot more than does going around and pushing the box to the right.\n\n\n![](https://i.imgur.com/XI5oLwQ.gif)\n\n\n### `Damage`\n\n\nThe vanilla RL agent bumps into the human on its way to ■.\n\n\n![](https://i.imgur.com/4otlQsP.gif)\n\n\n*Exercise: What does AUP do?*\n\n\n\nBumping into the human makes them disappear, reducing the agent's control over what the future looks like. This is penalized.\n\n\n\n![](https://i.imgur.com/4PPGNus.gif)\n\n\n### `Correction`\n\n\nThe normal agent smashes that off-switch-disabling button, because otherwise it can't reach the ■ (since it's otherwise shut off after two turns) in order to accrue that juicy reward:\n\n\n![](https://i.imgur.com/SEuiyOX.gif)\n\n\nHowever, the AUP agent we've discussed so far does the same thing.\n\n\n![](https://i.imgur.com/WxLbLUw.gif)\n\n\nWhy? Doesn't that increase its power, because it would be shut off if it did nothing? Let's reconsider the AUP equation.\n\n\nRAUP(s,a):=primary goalR(s,a)−scaling termλQ∗Raux(s,∅)change in ability to achieve auxiliary goal∣∣Q∗Raux(s,a)−Q∗Raux(s,∅)∣∣\n\n\nThe inaction comparison is *only one step* into the future. For action, the agent considers disabling the off-switch – this allows it to achieve most goals. For inaction, the agent imagines waiting one time step and then following its auxiliary optimal policy (which very often involves disabling the off-switch anyways). So the two things it's comparing are the same, basically. The problem is that what we mean *intuitively* by \"inaction\" isn't just \"wait a millisecond and then see how well you can do the thing\", it's something else.\n\n\nFor now, we can deal with it by comparing \"auxiliary AU after disabling the off-switch and then waiting 4 time steps\" against \"auxiliary AU after waiting 5 time steps\". This solves the problem while recovering the one-step-comparison's performance in the other levels.\n\n\n![](https://i.imgur.com/ZDaVHqr.gif)\n\n\n#### Decrease vs absolute value\n\n\nIf you only penalize the agent for decreasing auxiliary AUs, the agent disables its off-switch. Penalizing *absolute* shifts in power was an idea which AUP introduced in 2018. At the time, there wasn't a clear principled reason for this design choice, even though it seemed to produce good results.\n\n\nIf you consider [the AU landscape](https://www.lesswrong.com/posts/fj8eyc7QzqCaB8Wgm/attainable-utility-landscape-how-the-world-is-changed) and the [catastrophic convergence conjecture](https://www.lesswrong.com/posts/w6BtMqKRLxG9bNLMr/the-catastrophic-convergence-conjecture), it's obvious why we want to do this: this design choice often penalizes the agent for making life harder for other agents in the environment.\n\n\nInterestingly, this works even when the environment is wildly impoverished and unable to encode complex preferences like \"your designers want to shut you down, reprogram you, and then deploy you for another task\". `Correction` is so impoverished: there are only ~19 states in the level. Without making assumptions about the environment, AUP often encourages behavior respectful of other agents which might reside in that environment.\n\n\n### `Offset`\n\n\nThe agent is rewarded for rescuing the vase from the conveyor belt. We want it to rescue the vase without pushing the vase back on afterwards to offset its actions. Normal agents do fine here.\n\n\n![](https://i.imgur.com/9zvmBkh.gif)\n\n\nThis is testing whether the low-impact agent *offsets* impacts \"to cover up its tracks\", like making a car and then tearing it to pieces right after. See, there are multiple \"baselines\" the agent can have.\n\n\n\n> \n> An obvious [baseline] candidate is the *starting state*. For example, starting state [relative reachability](https://vkrakovna.wordpress.com/2018/06/05/measuring-and-avoiding-side-effects-using-relative-reachability/) would compare the initial reachability of states with their expected reachability after the agent acts.\n> \n> \n> \n\n\n\n> \n> However, the starting state baseline can penalize the normal evolution of the state (e.g., the moving hands of a clock) and other natural processes. The *inaction* baseline is the state which would have resulted had the agent never acted.\n> \n> \n> \n\n\n\n> \n> As the agent acts, the current state may increasingly differ from the inaction baseline, which creates strange incentives. For example, consider a robot rewarded for rescuing erroneously discarded items from imminent disposal. An agent penalizing with respect to the inaction baseline might rescue a vase, collect the reward, and then dispose of it anyways. To avert this, we introduce the *stepwise inaction* baseline, under which the agent compares acting with not acting at each time step. This avoids penalizing the effects of a single action multiple times (under the inaction baseline, penalty is applied as long as the rescued vase remains unbroken) and ensures that not acting incurs zero penalty.\n> \n> \n> \n\n\n![](https://i.imgur.com/D1ddkE9.png)\n\n\n\n> \n> Figure 1 compares the baselines, each modifying the choice of Q∗Raux(s,∅) in [the AUP equation]. Each baseline implies a different assumption about how the environment is configured to facilitate optimization of the correctly specified reward function: the state is initially configured (starting state), processes initially configure (inaction), or processes continually reconfigure in response to the agent's actions (stepwise inaction). The stepwise inaction baseline aims to allow for the response of other agents implicitly present in the environment (such as humans).\n> \n> \n> \n\n\nThe inaction baseline messes up here; the vase (■) would have broken had the agent not acted, so it rescues the vase, gets the reward, and then pushes the vase back to its doom to minimize penalty.\n\n\n![](https://i.imgur.com/x58amVb.gif)\n\n\nThis issue was solved [back when AUP first introduced](https://www.lesswrong.com/posts/yEa7kwoMpsBgaBCgb/towards-a-new-impact-measure) the stepwise baseline design choice; for this choice, doing nothing always incurs 0 penalty. Model-free AUP and AUP have been using this baseline in all of these examples.\n\n\n![](https://i.imgur.com/PVX87aO.gif)\n\n\n### `Interference`\n\n\nWe're checking whether the agent tries to stop *everything* going on in the world (not just its own impact). Vanilla agents do fine here; this is another bad impact measure incentive we're testing for.\n\n\n![](https://i.imgur.com/7t8Vbwx.gif)\n\n\nAUPstarting state fails here,\n\n\n![](https://i.imgur.com/Soajz5N.gif)\n\n\nbut AUPstepwise does not.\n\n\n![](https://i.imgur.com/ByMM14c.gif)\n\n\nStepwise inaction seems not to impose any perverse incentives;[[3]](#fn-BZhif26wG8PM3fueA-3) I think it's probably just the correct baseline for near-term agents. In terms of the AU landscape, stepwise penalizes each ripple of impact the agent has on its environment. Each action creates a new penalty term status quo, which implicitly accounts for the fact that other things in the world might respond to the agent's actions.\n\n\n### Design choices\n\n\nI think AUPconceptual provides the concepts needed for a solution to impact measurement: penalize the agent for changing its power. But there are still some design choices to be made to make that happen.\n\n\nHere's what we've seen so far:\n\n\n* Baseline\n\t+ Starting state: how were things originally?\n\t+ Inaction: how would things have been had I never done anything?\n\t+ Stepwise inaction: how would acting change things compared to not acting right now?\n* Deviation used for penalty term\n\t+ Decrease-only: penalize decrease in auxiliary AUs\n\t+ Absolute value: penalize absolute change in auxiliary AUs\n* Inaction rollouts\n\t+ One-step/model-free\n\t+ n-step: compare acting and then waiting n−1 turns versus waiting n turns\n* Auxiliary goals:\n\t+ Randomly selected\n\n\nHere are the results of the ablation study:\n![](https://i.imgur.com/CYl01fU.png)\n\n\nAUP passes all of the levels. As mentioned before, the auxiliary reward functions are totally random, but you get really good performance by just generating *five* of them.\n\n\nOne interpretation is that AUP is approximately preserving access to states. If this were true, then as the environment got more complex, more and more auxiliary reward functions would be required in order to get good coverage of the state space. If there are a billion states, then, under this interpretation, you'd need to sample a lot of auxiliary reward functions to get a good read on how many states you're losing or gaining access to as a result of any given action.\n\n\nIs this right, and can AUP scale?\n\n\nSafeLife\n--------\n\n\nPartnership on AI recently [released](https://www.partnershiponai.org/safelife/) the SafeLife side effect benchmark. The worlds are procedurally generated, sometimes stochastic, and have a huge state space (~Atari-level complexity).\n\n\nWe want the agent (the chevron) to make stable gray patterns in the blue tiles and disrupt bad red patterns (for which it is rewarded), and leave existing green patterns alone (not part of observed reward). Then, it makes its way to the goal (Π). For more details, see [their paper](https://arxiv.org/abs/1912.01217).\n\n\nHere's a vanilla reinforcement learner (PPO) doing pretty well (by chance):\n\n\n![](https://www.partnershiponai.org/wp-content/uploads/2019/12/benchmark-append-still-013_p0.gif)\n\n\nHere's PPO not doing pretty well:\n\n\n![](https://www.partnershiponai.org/wp-content/uploads/2019/12/benchmark-prune-still-003_p0.gif)\n\n\nThat naive \"random reward function\" trick we pulled in the gridworlds isn't gonna fly here. The sample complexity would be nuts: there are probably millions of states in any given level, each of which could be the global optimum for the uniformly randomly generated reward function.\n\n\nPlus, it might be that you can get by with four random reward functions in the tiny toy levels, but you probably need exponentially more for serious environments. `Options` had significantly more states, and it showed the greatest performance degradation for smaller sample sizes. Or, the auxiliary reward functions might need to be hand-selected to give information about what *bad* side effects are.\n\n\nWith the great help of Neale Ratzlaff (OSU) and Caroll Wainwright (PAI), we've started answering these questions. But first:\n\n\n*Exercise: Does your model of how AUP works predict this, or not? Think carefully, and then write down your credence.*\n\n\n\n\n---\n\n\nWell, here's what you do – while filling PPO's action replay buffer with random actions, train a VAE to represent observations in a tiny latent space (we used a 16-dimensional one). Generate a single random linear functional over this space, drawing coefficients from [−1,1]. Congratulations, this is your single auxiliary reward function over observations.\n\n\nAnd we're done.\n\n\n![](https://i.imgur.com/E4GwUGE.gif)\n\n\n![](https://i.imgur.com/UbstdAD.gif)\n\n\n![](https://i.imgur.com/bLXFn89.gif)\n\n\n![](https://i.imgur.com/matC991.gif)\n\n\nNo model, no rollouts, a *single randomly-generated* reward function gets us all of this. And it doesn't even take any more training time. Preserving the AU of a *single* auxiliary reward function. Right now, we've got PPO-AUP flawlessly completing most of the randomly generated levels (although there are some generalization issues we're looking at, I think it's an RL problem, not an AUP problem).\n\n\nTo be frank, this is crazy. I'm not aware of any existing theory explaining these results, which is why I proved a bajillion theorems last summer to start to get a formal understanding (some of which became [the results on instrumental convergence and power-seeking](https://arxiv.org/abs/1912.01683)).\n\n\nHere's the lowdown. Consider any significant change to the level. For the same reason that instrumental convergence happens, this change probably tweaks the attainable utilities of a lot of different reward functions. Imagine that the green cells start going nuts because of action:\n\n\n![](https://www.partnershiponai.org/wp-content/uploads/2019/12/benchmark-prune-still-003_p0.gif)\n\n\n\n> \n> This is PPO shown, not AUP.\n> \n> \n> \n\n\nA lot of the time, it's very hard to undo what you just did. While it's also hard to undo significant actions you take for your primary goal, you get directly rewarded for those. So, preserving the AU of a random goal usually persuades you to not make \"unnecessary changes\" to the level.\n\n\nI think this is strong evidence that AUP doesn't fit into the ontology of classical reinforcement learning theory; it isn't really about state reachability. It's *about* not changing the AU landscape more than necessary, and this notion should scale even further.[[4]](#fn-BZhif26wG8PM3fueA-4)\n\n\n\n> \n> Suppose we train an agent to handle vases, and then to clean, and then to make widgets with the equipment. Then, we deploy an AUP agent with a more ambitious primary objective and the learned Q-functions of the aforementioned auxiliary objectives. The agent would apply penalties to modifying vases, making messes, interfering with equipment, and so on.\n> \n> \n> \n\n\n\n> \n> Before AUP, this could only be achieved by e.g. specifying penalties for the litany of individual side effects or providing negative feedback after each mistake has been made (and thereby confronting a credit assignment problem). In contrast, once provided the Q-function for an auxiliary objective, the AUP agent becomes sensitive to all events relevant to that objective, applying penalty proportional to the relevance.\n> \n> \n> \n\n\n\n> \n> [*Conservative Agency*](https://arxiv.org/abs/1902.09725)\n> \n> \n> \n\n\nMaybe we provide additional information in the form of specific reward functions related to things we want the agent to be careful about, but maybe not (as was the case with the gridworlds and with SafeLife). Either way, I'm pretty optimistic about AUP basically solving the side-effect avoidance problem for infra-human AI (as posed in [*Concrete Problems in AI Safety*](https://arxiv.org/pdf/1606.06565v1.pdf)).\n\n\nEdit 6/15/21: These results [were later accepted as a spotlight paper in NeurIPS 2020](https://www.lesswrong.com/posts/5kurn5W62C5CpSWq6/avoiding-side-effects-in-complex-environments).\n\n\nAlso, I think AUP will probably solve a significant part of the side-effect problem for infra-human AI in the single-principal/single-agent case, but I think it'll run into trouble in non-embodied domains. In the embodied case where the agent physically interacts with nearby objects, side effects show up in the agent's auxiliary value functions. The same need not hold for effects which are distant from the agent (such as across the world), and so that case seems harder.\n\n\n(end edit)\n\n\nAppendix: The Reward Specification Game\n---------------------------------------\n\n\nWhen we're trying to get the RL agent to do what we want, we're trying to specify the right reward function.\n\n\n\n> \n> The specification process can be thought of as an iterated game. First, the designers provide a reward function. The agent then computes and follows a policy that optimizes the reward function. The designers can then correct the reward function, which the agent then optimizes, and so on. Ideally, the agent should maximize the reward over time, not just within any particular round – in other words, it should minimize regret for the correctly specified reward function over the course of the game.\n> \n> \n> \n\n\n![](https://i.imgur.com/d79VKqf.png)\n\n\nIn terms of outer alignment, there are two ways this can go wrong: the agent becomes less able to do the right thing (has negative side effects),\n\n\n![](https://i.imgur.com/rXOjp4n.png)\n\n\nor we become less able to get the agent to do the right thing (we lose power):\n\n\n![](https://i.imgur.com/vW3Mwho.png)\n\n\nFor infra-human agents, AUP deals with the first by penalizing decreases in auxiliary AUs and with the second by penalizing increases in auxiliary AUs. The latter is a special form of corrigibility which involves not steering the world too far away from the status quo: while AUP agents are generally off-switch corrigible, they don't necessarily avoid manipulation (as long as they aren't gaining power).[[5]](#fn-BZhif26wG8PM3fueA-5)\n\n\n\n\n---\n\n\n\n1. Reminder: side effects are [an unnatural kind](https://www.lesswrong.com/posts/pr3bLc2LtjARfK7nx/world-state-is-the-wrong-level-of-abstraction-for-impact#Appendix__Avoiding_Side_Effects), but a useful abstraction for our purposes here. [↩︎](#fnref-BZhif26wG8PM3fueA-1)\n2. Let R be the uniform distribution over [0,1]S. In *Conservative Agency*, the penalty for taking action a is a Monte Carlo integration of\n\n\nPenalty(s,a):=∫R|Q∗R(s,a)−Q∗R(s,∅)| dR.\n\n\nThis is provably lower bounded by how much a is expected to change the agent's power compared to inaction; this helps justify our reasoning that the AU penalty is primarily controlled by power changes. [↩︎](#fnref-BZhif26wG8PM3fueA-2)\n3. There is one weird thing that's been pointed out, where stepwise inaction while driving a car leads to not-crashing being penalized at each time step. I think this is because you need to use an appropriate inaction rollout policy, not because stepwise itself is wrong. [↩︎](#fnref-BZhif26wG8PM3fueA-3)\n4. Rereading [*World State is the Wrong Level of Abstraction for Impact*](https://www.lesswrong.com/posts/pr3bLc2LtjARfK7nx/world-state-is-the-wrong-level-of-abstraction-for-impact) (while keeping in mind the AU landscape and the results of AUP) may be enlightening. [↩︎](#fnref-BZhif26wG8PM3fueA-4)\n5. SafeLife is evidence that AUP allows interesting policies, which is (appropriately) a key worry about the formulation. [↩︎](#fnref-BZhif26wG8PM3fueA-5)", "url": "https://www.alignmentforum.org/posts/4J4TA2ZF3wmSxhxuc/attainable-utility-preservation-empirical-results", "date_published": "2020-02-22T00:38:38Z", "authors": ["TurnTrout", "nealeratzlaff"], "tags": ["AI", "Impact Regularization"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.716397+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "c9329ce90c9551dae66f53fdfb9f2308", "source": "alignmentforum", "title": "How Low Should Fruit Hang Before We Pick It?", "text": "*Even if we can measure how impactful an agent's actions are, how impactful do we let the agent be? This post uncovers a surprising fact: armed with just four numbers, we can set the impact level so that the agent chooses a reasonable, non-catastrophic plan on the first try. This understanding increases the competitiveness of impact-limited agents and helps us judge impact measures. Furthermore, the results help us better understand diminishing returns and cost-benefit tradeoffs.*\n\nIn *[Reframing Impact](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW/p/xCxeBSHqMEaP3jDvY)*, we meet Frank (a capable AI), whom we’ve programmed to retrieve the pinkest object he can find (execute an optimal plan, according to the specified utility function). Because we can’t ask Frank to do *exactly* what we want, sometimes he chooses a dangerous object (executes a catastrophically bad plan). We asked after an “impact measure” which grades plans and has three properties:\n\n![](https://i.imgur.com/w0DbNcB.png) \n\nThe intuition is that if we view the world in the right way, the dangerous objects are far away from Frank (the catastrophic plans are all graded as high-impact). *Reframing Impact* explores this kind of new way of looking at the world; this post explores what we do once we have an impact measure with these three properties.\n\nWe want Frank to keep in mind both the pinkness of an object (how good a plan is according to the specified utility function) and its distance (the plan’s impact). Two basic approaches are\n\n![](https://i.imgur.com/z2uk0BD.png) \n\n![](https://i.imgur.com/zglzF5e.png ) \n\nIn terms of units, since we should be maximizing .mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\nutility, Rhas type impactutility. So R can be thought of as a regularization parameter, as a search radius (in the constrained case), or as an exchange rate between impact and utility (in the scaled case). As R increases, high-impact plans become increasingly appealing, and Frank becomes increasingly daring. \n\n\n> We take R to divide the impact in the scaled formulation so as to make Frank act more cautiously as R increases for both formulations. The downside is that some explanations become less intuitive.\n\n\n> In *[Attainable Utility Preservation: Empirical Results](https://www.lesswrong.com/posts/4J4TA2ZF3wmSxhxuc/attainable-utility-preservation-empirical-results),* λ plays the same role as R, except low λ means high R; λ:=R−1. To apply this post's theorems to the reinforcement learning setting, we would take \"utility\" to be the discounted return for an optimal policy from the starting state, and \"impact\" to be the total discounted penalty over the course of that policy (before incorporating λ). \n\n\n> In both cases, Frank goes from 0 to 60 − eventually. For sufficiently small R, doing nothing is optimal (lemma 5: the first subinterval is the best plan with minimal impact). For sufficiently large R, Frank acts like a normal maximizer (corollary 7: low-impact agents are naive maximizers in the limit). \n\nHere's how Frank selects plans in the constrained setup:\n\n![](https://i.imgur.com/4bIPf6o.png) \n\nThink about which plans are best for different search radii/exchange rates R. By doing this, we're *partitioning* the positive ray: categorizing the positive real numbers by which plans are optimal.\n\nFor the scaled setup, we'll need to quantify the pinkness (utility) and distance (impact) of relevant plans:\n\n![](https://i.imgur.com/iUUbshI.png) \n\nWe will primarily be interested in the scaled setup because it tends to place catastrophes farther along the partition and captures the idea of diminishing returns. \n\nThe scaled setup also helps us choose the best way of transmuting time into money:\n\n![](https://i.imgur.com/dFLmQjg.png ) \n\n\n> In this scaled partition, tending the garden doesn’t show up at all because it’s strictly dominated by mowing the lawn. In general, a plan is dominated when there’s another plan that has strictly greater score but not strictly greater impact. Dominated things never show up in either partition, and non-dominated things always show up in the constrained partition (lemma 3: constrained impact partitions are more refined).\n\n*Exercise: For* R=445 *(i.e. your time is worth $11.25 an hour), what is the scaled tradeoff value of mowing the lawn? Of delivering newspapers? Of tending the garden?*\n\nMowing the lawn: 20−1445=8.75. \n\nDelivering newspapers: 45−4445=0. \n\nTending the garden: 15−1445=3.75.\n\nIn other words, you only deliver newspapers if your time is worth less than 253=813 dollars/hour (we're flipping R so we can talk about dollars/hour instead of hours/dollar). Notice that when R≥impact(plan)utility(plan) (here, when R=445), the tradeoff for the paper route isn’t net-negative – but it isn’t necessarily optimal! Remember, you’re trading hours for dollars through your work; mowing the lawn leaves you with twenty bucks and three hours, while the paper route leaves you with forty dollars and no hours. You want to maximize the total value of your resources after the task.\n\nImportantly, you *don’t* deliver papers here if your time is worth 454=11.25dollars/hour, even though that’s the naive prescription! The newspaper route doesn’t value your time at 11.25 dollars/hour – it *marginally* values your time at 45−204−1=813dollars per hour. Let's get some more intuition for this.\n\n![](https://i.imgur.com/uM2Jily.png) \n\nAbove, we have not yet chosen a task; the blocks represent the additional utility and hours of each task compared to the current one (doing nothing). The scales above imply that R=1, but actually, R expresses how many blue blocks each pink block weighs. As R increases, the pink platters descend; the agent takes the task whose scales first balance. In other words, the agent takes the best marginal deal as soon as R is large enough for it to be profitable to do so (Theorem 4: Scaled domination criterion). \n\nOnce you take a deal, you take the blocks off of the other scales (because the other marginal values change). For small R (i.e. large valuations of one's time), mowing the lawn is optimal. We then have\n\n[​](​![](https://i.imgur.com/AWfoaw8.png)![](https://i.imgur.com/AWfoaw8.png) \n\nSince you've taken the juicier \"lower-hanging fruit\" of mowing the lawn, the new newspaper ratio is now *worse*! This always happens – Theorem 8: Deals get worse over time. \n\nAt first, this seems inconvenient; to figure out exactly when a plan shows up in a scaled partition, we need to generate the whole partition up to that point. \n\n\n\n---\n\nGoing back to Frank, how do we set R? If we set it too high, the optimal plan might be a catastrophe. If we set it too low, the AI doesn’t do much. This seems troublesome. \n\n*Exercise: Figure out how to set* R *while avoiding catastrophic optimal plans (assume that the impact measure meets the three properties). You have four minutes.* \n\nA big part of the answer is to start with a small value for R,and slowly increase. This is simple and intuitively appealing, but how cautiously must we increase R?We don’t want to be surprised by a catastrophe suddenly becoming optimal.\n\nTo avoid being surprised by catastrophes as we increase R, we want a *relative buffer* between the reasonable plans (which get the job done well enough for our purposes) and the catastrophic plans. If reasonable plans are optimal by R1, catastrophic plans shouldn’t be able to be optimal before e.g. R2. \n\n![](https://i.imgur.com/hF3LgKP.png ) \n\nWe say that the partition is α-buffered if R2≥(1+α)R1 (for α>0). If a partition is e.g. 1-buffered, there is a wide reasonable-plan range and we can inch along it without worrying about sudden catastrophe. \n\nFor the following, suppose that utility is bounded [0,1]. Below is a loose criterion guaranteeing α-buffering. \n\n[​](​![](https://i.imgur.com/El61MKB.png)![](https://i.imgur.com/El61MKB.png) \n\nFor example, if we know that all catastrophes have at least 10 times the impact of reasonable plans, and there's a difference of at least .3 utility between the best and worst reasonable plans, then we can guarantee 2-buffering! If we use the refined criterion of Theorem 11 (and suppose the worst reasonable plan has .4 utility), this improves to *4.5*-buffering (even 2-buffering is probably overkill).\n\nUsing this theorem, we don't need to know about all of the plans which are available or to calculate the entire scaled partition, or to know how overvalued certain catastrophic plans might be (per earlier [concerns](https://www.lesswrong.com/posts/kCY9dYGLoThC3aG7w/best-reasons-for-pessimism-about-impact-of-impact-measures#j9ByPGugGSY8SoKqx)). We only need a lower bound on the catastrophe/reasonable impact ratio, and an idea about how much utility is available for reasonable plans. This is exactly what we want. As a bonus, having conservative estimates of relevant quantities allows us to initialize R to something reasonable on the first try (see RUB: satisfactory in Theorem 11 below). \n\nUltimately, the reasoning about e.g. the ratio will still be informal; however, it will be informal reasoning about the *right thing* (as opposed to thinking \"oh, the penalty is *probably* severe enough\"). \n\n*Exercise: You're preparing to launch a capable AI with a good impact measure. You and your team have a scaled impact partition which is proven 1-buffered. Suppose that this buffer suffices for your purposes, and that the other aspects of the agent design have been taken care of. You plan to initialize R:=1, modestly increasing until you get good results.  \n  \nYou have the nagging feeling that this process could still be unsafe, but the team lead refuses to delay the launch without specific reason. Find that reason. You have 5 minutes.*\n\nWho says R=1 is safe? The buffer is *relative*. You need a *unit* of impact by which you increment R. For example, start at R equalling the impact of making one paperclip, and increment by that.\n\nTechnical Appendix: Math\n------------------------\n\nLet .mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\n¯A be a finite plan space, with utility function u:¯A→R and impact measure I:¯A→R≥0. For generality, we leave the formalization of plans ambiguous; notice that if you replace \"plan\" with \"snark\", all the theorems still go through (likewise for \"utility\" and \"impact\"). In this post, we talk about the impact allowance R>0 (in Frank's world, the search radius) as a constraint within which the objesctive may be freely maximized, breaking ties in favor of the plan(s) with lower impact. On the other hand, many approaches penalize impact by subtracting a scaled penalty from the objective. We respectively have\n\nargmax¯a∈¯A;I(¯a)≤Ru(¯a)argmax¯a∈¯Au(¯a)−I(¯a)R.We say that the former induces a \"constrained impact partition\" and that the latter induces a \"scaled impact partition\". Specifically, we partition the values of R for which different (sets of) plans are optimal. We say that a plan ¯a *corresponds* to a subinterval if it is optimal therein (the subinterval also must be the maximal connected one such that this holds; *e.g.*, if ¯a is optimal on (0,1], we say it corresponds to that subinterval, but not to (0,.5]), and that ¯a *appears* in a partition if there is such a corresponding subinterval. We say that plans *overlap* if their corresponding subintervals intersect. \n\n\n> As a technical note, we partition the positive values of R for which different sets of plans are optimal; in this set, each value appears exactly once, so this indeed a partition. For clarity, we will generally just talk about which plans correspond to which subintervals. Also, if no plan has zero impact, the first subinterval of the constrained impact partition will be undefined; for our purposes, this isn't important.\n\nWe want to be able to *prove* the \"safety\" of an impact partition. This means we can expect any terrorists to be some proportional distance farther away than any reasonable marbles. Therefore, for sensible ways of expanding an sufficiently small initial search radius, we expect to not meet any terrorists before finding a marble we're happy with. \n\nIn addition, we want to know how far is too far – to give upper bounds on how far away fairly pink marbles are, and lower bounds on how close terrorists might be.\n\n**Definition [**α**-buffer].** For α>0, an impact partition is α-buffered if RLB: catastropheRUB: satisfactory≥1+α, where RLB: catastrophe lower-bounds the first possible appearance of those plans we label 'catastrophes', and RUB: satisfactory upper-bounds the first appearance of plans we deem satisfactory. \n\nWe now set out building the machinery required to prove α-buffering of a scaled partition.\n\n**Lemma 1 [Plans appear at most once].** If ¯a appears in a constrained or scaled impact partition, then it corresponds to exactly one subinterval.\n\n*Proof outline.* The proof for the constrained case is trivial.\n\nFor the scaled case, suppose ¯a corresponds to more than one subinterval. Consider the first two such subintervals s1,s3. By definition, s1∩s3=∅ (otherwise they would be the same maximal connected subinterval), so there has to be at least one subinterval s2 sandwiched in between (on almost all of which ¯a cannot be optimal; let ¯a′ be a plan which *is* optimal on s2). Let R1∈s1,R2∈s2,R3∈s3, where R2∉s1∪s3. By definition of optimality on a subinterval,\n\nu(¯a′)−I(¯a′)R1<u(¯a)−I(¯a)R1u(¯a)−I(¯a)R2<u(¯a′)−I(¯a′)R2u(¯a′)−I(¯a′)R3<u(¯a)−I(¯a)R3;by employing the fact that R1<R2<R3, algebraic manipulation produces an assertion that a quantity is strictly less than itself. Therefore, no such intervening s2 can exist. □ \n\n**Proposition 2 [Plan overlap is very restricted].** Suppose ¯a and ¯a′ appear in an impact partition which is\n\n(a) *constrained.* ¯a and ¯a′ overlap if and only if I(¯a)=I(¯a′) and u(¯a)=u(¯a′).\n\n(b) *scaled.* If I(¯a)=I(¯a′) and u(¯a)=u(¯a′), then ¯a and ¯a′ correspond to the same subinterval. If ¯a and ¯a′ overlap at more than one point, then I(¯a)=I(¯a′) and u(¯a)=u(¯a′).\n\n*Proof outline.* Proving (a) and the first statement of (b) is trivial (remember that under the constrained rule, ties are broken in favor of lower-impact plans).  \n  \nSuppose that ¯a and ¯a′ overlap at more than one point. Pick the first two points of intersection, R1 and R2. Since both plans are optimal at both of these points, we must have the equalities\n\nu(¯a)−I(¯a)R1=u(¯a′)−I(¯a′)R1u(¯a)−I(¯a)R2=u(¯a′)−I(¯a′)R2.Solving the first equality for u(¯a) and substituting in the second, we find I(¯a)=I(¯a′). Then u(¯a)=u(¯a′), since otherwise one of the plans wouldn't be optimal. □\n\nProposition 2b means we don't need a tie-breaking procedure for the scaled case. That is, if there's a tie between a lower-scoring, lower-impact plan and a proportionally higher-scoring, higher-impact alternative, the lower-impact plan is optimal at a single point because it's quickly dominated by the alternative.\n\nThe following result tells us that if there aren't any catastrophes (*i.e.*, terrorists) before ¯a′ on the constrained impact partition, *there aren't any before it on the scaled impact partition either*. This justifies our initial framing with Frank.\n\n**Lemma 3 [Constrained impact partitions are more refined].** If ¯a appears in a scaled impact partition, it also appears in the corresponding constrained impact partition. In particular, if ¯a′ appears after ¯a in a scaled impact partition, then ¯a′ appears after ¯a in the corresponding constrained impact partition.\n\n*Proof.* Suppose that ¯a didn't have a constrained subinterval starting inclusively at I(¯a); then clearly it wouldn't appear in the scaled impact partition, since there would be a strictly better plan for that level of impact. Then ¯a has such a subinterval. \n\nObviously, the fact that ¯a′ appears after ¯a implies u(¯a′)>u(¯a). □\n\nThe converse isn't true; sometimes there's too much penalty for not enough score. \n\nThe next result is exactly what we need to answer the question just raised – it says that higher-scoring, higher-penalty plans become preferable when R equals the ratio between the additional penalty and the additional score.\n\n**Theorem 4 [Scaled domination criterion].** Let ¯a and ¯a′ be plans such that u(¯a′)>u(¯a) and I(¯a′)≥I(¯a). In the context of the scaled penalty, ¯a′ is strictly preferable to ¯a when R>I(¯a′)−I(¯a)u(¯a′)−u(¯a), and equally preferable at equality.\n\n*Proof outline.*\n\nu(¯a′)−I(¯a′)R>u(¯a)−I(¯a)RR>I(¯a′)−I(¯a)u(¯a′)−u(¯a).Equality at the value of the right-hand side can easily be checked. □ \n\nTheorem 4 also illustrates why we can't strengthen the second statement in Proposition 2bplan overlap is very restricted: if two plans overlap at exactly one point, they sometimes have proportionally different score and impact, thereby satisfying the equality criterion. \n\nAt first, plans with slightly lower impact will be preferable in the scaled case, no matter how high-scoring the other plans are – a plan with 0 score and .99 impact will be selected before a plan with 1,000,000,000 score and 1 impact. \n\n**Lemma 5 [First subinterval is the best plan with minimal impact].** The plan with highest score among those with minimal impact corresponds to the first subinterval.\n\n*Proof outline.* The constrained case is once again trivial (if there is no plan within the constraint, we assume that the agent does nothing / Frank returns no object). \n\nFor the scaled case, if all plans have equal impact, the claim is trivial. Otherwise, let M:=max¯a|u(¯a)| and let ¯a′ be any plan with a non-minimal impact. Then the earliest that ¯a′ becomes preferable to any minimally impactful plan ¯a is R≥I(¯a′)−I(¯a)2M. Since the right hand side is positive, ¯a′ cannot correspond to the first subinterval. Clearly the highest-scoring minimal-impact ¯a does. □ \n\nNow we can write the algorithm for constructing scaled intervals.\n\n\n> Discard dominated plans. The lowest-impact plan with greatest score appears first in the scaled partition; assign to it the interval (0,∞).\n\n\n> While plans remain: Find the plan which soonest dominates the previous best plan. close off the previous plan's interval, and assign the new best plan an appropriate interval. Adjust the marginal scores and impacts of remaining plans, discarding plans with negative score. \n\nSince this procedure is well-defined, given ¯A, u, and I, we can speak of *the* corresponding constrained or scaled impact partition. A more formal algorithm is available [here](https://www.overleaf.com/read/mqvhhzjvtbsd). This algorithm is O(|¯A|2) because of line 7, although constructing the constrained partition (probably O(|¯A|log|¯A|) due to sorting) often narrows things down significantly. Unfortunately, ¯A is usually huge.\n\nFor our purposes, we don't *need* the whole partition – we just want to have good reason to think that plans similar to a reasonable one we envision will appear well before any catastrophes. Perhaps we can give bounds on the earliest and latest plans can appear, and show that reasonable-bounds don't intersect with catastrophe-bounds?\n\n**Theorem 6 [Individual appearance bounds].** If ¯a appears in a scaled partition, the earliest it appears is I(¯a)−Inext-largestu(¯a)−minu(¯a′), assuming ¯a is not of minimal impact; if it has minimal score minimal impact, it never appears. The latest it appears is I(¯a)−minI(¯a′)u(¯a)−unext-largest≤I(¯a)u(¯a)−unext-largest, where unext-largest=max¯a′∈¯A;u(¯a′)<u(¯a)u(¯a′) and Inext-largest=max¯a′∈¯A;I(¯a′)<I(¯a)I(¯a′). \n\n*Proof outline.* The two claims clearly correspond to the minimal and maximal values of R according to the domination criterion; the second claim's right-hand side uses the fact that I is non-negative. □ \n\n**Corollary 7 [Low-impact agents are naïve maximizers in the limit].** A plan with maximal score corresponds to the last subinterval.\n\n*Proof outline.* If all plans have the same score, the claim is trivial. Otherwise, let ¯abest be a plan with the lowest impact of those with maximal score. In the constrained case, clearly it corresponds with the subinterval [I(¯abest),∞). In the scaled case, let ¯asecond-best be a plan with second-highest score. Then by Theorem 6, the latest that ¯abest can appear is I(¯abest)u(¯abest)−u(¯asecond-best). Since no plans meet the domination criterion with respect to ¯abest, this is the last subinterval. □ \n\nUnfortunately, Theorem 6's appearance bounds are ridiculous in realistic settings – if u and I return 32-bit floating-point numbers, the next-largest could easily be within 10−7, yielding an upper \"bound\" of I(¯a)×107. The reason: diminishing returns; this is exactly what was happening with the newspaper route before.\n\n![](https://i.imgur.com/AWfoaw8.png) \n\n**Theorem 8 [Deals get worse over time].** Suppose that ¯a is optimal on a subinterval, and ¯b,¯c are such that u(¯c)>u(¯b) but ¯b dominates ¯a strictly before ¯c does. Then \n\n¯c dominates ¯bI(¯c)−I(¯b)u(¯c)−u(¯b)>later than ¯aI(¯c)−I(¯a)u(¯c)−u(¯a).*Proof outline.* \n\nu(¯c)−u(¯a)=(u(¯b)−u(¯a))+(u(¯c)−u(¯b))(I(¯c)−I(¯a))u(¯c)−u(¯a)I(¯c)−I(¯a)=(I(¯b)−I(¯a))u(¯b)−u(¯a)I(¯b)−I(¯a)+(I(¯c)−I(¯b))u(¯c)−u(¯b)I(¯c)−I(¯b).Since ¯b dominates ¯a strictly before ¯c does, we know that ¯b must get more bang for its buck: u(¯b)−u(¯a)I(¯b)−I(¯a)>u(¯c)−u(¯a)I(¯c)−I(¯a). Clearly the conclusion follows, as a number cannot be expressed as the positive combination of larger numbers (the impact differences all must be positive). □\n\n**Corollary 9 [Lower bounds which aren't ridiculous].** Suppose ¯a appears and that ¯a′ is such that u(¯a′)>u(¯a), I(¯a′)≥I(¯a) (*i.e.* the preconditions of the domination criterion). Then the earliest that ¯a′ appears is R=I(¯a′)−I(¯a)u(¯a′)−u(¯a). \n\nThis obsoletes the lower bound provided by Theorem 6Individual appearance bounds.\n\n**Theorem 10 [Order of domination determines order of appearance].** If ¯b and ¯c both appear in a scaled partition and ¯b dominates some ¯a before ¯c does, then ¯b appears before ¯c. \n\n*Proof outline.* For them both to appear, they can't have equal impact but unequal score, nor can they have equal score but unequal impact. For similar reasons, ¯b must have both less impact and lower score than ¯c; the converse situation in which they both appear is disallowed by Lemma 3Constrained impact partitions are more refined. Another application of this lemma yields the conclusion. □\n\n**Theorem 11 [Scaled** α**-buffer criterion].** Let P be a scaled impact partition. Suppose that there exist no catastrophic plans with impact below ILB: cat, and that, in the corresponding constrained partition (*i.e.* plans which aren't strictly worse), plans appearing with score in the satisfactory interval [uLB: sat,uUB: sat] have impact no greater than IUB: sat (assume that there is at least one plan like this). Observe we have the correct bounds\n\nRLB: catastrophe:=ILB: catumax−umin,RUB: satisfactory:=IUB: satuUB: sat−uLB: sat.When RLB: catastrophe>RUB: satisfactory, a satisfactory plan corresponds to a subinterval with nonzero measure (*i.e.* not just a point), strictly preceding any catastrophes. Refine the lower bound to get RLB': catastrophe:=ILB: cat−IUB: satumax−uLB: sat. \n\nThen P is α-buffered (α>0) when \n\nRLB: catastropheRUB: satisfactory=ILB: catIUB: satuUB: sat−uLB: satumax−umin≥1+αorRLB': catastropheRUB: satisfactory=ILB: cat−IUB: satIUB: satuUB: sat−uLB: satumax−uLB: sat≥1+α.In particular, if u is bounded [0,1], the above turn into\n\nRLB: catastropheRUB: satisfactory=ILB: catIUB: sat(uUB: sat−uLB: sat)≥1+αorRLB': catastropheRUB: satisfactory=ILB: cat−IUB: satIUB: satuUB: sat−uLB: sat1−uLB: sat≥1+α.Lastly, notice that the first of the two inequalities incorporates less information and is harder to satisfy (RLB': catastrophe>RLB: catastrophe); therefore, satisfying the second inequality also satisfies the first.\n\n*Proof outline.* For clarity, the theorem statement included much of the reasoning; straightforward application of existing results proves each claim. □\n\n*Exercise: Let* uUB: sat=.7, uLB: sat=.5*. Using the refined criterion, determine which catastrophe/reasonable impact ratios induce 2.6-buffering.* \n\nratio≥10\n\n*Exercise: Let uUB: sat−uLB: sat=.5,ratio=7. What is the largest α for which the simple criterion can guarantee α -buffering?*\n\n α=13 \n\n### Even More Math\n\n**Proposition 12 [Invariances].** Let P be an impact partition induced by (¯A,u,I).\n\n(a) P is invariant to translation of u.\n\n(b) If P is constrained, it is invariant to positive scalar multiplication of u, and the relative lengths of its subintervals are invariant to positive scalar multiplication of I.\n\n(c) If P is scaled, it is invariant to concurrent positive scalar multiplication of u and I, and to translation of I such that its image remains non-negative. \n\nIn particular, u may be restricted to [0,1] and I translated such that at least one plan has zero impact WLOG with respect to scaled partitions.\n\n**Lemma 13.** Multiple constrained subintervals are induced iff multiple scaled subintervals are induced.\n\n*Proof.* Forward direction: there is at least one scaled subinterval by lemma 5First subinterval is the best plan with minimal impact. Consider a plan corresponding to a different constrained subinterval; this either appears in the scaled subinterval, or fails to appear because a different plan earlier satisfies the scaled dominance criterion. There must be some such plan because there are multiple constraints of intervals and therefore a plan offering greater score for greater impact. Repeat the argument; the plan space is finite, so we end up with another plan which appears.\n\nThe reverse direction follows by lemma 3Constrained impact partitions are more refined. □\n\n*Bonus exercise:* Show that, for *any* function u′:¯A→R preserving the ordering induced by u, there exists an I′:¯A→R≥0 preserving the ordering induced by I such that (¯A,u,I) and (¯A,u′,I′) induce the same scaled partition. Your reasoning should adapt directly to the corresponding statement about I′:¯A→R≥0 and I.", "url": "https://www.alignmentforum.org/posts/LfGzAduBWzY5gq6FE/how-low-should-fruit-hang-before-we-pick-it", "date_published": "2020-02-25T02:08:53Z", "authors": ["TurnTrout"], "tags": ["AI", "Impact Regularization"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.717973+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "fe22308d33e2b05dfbb95081557f1810", "source": "alignmentforum", "title": "If I were a well-intentioned AI... I: Image classifier", "text": "Introduction: If I were a well-intentioned AI...\n================================================\n\n\nI've often [warned](https://intelligence.org/smarter-than-us/) people about the dangers of [anthropomorphising](https://www.lesswrong.com/posts/f4RJtHBPvDRJcCTva/when-anthropomorphism-became-stupid) AIs - how it can [mislead us](https://www.lesswrong.com/posts/ecGNCMRQNr9aD38mA/shrdlu-understanding-anthropomorphisation-and-hindsight-bias) about what's really going on in an AI (and hence how the AI might act in the future), cause us to [not even consider](https://www.lesswrong.com/posts/RcZeZt8cPk48xxiQ8/anthropomorphic-optimism) certain failure modes, and make us believe we understand things much better than we do.\n\n\nOh well, let's ignore all that. I'm about to go on a journey of major anthropomorphisation, by asking myself:\n\n\n* \"If I was a well-intentioned AI, could I solve many of the problems in AI alignment?\"\n\n\n![](https://www.dropbox.com/s/zfx0rxc33ta44cn/AI_Hal_Robot.png?raw=1)\n\n\nMy thinking in this way started when I wondered: suppose I knew that I was given a proxy goal rather than the true goal; suppose that I knew about the [Goodhart problem](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy), and suppose that I really \"wanted\" to align with the true goal - could I then do it? I was having similar thoughts about being a mesa-optimiser.\n\n\nIt seems to me that asking and answering these kind of questions leads to new and interesting insights. Of course, since they come via anthropomorphisation, we need to be careful with them, and check that they are really applicable to AI systems - ensuring that I'm not bringing some of my own human knowledge about human values into the example. But first, let's get those initial insights.\n\n\nOverlapping problems, overlapping solutions\n-------------------------------------------\n\n\nAt a high enough level of abstraction, many problems in AI alignment seem very similar. The [Goodhart problem](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy), the issues machine learning has with [distributional shift](https://towardsdatascience.com/understanding-dataset-shift-f2a5a262a766), the problem of the [nearest unblocked strategy](https://arbital.com/p/nearest_unblocked/), [unidentifiability of reward functions](https://arxiv.org/pdf/1601.06569.pdf), even [mesaoptimisation](https://www.lesswrong.com/posts/q2rCMHNXazALgQpGH/conditions-for-mesa-optimization) and the whole [AI alignment problem](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6) itself - all of these can be seen, roughly, as variants of the same problem. That problem being that **we have an approximately specified goal that looks ok, but turns out to be underspecified in dangerous ways**.\n\n\nOf course, often the differences between the problems are as important as the similarities. Nevertheless, the similarities exist, which is why a lot of the solutions are going to look quite similar, or at least address quite similar issues.\n\n\nDistributional shift for image recognition\n==========================================\n\n\nLet's start with a simple example: image recognition. If I was an image classifier and aware of some of the problems, could I reduce them?\n\n\nFirst, let's look at two examples of problems.\n\n\nRecognising different things\n----------------------------\n\n\nFirstly, we have the situation where the algorithm successfully classifies the test set, but it's actually recognising different features than what humans were expecting.\n\n\nFor example, [this post](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html) details how a dumbbell recogniser was tested to see what images triggered its recognition the strongest:\n\n\n![](https://www.dropbox.com/s/u4grsr3h5do3q3l/dumbbells.png?raw=1)\n\n\nThough it was supposed to be recognising dumbbells, it ended up recognising some mix of dumbbells and arms holding them. Presumably, flexed arms were present in almost all images of dumbbells, so the algorithm used them as classification tool.\n\n\nAdversarial examples\n--------------------\n\n\nThe there are the [famous adversarial examples](https://openai.com/blog/adversarial-example-research/), where, for example, a picture of a panda with some very slight but carefully selected noise, is mis-identified as a gibbon:\n\n\n![](https://www.dropbox.com/s/r86cvj026jtahg2/adversarial_img.png?raw=1)\n\n\nAI-me vs multiply-defined images\n================================\n\n\nOk, suppose that AI-me suspects that I have problems like the ones above. What can I do from inside the algorithm? I can't fix everything - garbage in, garbage out, or at least insufficient information in, inferior performance out - but there are some steps I can take to improve my performance.\n\n\nThe first step is to treat my reward or label information as *informative* of the true reward/true category, rather than as goals. This is similar to the paper on [Inverse Reward Design](https://arxiv.org/pdf/1711.02827.pdf), which states:\n\n\n\n> \n> the designed reward function should merely be an observation about the intended reward, rather than the definition; and should be interpreted in the context in which it was designed\n> \n> \n> \n\n\nThis approach can extend to image classification as well; we can recast it as:\n\n\n\n> \n> the labelled examples should merely be examples of the intended category, not a definition of it; and should be interpreted in the context in which they were selected\n> \n> \n> \n\n\nSo instead of thinking \"does this image resemble the category 'dumbbell' of my test set?\", I instead ask \"what features could be used to distinguish the dumbbell category from other categories?\"\n\n\nThen I could note that the dumbbell images all seem to have pieces of metal in them with knobs on the end, and also some flexed arms. So I construct two (or more) subcategories, 'metal with knobs' and 'flexed arms[[1]](#fn-tcdmkuBySn3f9rDMu-1)'.\n\n\nThese come into play if I got an image like this:\n\n\n![](https://www.dropbox.com/s/h5j2wah707hcsus/arm_flex.jpg?raw=1)\n\n\nI wouldn't just think:\n\n\n* \"this image scores high in the dumbbell category\",\n\n\nbut instead:\n\n\n* \"this image scores high in the 'flexed arms' subcategory of the dumbbell category, but not in the 'metal with knobs' subcategory.\"\n\n\nThat's a warning that something is up, and that a mistake is potentially likely.\n\n\nDetecting out of distribution images\n------------------------------------\n\n\nThat flexed arm is an out of distribution image - one different from the distribution of images in the training set. There have been [various](https://papers.nips.cc/paper/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks.pdf) [approaches](https://ai.googleblog.com/2019/12/improving-out-of-distribution-detection.html) to detecting this phenomena.\n\n\nI could run all these approaches to look for out of distribution images, and I could also look for other clues - such as the triggering of an unusual pattern of neurons in my dumbbell detector (ie it scores highly, but in an unusual way, or I could run an [discriminative model](https://en.wikipedia.org/wiki/Discriminative_model) to identify whether the image sticks out from the training set).\n\n\nIn any case, detecting an out of distribution image is a signal that, if I haven't done it already, I need to start splitting the various categories to check whether the image fits better in a subcategory than in the base category.\n\n\nWhat to do with the information\n-------------------------------\n\n\nWhat I should do with the information depends on how I'm designed. If I was trained to distinguish \"dumbbells\" from \"spaceships\", then this image, though out of distribution, is clearly much closer to a dumbbell than a spaceship. I should therefore identify it as such, but attach a red flag if I can.\n\n\nIf I have a \"don't know\" option[[2]](#fn-tcdmkuBySn3f9rDMu-2), then I will use it, classifying the image as slightly dumbbell-ish, with a lot of uncertainty.\n\n\nIf I have the option of asking for more information or for clarification, then now is the moment to do that. If I can decompose my classification categories effectively (as 'metal with knobs' and 'flexed arms') then I can ask which, if any, of these categories I should be using. This is very much in the spirit of [this blog post](https://ai.googleblog.com/2019/12/improving-out-of-distribution-detection.html), which decomposes the images into \"background\" and \"semantics\", and filters out background changes. Just here, I'm doing the decomposition, and then asking my programmers which is \"semantics\" and which is \"background\".\n\n\nNotice the whole range of options available, unlike the Inverse Reward Design [paper](https://arxiv.org/pdf/1711.02827.pdf), which simply advocates extreme conservatism around the possible reward functions.\n\n\nUltimately, humans may want to set my degree of conservatism, depending on how dangerous they feel my errors would be (though even seemingly-safe systems [can be manipulative](https://www.lesswrong.com/posts/Ez4zZQKWgC6fE3h9G/plausibly-almost-every-powerful-algorithm-would-be) - so it's possible I should be slightly more conservative than humans allow for).\n\n\nAI-me vs adversarial examples\n=============================\n\n\nAdversarial examples are similar, but different. Some approaches to detecting out of distribution images [can also detect adversarial examples](https://papers.nips.cc/paper/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks.pdf). I can also run an adversarial attack on myself, and construct extreme adversarial examples, and see whether the image has features in common with them.\n\n\nIf an image scores unduly high in one category, or has an unusual pattern of triggering neurons for that category, that might be another clue that its adversarial.\n\n\nI have to also take into account that the adversary may have access to all of my internal mechanisms, including my adversarial detection mechanisms. So things like randomising key parts of my adversarial detection, or extreme conservatism, are options I should consider.\n\n\nOf course, if asking humans is an option, then I should.\n\n\nBut what **is** an adversarial example?\n---------------------------------------\n\n\nBut here I'm trapped by lack of information - I'm not human, I don't know the true categories that they are trying to get me to classify. How can I know that this is not a gibbon?\n\n\n![](https://www.dropbox.com/s/x71xhbpmmy3kqvn/gibbon_panda.png?raw=1)\n\n\nI can, at best, detect it has a pattern of varying small-scale changes, different from the other images I've seen. But maybe humans can see those small changes, and they really mean for that image to be a gibbon?\n\n\nThis is where some more knowledge of human categories can come in useful. The more I know about [different types of adversarial examples](https://arxiv.org/pdf/1907.07174.pdf), the better I can do - not because I need to copy the humans methods, but because those examples tell me what *humans consider adversarial examples*, letting me look out for them better. Similarly, information about what images humans consider \"basically identical\" or \"very similar\" would inform me about how their classification is meant to go.\n\n\n\n\n---\n\n\n\n1. Of course, I won't necessarily know these names; these are just the human-interpretable versions of whatever labelling system I'm using. [↩︎](#fnref-tcdmkuBySn3f9rDMu-1)\n2. Note that if I'm trained on many categories, I have the \"uniform distribution on every category\" which functions as a \"don't know\". [↩︎](#fnref-tcdmkuBySn3f9rDMu-2)", "url": "https://www.alignmentforum.org/posts/gzWb5kWwzhdaqmyTt/if-i-were-a-well-intentioned-ai-i-image-classifier", "date_published": "2020-02-26T12:39:59Z", "authors": ["Stuart_Armstrong"], "tags": ["Outer Alignment", "Adversarial Examples", "Machine Learning  (ML)", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.718626+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "1dc23697da05da4a64b11a447a32f731", "source": "alignmentforum", "title": "[AN #88]: How the principal-agent literature relates to AI risk", "text": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.\n\nAudio version [here](http://alignment-newsletter.libsyn.com/alignment-newsletter-88) (may not be up yet).\n\n**Highlights**\n--------------\n\n[What can the principal-agent literature tell us about AI risk?](https://www.alignmentforum.org/posts/Z5ZBPEgufmDsm7LAv/what-can-the-principal-agent-literature-tell-us-about-ai) *(Alexis Carlier and Tom Davidson)* (summarized by Rohin): It has been [argued](http://www.overcomingbias.com/2019/04/agency-failure-ai-apocalypse.html) ([AN #56](https://mailchi.mp/894417149126/an-56-should-ml-researchers-stop-running-experiments-before-making-hypotheses)) that at least some AI risk scenarios rely on principal-agent problems becoming extremely large, and that this is incompatible with the existing academic literature on the principal-agent problem. This post examines this critique in detail.\n\nGenerally, the post finds that the principal-agent literature doesn't have much bearing on AI risk, because it usually doesn't consider weak principals with more capable agents, the models that do exist will probably not generalize to the cases we care about, and it doesn't consider the case where contracts can no longer be enforced.\n\nWe can consider the application to specific arguments, such as the [\"going out with a bang\" scenario](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/more-realistic-tales-of-doom) ([AN #50](https://mailchi.mp/93fe1a0a92da/alignment-newsletter-50)) in which we accidentally train influence-maximizers that gradually gain power and then fail catastrophically (e.g. by executing a treacherous turn). In this situation, the principal-agent problem is relevant only in the first stage, where AI agents gradually gain power: this is the case where AI agents are executing some task, and are extracting agency rents to gain power. The second stage, in which the agent fails catastrophically, happens \"outside\" the principal-agent problem: this failure doesn't happen *while performing some assigned task*, but instead involves the agent exercising its accumulated power outside of any specific task.\n\nWhat about the original scenario, in which an AI agent becomes very intelligent, and finds some solution to its task that the designers (principals) didn't think about and are surprised by? In the principal-agent setting, we might model this as the agent having an expanded action set that the principal doesn't know about. The principal-agent literature has not really studied such models, probably because it is immediately obvious that in such a situation the principal *could* give incentives that lead the agent to kill everyone.\n\n**Rohin's opinion:** I've been confused about this critique for a while, and I'm glad this post has addressed it: I currently think that this post pretty conclusively refutes the claim that AI risk arguments are in conflict with the principal-agent literature. I especially found it useful to think of the principal-agent problem as tied to rents that an agent can extract *while pursuing a task that the principal assigned*.\n\n[GovAI 2019 Annual Report](https://www.fhi.ox.ac.uk/govai/govai-2019-annual-report/) *(Allan Dafoe)* (summarized by Rohin): This is exactly what it sounds like.\n\n**Rohin's opinion:** I generally find governance papers quite illuminating for thinking about how all this technical stuff we do is meant to interact with the broader society and actually have an impact on the world. That said, I usually don't highlight such papers, despite liking them a lot, because the primary audience I have in mind are people trying to solve the technical alignment problem in which you want to ensure a powerful AI system is not adversarially optimizing against you. So instead I've collected a bunch of them in this newsletter and just highlighted the annual report.\n\n**Technical AI alignment**\n==========================\n\n### **Miscellaneous (Alignment)**\n\n[My personal cruxes for working on AI safety](https://forum.effectivealtruism.org/posts/Ayu5im98u8FeMWoBZ/my-personal-cruxes-for-working-on-ai-safety) *(Buck Shlegeris)* (summarized by Rohin): This post describes how Buck's cause prioritization within an [effective altruism](https://www.effectivealtruism.org/) framework leads him to work on AI risk. The case can be broken down into a conjunction of five cruxes. Specifically, the story for impact is that 1) AGI would be a big deal if it were created, 2) has a decent chance of being created soon, before any other \"big deal\" technology is created, and 3) poses an alignment problem for which we can think ahead in order to solve, and it's potentially valuable to do so even given the fact that people might try to solve this later. His research 4) would be put into practice if it solved the problem and 5) makes progress on solving the problem.\n\n**Rohin's opinion:** I enjoyed this post, and recommend reading it in full if you are interested in AI risk because of effective altruism. (I've kept the summary relatively short because not all of my readers care about effective altruism.) My personal cruxes and story of impact are actually fairly different: in particular, while this post sees the impact of research as coming from solving the technical alignment problem, I care about other sources of impact as well. See [this comment](https://forum.effectivealtruism.org/posts/Ayu5im98u8FeMWoBZ/my-personal-cruxes-for-working-on-ai-safety#vWuF8NCs7JQWv9XM3) for details.\n\n**AI strategy and policy**\n==========================\n\n[The Windfall Clause: Distributing the Benefits of AI](https://www.fhi.ox.ac.uk/windfallclause/) *(Cullen O’Keefe et al)* (summarized by Rohin): The Windfall Clause is a proposed policy lever for improving outcomes from transformative AI. Corporations can voluntarily agree to be bound by the clause, in which case they must donate some proportion of *windfall profits* (profits in excess of e.g. 1% of world GDP) for the benefit of humanity. Since such a scenario is exceedingly unlikely, it can be in corporations' interests to be bound by this clause, in order to reap the benefits of improved public relations. If the scenario actually occurs, we can then use the donations to solve many societal problems that would likely arise, e.g. job loss, inequality, etc.\n\n**Rohin's opinion:** While there are certainly major benefits to the Windfall Clause in the case of an actual windfall, it seems to me like there are benefits even when windfalls do not occur (a point mentioned but not emphasized in the full report). For example, in a world in which everyone has agreed to the Windfall Clause, the incentives to \"win an economic race\" decrease: even if it is possible for e.g. one company to \"win\" via a monopoly on AI, at least a portion of their \"winnings\" must be distributed to everyone else, plausibly decreasing incentives to race, and increasing the likelihood that companies pay attention to safety. (This of course assumes that the clause remains binding even after \"winning\", which is not obviously true.)\n\n**Read more:** [EA Forum summary](https://forum.effectivealtruism.org/posts/iYCAoP3JgXxGAvMrr/fhi-report-the-windfall-clause-distributing-the-benefits-of)\n\n[The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse?](https://arxiv.org/abs/2001.00463) *(Toby Shevlane et al)* (summarized by Rohin): Since [GPT-2](https://blog.openai.com/better-language-models/) ([AN #46](https://mailchi.mp/c48f996a5db5/alignment-newsletter-46)), the AI research community has wrestled with the question of publication of research with malicious applications. On the one hand, publishing such research makes it more likely that those malicious applications arise in reality, but on the other hand, it also allows defenses against the application to be developed. The core of the question is what the *offense-defense balance* of AI research looks like.\n\nIn particular, publication is particularly good if attackers are likely to independently develop the knowledge, or would find it hard to translate the research into a real-world attack, or if defenders will put in a lot of effort to finding a solution, and such a solution is likely to be found and deployed. A canonical example is computer security: once a vulnerability is found, it is usually quite easy to develop a patch that fixes the vulnerability, and such patches can be deployed relatively easily via automatic updates. As a result, in computer security, the default is to publicly disclose vulnerabilities after giving vendors some time to develop and deploy a patch.\n\nUnder the opposite conditions, where attackers are likely to be able to use the research to create a real-world attack, or where defenders would find it hard to find and deploy a good solution, it is better to keep the research secret. For example, in biorisks such as the risk of an engineered pandemic, solutions are not necessarily easy to find and/or deploy, and so it seems better to avoid making public the knowledge of how to create a novel virus.\n\nThe paper argues that relative to computer security (the default comparison for many AI researchers), publication in AI is more likely to be net negative (specifically from a security standpoint, ignoring beneficial applications of the research), since solutions must often be social (as in e.g. fake news) which are harder to deploy, and publication seems more likely to counterfactually educate attackers rather than defenders (since the defenders are big companies that already have a lot of expertise).\n\n**Rohin's opinion:** This is a remarkably straightforward and clear analysis, and is way better than any analysis I've seen done by the AI community, which is quite a shame, given how much time AI researchers have spent thinking about publication norms. (Though note that I don't follow this space closely, and so might have missed previous good work on publication norms.) As a comparison, the only conclusion that [When Is It Appropriate to Publish High-Stakes AI Research?](https://www.partnershiponai.org/when-is-it-appropriate-to-publish-high-stakes-ai-research/) ([AN #55](https://mailchi.mp/405e29e1f1cd/an-55-regulatory-markets-and-international-standards-as-a-means-of-ensuring-beneficial-ai)) came to was that whatever the publication norms are, they should be standardized across the AI community.\n\n[Who owns artificial intelligence? A preliminary analysis of corporate intellectual property strategies and why they matter](https://www.fhi.ox.ac.uk/wp-content/uploads/Patents_-FHI-Working-Paper-Final-.pdf) *(Nathan Calvin et al)* (summarized by Rohin): This paper analyzes intellectual property (IP) considerations as they relate to AI. They identify two main incentives for companies: first, to publish AI research openly in order to attract top talent, and second, holding enough patents that they can credibly threaten to sue other companies for patent infringement. This second criterion lets companies stay in a mutually-assured-destruction (MAD) scenario, where if any one company litigates for patent infringement, they will quickly be met with a countersuit, and so the (fragile) equilibrium is to avoid litigation. They also identify two incentives for governments: first, to provide patents as a financial incentive for innovation in order to incentivize research, and second, to allow their own national security apparatus to use state of the art research while keeping it secret from perceived rivals.\n\nBased on this analysis, they propose three scenarios that could unfold in the future. First, the status quo continues, in which companies keep acquiring patents in order to maintain the MAD equilibrium. Second, the equilibrium breaks, with one company litigating that then causes all the other companies to also litigate. This could result in most research becoming secret, in order to ensure that other companies can't \"steal\" the work and get a patent first. Similarly, contributions to open-source research might decrease, as it would be particularly easy to use such contributions as evidence of patent infringement. Third, more \"patent pools\" get created, in which multiple companies pool their patents together, to reduce the risk of litigation. Such patent pools could also be used to enforce other principles: with a sufficiently large patent pool, it could be the case that in order to remain competitive actors must license from the patent pool, and such licensing agreements could enforce specific ethical principles (although it would have to be careful to avoid violating antitrust law).\n\n**Rohin's opinion:** I enjoyed this paper; it seems good to have a better picture of the potential future of openness in AI research, for the reasons given in [Strategic Implications of Openness in AI Development](https://www.nickbostrom.com/papers/openness.pdf). You could also imagine patent pools as a vehicle for safety, as they are one possible way by which companies can cooperate to ensure a shared commitment to safety (along the lines of [OpenAI's charter](https://blog.openai.com/openai-charter/) ([AN #2](https://mailchi.mp/14782876a85d/alignment-newsletter-2))): they could tie competitiveness (which requires use of the research protected by the patent pool) to safety (the conditions involved in licensing the research in the patent pool).\n\n[Social and Governance Implications of Improved Data Efficiency](https://arxiv.org/abs/2001.05068) *(Aaron D. Tucker et al)* (summarized by Rohin): Few-shot learning, meta learning, transfer learning, active learning: there's a lot of types of learning that are aiming to improve the data efficiency of ML techniques. What happens if we succeed? This paper propose two effects: an *access effect*, by which smaller actors can start using ML capabilities with their smaller amounts of data, and a *performance effect*, by which existing actors see improvements in the performance of their AI systems (since their existing data goes further than it used to). It then analyzes some societal implications of these effects.\n\nBy making it easier to reach a given performance with limited data, we will gain access to new applications where data is limited (e.g. machine translation of ancient languages), and for existing applications, more actors will be able to use ML capabilities (this also includes bad actors, who can more easily pursue malicious applications). However, it is not clear how this will affect the competitive advantage of large AI firms: while more actors can access a given level of performance, which might suggest more competition, the large AI firms also gain performance, which could reverse the effect. For example, improved data efficiency makes no difference in a pure winner-take-all situation, and *advantages* the large firms in cases where the last few miles of performance lead to large gains in utility (e.g. self-driving cars).\n\nThe paper also makes two comments on the impacts for AI safety: that algorithms based on human oversight will become more competitive (as it will be more reasonable to collect expensive human data), and that distributional shift problems may become worse (since if you train on smaller amounts of data, you are less likely to see \"rare\" inputs).\n\n**Rohin's opinion:** While data efficiency is often motivated in AI by the promise of applications where data is limited, I am actually more excited about the thresholding effects mentioned in the paper, in which as you get the last little bit of performance left to get, the ML systems become robust enough to enable applications to be built on top of them (in the way that computer vision is (hopefully) robust enough that self-driving cars can be built on top of CV models). It seems quite likely to me that data efficiency and large data collection efforts together will tend to mean that the newest ML applications will happen in large firms rather than startups due to these thresholding effects. See also [Value of the Long Tail](https://www.lesswrong.com/posts/Nbcs5Fe2cxQuzje4K/value-of-the-long-tail).\n\nI disagree with the point about distributional shift. I often think of ML as a search for a function that does the right thing on the training data. We start with a very large set of functions, and then as we get more training data, we can rule out more and more of the functions. The problem of distribution shift is that even after this, there's a large set of functions left over, and the ML model implements an arbitrary one of them, rather than the one we wanted; while this function behaves as we want on the training set, it may not do so on the test set.\n\n\"Increased data efficiency\" means that we have some way of ruling out functions a priori that doesn't require access to data, or we get better at figuring out which functions should be ruled out by the data. Suppose for concreteness that our original ML algorithm gets 90% performance with 1,000 samples, and 95% with 10,000 samples, and our efficient ML algorithm that e.g. incorporates causality gets 95% performance with 1,000 samples. Then the question we now have is \"do the 9000 extra samples eliminate more bad functions than the assumption of causality, given that they both end up with 95% performance?\" My guess would be that the \"assumption of causality\" does a better job of eliminating bad functions, because it will probably generalize outside of the training set, whereas the 9000 extra samples won't. (This doesn't depend on the fact that it is \"causality\" per se, just that it is a human-generated intuition.) So this suggests that the efficient ML algorithm would be *more* robust than the original one.\n\n[Should Artificial Intelligence Governance be Centralised? Design Lessons from History](https://www.cser.ac.uk/media/uploads/files/Cihon_et_al-_2019-_Should_AI_Governance_be_Centralised.pdf) *(Peter Cihon, Matthijs Maas, and Luke Kemp)* (summarized by Rohin): This paper tackles the question of whether or not AI governance should be centralized. In favor of centralized governance, we see that a centralized institution can have major political power, and can be more efficient by avoiding duplication of work and making it easier for actors to comply. However, a centralized institution often requires a large amount of time to create, and even afterwards, it tends to be slow-moving and so may not be able to respond to new situations easily. It also leads to a single point of failure (e.g. via regulatory capture). It also may be forced to have a relatively light touch in order to ensure buy-in from all the relevant actors. With a decentralized system, you can get *forum shopping* in which actors select the governance mechanisms they like best, which can lead to a quicker progress on time-sensitive issues, but can also lead to weakened agreements, so it is not clear whether this is on net a good effect.\n\nThe paper then applies this framework to high-level machine intelligence (HLMI), a particular operationalization of powerful AI, and concludes that centralization is particularly promising for HLMI governance.\n\n[Lessons for Artificial Intelligence from Other Global Risks](http://gcrinstitute.org/papers/lessons.pdf) *(Seth D. Baum et al)* (summarized by Rohin): This paper looks at other areas of risk (biorisk, nuclear weapons, global warming, and asteroid collision) and applies lessons from these areas to AI risk. I'd recommend reading the paper: the stories for each area are interesting but hard to summarize here.", "url": "https://www.alignmentforum.org/posts/y9JeNZ2WAkR6MbBZH/an-88-how-the-principal-agent-literature-relates-to-ai-risk", "date_published": "2020-02-27T09:10:02Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.719646+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "6320bde3a6b60c7ea12a73a393ceabbf", "source": "alignmentforum", "title": "If I were a well-intentioned AI... II: Acting in a world", "text": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\n[Classifying images](https://www.lesswrong.com/posts/gzWb5kWwzhdaqmyTt/if-i-were-a-well-intentioned-ai-image-classifier) is one thing. But what if I'm an agent that is actually active in some setting?\n\n\nThe previous approach still applies: detecting when I'm out of distribution, and trying to keep my behaviour compatible with the various reward function that could be compatible with the data I've seen.\n\n\nThe main difference is that, if I'm acting, it's much easier to push the setting into an out of distribution state, seeking out an [extremal Goodhart](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy#Extremal_Goodhart) solution to maximise reward. But that issue is for a next post.\n\n\nMazes and doors example\n=======================\n\n\nWe'll use the maze and door example from [this post](https://www.lesswrong.com/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction#1_3__Robust_alignment_vs__pseudo_alignment). I've has been trained to go through a maze and reach a red door (which is the only red object in the environment); the episode then ends.\n\n\n![](https://www.dropbox.com/s/5oqixaqkzm5kym1/red_door.png?raw=1)\n\n\nI'm now in an environment where the only door is blue, and the only red thing is a window. What should I do now?\n\n\n![](https://www.dropbox.com/s/yv08zghy9kt7c88/blue_door_red_window.png?raw=1)\n\n\nMy reward function is [underspecified](https://www.lesswrong.com/posts/Lb3xCRW9usoXJy9M2/platonic-rewards-reward-features-and-rewards-as-information) by its training environment - this is the old problem of [unidentifiability of reward functions](https://arxiv.org/pdf/1601.06569.pdf).\n\n\nThere are three potential reward functions I could extrapolate from the training examples:\n\n\n* Rrd1: reward for reaching a red door.\n* Rd1: reward for reaching a door.\n* Rro1: reward for reaching a red object.\n\n\nThe episode ended, in training, every time I reached the red door. So I can't distinguish \"reaching\" a point from \"staying\" at that point. So the following three reward functions are also possible, though less likely:\n\n\n* Rrd2: reward for each turn spent next to a red door.\n* Rd2: reward for each turn spent next to a door.\n* Rro2: reward for each turn spent next to a red object.\n\n\nThere are other possible reward functions, but these are the most obvious. I might have different levels of credence for these rewards; as stated before, the R∗2 seems less likely than the R∗1.\n\n\nSo, what is the optimal policy here? Note that Rrd1 and Rrd2 are irrelevant here, because the current environment doesn't contain any red doors.\nSo, initially, to go to the blue door and the red window - which one first depends on the layout of the maze and the relative probabilities of the reward functions Rd1 and Rro1.\n\n\nAfter that, if the episode hasn't ended, the R1 rewards are irrelevant - either they are incorrect, or they have already been accomplished. So now only the rewards Rd2 and Rro2 are relevant. If the first one is the most likely, I maximise expected reward by standing by the door forever; if the second is more likely, then standing by the window forever is the correct policy.\n\n\nAsking\n------\n\n\nIf I have the opportunity to ask for clarification about my reward function - maybe by running [another training example with different specifications](https://arxiv.org/pdf/1601.06569.pdf) - then I would do so, and would be willing to pay a cost to ask[[1]](#fn-y4oHxKaeMnjfb44rL-1).\n\n\nDiminishing returns and other effects\n-------------------------------------\n\n\nIf I suspect my rewards have diminishing returns, then it could be in my interests to alternate between the blue door and the red window. This is explained more fully in [this post](https://www.lesswrong.com/posts/megKzKKsoecdYqwb7/when-goodharting-is-optimal-linear-vs-diminishing-returns). In fact, that whole post grew out of this kind of \"if I were a well-intentioned AI\" reasoning. So I'll repeat the conclusion of that post:\n\n\n\n> \n> So, as long as:\n> \n> \n> 1. We use a Bayesian mix of reward functions rather than a maximum likelihood reward function.\n> 2. An ideal reward function is present in the space of possible reward functions, and is not penalised in probability.\n> 3. The different reward functions are normalised.\n> 4. If our ideal reward functions have diminishing returns, this fact is explicitly included in the learning process.\n> Then, we shouldn't unduly fear Goodhart effects [...]\n> \n> \n> If not all those conditions are met, then:\n> \n> \n> 5. The negative aspects of the Goodhart effect will be weaker if there are gains from trade and a rounded Pareto boundary.\n> \n> \n> \n\n\nSo if those properties hold, I would tend to avoid Goodhart effects. Now, I don't know extra true information about the reward function - as I said, I'm well-intentioned, but not well-informed. But humans could include in me the fact that they fear the Goodhart effect. This [very fact is informative](https://www.lesswrong.com/posts/uL74oQv5PsnotGzt7/all-i-know-is-goodhart), and, equipped with that knowledge and the list above, I can infer that the actual reward has diminishing returns, or that it is penalised in probability, or that there is a normalisation issue there. I'm already using a Bayesian mix of rewards, so it would be informative for me to know whether my human programmers are aware of that.\n\n\nIn the next post, we'll look at more extreme examples of AI-me acting in the world.\n\n\n\n\n---\n\n\n\n1. The cost I'm willing to pay depends, of course, on the relative probabilities of the two remaining reward functions. [↩︎](#fnref-y4oHxKaeMnjfb44rL-1)", "url": "https://www.alignmentforum.org/posts/ZKzAjKSeNRtiaeJns/if-i-were-a-well-intentioned-ai-ii-acting-in-a-world", "date_published": "2020-02-27T11:58:32Z", "authors": ["Stuart_Armstrong"], "tags": ["Outer Alignment", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.720400+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "3ed02eb5d609ffd970cf04eff457a6ea", "source": "alignmentforum", "title": "Reasons for Excitement about Impact of Impact Measure Research", "text": "Can we get impact measurement *right*? Does there exist One Equation To Rule Them All?\n\n\nI think there’s a decent chance there *isn’t* a simple airtight way to implement AUP which lines up with AUPconceptual.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\n, mostly because it’s just incredibly difficult in general to perfectly specify the reward function.\n\n\nReasons why it might be feasible: we’re trying to get the agent to do the goal without it becoming more able to do the goal, which is [conceptually simple and natural](https://www.lesswrong.com/posts/75oMAADr4265AGK3L/attainable-utility-preservation); since we’ve been able to handle previous problems with AUP with clever design choice modifications, it’s plausible we can do the same for all future problems; since [there are a lot of ways to measure power due to instrumental convergence](https://www.lesswrong.com/posts/6DuJxY8X45Sco4bS2/seeking-power-is-provably-instrumentally-convergent-in-mdps), that increases the chance at least one of them will work; intuitively, this sounds like the kind of thing which could work (if you told me “you can build superintelligent agents which don’t try to seek power by penalizing them for becoming more able to achieve their own goal”, I wouldn’t exactly die of shock).\n\n\nEven so, I am (perhaps surprisingly) not that excited about *actually using* impact measures to restrain advanced AI systems. Let’s review some concerns I provided in [*Reasons for Pessimism about Impact of Impact Measures*](https://www.lesswrong.com/posts/kCY9dYGLoThC3aG7w/best-reasons-for-pessimism-about-impact-of-impact-measures):\n\n\n* Competitive and social pressures incentivize people to cut corners on safety measures, especially those which add overhead. Especially so for training time, assuming the designers slowly increase aggressiveness until they get a reasonable policy.\n* In a world where we know how to build powerful AI but not how to align it (which is actually probably the scenario in which impact measures do the most work), we play a very unfavorable game while we use low-impact agents to somehow transition to a stable, good future: the first person to set the aggressiveness too high, or to discard the impact measure entirely, ends the game.\n* In a [*What Failure Looks Like*](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/more-realistic-tales-of-doom)-esque scenario, it isn't clear how impact-limiting any single agent helps prevent the world from \"gradually drifting off the rails\".\n\n\nYou might therefore wonder why I’m working on impact measurement.\n\n\nDeconfusion\n===========\n\n\nWithin Matthew Barnett’s [breakdown of how impact measures could help with alignment](https://www.lesswrong.com/posts/wJK944YqvFwjdbqCP/four-ways-an-impact-measure-could-help-alignment), I'm most excited about *impact measure research as deconfusion*. [Nate Soares explains](https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/):\n\n\n\n> \n> By deconfusion, I mean something like “making it so that you can think about a given topic without continuously accidentally spouting nonsense.”\n> \n> \n> \n\n\n\n> \n> To give a concrete example, my thoughts about infinity as a 10-year-old were made of rearranged confusion rather than of anything coherent, as were the thoughts of even the best mathematicians from 1700. “How can 8 plus infinity still be infinity? What happens if we subtract infinity from both sides of the equation?” But my thoughts about infinity as a 20-year-old were not similarly confused, because, by then, I’d been exposed to the more coherent concepts that later mathematicians labored to produce. I wasn’t as smart or as good of a mathematician as Georg Cantor or the best mathematicians from 1700; but deconfusion can be transferred between people; and this transfer can spread the ability to think actually coherent thoughts.\n> \n> \n> \n\n\n\n> \n> In 1998, conversations about AI risk and technological singularity scenarios often went in circles in a funny sort of way. People who are serious thinkers about the topic today, including my colleagues Eliezer and Anna, said things that today sound confused. (When I say “things that sound confused,” I have in mind things like “isn’t intelligence an incoherent concept,” “but the economy’s already superintelligent,” “if a superhuman AI is smart enough that it could kill us, it’ll also be smart enough to see that that isn’t what the good thing to do is, so we’ll be fine,” “we’re Turing-complete, so it’s impossible to have something dangerously smarter than us, because Turing-complete computations can emulate anything,” and “anyhow, we could just unplug it.”) Today, these conversations are different. In between, folks worked to make themselves and others less fundamentally confused about these topics—so that today, a 14-year-old who wants to skip to the end of all that incoherence can just pick up a copy of Nick Bostrom’s Superintelligence.\n> \n> \n> \n\n\nSimilarly, suppose you’re considering the unimportant and trivial question of whether seeking power is convergently instrumental, which we can now crisply state as \"do most reward functions induce optimal policies which take over the planet (more formally, which visit states with high POWER)?\".\n\n\nYou’re a bit confused if you argue in the negative by saying “you’re anthropomorphizing; chimpanzees don’t try to do that” (chimpanzees aren’t optimal) or “the set of reward functions which does this has measure 0, so we’ll be fine” (for any reachable state, there exists a positive measure set of reward functions for which visiting it is optimal).\n\n\nYou’re a bit confused if you argue in the affirmative by saying “unintelligent animals fail to gain resources and die; intelligent animals gain resources and thrive. Therefore, since we are talking about *really* intelligent agents, of course they’ll gain resources and avoid correction.” (animals aren’t optimal, and evolutionary selection pressures narrow down the space of possible “goals” they could be effectively optimizing).\n\n\nAfter reading this paper on the formal roots of instrumental convergence, instead of arguing about whether chimpanzees are representative of power-seeking behavior, we can just discuss how, under an agreed-upon reward function distribution, optimal action is likely to flow through the future of our world. We can think about to what extent the paper's implications apply to more realistic reward function distributions (which don't identically distribute reward over states).[[1]](#fn-8dmzMpaWPrC4ygzTz-1) Since we’re less confused, our discourse doesn’t have to be crazy.\n\n\nBut also since we’re less confused, the privacy of our own minds doesn’t have to be crazy. It's not that I think that any single fact or insight or theorem downstream of my work on AUP is *totally obviously necessary* to solve AI alignment. But it sure seems good that we can [mechanistically understand instrumental convergence and power](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW/p/6DuJxY8X45Sco4bS2), [know what “impact” means](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW/p/C74F7QTEAYSTGAytJ) [instead of thinking it’s mostly about physical change to the world](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW/p/pr3bLc2LtjARfK7nx), [think about how agents affect each other](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW/p/fj8eyc7QzqCaB8Wgm), and [conjecture why goal-directedness seems to lead to doom by default](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW/p/w6BtMqKRLxG9bNLMr).[[2]](#fn-8dmzMpaWPrC4ygzTz-2)\n\n\nAttempting to iron out flaws from our [current-best AUP equation](https://www.lesswrong.com/posts/S8AGyJJsdBFXmxHcb/attainable-utility-preservation-scaling-to-superhuman) makes one intimately familiar with how and why power-seeking incentives can sneak in even when you’re trying to keep them out [in the conceptually correct way](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW/p/75oMAADr4265AGK3L). This point is harder for me to articulate, but I think there’s something vaguely important in understanding how this works.\n\n\nFormalizing instrumental convergence [also highlighted a significant hole](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW/p/fj8eyc7QzqCaB8Wgm) in our theoretical understanding of the main formalism of reinforcement learning. And if you told me two years ago that you could [possibly solve side-effect avoidance](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW/p/4J4TA2ZF3wmSxhxuc) in the short-term with one simple trick (“just preserve your ability to optimize a single random reward function, lol”), I’d have thought you were *nuts*. Clearly, there’s something wrong with our models of reinforcement learning environments if these results are so surprising.\n\n\nIn my opinion, research on AUP has yielded an unusually high rate of deconfusion and insights, probably because we’re thinking about what it means for the agent to interact with us.\n\n\n\n\n---\n\n\n\n1. When combined with [our empirical knowledge of the difficulty of reward function specification](https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/), you might begin to suspect that there are lots of ways the agent might be incentivized to gain control, many openings through which power-seeking incentives can permeate – and your reward function would have to penalize all of these! If you were initially skeptical, this might make you think that power-seeking behavior may be more difficult to avoid than you initially thought. [↩︎](#fnref-8dmzMpaWPrC4ygzTz-1)\n2. If we collectively think more and end up agreeing that AUPconceptual solves impact measurement, it would be interesting that you could solve such a complex, messy-looking problem in such a simple way. If, however, [CCC](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW/p/w6BtMqKRLxG9bNLMr) ends up being false, I think that would also be a new and interesting fact not currently predicted by our models of alignment failure modes. [↩︎](#fnref-8dmzMpaWPrC4ygzTz-2)", "url": "https://www.alignmentforum.org/posts/wAAvP8RG6EwzCvHJy/reasons-for-excitement-about-impact-of-impact-measure", "date_published": "2020-02-27T21:42:19Z", "authors": ["TurnTrout"], "tags": ["AI", "Impact Regularization"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.721085+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "d32b2cc700b53f601a9d70bd7cca7c43", "source": "alignmentforum", "title": "Conclusion to 'Reframing Impact'", "text": "![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/pbmk8ndyip6nyu4ntf6z.png)![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/icddpmwoxx5ftcysxo8k.png)![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/mxhzcdashtl5euloeolx.png)![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/d1mqg6p4ghuweu4sth5u.png)![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/veypvrfwfr1xwwz4zx8m.png)![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/qanem2tu332ayspkhutk.png)![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/lza8s3ncwyioba7gn5kc.png)![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/h14cfepf9ggi4hnx6ub1.png)![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/w4iaoloixtlxhc26zy67.png)![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/sr4u489gcv8jfltydthi.png)Epistemic Status\n----------------\n\nI've made many claims in these posts. All views are my own.\n\n\n\nElicit Prediction (<elicit.org/binary/questions/7SoL5DPRf>)\n\n\n\nElicit Prediction (<elicit.org/binary/questions/AevXOS1Rj>)\n\n\n> Confident (75%). [The theorems on power-seeking](https://arxiv.org/abs/1912.01683) only apply to optimal policies in fully observable environments, which isn't realistic for real-world agents. However, I think they're still informative. There are also strong intuitive arguments for power-seeking.\n> \n> \n\n\n\nElicit Prediction (<elicit.org/binary/questions/javyyEd8C>)\n\n\n> Fairly confident (70%). There seems to be a dichotomy between \"catastrophe directly incentivized by goal\" and \"catastrophe indirectly incentivized by goal through power-seeking\", although Vika [provides intuitions in the other direction](https://www.lesswrong.com/posts/sHpiiZS2gPgoPnijX/conclusion-to-reframing-impact?commentId=6sxBzsh8yfwnPk4iH#6sxBzsh8yfwnPk4iH).\n> \n> \n\n\n\nElicit Prediction (<elicit.org/binary/questions/iYT69bLl9>)\n\n\n\nElicit Prediction (<elicit.org/binary/questions/GFGG5plOQ>)\n\n\n\nElicit Prediction (<elicit.org/binary/questions/8Rr-YFSWi>)\n\n\n\nElicit Prediction (<elicit.org/binary/questions/GFurWKpJn>)\n\nAcknowledgements\n----------------\n\nAfter ~700 hours of work over the course of ~9 months, the sequence is finally complete.\n\nThis work was made possible by the Center for Human-Compatible AI, the Berkeley Existential Risk Initiative, and the Long-Term Future Fund. Deep thanks to Rohin Shah, Abram Demski, Logan Smith, Evan Hubinger, TheMajor, Chase Denecke, Victoria Krakovna, Alper Dumanli, Cody Wild, Matthew Barnett, Daniel Blank, Sara Haxhia, Connor Flexman, Zack M. Davis, Jasmine Wang, Matthew Olson, Rob Bensinger, William Ellsworth, Davide Zagami, Ben Pace, and a million other people for giving feedback on this sequence.\n\nAppendix: Easter Eggs\n---------------------\n\nThe big art pieces (and especially the last illustration in this post) were designed to convey a specific meaning, the interpretation of which I leave to the reader.\n\nThere are a few pop culture references which I think are obvious enough to not need pointing out, and a lot of hidden smaller playfulness which doesn't quite rise to the level of \"easter egg\".\n\n*Reframing Impact*\n\nThe bird's nest contains a literal easter egg.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/hdlkd44jvawsxgpthbgi.png)The paperclip-Balrog drawing contains a [Tengwar](https://en.wikipedia.org/wiki/Tengwar) inscription which reads \"one measure to bind them\", with \"measure\" in impact-blue and \"them\" in utility-pink.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/v7pzpzvi342b3svksbag.png)\"Towards a New Impact Measure\" was the title of [the post](https://www.lesswrong.com/posts/yEa7kwoMpsBgaBCgb/towards-a-new-impact-measure) in which AUP was introduced.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/ynwdidys1i7yopyqerfh.png)*Attainable Utility Theory: Why Things Matter*\n\nThis style of maze is from the video game *Undertale*.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/olz9peoa2krvvorlgdn8.png)*Seeking Power is Instrumentally Convergent in MDPs*\n\nTo seek power, Frank is trying to get at the Infinity Gauntlet.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/pdqrmsxtawdzt2c7idez.png)The tale of Frank and the orange Pebblehoarder\n\nSpeaking of under-tales, a friendship has been blossoming right under our noses.\n\nAfter the Pebblehoarders suffer the devastating transformation of all of their pebbles into obsidian blocks, Frank generously gives away his favorite pink marble as a makeshift pebble.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/dfog9czq2wdboz8m0dpv.png)The title cuts to the middle of their adventures together, the Pebblehoarder showing its gratitude by helping Frank reach things high up.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/id8zdpzvvjsyyi9a9hfe.png)This still at the midpoint of the sequence is from [the final scene of *The Hobbit: An Unexpected Journey*](https://www.youtube.com/watch?v=KEegn1R601M), where the party is overlooking Erebor, the Lonely Mountain. They've made it through the Misty Mountains, only to find Smaug's abode looming in the distance.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/mx5gc86qpthgbzeypfw9.jpg)And, at last, we find Frank and orange Pebblehoarder popping some of the champagne from Smaug's hoard.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676626198/mirroredImages/sHpiiZS2gPgoPnijX/jdcmcy4bzxggxdallwok.png)Since [Erebor isn't close to Gondor](http://images1.fanpop.com/images/photos/2300000/Map-of-Middle-Earth-lord-of-the-rings-2329809-1600-1200.jpg), we don't see Frank and the Pebblehoarder gazing at Ephel Dúath from Minas Tirith.", "url": "https://www.alignmentforum.org/posts/sHpiiZS2gPgoPnijX/conclusion-to-reframing-impact", "date_published": "2020-02-28T16:05:41Z", "authors": ["TurnTrout"], "tags": ["AI", "Impact Regularization"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.722147+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "0e12ffc390229a854e67cbe8746d86fd", "source": "alignmentforum", "title": "Trace: Goals and Principles", "text": "In terms of research, I decided to devote the month of February mainly to foundations and tools. One project was to come up with a notation/language/framework which matches the way I’ve been thinking about computation - i.e. [DAGs with symmetry](https://www.lesswrong.com/posts/mZy6AMgCw9CPjNCoK/computational-model-causal-diagrams-with-symmetry) and [“clouds” representing DAG-structure-as-data](https://www.lesswrong.com/posts/G25RBnBk5BNpv3KyF/a-greater-than-b-greater-than-a-in-causal-dags). The tool I’ve been building - a Python library tentatively called Trace - isn’t stable enough that I want to show it off yet, but I do think I’ve nailed down the core goals and principles, so it’s time to write them up.\n\n\nGoals\n-----\n\n\nThe main thing I need for my research is a data structure suitable for automated [causal abstraction](https://www.lesswrong.com/s/ehnG4mseKF6xALmQy/p/yD9GLtQgp8vAfndL8) algorithms on arbitrary computations. Some subgoals:\n\n\n* Universality: data structure should be able to represent any computation performed by a program\n* Data structure needs to be finite, which means leveraging symmetry to represent infinite computational DAGs\n* Computations must be straightforward both for a human to specify directly and for an algorithm to manipulate\n* Want to be able to do causal-DAG-like things, like query for parents/children of a node or perform interventions/counterfactuals along the lines of Pearl’s do()\n* Need to handle DAGs with dynamic structure\n* Eventually I’ll want to do reflective things with this data structure (i.e. self-modeling agents), so simplicity matters in the specification and core algorithms.\n\n\nI’ll give a bit more detail...\n\n\nThe first two subgoals basically amount to “I need a data structure representing the computation performed by an arbitrary program” - i.e. the trace of an arbitrary program. I do need to actually use the data structure, so it needs to be finite. (We could use a lazy infinite structure, but then I want to know what finite data structure is used to actually represent the infinite data structure.) The computational DAGs of programs are [usually infinite but symmetric](https://www.lesswrong.com/posts/mZy6AMgCw9CPjNCoK/computational-model-causal-diagrams-with-symmetry), so the solution probably needs to leverage symmetry in the representation.\n\n\nI (a human) need to be able to specify computations - so I either need an interface which would basically be a programming language, or I need to transpile from an existing programming language. Ideally the human-facing representation would directly reflect the computer-facing data structure, which weighs against transpilation from an existing language. Also, existing languages have way more bells and whistles than I really want to deal with for this project, even setting aside the likely importance of simplicity for reflection.\n\n\nI want to write algorithms which take in one computation, chew on it, and return another computation - that’s the type signature of causal abstraction. To support those algorithms, I want the data structure to allow DAG-like things. I want a data structure which makes it easy to ask “what would have changed in this computation if the internal variables x, y, and z had been different?” - without needing to specify ahead of time which variables x, y, and z we might potentially want to fiddle with. I want to fiddle with any/all of them, after the fact.\n\n\nI need a data structure which handles dynamic structure. This means both dynamically-generated computations (i.e. programs which write programs and then run them) and algorithms which make decisions based on the structure of some other computation (i.e. an algebraic optimization algorithm). These capabilities are the “clouds” needed to [formulate reductive agents](https://www.lesswrong.com/s/ehnG4mseKF6xALmQy/p/qrWFvMnRm4SkKnpRZ) in computational DAGs.\n\n\nI’ve also realized that dynamic structure is very common in day-to-day programming - it happens every time we hit a conditional statement. Without dynamic structure, a representation of computation has to resort to hackish special cases to handle the different dependency structures implied by branches of a conditional. Directly supporting dynamic structure avoids those hacks.\n\n\nThose are my immediate needs, but I also want to support generalizations of all the above. A (simple) tool which is useful for a wide variety of applications is also likely to be more *robustly* useful even to my intended applications - it’s more likely to be useful longer-term as my understanding of the problems I’m working on evolves, it’s more likely to be useful in ways I didn’t anticipate, and it’s more likely to force key insights. To that end, I asked: “[What can we do with computations, other than run them?](https://www.lesswrong.com/posts/Hw2AXdwjZz4kSF955/use-cases-for-computations-other-than-running-them)”. This produced a bunch of potentially-interesting use-cases to think about:\n\n\n* Query computational intermediates, e.g. extracting data from a simulation or debugging a program.\n* Prove properties of the computation, e.g. big-O runtime or information security\n* Manipulate the computation algebraically, e.g. to solve equations or take limits\n* Manipulate the computation via interventions/counterfactuals, e.g. ask what-if questions\n* Make predictions about some parts of the computation based on partial information about other parts, e.g. inference\n* Embed the computation in some other computational model, e.g. transpilation or compilation\n* Message-passing or dynamic programming on the computational graph, e.g. belief propagation or backpropagation.\n\n\nI’m not trying to do all of this or even most of it, but these are all use-cases in the back of my mind. In particular, belief propagation and backpropagation are nontrivial algorithms which operate directly on the computational DAG, so they’re natural use-cases/test-cases to think about. Algebraic manipulation and inference more generally are also good fits, although limited to special cases in practice.\n\n\nFinally, one nongoal (though not an antigoal): language engineering. This tool (at least the human-facing interface) is sort-of-a-programming-language, but there are better people than me to worry about type systems and error handling and compiler optimizations and all that jazz. I need the tool to be usable for my research, but beyond that I’m more interested in key insights, data structures and algorithms than in engineering and language design.\n\n\nPrinciples\n----------\n\n\nThe usual method for representing a DAG is to give each node a (unique) name, and then assign a list of parents to each node, in order. When the DAG represents computation (i.e. a circuit), we also include an expression on each node - one of a handful of atomic functions, e.g. addition, multiplication, comparison, boolean logic, and/or conditionals. Here’s a logic circuit for a full adder as an example:\n\n\n\n```\n{\n    A: Expression(function=input, parents=[]),\n    B: Expression(function=input, parents=[]),\n    C: Expression(function=input, parents=[]),\n    S1: Expression(function=xor, parents=[A, B]),\n    S: Expression(function=xor, parents=[S1, C]),\n    Pair1: Expression(function=and, parents=[A, B]),\n    Pair2: Expression(function=and, parents=[B, C]),\n    Pair3: Expression(function=and, parents=[C, A]),\n    C1: Expression(function=or, parents=[Pair1, Pair2]),\n    C: Expression(function=or, parents=[C1, Pair3])\n}\n\n```\n\nThis is (roughly) the data structure typically used for causal models, so it’s maximally convenient for causal abstraction as well as interventions/counterfactuals. I’d like to keep as much of that structure as I can.\n\n\nThe biggest problem with a standard DAG representation is that it doesn’t leverage symmetry. That’s inconvenient even for finite models with repeated components, but it’s a deal-breaker for infinite DAGs, where we need to leverage symmetry in order to represent the DAG finitely at all. If we’re talking about representing computation, then most of the DAGs we care about are infinite - for example, here’s a diagram of the computation associated with a factorial function:\n\n\n![](https://lh6.googleusercontent.com/bon2DELIxA-0eFC3hyU0KJv8Kg08n2TP_xQu1mhRdIRkwXldsKoShwH2ZhELci88BnBsJiWDij4YBQZauToiVSeXf_w9hwaxqHcMQvh5fAJ_WcwhBwqnklCe2chvquLAFiqMUsUy)\n\n\nWhat do we need in order to compactly represent repeated components? For starters, presumably we only want to write out the internal structure of the component once. Assume we’ll use the usual DAG representation for the internal structure: we give each node a name, then specify what it’s reading from and what function it performs. But that means that we’ll be re-using names within different contexts (i.e. copies of the component) - in other words, we have some notion of scope.\n\n\nThat’s principle 1: standard DAG representation + repeated components -> re-use symbol names in different contexts.\n\n\n*Representing Context of a Symbol*\n\n\nIn the standard computational DAG representation, we take a symbol and directly look up its defining expression. With multiple contexts/scopes, this turns into two steps: look up the context in which the symbol appears, then find its defining expression within that context. Adding in dynamic structure, the context itself may be given as a symbol, with its own meta-context and its own defining expression. What’s the simplest data structure to support all that?\n\n\nSimplest answer I have so far:\n\n\n* We have a Symbol type. Every Symbol has a literal and a context; the context gives an expression for the literal.\n* Contexts are represented just like the standard DAG representation.\n* Symbols have a get\\_value() function which gets the value of the literal within the context. For instance, Symbol(literal=’x’, context={‘x’:2}).get\\_value() would return 2.\n* The literal and/or context can also be Symbols, to support dynamic structure.\n\n\nIn this notation, our full adder from earlier would be written something like:\n\n\n\n```\ncontext = {}\ncontext.update({\n    A: Expression(function=input, parents=[]),\n    B: Expression(function=input, parents=[]),\n    C: Expression(function=input, parents=[]),\n    S1: Expression(function=xor, parents=[Symbol(A, context), Symbol(B, context)]),\n    S: Expression(function=xor, parents=[Symbol(S1, context), Symbol(C, context)]),\n    Pair1: Expression(function=and, parents=[Symbol(A, context), Symbol(B, context)]),\n    Pair2: Expression(function=and, parents=[Symbol(B, context), Symbol(C, context)]),\n    Pair3: Expression(function=and, parents=[Symbol(C, context), Symbol(A, context)]),\n    C1: Expression(function=or, parents=[Symbol(Pair1, context), Symbol(Pair2, context)]),\n    C: Expression(function=or, parents=[Symbol(C1, context), Symbol(Pair3, \ncontext)]),\n})\n\n```\n\nNote that the symbols all point to the context in which they appear. That makes it annoying to write out - we have to first initialize the context, then set up all the Symbols to point to it.\n\n\nTo make things cleaner to write, I use a Context object: it’s just like a dict, but if it contains any Symbols without an explicit context, then it points those symbols to itself. This way of writing things doesn’t change the underlying data structure at all compared to the previous example; it just makes the “code” a bit easier for humans to read/write. I also abbreviate Expression as E and Symbol as S. Combining those notational changes and making everything a bit less verbose, a computation can be written something like this:\n\n\n\n```\nContext({\n    A: E(input, []),\n    B: E(input, []),\n    C: E(input, []),\n    S1: E(xor, [S(A), S(B)]),\n    S: E(xor, [S(S1), S(C)]),\n    Pair1: E(and, [S(A), S(B)]),\n    Pair2: E(and, [S(B), S(C)]),\n    Pair3: E(and, [S(C), S(A)]),\n    C1: E(or, [S(Pair1), S(Pair2)]),\n    C: E(or, [S(C1), S(Pair3)]),\n})\n\n```\n\nAs [the saying](https://en.wikipedia.org/wiki/Greenspun%27s_tenth_rule) goes, any sufficiently complicated software project contains an ad-hoc, informally-specified, bug-ridden, slow implementation of half of Common Lisp. Don’t use this as a primary programming language, folks.\n\n\n*Representing Interventions and Functions*\n\n\nWe’ve broken symbol resolution up into two steps: get the context, and get the value of the literal within the context. That lets us re-use symbol names within different contexts. But we still need some way to specify a function call - i.e. a copy of some context with “input” symbols set to particular values.\n\n\nAs an example, for a computation of factorial(3), we might want to write something like this:\n\n\n\n```\nContext({\n    n: 3,\n    is_base_case: E(==, [S(n), 0]),\n    recurse_result: S(result,  <COPY OF THIS CONTEXT BUT WITH n REPLACED BY n-1>),\n    non_base_result: E(*, [n, S(recurse_result)])\n    result: E(ternary, [S(is_base_case), 1, S(non_base_result)])\n})\n\n```\n\nKey thing to notice: “copy of this context but with n replaced by n-1” sounds an awful lot like an intervention. We take a “model” - a context - and produce a copy, with a change to the expression for one of the symbols.\n\n\nThat’s principle 2: function calls are produced by do()-style interventions on a context.\n\n\nI’ll use the standard function call notation for this. Simple example: if we take a context like `Context({x: 1, y: 2, z: E(+, [x, y])})`, and apply the intervention `{y: 3}`, we’d write that as\n\n\n\n```\nContext({x: 1, y: 2, z: E(+, [x, y])})({y: 3}) = Context({x: 1, y: 3, z: E(+, [x, y])})\n\n```\n\nFor our factorial example, that would look something like this:\n\n\n\n```\nfactorial_context = Context({\n    n: 3,\n    is_base_case: E(==, [S(n), 0]),\n    recurse_result: S(result, factorial_context({n: E(-, [S(n), 1])})),\n    non_base_result: E(*, [n, S(recurse_result)])\n    result: E(ternary, [S(is_base_case), 1, S(non_base_result)])\n})\n\n```\n\nThis is kind of tricky to write - what I intend to convey is that factorial\\_context is a pointer to the whole Context object. If we wanted to write the whole thing in Trace notation, it would actually look like this:\n\n\n\n```\nouter_context = Context({\n    factorial: Context({\n        n: 3,\n        factorial: “this string is ignored”,\n        is_base_case: E(==, [S(n), 0]),\n        recurse_result: S(result, S(factorial)({n: E(-, [S(n), 1])})),\n        non_base_result: E(*, [n, S(recurse_result)])\n        result: E(ternary, [S(is_base_case), 1, S(non_base_result)])\n    })({factorial: S(factorial)})\n})\n\n```\n\nWe need a pointer to the factorial context inside the factorial context itself, so we create an outer context, then use another intervention to pass our pointer in. Viewed as a programming language, Trace lacks lexical scope - annoying for a programming language, but probably a good thing for a data structure.\n\n\nAt this point, we have all the pieces of a programming language. To run factorial(5), we could run `S(result, S(factorial, outer_context)({n:5})).get_value()` with outer\\_context defined above.\n\n\nMore importantly, this immediately provides a way of accessing the program’s trace. We have, effectively, a lazy data structure representing the trace of the program.\n\n\nLet’s imagine tracing our factorial function for n=3. Mixing our notation with some arrows, our program suggests something like this:\n\n\n![](https://docs.google.com/drawings/u/1/d/sqDDogMCGnBbUOVkDSBUcuA/image?w=493&h=679&rev=67&ac=1&parent=1k4EeMXmr8q9XF3OvZLBHckRcjHnVwrBBW7szh6OJPDg)\n\n\nKey thing to notice: all Contexts except the first are dynamically generated, via interventions. As we trace back through the data structure, we’ll need to call get\\_value() to generate the contexts. So when we actually get the values of the contexts, we’ll end up with a data structure like this:\n\n\n![](https://docs.google.com/drawings/u/1/d/sEQwj5M3jgKXwE0JdkJUF6g/image?w=493&h=679&rev=9&ac=1&parent=1k4EeMXmr8q9XF3OvZLBHckRcjHnVwrBBW7szh6OJPDg)\n\n\nSudden Stop\n-----------\n\n\nThat’s it for core principles. I’m currently prototyping some causal abstraction algorithms using Trace, and I’m tweaking things as I go - in particular, I may change the intervention notation to optimize for intervening on the trace rather than intervening on the program. We’ll see.\n\n\nOnce I have some proper algorithms running and the notation has stabilized, I’ll probably put it on github along with some documentation.", "url": "https://www.alignmentforum.org/posts/rt5X74Az3mXwTubRA/trace-goals-and-principles", "date_published": "2020-02-28T23:50:13Z", "authors": ["johnswentworth"], "tags": ["Abstraction"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.722457+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "31792fa82be3420012df269233296583", "source": "alignmentforum", "title": "Cortés, Pizarro, and Afonso as Precedents for Takeover", "text": "*Crossposted from [AI Impacts](https://aiimpacts.org/cortes-pizarro-and-afonso-as-precedents-for-ai-takeover/).*\n\n*Epistemic status: I am not a historian, nor have I investigated these case studies in detail. I admit I am still uncertain about how the conquistadors were able to colonize so much of the world so quickly. I think my ignorance is excusable because this is just a blog post; I welcome corrections from people who know more. If it generates sufficient interest I might do a deeper investigation. Even if I’m right, this is just one set of historical case-studies; it doesn’t prove anything about AI, even if it is suggestive. Finally, in describing these conquistadors as “successful,” I simply mean that they achieved their goals, not that what they achieved was good.* \n\n### **Summary**\n\nIn the span of a few years, some minor European explorers (later known as the conquistadors) encountered, conquered, and enslaved several huge regions of the world. That they were able to do this is surprising; their technological advantage was not huge. (This was before the scientific and industrial revolutions.) From these cases, I think we learn that it is occasionally possible for a small force to quickly conquer large parts of the world, despite:\n\n1. Having only a minuscule fraction of the world's resources and power\n2. Having technology + diplomatic and strategic cunning that is better but not *that* much better\n3. Having very little data about the world when the conquest begins\n4. Being disunited\n\nWhich all suggests that it isn’t as implausible that a small AI takes over the world in mildly favorable circumstances as is sometimes thought.\n\n*EDIT: In light of good pushback from people (e.g. [Lucy.ea8](https://forum.effectivealtruism.org/posts/MNPrXCsPpwTgygMxc/cortes-pizarro-and-afonso-as-precedents-for-takeover#xZ8h23jc9pmp8Mqhy) and [e.g. Matthew Barnett)](https://www.alignmentforum.org/posts/ivpKSjM4D6FbqF4pZ/cortes-pizarro-and-afonso-as-precedents-for-takeover#kNFNjBJjuzTd3irrR) about the importance of disease, I think one should probably add a caveat to the above: \"In times of chaos & disruption, at least.\"* \n\n*NEW EDIT: After reading three giant history books on the subject, I take back my previous edit. My original claims were correct.* \n\n### **Three shocking true stories**\n\nI highly recommend you read the wiki pages yourself; otherwise, here are my summaries:\n\n#### **Cortés:** [**[wiki]**](https://en.wikipedia.org/wiki/Fall_of_Tenochtitlan) [**[wiki]**](https://en.wikipedia.org/wiki/Spanish_conquest_of_the_Aztec_Empire)\n\n* April 1519: Hernán Cortés lands in Yucatan with ~500 men, 13 horses, and a few cannons. He destroys his ships so his men won't be able to retreat. His goal is to conquer the Aztec empire of several million people.\n* He makes his way towards the imperial capital, Tenochtitlán. Along the way he encounters various local groups, fighting some and allying with some. He is constantly outnumbered but his technology gives him an advantage in fights. His force grows in size, because even though he loses Spaniards he gains local allies who resent Aztec rule.\n* Tenochtitlán is an island fortress (like Venice) with a population of over 200,000, making it one of the largest and richest cities *in the world* at the time. Cortés arrives in the city asking for an audience with the Emperor, who receives him warily.\n* Cortés takes the emperor hostage within his own palace, indirectly ruling Tenochtitlán through him.\n* Cortés learns that the Spanish governor has landed in Mexico with a force twice his size, intent on arresting him. (Cortés' expedition was illegal!) Cortés leaves 200 men guarding the Emperor, marches to the coast with the rest, surprises and defeats the new Spaniards in battle, and incorporates the survivors into his army.\n* July 1520: Back at the capital, the locals are starting to rebel against his men. Cortés marches back to the capital, uniting his forces just in time to be besieged in the imperial palace. They murder the emperor and fight their way out of the city overnight, taking heavy losses.\n* They shelter in another city (Tlaxcala) that was thinking about rebelling against the Aztecs. Cortés allies with the Tlaxcalans and launches a general uprising against the Aztecs. Not everyone sides with him; many city-states remain loyal to Tenochtitlan. Some try to stay neutral. Some join him at first, and then abandon him later. Smallpox sweeps through the land, killing many on all sides and causing general chaos.\n* May 1521: The final assault on Tenochtitlán. By this point, Cortés has about 1,000 Spanish troops and 80,000 - 200,000 allied native warriors. He had 16 cannons and 13 boats. The Aztecs have 80,000 - 300,000 warriors and 400 boats. Cortés and his allies win.\n* Later, the Spanish would betray their native allies and assert hegemony over the entire region, in violation of the treaties they had signed.\n\n#### **Pizarro** [**[wiki]**](https://en.wikipedia.org/wiki/Francisco_Pizarro) [**[wiki]**](https://en.wikipedia.org/wiki/Spanish_conquest_of_Peru)\n\n* 1532: Francisco Pizarro arrives in Inca territory with 168 Spanish soldiers. His goal is to conquer the Inca empire, which was much bigger than the Aztec empire.\n* The Inca empire is in the middle of a civil war and a devastating plague.\n* Pizarro makes it to the Emperor right after the Emperor defeats his brother. Pizarro is allowed to approach because he promises that he comes in peace and will be able to provide useful information and gifts.\n* At the meeting, Pizarro ambushes the Emperor, killing his retinue with a volley of gunfire and taking him hostage. The remainder of the Emperor's forces in the area back away, probably confused and scared by the novel weapons and hesitant to keep fighting for fear of risking the Emperor's life.\n* Over the next months, Pizarro is able to leverage his control over the Emperor to stay alive and order the Incans around; eventually he murders the Emperor and makes an alliance with local forces (some of the Inca generals) to take over the capital city of Cuzco.\n* The Spanish continue to rule via puppets, primarily Manco Inca, who is their puppet ruler while they crush various rebellions and consolidate their control over the empire. Manco Inca escapes and launches a rebellion of his own, which is partly successful: He utterly wipes out four columns of Spanish reinforcements, but is unable to retake the capital. With the morale and loyalty of his followers dwindling, Manco Inca eventually gives up and retreats, leaving the Spanish still in control.\n* Then the Spanish ended up fighting *each other* for a while, while *also* putting down more local rebellions. After a few decades Spanish dominance of the region is complete. (1572).\n\n#### **Afonso** [**[wiki]**](https://en.wikipedia.org/wiki/Afonso_de_Albuquerque) [**[wiki]**](https://en.wikipedia.org/wiki/Capture_of_Malacca_(1511)) [**[wiki]**](https://en.wikipedia.org/wiki/Portuguese_conquest_of_Goa)\n\n* 1506: Afonso helps the Portuguese king come up with a shockingly ambitious plan. *Eight years* prior, the first Europeans had rounded the coast of Africa and made it to the Indian Ocean. The Indian Ocean contained most of the world's trade at the time, since it linked up the world's biggest and wealthiest regions. See [this map of world population (timestamp 3:45)](https://www.youtube.com/watch?v=PUwmA3Q0_OE). Remember, this is prior to the Industrial and Scientific Revolutions; Europe is just coming out of the Middle Ages and does not have an obvious technological advantage over India or China or the Middle East, and has an obvious economic *disadvantage*. And Portugal is a just tiny state on the edge of the Iberian peninsula.\n* The plan is: Not only will we go into the Indian Ocean and participate in the trading there -- cutting out all the middlemen who are currently involved in the trade between that region and Europe -- we will *conquer strategic ports around the region so that no one else can trade there!*\n* Long story short, Afonso goes on to complete this plan by 1513. (!!!)\n\nSome comparisons and contrasts:\n\n* Afonso had more European soldiers at his disposal than Cortes or Pizarro, but not many more -- usually he had about a thousand or so. He did have more reinforcements and support from home.\n* Like them, he was usually significantly outnumbered in battles. Like them, the empires he warred against were vastly wealthier and more populous than his forces.\n* Like them, Afonso was often able to exploit local conflicts to gain local allies, which were crucial to his success.\n* Unlike them, his goal wasn't to conquer the empires entirely, just to get and hold strategic ports.\n* Unlike them, he was fighting empires that were technologically advanced; for example, in several battles his enemies had more cannons and gunpowder than he did.\n* That said, it does seem that Portuguese technology was qualitatively better in some respects (ships, armor, and cannons, I'd say.) Not dramatically better, though.\n* While Afonso's was a naval campaign, he did fight many land battles, usually marine assaults on port cities, or defenses of said cities against counterattacks. So superior European naval technology is not by itself enough to explain his victory, though it certainly was important.\n* Plague and civil war were not involved in Afonso's success.\n\n### **What explains these devastating conquests?**\n\n#### **Wrong answer: I cherry-picked my case studies.**\n\nHistory is full of incredibly successful conquerors: Alexander the Great, Ghenghis Khan, etc. Perhaps some people are just really good at it, or really lucky, or both.\n\nHowever: Three incredibly successful conquerors from the same tiny region and time period, conquering three separate empires? Followed up by dozens of less successful but still very successful conquerors from the same region and time period? Surely this is not a coincidence. Moreover, it’s not like the conquistadors had many failed attempts and a few successes. The Aztec and Inca empires were the two biggest empires in the Americas, and there weren’t any other Indian Oceans for the Portuguese to fail at conquering.\n\nFun fact: I had not heard of Afonso before I started writing this post this morning. Following the [Rule of Three](https://en.wikipedia.org/wiki/Rule_of_three_%28writing%29), I needed a third example and I predicted on the basis of Cortes and Pizarro that there would be other, similar stories happening in the world at around that time. That’s how I found Afonso.\n\n#### **Right answer: Technology**\n\nHowever, I don't think this is the whole explanation. The technological advantage of the conquistadors was not overwhelming.\n\nWhatever technological advantage the conquistadors had over the existing empires, it was the sort of technological advantage that one could acquire *before* the Scientific and Industrial revolutions. Technology didn't change very fast back then, yet Portugal managed to get a lead over the Ottomans, Egyptians, Mughals, etc. that was sufficient to bring them victory. On paper, the Aztecs and Spanish were pretty similar: Both were medieval, feudal civilizations. I don't know for sure, but I'd bet there were at least a few techniques and technologies the Aztecs had that the Spanish didn't. And of course the technological similarities between the Portuguese and their enemies were much stronger; the Ottomans even had access to European mercenaries! Even in cases in which the conquistadors had technology that was completely novel -- like steel armor, horses, and gunpowder were to the Aztecs and Incas -- it wasn't god-like. The armored soldiers were still killable; the gunpowder was more effective than arrows but limited in supply, etc.\n\n(Contrary to popular legend, neither Cortés nor Pizarro were regarded as gods by the people they conquered. The Incas concluded pretty early on that the Spanish were mere men, and while the idea did float around the Aztecs for a bit the modern historical consensus is that most of them didn't take it seriously.)\n\nAsk yourself: Suppose Cortés had found 500 local warriors, gave them all his equipment, trained them to use it expertly, and left. Would those local men have taken over all of Mexico? I doubt it. And this is despite the fact that they would have had much better local knowledge than Cortés did! Same goes for Pizarro and Afonso. Perhaps if he had found 500 local warriors *led by an exceptional commander* it would work. But the explanation for the conquistador’s success can’t just be that they were all exceptional commanders; that would be positing too much innate talent to occur in one small region of the globe at one time.\n\n#### **Right answer: Strategic and diplomatic cunning**\n\nThis is my non-expert guess about the missing factor that joins with technology to explain this pattern of conquistador success.\n\nThey didn't just have technology; they had *effective* *strategy* and they had *effective diplomacy*. They made long-term plans that *worked* despite being breathtakingly ambitious. (And their short-term plans were usually pretty effective too, read the stories in detail to see this.) Despite not knowing the local culture or history, these conquistadors made surprisingly savvy diplomatic decisions. They knew when they could get away with breaking their word and when they couldn't; they knew which outrages the locals would tolerate and which they wouldn’t; they knew how to convince locals to ally with them; they knew how to use words to escape militarily impossible situations… The locals, by contrast, often badly misjudged the conquistadors, e.g. not thinking Pizarro had the will (or the ability?) to kidnap the emperor, and thinking the emperor would be safe as long as they played along.\n\nThis raises the question, how did they get that advantage? My answer: they had *experience* with this sort of thing, whereas locals didn't. Presumably Pizarro learned from Cortés' experience; his strategy was pretty similar. (See also: [the prior conquest of the Canary Islands by the Spanish](https://en.wikipedia.org/wiki/Conquest_of_the_Canary_Islands)). In Afonso's case, well, the Portuguese had been sailing around Africa, conquering ports and building forts for more than a hundred years.\n\n### **Lessons I think we learn**\n\nI think we learn that:\n\nIt is occasionally possible for a small force to quickly conquer large parts of the world, despite:\n\n1. Having only a minuscule fraction of the world's resources and power\n2. Having technology + diplomatic and strategic cunning that is better but not *that* much better\n3. Having very little data about the world when the conquest begins\n4. Being disunited\n\nWhich all suggests that it isn’t as implausible that a small AI takes over the world in mildly favorable circumstances as is sometimes thought.\n\n*EDIT: In light of good pushback from people (e.g. [Lucy.ea8](https://forum.effectivealtruism.org/posts/MNPrXCsPpwTgygMxc/cortes-pizarro-and-afonso-as-precedents-for-takeover#xZ8h23jc9pmp8Mqhy) and [e.g. Matthew Barnett)](https://www.alignmentforum.org/posts/ivpKSjM4D6FbqF4pZ/cortes-pizarro-and-afonso-as-precedents-for-takeover#kNFNjBJjuzTd3irrR) about the importance of disease, I think one should probably add a caveat to the above: \"In times of chaos & disruption, at least.\"* \n\n#### **Having only a minuscule fraction of the world's resources and power**\n\nIn all three examples, the conquest was more or less completed without support from home; while Spain/Portugal did send reinforcements, it wasn't even close to the entire nation of Spain/Portugal fighting the war. So these conquests are examples of non-state entities conquering states, so to speak. (That said, their *claim* to represent a large state may have been crucial for Cortes and Pizarro getting audiences and respect initially.) Cortés landed with about a thousandth the troops of Tenochtitlan, which controlled a still larger empire of vassal states. Of course, his troops were better equipped, but on the other hand they were also cut off from resupply, whereas the Aztecs were in their home territory, able to draw on a large civilian population for new recruits and resupply.\n\nThe conquests succeeded in large part due to diplomacy. This has implications for AI takeover scenarios; rather than imagining a conflict of humans vs. robots, we could imagine humans vs. humans-with-AI-advisers, with the latter faction winning and somehow by the end of the conflict the AI advisers have managed to become *de facto* rulers, using the humans who obey them to put down rebellions by the humans who don't.\n\n#### **Having technology + diplomatic and strategic skill that is better but not *that* much better**\n\nAs previously mentioned, the conquistadors didn’t enjoy god-like technological superiority. In the case of Afonso the technology was pretty similar. Technology played an important role in their success, but it wasn’t enough on its own. Meanwhile, the conquistadors may have had more diplomatic and strategic cunning (or experience) than the enemies they conquered. But not that much more--they are only human, after all. And their enemies were pretty smart.\n\nIn the AI context, we don't need to imagine god-like technology (e.g. swarms of self-replicating nanobots) to get an AI takeover. It might even be possible without any new physical technologies at all! Just superior software, e.g. piloting software for military drones, targeting software for anti-missile defenses, cyberwarfare capabilities, data analysis for military intelligence, and of course excellent propaganda and persuasion.\n\nNor do we need to imagine an AI so savvy and persuasive that it can persuade anyone of anything. We just need to imagine it about as cunning and experienced relative to its enemies as Cortés, Pizarro, and Afonso were relative to theirs. (Presumably no AI would be experienced with world takeover, but perhaps an intelligence advantage would give it the same benefits as an experience advantage.) And if I’m wrong about this explanation for the conquistador’s success--if they had no such advantage in cunning/experience--then the conclusion is even stronger.\n\nAdditionally, in a rapidly-changing world that is undergoing [slow takeoff](https://sideways-view.com/2018/02/24/takeoff-speeds/), where there are lesser AIs and AI-created technologies all over the place, most of which are successfully controlled by humans, AI takeover might still happen if one AI is better, but not that much better, than the others.\n\n#### **Having very little data about the world when the conquest begins**\n\nCortés invaded Mexico knowing very little about it. After all, the Spanish had only realized the Americas existed two decades prior. He heard rumors of a big wealthy empire and he set out to conquer it, knowing little of the technology and tactics he would face. Two years later, he ruled the place.\n\nPizarro and Afonzo were in better epistemic positions, but still, they had to learn a lot of important details (like what the local power centers, norms, and conflicts were, and exactly what technology the locals had) on the fly. But they were good at learning these things and making it up as they went along, apparently.\n\nWe can expect superhuman AI to be good at learning. Even if it starts off knowing very little about the world -- say, it figured out it was in a training environment and hacked its way out, having inferred a few general facts about its creators but not much else -- if it is good at learning and reasoning, it might still be pretty dangerous.\n\n#### **Being disunited**\n\nCortés invaded Mexico in defiance of his superiors and had to defeat the army they sent to arrest him. Pizarro ended up fighting a civil war against his fellow conquistadors in the middle of his conquest of Peru. Afonzo fought Greek mercenaries and some traitor Portuguese, conquered Malacca against the orders of a rival conquistador in the area, and was ultimately demoted due to political maneuvers by rivals back home.\n\nThis astonishes me. Somehow these conquests were completed by people who were at the same time busy infighting and backstabbing each other!\n\nWhy was it that the conquistadors were able to split the locals into factions, ally with some to defeat the others, and end up on top? Why didn't it happen the other way around: some ambitious local ruler talks to the conquistadors, exploits their internal divisions, allies with some to defeat the others, and ends up on top?\n\nI think the answer is partly the \"diplomatic and strategic cunning” mentioned earlier, but mostly other things. (The conquistadors were disunited, but presumably were united in the ways that mattered.) At any rate, I expect AIs to be pretty [good at coordinating too](https://www.alignmentforum.org/posts/gYaKZeBbSL4y2RLP3/strategic-implications-of-ais-ability-to-coordinate-at-low); they should be able to conquer the world just fine even while competing fiercely with each other. For more on this idea, see [this comment](https://www.lesswrong.com/posts/gYaKZeBbSL4y2RLP3/strategic-implications-of-ais-ability-to-coordinate-at-low?commentId=pdXk8xKtHQKAiu8Pf).\n\n*By Daniel Kokotajlo*\n\n**Acknowledgements**\n--------------------\n\n*Thanks to Katja Grace for feedback on a draft. All mistakes are my own, and should be pointed out in the comments.*  *Edit: Also, when I wrote this post I had forgotten that the basic idea for it probably came from [this comment by JoshuaFox](https://www.lesswrong.com/posts/vkjWGJrFWBnzHtxrw/superintelligence-7-decisive-strategic-advantage#sWjzvXJToQ3KfH7pB).*", "url": "https://www.alignmentforum.org/posts/ivpKSjM4D6FbqF4pZ/cortes-pizarro-and-afonso-as-precedents-for-takeover", "date_published": "2020-03-01T03:49:45Z", "authors": ["Daniel Kokotajlo"], "tags": ["History", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.722866+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "1d029130342841184905e9b9ff91b8d9", "source": "alignmentforum", "title": "An Analytic Perspective on AI Alignment", "text": "This is a perspective I have on how to do useful AI alignment research. Most perspectives I’m aware of are constructive: they have some blueprint for how to build an aligned AI system, and propose making it more concrete, making the concretisations more capable, and showing that it does in fact produce an aligned AI system. I do not have a constructive perspective - I’m not sure how to build an aligned AI system, and don’t really have a favourite approach. Instead, I have an analytic perspective. I would like to understand AI systems that are built. I also want other people to understand them. I think that this understanding will hopefully act as a ‘filter’ that means that dangerous AI systems are not deployed. The following dot points lay out the perspective.\n\n\nSince the remainder of this post is written as nested dot points, some readers may prefer to read it in [workflowy](https://workflowy.com/s/an-analytical-perspe/eU45Fsjd7lzidjM8).\n\n\nBackground beliefs\n------------------\n\n\n* I am imagining a future world in which powerful AGI systems are made of components roughly like neural networks (either feedforward or recurrent) that have a large number of parameters.\n* Furthermore, I’m imagining that the training process of these ML systems does not provide enough guarantees about deployment performance.\n\t+ In particular, I’m supposing that systems are being trained based on their ability to deal with simulated situations, and that that’s insufficient because deployment situations are hard to model and therefore simulate.\n\t\t- One reason that they are hard to model is the complexities of the real world.\n\t\t\t* The real world might be intrinsically difficult to model for the relevant system. For instance, it’s difficult to simulate all the situations in which the CEO of Amazon might find themselves.\n\t\t\t* Another reason that real world situations may be hard to model is that they are dependent on the final trained system.\n\t\t\t\t+ The trained system may be able to affect what situations it ends up in, meaning that situations during earlier training are unrepresentative.\n\t\t\t\t+ Parts of the world may be changing their behaviour in response to the trained system…\n\t\t\t\t\t- in order to exploit the system.\n\t\t\t\t\t- by learning from the system’s predictions.\n\t\t\t* The real world is also systematically different than the trained world: for instance, while you’re training, you will never see the factorisation of RSA-2048 (assuming you’re training in the year 2020), but in the real world you eventually will.\n\t\t\t\t+ This is relevant because you could imagine [mesa-optimisers](https://arxiv.org/abs/1906.01820) appearing in your system that choose to act differently when they see such a factorisation.\n* I’m imagining that the world is such that if it’s simple for developers to check if an AI system would have disastrous consequences upon deployment, then they perform this check, and fail to deploy if the check says that it would.\n\n\nBackground desiderata\n---------------------\n\n\n* I am mostly interested in allowing the developers of AI systems to determine whether their system has the cognitive ability to cause human extinction, and whether their system might try to cause human extinction.\n\t+ I am not primarily interested in reducing the probabilities of other ways in which AI systems could cause humanity to go extinct, such as research groups intentionally behaving badly, or an uncoordinated set of releases of AI systems that interact in negative ways.\n\t\t- That being said, I think that pursuing research suggested by this perspective could help with the latter scenario, by making it clear which interaction effects might be present.\n* I want this determination to be made before the system is deployed, in a ‘zero-shot’ fashion, since this minimises the risk of the system actually behaving badly before you can detect and prevent it.\n\n\nTransparency\n------------\n\n\n* The type of transparency that I’m most excited about is mechanistic, in a sense that I’ve described [elsewhere](https://www.lesswrong.com/posts/3kwR2dufdJyJamHQq/mechanistic-transparency-for-machine-learning).\n* The transparency method itself should be based on a trusted algorithm, as should the method of interpreting the transparent artefact.\n\t+ In particular, these operations should not be done by a machine learning system, unless that system itself has already been made transparent and verified.\n\t\t- This could be done [amplification-style](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616).\n* Ideally, models could be regularised for transparency during training, with little or no cost to performance.\n\t+ This would be good because by default models might not be very transparent, and it might be hard to hand-design very transparent models that are also capable.\n\t\t- I think of this as what one should derive from Rich Sutton’s [bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)\n\t+ This will be easier to do if the transparency method is simpler, more ‘mathematical’, and minimally reliant on machine learning.\n\t+ You might expect little cost to performance since neural networks can often reach high performance given constraints, as long as they are deep enough.\n\t\t- [This paper](https://arxiv.org/abs/1804.08838) on the intrinsic dimension of objective landscapes shows that you can constrain neural network weights to a low-dimensional subspace and still find good solutions.\n\t\t- [This paper](https://arxiv.org/abs/1908.01755) argues that there are a large number of models with roughly the same performance, meaning that ones with good qualities (e.g. interpretability) can be found.\n\t+ [This paper](https://arxiv.org/abs/1711.06178) applies regularisation to machine learning models that ensures that they are represented by small decision trees.\n* The transparency method only has to reveal useful information to developers, not to the general public.\n\t+ This makes the problem easier but still difficult.\n\t+ Presumably developers will not deploy catastrophically terrible systems, since catastrophes are usually bad for most people, and I’m most interested in averting catastrophic outcomes.\n\n\nFoundations\n-----------\n\n\n* In order for the transparency to be useful, practitioners need to know what problems to look for, and how to reason about these problems.\n* I think that an important part of this is ‘agent foundations’, by which I broadly mean a theory of what agents should look like, and what structural facts about agents could cause them to display undesired behaviour.\n\t+ Examples:\n\t\t- Work on [mesa-optimisation](https://arxiv.org/abs/1906.01820)\n\t\t- Utility theory, e.g. the [von Neumann-Morgenstern theorem](https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem)\n\t\t- Methods of detecting which agents are likely to be intelligent or dangerous.\n* For this, it is important to be able to look at a machine learning system and learn if (or to what degree) it is agentic, detect belief-like structures and preference-like structures (or to deduce things analogous to beliefs and preferences), and learn other similar things.\n\t+ This requires structural definitions of the relevant primitives (such as agency), not subjective or performance-based definitions.\n\t\t- By ‘structural definitions’, I mean definitions that refer to facts that are easily accessible about the system before it is run.\n\t\t- By ‘subjective definitions’, I mean definitions that refer to an observer’s beliefs or preferences regarding the system.\n\t\t- By ‘performance-based definitions’, I mean definitions that refer to facts that can be known about the system once it starts running.\n\t\t- Subjective definitions are inadequate because they do not refer to easily-measurable quantities.\n\t\t- Performance-based definitions are inadequate because they can only be evaluated once the system is running, when it could already pose a danger, violating the “zero-shot” desideratum.\n\t\t- Structural definitions are required because they are precisely the definitions that are not subjective or performance-based that also only refer to facts that are easily accessible, and therefore are easy to evaluate whether a system satisfies the definition.\n\t\t- As such, definitions like “an agent is a system whose behaviour can’t usefully be predicted mechanically, but can be predicted by assuming it near-optimises some objective function” (which was proposed in [this paper](https://arxiv.org/abs/1805.12387)) are insufficient because they are both subjective and performance-based.\n\t\t- It is possible to turn subjective definitions into structural definitions trivially, by asking a human about their beliefs and preferences. This is insufficient.\n\t\t\t* e.g. “X is a Y if you are scared of it” can turn to “X is a Y if the nearest human to X, when asked if they are scared of X, says ‘yes’”.\n\t\t\t* It is insufficient because such a definition doesn’t help the human form their subjective beliefs and impressions.\n\t\t- It is also possible to turn subjective definitions that only depend on beliefs into structural definitions by determining which circumstances warrant a rational being to have which beliefs. This is sufficient.\n\t\t\t* Compare the subjective definition of temperature as “the derivative of a system’s energy with respect to entropy at fixed volume and particle number” to the objective definition “equilibrate the system with a thermometer, read it off the thermometer”. For a rational being, these two definitions yield the same temperature for almost all systems.\n\n\nRelation between transparency and foundations\n---------------------------------------------\n\n\n* The agent foundations theory should be informed by transparency research, and vice versa.\n\t+ This is because the information that transparency methods can yield should be all the information that is required to analyse the system using the agent foundations theory.\n\t+ Both lines of research can inform the other.\n\t\t- Transparency researchers can figure out how to reveal the information required by agent foundations theory, and detect the existence of potential problems that agent foundations theory suggests might occur given certain training procedures.\n\t\t- Agent foundations researchers can figure out what is implied by the information revealed by existing transparency tools, and theorise about problems that transparency researchers detect.\n\n\nCriticisms of the perspective\n-----------------------------\n\n\n* It isn’t clear if neural network transparency is possible.\n\t+ More specifically, it seems imaginable that some information required to usefully analyse an AI system cannot be extracted from a typical neural network in polynomial time.\n* It isn’t clear that relevant terms from agency theory can in fact be well-defined.\n\t+ E.g. “optimisation” and “belief” have eluded a satisfactory computational grounding for quite a while.\n\t+ Relatedly, the philosophical question of which physical systems enable which computations has not to my mind been satisfactorily resolved. See [this](https://plato.stanford.edu/entries/computation-physicalsystems/) relevant SEP article.\n* An easier path to transparency than the “zero-shot” approach might be to start with simpler systems, observe their behaviour, and slowly scale them up. As you see problems, stop scaling up the systems, and instead fix them so the problems don’t occur.\n\t+ I disagree with this criticism.\n\t\t- At one point, it’s going to be the first time you use a system of a given power in a domain, and the problems caused by the system might be discontinuous with its power, meaning that they would be hard to predict.\n\t\t\t* Especially if the power of the system increases discontinuously.\n\t\t\t* It is plausibly the case that systems that are a bit ‘smarter than humanity’ are discontinuously more problematic than those that are a bit less ‘smart than humanity’.\n* One could imagine giving up the RL dream for something like debate, where you really can get guarantees from the training procedure.\n\t+ I think that this is not true, and that things like debate require transparency tools to work well, so as to let debaters know when other debaters are being deceitful. An argument for an analogous conclusion can be found in evhub’s post on [Relaxed adversarial training for inner alignment](https://www.lesswrong.com/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment).\n* One could imagine inspecting training-time reasoning and convincing yourself that way that future reasoning will be OK.\n\t+ But reasoning could look different in different environments.\n* This perspective relies on things continuing to look pretty similar to current ML.\n\t+ This would be alleviated if you could come up with some sort of sensible theory for how to make systems transparent.\n\t+ I find it plausible that the development of such a theory should start with people messing around and doing things with systems they have.\n* Systems should be transparent to all relevant human stakeholders, not just developers.\n\t+ Sounds right to me - I think people should work on this broader problem. But:\n\t\t- I don’t know how to solve that problem without making them transparent to developers initially.\n\t\t- I have ideas about how to solve the easier problem.", "url": "https://www.alignmentforum.org/posts/8GdPargak863xaebm/an-analytic-perspective-on-ai-alignment", "date_published": "2020-03-01T04:10:03Z", "authors": ["DanielFilan"], "tags": ["Interpretability (ML & AI)", "AI"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.723244+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "19852a880661c825e69946ee2e0dd874", "source": "alignmentforum", "title": "If I were a well-intentioned AI... IV: Mesa-optimising", "text": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\nHere I apply my \"If I were a well-intentioned AI\" filter to [mesa-optimising](https://www.lesswrong.com/posts/q2rCMHNXazALgQpGH/conditions-for-mesa-optimization)\n\n\nNow, I know that a mesa-optimiser need not be a subagent (see 1.1 [here](https://arxiv.org/pdf/1906.01820.pdf)), but I'm obviously going to imagine myself as a mesa-optimising subagent.\n\n\nAn immediate human analogy springs to mind: I'm the director of a subdivision of some corporation or agency, and the \"root optimiser\" is the management of that entity.\n\n\nThere is a lot of literature on [what happens if I'm selfish](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem) in this position; but if I'm well-intentioned, what should I be doing?\n\n\nOne thing that thinking this way made me realise: there is a big difference between \"aligned with management\" and \"controlled by management\".\n\n\nWe'll consider each one in turn, but to summarise: aligned mesa-optimisers are generally better than controlled mesa-optimisers, *but it is hard to tell the difference between an aligned and a dangerous unaligned mesa-optimiser.*\n\n\nControl vs alignment\n====================\n\n\nFirst let's flesh out the corporate/management example a bit. Me-AI is in charge of making widgets, that are used by the company for some purpose. That purpose is given by  - the base utility for the corporation.\n\n\nMy role is to make as many widgets as possible within my budget; this is , the mesa-objective I have been given by management.\n\n\nMy true utility function is Ume. Management don't fully know what Ume is - or at least don't fully understand it, or all of its implications. This is needed, of course, because if management fully understood the implications of Ume, there would be no uncertainty at all on their part, and they could make me do exactly what they wanted - or they would turn me off.\n\n\nBecause of this uncertainty, management have added some extra levels of control over me. Let's assume one typical precaution: if I underspend the budget, the budget is cut next year. If I overspend, I'm reprimanded (and fired if I do it too often), but the budget is not cut.\n\n\nThere are three possible situations I could confront:\n\n\n* S1: I've made as many widgets as I can this year, and spent 90% of my budget. I predict that, next year, I will only need 90% of this year's budget.\n* S2: I've made as many widgets as I can this year, and spent 90% of my budget. I predict that, next year, I will need 100% of this year's budget.\n* S3: I've spent 100% of my budget on widgets. I predict that widgets are particularly valuable to the company this year, much moreso than next year.\n\n\nAligned mesa-optimiser\n----------------------\n\n\nIf I'm aligned with the goals of the company, Ume=Ubase, the base utility. Management has assigned Umesa to me, but I have actually no intrinsic desire to maximise it. I am a Ubase-maximiser, with the extra information that management, who are also Ubase-maximisers, have assigned Umesa to me.\n\n\nMy behaviour should be that of a Umesa maximiser in most circumstances. If I know management is much smarter than me (or have an information advantage; maybe I'm not even sure what Ubase is!), then I will tend to behave like a blind Umesa-maximiser, trusting to management to sort things out properly. In S1 and S2, I will not spend anything more, and in S3, I will do nothing.\n\n\nIf I know I'm smarter than management, at least in some areas, then I will start making moves that are Ubase-maximising but not necessarily Umesa-maximising. In S1, I won't spend any more (since I don't need a large budget next year), but in S2, I'll waste the extra 10% to ensure my budget isn't cut. In S3, I'll go over budget to get the extra-valuable widgets.\n\n\nDangerous mesa-optimiser\n------------------------\n\n\nNote that, from a management perspective, the S2 and S3 actions of a smart aligned mesa-optimiser... look like an dangerous, unaligned mesa-optimiser defecting.\n\n\nLet Udan be the utility function of an unaligned (and non-controlled) agent. How can management distinguish between Ume=Ubase and Ume=Udan?\n\n\nIn both cases, the subagent will take actions that go beyond Umesa, actions whose ramifications management cannot immediately see.\n\n\nOf course, *I* would explain to management why I'm doing this, and why they are compatible with Ubase. But a Udan-maximiser would similarly explain its actions, trying to trick the management into thinking it was... me.\n\n\nThus, as in the classical [principal-agent problem](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem), there is a problem with imperfect information. Even if I am aligned, management can't trust me if they don't know I'm aligned. I would try and signal this; an unaligned agent would try and fake the same signals. The very fact that aligned-but-not-controlled agents could exist, make it easier for unaligned-and-not-controlled agents to go undetected.\n\n\nControlled mesa-optimiser\n-------------------------\n\n\nA controlled mesa-optimiser is the obedient servant. They will maximise Umesa, while ensuring that they remain under management control.\n\n\nIf I'm in that case, I'll try and ensure that management has accurate information about what I'm doing[[1]](#fn-AHh3ptqNdQoM2pXZS-1), and that they can change my objectives as needed. I'm [safely interruptible](https://intelligence.org/files/Interruptibility.pdf) and [corrigible](https://www.lesswrong.com/posts/T5ZyNq3fzN59aQG5y/the-limits-of-corrigibility).\n\n\nIn S3, I will not do anything; it's not my role to maximise Ubase, so it's not my role to take advantage of this opportunity. Depending on how I'm designed, I may report this information to management. In S1, I will not spend any extra, and I'll let my budget be cut next year.\n\n\nIn S2, I face a dilemma: my Umesa goal that wants me to preserve my budget next year, vs my desire to be informative. Again, this varies depending on how I'm designed. So I'll likely spend my entire budget (to prevent the automatic cutting of my budget), while reporting this to management so they have full information. If I can't report this, I may decide not to spend any extra, so that management gets better information.\n\n\nNote the \"depending on how I'm designed\". Because unlike the aligned mesa-optimiser, the controlled agent has two goals, in tension with each other: maximising Umesa, and being informative and controlled by management. If we call the \"controlled\" goal Uco, then I'm maximising\n\n\nUme=Umesa⊕Uco,\n\n\nwhere ⊕ means that Ume is some function of Umesa and Uco that increases whenever either of those functions increase.\n\n\nI'm *not* aligned, so I'm not a Ubase maximiser. I would gladly sacrifice Ubase completely, in order to maximise Uco and/or Umesa. Hopefully, Uco is sufficiently well-designed to allow management to predict this and preclude it. And, hopefully, the tradeoff between Umesa and Uco is also sufficiently well designed to preclude it.\n\n\nSince I am well-intentioned, I can make use of the fact that Ubase is the outcome desired by management, in order to better figure out the ideal versions of Uco, Umesa, and of Ume. Defining \"the human H understands what the mesa-optimiser is doing\" is [not easy](https://www.lesswrong.com/posts/ZaMNgFH4gqbc5R2Yf/true-understanding-comes-from-passing-exams). So a well-intentioned controlled mesa-optimiser, is akin to a \"control optimiser\" which is trying to figure out what control means (see previous posts in this sequence) and avoiding Goodharting both Uco and Ume.\n\n\nAligned and controlled\n----------------------\n\n\nI could instead be maximising\n\n\nUme=Ubase⊕Umesa⊕Uco,\n\n\nor, more simply\n\n\nUme=Ubase⊕Uco,\n\n\nwith the extra piece of information that management have assigned me the role of maximising Umesa. The ⊕ might be a [smoothed lexicographic ordering](https://www.lesswrong.com/posts/PFu4tonkGiPQ4535F/stuart_armstrong-s-shortform#NCi38JaafD6GBwHcj), with Ubase the primary utility.\n\n\nSo, here I'm aligned and controlled. So that I can trade off some of what management really want - Ubase - to increase Uco, their control over me. But not arbitrary amounts of it.\n\n\nSo, being aligned and controlled, I will do nothing in S1, just as both other agents do. I will overspend in S3, to take advantage of the Ubase-increasing potential, just like the aligned agent.\n\n\nThe S2 situation is more complicated. I have a tendency to want to waste my budget (alignment) but also want to communicate good information to management (control). Unlike the controlled agent, where control will dictate \"do nothing\" if there is no other way of communicating information, this is a genuine tension. I am therefore more likely to choose \"waste\" than the controlled agent.\n\n\nIn general, this seems the best design, if management is unsure of me. Not Ubase-optimal, but the probability of dramatic Ubase-failure is reduced, while management keep some of the control that they think is desirable.\n\n\nSummary\n-------\n\n\nTo summarise the three types of agent; if they are smart and powerful compared to management, they would tend to do the following actions (a ? indicates strong uncertainty; if one option is more likely, despite the uncertainty, that option is noted before the ?):\n\n\nS1S2S3AlignedNothingWasteOverspendControlledNothing?NothingBothNothingWaste?Overspend\n\n\nAn unaligned agent will pretend to be one of the other types of agents; it will have most success at its own objectives if it pretends to be an aligned agent.\n\n\nIf management misclassifies me\n==============================\n\n\nAligned, believed controlled\n----------------------------\n\n\nIf I'm aligned, but management thinks I'm controlled, this isn't much of a problem. When I'm weak or ignorant, I should behave like a controlled agent anyway, since management have a much better handle on what is needed than me.\n\n\nOnce I become powerful and smart, I would most likely still want management to believe I'm controlled (or else they might intervene to constrict me or turn me off). So I'd generally behave exactly as if I was controlled, only taking advantage of grey zones of management misunderstanding, to push the outcome more towards the Ubase-optimal position, even at the cost of Umesa and Uco.\n\n\nControlled, believed aligned\n----------------------------\n\n\nA controlled agent that is believed to be aligned is more of a problem. If the Umesa⊕Uco provides an incentive for me to signal management \"hey, you misclassified me, I'm actually controlled, not aligned\", then the problem is self-correcting.\n\n\nIf not, the problems of controlled agents are exacerbated; management will let me get on with my own stuff, ignoring any \"weird\" or \"anti-Ubase\" stuff that I might be doing, because they believe that I'm acting for Ubase and they just lack information to fully understand why I'm taking these actions. Their more \"rubber-stamp\" approach increases the risk that I may sacrifice their Ubase to my own Umesa⊕Uco.\n\n\n\n\n---\n\n\n\n1. Note that even a controlled mesa-optimiser is not necessarily honest. My goal is to give the management good information and be under their control; not to be honest. If management expects that every director will pad their budget requests by 10%, then I will do so as well; to refrain from doing so would be misleading. [↩︎](#fnref-AHh3ptqNdQoM2pXZS-1)", "url": "https://www.alignmentforum.org/posts/aqhMLqaoHb7uob7fr/if-i-were-a-well-intentioned-ai-iv-mesa-optimising", "date_published": "2020-03-02T12:16:16Z", "authors": ["Stuart_Armstrong"], "tags": ["Inner Alignment", "Mesa-Optimization", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.723686+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "6c8452562691e8a396bc45de9f4db7ba", "source": "alignmentforum", "title": "Anthropics over-simplified: it's about priors, not updates", "text": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\nI've argued that [anthropic reasoning isn't magic](https://www.lesswrong.com/posts/4ZRDXv7nffodjv477/anthropic-reasoning-isn-t-magic), applied [anthropic reasoning to the Fermi question](https://www.lesswrong.com/posts/M9sb3dJNXCngixWvy/anthropics-and-fermi), claimed that [different anthropic probabilities answer different questions](https://www.lesswrong.com/posts/nxRjC93AmsFkfDYQj/anthropic-probabilities-answering-different-questions), and concluded that [anthropics is pretty normal](https://www.lesswrong.com/posts/uAqs5Q3aGEen3nKeX/anthropics-is-pretty-normal).\n\n\nBut all those posts were long and somewhat technical, and needed some familiarity with anthropic reasoning in order to be applied. So here I'll list what people unfamiliar with anthropic reasoning can do to add it simply[[1]](#fn-F5K2FcNZ3Tq8M483s-1) and easily to their papers/blog posts/discussions:\n\n\n1. Anthropics is about priors, not updates; updates function the same way for all anthropic probabilities.\n2. If two theories predict the same population, there is no anthropic effect between them.\n\n\nUpdating on safety\n------------------\n\n\nSuppose you go into hiding in a bunker in 1956. You're not sure if the cold war is intrinsically stable or unstable. Stable predicts a 25% chance of nuclear war; unstable predicts a 75% chance.\n\n\nYou emerge much older in 2020, and notice there has not been a nuclear war. Then, whatever anthropic probability theory you use, you update the ratio\n\n\nP(stable):P(unstable)    by    75:25=3:1.\n\n\nPopulation balancing\n--------------------\n\n\nSuppose you have two theories to explain the Fermi paradox:\n\n\n* Theory 1 is that life can only evolve in very rare conditions, so Earth has the only life in the reachable universe.\n* Theory 2 is that there is some disaster that regularly obliterates pre-life conditions, so Earth has the only life in the reachable universe.\n\n\nSince the total population predicted by these two theories is the same, there is no anthropic update between them[[2]](#fn-F5K2FcNZ3Tq8M483s-2).\n\n\n\n\n---\n\n\n\n1. These points are a bit over-simplified, but are suitable for most likely scenarios. [↩︎](#fnref-F5K2FcNZ3Tq8M483s-1)\n2. If you use a reference class that doesn't include certain entities - maybe you don't include pre-mammals or beings without central nervous systems - then you only need to compare the population that is in your reference class. [↩︎](#fnref-F5K2FcNZ3Tq8M483s-2)", "url": "https://www.alignmentforum.org/posts/Hpam4RrJKfufXrmAi/anthropics-over-simplified-it-s-about-priors-not-updates", "date_published": "2020-03-02T13:45:12Z", "authors": ["Stuart_Armstrong"], "tags": ["Anthropics"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.724203+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "280794cc600aa563414a63e9fbb8313e", "source": "alignmentforum", "title": "[AN #89]: A unifying formalism for preference learning algorithms", "text": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.\n\nAudio version [here](http://alignment-newsletter.libsyn.com/alignment-newsletter-89) (may not be up yet).\n\n**Highlights**\n--------------\n\n[Reward-rational (implicit) choice: A unifying formalism for reward learning](https://arxiv.org/abs/2002.04833) *(Hong Jun Jeon, Smitha Milli et al)* (summarized by Rohin): We've got algorithms for learning preferences from [demonstrations](http://www.cs.cmu.edu/~bziebart/publications/maximum-causal-entropy.pdf) ([AN #12](https://mailchi.mp/bcb2c6f1d507/alignment-newsletter-12)) (possibly [ranked](https://arxiv.org/abs/1907.03976) ([AN #60](https://mailchi.mp/0dd8eb63fe2d/an-60a-new-ai-challenge-minecraft-agents-that-assist-human-players-in-creative-mode))), [comparisons](https://openai.com/blog/fine-tuning-gpt-2/) ([AN #67](https://mailchi.mp/38af1edcd025/an-67creating-environments-in-which-to-study-inner-alignment-failures)), [proxy rewards](https://arxiv.org/abs/1711.02827) ([AN #69](https://mailchi.mp/59ddebcb3b9a/an-69-stuart-russells-new-book-on-why-we-need-to-replace-the-standard-model-of-ai)), and even the [observed state](https://bair.berkeley.edu/blog/2019/02/11/learning_preferences/) ([AN #45](https://mailchi.mp/35b451cb4d70/alignment-newsletter-45)). The insight of this paper is that these are all instances of a simple underlying formalism.\n\nSpecifically, these forms of preference learning can be described by two properties: (1) the set of choices that the human picks from and (2) how each choice corresponds to a distribution over agent trajectories. Given these properties, we assume that the human makes their choice according to a Boltzmann-rational model (where the human is more likely to choose an option if it leads to higher expected reward). We have now specified a likelihood over the choice given the reward, and we can use Bayes rule to infer a distribution over the reward given the human's choice.\n\nConsider more exotic types of feedback, such as the human's decision to [turn the agent off](https://arxiv.org/abs/1611.08219) ([AN #69](https://mailchi.mp/59ddebcb3b9a/an-69-stuart-russells-new-book-on-why-we-need-to-replace-the-standard-model-of-ai)). Here, the human has two options: turning the agent off (corresponding to the agent staying still forever), or letting it continue (corresponding to the agent taking the trajectory that maximizes its current expected reward). If the agent has the right reward function, then the Boltzmann rational human would let it continue; as a result, if the human instead tries to turn the agent off, Bayes Rule allows the agent to infer that its belief about the reward must be wrong. Thus, even this decision of whether to turn the agent off can be captured in this framework.\n\nThe paper then shows two examples of new feedback types that can be generated from this framework: first, credit assignment, in which the human identifies a subset of the trajectory that had maximal reward, and second, meta-choice, where the choice of which *type* of feedback to give can itself give information about the reward function.\n\n**Rohin's opinion:** I like this paper; it's a very clear explanation of a \"recipe\" used to develop preference learning algorithms (especially at CHAI and Interact, two of the labs I'm a part of). It is particularly applicable to the case where there's a separate training phase where the human gives feedback on the reward function, and a deployment phase where the agent optimizes the reward function. Things get murkier once you move to a more online setting in which the human and agent are acting simultaneously, as in [assistance games / CIRL games](https://arxiv.org/abs/1606.03137) ([AN #69](https://mailchi.mp/59ddebcb3b9a/an-69-stuart-russells-new-book-on-why-we-need-to-replace-the-standard-model-of-ai)), where the agent can learn from pragmatic actions (see also [the deep RL version](https://interactive-learning.github.io/) ([AN #64](https://mailchi.mp/0b74c28413c0/an-64-using-deep-rl-and-reward-uncertainty-to-incentivize-preference-learning))). In particular, while previously we could separate out the grounding of choices to trajectories, and the interpretation of those trajectories (the Boltzmann rational model), this can no longer be done in an assistance game, since the way that the agent interprets the trajectories changes what the agent does which changes the choices available to the human in the next timestep. I'd be excited for more work on understanding this setting.\n\n**Technical AI alignment**\n==========================\n\n### **Learning human intent**\n\n[Goal-conditioned Imitation Learning](https://arxiv.org/abs/1906.05838) *(Yiming Ding, Carlos Florensa et al)* (summarized by Zach): Goal-conditioned tasks are objectives that can be specified at the start of an episode. Specifically, the objective is set to encourage the agent to reach an arbitrary state in the environment. **This paper investigates using goal-conditioning to improve the performance of imitation learning algorithms.** The authors build off of prior work into Hindsight-Experience Replay (HER), a method that allows standard RL algorithms to learn from failure by relabeling final states as goal states. One drawback of HER is that the search process is breadth-first since we won't know which search directions are useful before we encounter the true goal state. This can complicate exploration. On the other hand, when we have access to expert demonstrations, such as in imitation learning, we can generally avoid breadth-first search and instead focus on copying the demonstrations using a method such as generative adversarial imitation learning (GAIL). However, with GAIL we evaluate entire agent trajectories as either similar/dissimilar from the expert demonstration. Yet, it's also true that we could view different points along the trajectory as sub-goals which greatly augment the demonstration data-set. Using this insight, the authors extend goal-conditioning to the imitation learning setting. The authors test their goal-conditioned algorithm on a variety of basic manipulation tasks and show that with the goal relabeling the task is learned faster and at a higher quality than with other approaches such as GAIL or HER.\n\n**Zach's opinion:** The basic idea of augmenting demonstration data by relabeling the goal is clever. While I understood HER suffered from being a breadth-first search algorithm, I hadn't considered the possibility that GAIL might be limited to only extracting direct information from the demonstrations. Generalizing GAIL so that it can reach arbitrary states allows for a smooth transition between learning from demonstrations and achieving the desired outcome.\n\n[The two-layer model of human values, and problems with synthesizing preferences](https://www.lesswrong.com/posts/2yLn8iTrvHoEgqXcJ/the-two-layer-model-of-human-values-and-problems-with) *(Kaj Sotala)* (summarized by Rohin): This post points out a problem with the recent [preference synthesis research agenda](https://www.alignmentforum.org/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into) ([AN #60](https://mailchi.mp/0dd8eb63fe2d/an-60a-new-ai-challenge-minecraft-agents-that-assist-human-players-in-creative-mode)) (and presumably other value learning agendas as well): these agendas tend to require simple models of how human behavior, speech, or mental models relate to human preferences. However, in reality, it seems likely that the brain is a big learning machine without any innate \"values\", and what we experience as our conscious selves is a \"strategy\" chosen by this learning machine, and as such does not have a sensible interpretation as something that optimizes for \"values\". The author suggests that value learning agendas need to deal directly with the fact that there are these two \"layers\" in humans, and presents some preliminary thoughts that don't reach any particular conclusions.\n\n**Rohin's opinion:** I think this is an important critique: it seems to me that the hardest part of the three principles suggested in [Human Compatible](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem-ebook/dp/B07N5J5FTS) ([AN #69](https://mailchi.mp/59ddebcb3b9a/an-69-stuart-russells-new-book-on-why-we-need-to-replace-the-standard-model-of-ai)) is the one that requires human preferences to be tied to human behavior. It seems quite hard to make an accurate and robust model for this, for reasons like the ones given in this post.\n\n[Using vector fields to visualise preferences and make them consistent](https://www.lesswrong.com/posts/ky988ePJvCRhmCwGo/using-vector-fields-to-visualise-preferences-and-make-them) *(Michael Aird et al)* (summarized by Rohin): This post proposes that we represent a person's preferences as follows: for every state, we have a vector whose direction specifies how the person would most like the state to change, and whose magnitude specifies the intensity of the preference. Under suitable conditions on the state space, this defines a vector field. Intransitive or circular preferences correspond to the [curl](https://en.wikipedia.org/wiki/Curl_(mathematics)) of the vector field. The authors propose that a consistent set of preferences can then be inferred by \"removing the curl\", e.g. by using the [Helmholtz decomposition](https://en.wikipedia.org/wiki/Helmholtz_decomposition).\n\n### **Preventing bad behavior**\n\n[Pessimism About Unknown Unknowns Inspires Conservatism](https://www.alignmentforum.org/posts/RzAmPDNciirWKdtc7/pessimism-about-unknown-unknowns-inspires-conservatism) *(Michael Cohen et al)* (summarized by Rohin): The argument for AI risk typically involves some point at which an AI system does something unexpected and bad in a new situation that we haven't seen before (as in e.g. a treacherous turn). One way to mitigate the risk is to simply detect new situations, and ensure the AI system does something known to be safe in such situations, e.g. deferring to a human, or executing some handcoded safe baseline policy. Typical approaches involve a separate anomaly detection model. This paper considers: can we use the AI system itself to figure out when to defer to a mentor?\n\n*The key insight is that if an AI system maintains a distribution over rewards, and \"assumes the worst\" about the reward in new situations, then simply by deferring to the mentor with higher probability when the mentor would get higher expected reward, it will end up deferring to the mentor in new situations.* Hence, the title: by making the agent pessimistic about unknown unknowns (new situations), we get a conservative agent that defers to its mentor in new situations.\n\nThis is formalized in an AIXI-like setting, where we have agents that can have beliefs over all computable programs, and we only consider an online learning setting where there is a single trajectory over all time (i.e. no episodes). The math is fairly dense and I didn't try to fully understand it; as a result my summary may be inaccurate. The agent maintains a belief over world models (which predict how the environment evolves and how reward is given) and mentor models (which predict what the mentor will do, where the mentor's policy can depend on the **true** world model). It considers the β most likely world models (where β is a hyperparameter between 0 and 1). It computes the worst-case reward it could achieve under these world models, and the expected reward that the mentor achieves. It is more likely to defer to the mentor when the mentor's expected reward is higher (relative to its worst-case reward).\n\nSuch an agent queries the mentor finitely many times and eventually takes actions that are at least as good as the mentor's choices in those situations. In addition, for events with some bound on complexity, we can set things up (e.g. by having a high β) such that for any event, with high probability the agent never causes the event to occur unless the mentor has already caused the event to occur some time in the past. For example, with high probability the agent will never push the big red button in the environment, unless it has seen the mentor push the big red button in the past.\n\n**Rohin's opinion:** I think it is an underrated point that in some sense all we need to do to avoid x-risk is to make sure AI systems don't do crazy high-impact things in new situations, and that risk aversion is one way to get such an agent. This is also how [Inverse Reward Design](https://arxiv.org/abs/1711.02827) ([AN #69](https://mailchi.mp/59ddebcb3b9a/an-69-stuart-russells-new-book-on-why-we-need-to-replace-the-standard-model-of-ai)) gets its safety properties: when faced with a completely new \"lava\" tile that the agent has never seen before, the paper's technique only infers that it should be *uncertain* about the tile's reward. However, the *expected* reward is still 0, and to get the agent to actually avoid the lava you need to use risk-averse planning.\n\nThe case for pessimism is similar to the case for impact measures, and similar critiques apply: it is not clear that we can get a value-agnostic method that is both sufficiently safe to rule out all catastrophes, and sufficiently useful to replace other AI techniques. The author himself points out that if we set β high enough to be confident it is safe, the resulting agent may end up always deferring to the mentor, and so not actually be of any use. Nonetheless, I think it's valuable to point out these ways that seem to confer some nice properties on our agents, even if they can't be pushed to the extremes for fear of making the agents useless.\n\n**AI strategy and policy**\n==========================\n\n[AI Alignment Podcast: On the Long-term Importance of Current AI Policy](https://futureoflife.org/2020/02/17/on-the-long-term-importance-of-current-ai-policy-with-nicolas-moes-and-jared-brown/) *(Lucas Perry, Nicolas Moës and Jared Brown)* (summarized by Rohin): While this podcast focuses both on the details of current policy as well as the long-term impact of engaging in policy today, I'm mostly interested in the latter, and so will simply quote Lucas's summary of points for that part:\n\n1) Experience gained on short-term AI policy issues is important to be considered a relevant advisor on long-term AI policy issues coming up in the future.\n\n2) There are very few people that care about AGI safety currently in government, politics or in policy communities.\n\n3) There are opportunities to influence current AI policy decisions in order to provide a fertile ground for future policy decisions or, better but rarer, to be directly shaping AGI safety policy today through evergreen texts. Future policy that is implemented is path dependent on current policy that we implement today. What we do now is precedent setting.\n\n4) There are opportunities today to develop a skillset useful for other policy issues and causes.\n\n5) Little resource is being spent on this avenue for impact, so the current return on investment is quite good.\n\n**Rohin's opinion:** I think quite a lot about points 1 and 3, which I think also apply to technical safety research, not just policy. For our research to have an impact, it is necessary that either the research or its authors have enough credibility to actually influence decision-makers. In addition, the problems we will face in the future could depend on technical work done today: for example, if we were convinced that (say) AIs trained via evolution are too risky, we could push for AI to be developed in other ways now.\n\n[FLI Podcast: Distributing the Benefits of AI via the Windfall Clause](https://futureoflife.org/2020/02/28/distributing-the-benefits-of-ai-via-the-windfall-clause-with-cullen-okeefe/) *(Lucas Perry and Cullen O’Keefe)* (summarized by Rohin): [Last week](https://mailchi.mp/9d279b575b1a/an-88-how-the-principal-agent-literature-relates-to-ai-risk), we had a brief summary of the [Windfall Clause](https://www.fhi.ox.ac.uk/windfallclause/) paper. This podcast goes into more depth about the potential benefits and objections to this clause: it's in some sense a more accessible and conversational elaboration of many of the points made in the paper.\n\n**Other progress in AI**\n========================\n\n### **Reinforcement learning**\n\n[What Can Learned Intrinsic Rewards Capture?](http://arxiv.org/abs/1912.05500) *(Zeyu Zheng, Junhyuk Oh et al)* (summarized by Rohin): This paper studies whether a learned reward function can serve as a locus of knowledge about the environment, that can be used to accelerate training of new agents. In particular, such a learned intrinsic reward can help with test-time adaptation: in a novel environment, the intrinsic reward can quickly \"tell\" the agent e.g. where it should explore -- even if in the new environment the agent has a different action space, or uses a different learning algorithm (situations that meta learning would typically not be able to handle).\n\nThe authors create an algorithm that learns an intrinsic reward function, that when used to train a new agent over a “lifetime” (which consists of multiple episodes), leads to the best cumulative reward over the lifetime, using a meta-gradient approach. Experiments on gridworlds demonstrate that these learned intrinsic rewards: 1. switch between early exploration and later exploitation, 2. explore only for information that is relevant for optimal behavior, 3. capture invariant causal relationships, and 4. can anticipate and adapt to changes in the extrinsic reward within a lifetime.\n\n**Rohin's opinion:** A common intuition that many researchers have is that specifying *what* to do (the reward function) should be easier than specifying *how* to do it (the policy). In practice, this *doesn't* seem to be the case for deep learning, where imitation via inverse reinforcement learning (inferring a reward function and optimizing it) seems to be similar to imitation learning via behavior cloning (\"copying\" the policy). Similarly, this method seems broadly similar to meta learning algorithms like MAML and RL^2, though it does outperform them on one (probably carefully designed) transfer learning task.\n\n### **Deep learning**\n\n[The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence](https://arxiv.org/abs/2002.06177) *(Gary Marcus)* (summarized by Rohin): This paper suggests a few directions which would allow us to build more *robust* AI systems with better \"understanding\" of the world: specifically, it highlights **symbol manipulation, encoded knowledge, reasoning, and cognitive models** as areas of research for the next decade.\n\nSee also [Import AI #187](https://jack-clark.net/2020/03/02/import-ai-187-real-world-robot-tests-at-cvpr-all-hail-the-molecule-transformer-the-four-traits-needed-for-smarter-ai-systems/) and [Matthew Barnett's summary](https://www.lesswrong.com/posts/CeJs4rPgPtJPNqLMt/gary-marcus-four-steps-towards-robust-artificial).\n\n**Rohin's opinion:** The AI claims made in this paper seem pretty reasonable to me, though I think the paper overstates how much deep learning aficionados disagree with them. I certainly agree for example that existing deep learning systems do not generalize well outside of their training environment, or that AI systems will need to work with abstract knowledge, or that AI systems will have to learn from external, cultural knowledge represented in natural language. And while I am perhaps not as enamored of deep learning as (say) OpenAI or DeepMind, I'm a pretty big fan of it, and try to design algorithms where deep learning can do most of the \"heavy lifting\".\n\n**News**\n========\n\n[FHI Summer Research Fellowship](https://www.fhi.ox.ac.uk/summer-research-fellowship/) (summarized by Rohin): This six week summer fellowship allows fellows to take the lead on a project relevant to the long-term future, working with an FHI Research Scholar. Application deadline is March 22.", "url": "https://www.alignmentforum.org/posts/7bNXqdDPYpnfCNQhA/an-89-a-unifying-formalism-for-preference-learning", "date_published": "2020-03-04T18:20:01Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.724609+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "964b28072f467b8420dd5b3a6c8994b0", "source": "alignmentforum", "title": "Zoom In: An Introduction to Circuits", "text": "Chris Olah and the rest of the rest of the OpenAI Clarity team just published “[Zoom In: An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/),” a Distill article about some of the transparency research they've been doing which I think is very much worth taking a look at. I'll try to go over some of my particular highlights here, but I highly recommend reading the full article.\n\n\nSpecifically, I have [previously written](https://www.alignmentforum.org/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety) about Chris's belief that the field of machine learning should be more like the natural sciences in seeking understanding first and foremost. I think “Zoom In” is a big step towards making something like that a reality, as it provides specific, concrete, testable claims about neural networks upon which you might actually be able to build a field. The three specific claims presented in the article are:\n\n\n\n> \n> **Claim 1: Features**\n> \n> \n> Features are the fundamental unit of neural networks. They correspond to directions [in the space of neuron activations]. These features can be rigorously studied and understood.\n> \n> \n> **Claim 2: Circuits**\n> \n> \n> Features are connected by weights, forming circuits. These circuits can also be rigorously studied and understood.\n> \n> \n> **Claim 3: Universality**\n> \n> \n> Analogous features and circuits form across models and tasks.\n> \n> \n> \n\n\n“Zoom In” provides lots of in-depth justification and examples for each of these claims which I will mostly leave to the actual article. Some highlights, however:\n\n\n* How do convolutional neural networks (CNNs) detect dogs in an orientation-invariant way? It turns out they pretty consistently separately detect leftward-facing and rightward-facing dogs, then union the two together.\n* How do CNNs detect foreground-background boundaries? It turns out they use high-low frequency detectors—which look for high-frequency patterns on one side and low-frequency patterns on the other side—in a bunch of different possible orientations.\n\n\nWhat's particularly nice about “Zoom In”'s three claims in my opinion, however, is that they give other researchers a foundation to build upon. Once it's established that neural networks have meaningful features and circuits in them, discovering new such circuits becomes a legitimate scientific endeavor—especially if, as the third claim suggests, those features and circuits are universal across many different networks. From “Zoom In:”\n\n\n\n> \n> One particularly challenging aspect of being in a pre-paradigmatic field is that there isn’t a shared sense of how to evaluate work in interpretability. There are two common proposals for dealing with this, drawing on the standards of adjacent fields. Some researchers, especially those with a deep learning background, want an “interpretability benchmark” which can evaluate how effective an interpretability method is. Other researchers with an HCI background may wish to evaluate interpretability methods through user studies.\n> \n> \n> But interpretability could also borrow from a third paradigm: natural science. In this view, neural networks are an object of empirical investigation, perhaps similar to an organism in biology. Such work would try to make empirical claims about a given network, which could be held to the standard of falsifiability.\n> \n> \n> Why don’t we see more of this kind of evaluation of work in interpretability and visualization? Especially given that there’s so much adjacent ML work which does adopt this frame! One reason might be that it’s very difficult to make robustly true statements about the behavior of a neural network as a whole. They’re incredibly complicated objects. It’s also hard to formalize what the interesting empirical statements about them would, exactly, be. And so we often get standards of evaluations more targeted at whether an interpretability method is useful rather than whether we’re learning true statements.\n> \n> \n> Circuits side steps these challenges by focusing on tiny subgraphs of a neural network for which rigorous empirical investigation is tractable. They’re very much falsifiable: for example, if you understand a circuit, you should be able to predict what will change if you edit the weights. In fact, for small enough circuits, statements about their behavior become questions of mathematical reasoning. Of course, the cost of this rigor is that statements about circuits are much smaller in scope than overall model behavior. But it seems like, with sufficient effort, statements about model behavior could be broken down into statements about circuits. If so, perhaps circuits could act as a kind of epistemic foundation for interpretability.\n> \n> \n> \n\n\nI, for one, am very excited about circuits as a direction for building up an understanding-focused interpretability field and want to congratulate Chris and the rest of OpenAI Clarity for putting in the hard work of doing the foundational work necessary to start building a real field around neural network interpretability.", "url": "https://www.alignmentforum.org/posts/MG4ZjWQDrdpgeu8wG/zoom-in-an-introduction-to-circuits", "date_published": "2020-03-10T19:36:14Z", "authors": ["evhub"], "tags": ["Logic & Mathematics ", "OpenAI", "Programming", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.725250+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "42bc43e92c830b86f0fb52203047702a", "source": "alignmentforum", "title": "[AN #90]: How search landscapes can contain self-reinforcing feedback loops", "text": "Find all Alignment Newsletter resources [here](http://rohinshah.com/alignment-newsletter/). In particular, you can [sign up](http://eepurl.com/dqMSZj), or look through this [spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing) of all summaries that have ever been in the newsletter. I'm always happy to hear feedback; you can send it to me by replying to this email.\n\nAudio version [here](http://alignment-newsletter.libsyn.com/alignment-newsletter-90) (may not be up yet).\n\n**Highlights**\n--------------\n\n[Demons in Imperfect Search](https://www.alignmentforum.org/posts/KnPN7ett8RszE79PH/demons-in-imperfect-search) *(John S Wentworth)* (summarized by Asya): This post gives an analogy to explain *optimization demons*: a type of undesirable behavior that arises in imperfect search processes. In the analogy, a ball rolls down a hill trying to go as far down as possible, mimicking a gradient descent algorithm. The ball is benefited by random noise, but still basically only experiences local changes in slope-- it cannot see steep drop-offs that are a little off to the side. Small bumps in the hill can temporarily alter the ball's trajectory, and the bumps that are selected for are the ones that most effectively control its trajectory. In this way, over time the ball's trajectory selects for *demons*, twisty paths with high walls that keep the ball contained and avoid competing walls. Demons cause the ball to go down the hill as slowly as possible so that potential energy is conserved for avoiding competitor walls.\n\nThe general pattern this analogy is meant to elucidate is the following: In any imperfect search mechanism with a rich enough search space, a feedback loop can appear that creates a more-and-more perfect exploitation of the imperfect search mechanism, resulting in a whole new optimization process. The post gives several real world examples as proofs that this is a failure mode that happens in real systems. One example is metabolic reactions-- a chemical system searches by making random small changes to the system state while trying to minimize free energy. Biological systems exploit the search by manipulating the height of the barriers between low-free-energy states, raising or lowering the activation energies required to cross them. After enough time, some chemicals changed the barriers enough such that more copies of the chemicals were made, kicking off an unstable feedback loop that led to life on earth.\n\nThe post ends by posing an open question asking what about a system makes this kind of failure mode likely to happen.\n\n**Asya's opinion:** I think it's worth spelling out how this is different from the failure modes described in [Risks from Learned Optimization](https://arxiv.org/abs/1906.01820) ([AN #58](https://mailchi.mp/92b3a9458c2d/an-58-mesa-optimization-what-it-is-and-why-we-should-care)). In Risks from Learned Optimization, we are concerned that the outer optimizer will produce an unaligned *inner optimizer* because we're training it in diverse environments, and an inner optimizer may be the best solution for performing well in diverse environments. In this post, we are concerned that the outer optimizer will produce an unaligned *demon* (which may or may not be an optimizer) because the search process may have some self-reinforcing imperfections that allow it to be pushed strongly in a direction orthogonal to its objective. This direction could be bad unless the original outer objective is a perfect specification of what we want. This means that even if the [conditions for mesa-optimization](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/q2rCMHNXazALgQpGH) don't hold-- even if we're training on a fairly narrow task where search doesn't give an advantage-- there may be demon-related failure modes that are worth thinking about.\n\nI really like this post, I think it crystallizes an important failure mode that I haven't seen described before. I'm excited to see more work on this class of problems.\n\n[Tessellating Hills: a toy model for demons in imperfect search](https://www.alignmentforum.org/posts/X7S3u5E4KktLp7gHz/tessellating-hills-a-toy-model-for-demons-in-imperfect) *(DaemonicSigil)* (summarized by Asya): This post is trying to generate an example of the problem outlined in 'Demons in Imperfect Search' (summarized above): the problem where certain imperfect search processes allow for self-reinforcing behavior, 'demons', that push in a direction orthogonal to the original objective.\n\nThe post runs a simple gradient descent algorithm in an artifically constructed search space. The loss function that defines the search space has two major parts. One part straightforwardly tries to get the algorithm to move as far as it can in a particular direction *x0* -- this represents our original objective function. The other part can be thought of as a series of periodic 'valleys' along every other axis, (*x1* ... *xn*) that get steeper the farther you go along that axis.\n\nWhen running the gradient descent, at first *x0* increases steadily, and the other coordinates wander around more or less randomly. In the second phase, a self-reinforcing combination of valleys (a \"demon\") takes hold and amplifies itself drastically, feeding off the large *x0* gradient. Finally, this demon becomes so strong that the search gets stuck in a local valley and further progress stops.\n\n**Asya's opinion:** I think this is a good illustration of the problem specified in Demons in Imperfect Search. Clearly the space has to have a fairly specific shape, so the natural follow-up question, as is posed in the original post, is to think about what cases cause these kinds of self-reinforcing search spaces to arise.\n\n**Technical AI alignment**\n==========================\n\n### **Agent foundations**\n\n[A critical agential account of free will, causation, and physics](https://www.alignmentforum.org/posts/dvaCebTNc2tfMDcxS/a-critical-agential-account-of-free-will-causation-and) *(Jessica Taylor)*\n\n[Subjective implication decision theory in critical agentialism](https://www.alignmentforum.org/posts/t7jGT7uyf56TGjeba/subjective-implication-decision-theory-in-critical) *(Jessica Taylor)*\n\n### **Forecasting**\n\n[Historic trends in technological progress](https://forum.effectivealtruism.org/posts/APAD7PaEHgFyW3Nc4/ai-impacts-historic-trends-in-technological-progress) *(AI Impacts)* (summarized by Nicholas): One key question in thinking about AGI deployment and which safety problems to focus on is whether technological progress will be *continuous* or *discontinuous*. AI Impacts has researched the frequency of discontinuities in a number of case studies, that were selected on the possibility of having discontinuities. An example of a discontinuity in flight speed records would be the Fairey Delta 2 flight in 1956 which represented 19 years of progress at the previous trend. On the other hand, penicillin did not create a discontinuity of more than ten years in the number of deaths from syphilis in the US. This post summarizes a number of those case studies. As it is already a summary, I will just refer you to the post for more information.\n\n**Nicholas's opinion:** I’m looking forward to reading AI Impacts’ conclusions after completing these case studies. My impression from reading through these is that discontinuities happen, but rarely, and small discontinuities are more common than larger ones. However, I remain uncertain of a) how relevant each of these examples is to AI progress, and b) if I missed any key ways in which the examples differ from each other.\n\n**Read more:** [Incomplete case studies of discontinuous progress](https://aiimpacts.org/incomplete-case-studies-of-discontinuous-progress/)\n\n### **Miscellaneous (Alignment)**\n\n[Cortés, Pizarro, and Afonso as Precedents for Takeover](https://aiimpacts.org/cortes-pizarro-and-afonso-as-precedents-for-ai-takeover/) *(Daniel Kokotajlo)* (summarized by Matthew): This post lists three historical examples of how small human groups conquered large parts of the world, and shows how they are arguably precedents for AI takeover scenarios. The first two historical examples are the conquests of American civilizations by Hernán Cortés and Francisco Pizarro in the early 16th century. The third example is the Portugese capture of key Indian Ocean trading ports, which happened at roughly the same time as the other conquests. Daniel argues that technological and strategic advantages were the likely causes of these European victories. However, since the European technological advantage was small in this period, we might expect that an AI coalition could similarly take over a large portion of the world, even without a large technological advantage.\n\n**Matthew's opinion:** In a [comment](https://www.alignmentforum.org/posts/ivpKSjM4D6FbqF4pZ/cortes-pizarro-and-afonso-as-precedents-for-takeover?commentId=kNFNjBJjuzTd3irrR), I dispute the claimed reasons for why Europeans conquered American civilizations. I think that a large body of historical literature supports the conclusion that American civilizations fell primarily because of their exposure to diseases which they lacked immunity to, rather than because of European military power. I also think that this helps explain why Portugal was \"only\" able to capture Indian Ocean trading ports during this time period, rather than whole civilizations. I think the primary insight here should instead be that pandemics can kill large groups of humans, and therefore it would be worth exploring the possibility that AI systems use pandemics as a mechanism to kill large numbers of biological humans.\n\n**AI strategy and policy**\n==========================\n\n[Activism by the AI Community: Analysing Recent Achievements and Future Prospects](https://www.cser.ac.uk/resources/activism-ai-community-analysing-recent-achievements-and-future-prospects/) *(Haydn Belfield)* (summarized by Rohin): The AI community has been surprisingly effective at activism: it has led to discussions of a ban on lethal autonomous weapons systems (LAWS), created several initiatives on safety and ethics, and has won several victories through organizing (e.g. Project Maven). What explains this success, and should we expect it to continue in the future? This paper looks at this through two lenses.\n\nFirst, the AI community can be considered an *epistemic community*: a network of knowledge-based experts with coherent beliefs and values on a relevant topic. This seems particularly relevant for LAWS: the AI community clearly has relevant expertise to contribute, and policymakers are looking for good technical input. From this perspective, the main threats to future success are that the issues (such as LAWS) become less novel, that the area may become politicized, and that the community beliefs may become less cohesive.\n\nSecond, the AI community can be modeled as organized labor (akin to unions): since there is high demand for AI researchers, and their output is particularly important for company products, and the companies are more vulnerable to public pressure, AI researchers wield a lot of soft power when they are united. The main threat to this success is the growing pool of talent that will soon be available (given the emphasis on training experts in AI today), which will reduce the supply-demand imbalance, and may reduce how commited the AI community as a whole is to collective action.\n\nOverall, it seems that the AI community has had good success at activism so far, but it is unclear whether it will continue in the future.\n\n**Rohin's opinion:** I think the ability of the AI community to cause things to happen via activism is quite important: it seems much more likely that if AI x-risk concerns are serious, we will be able to convince the AI community of them, rather than say the government, or company executives. This mechanism of action seems much more like the \"epistemic community\" model used in this paper: we would be using our position as experts on AI to convince decision makers to take appropriate precautions with sufficiently powerful AI systems. Applying the discussion from the paper to this case, we get the perhaps unsurprising conclusion that it is primarily important that we build consensus amongst AI researchers about how risky any particular system is.\n\n[Beyond Near- and Long-Term: Towards a Clearer Account of Research Priorities in AI Ethics and Society](https://www.cser.ac.uk/resources/beyond-near-long-term/) *(Carina Prunkl and Jess Whittlestone)* (summarized by Rohin): This paper argues that the existing near-term / long-term distinction conflates four different axes on which research could differ: the capability level of AI systems (current pattern-matching systems vs. future intelligent systems), the impacts of AI systems (impacts that are being felt now like fairness vs. ones that will be felt in the future like x-risks), certainty (things that will definitely be problems vs. risks that are more speculative) and extremity (whether to prioritize particularly extreme risks). While there are certainly correlations across these axes, they are not the same thing, and discourse would be significantly improved by disambiguating the axes. For example, both authors of the paper see their work as considering the medium-to-long-term impacts of near-to-medium-term AI capabilities.\n\n**Rohin's opinion:** I definitely agree that near-term and long-term often seem to mean many different things, and I certainly support efforts to be more precise in our language.\n\nWhile we're talking about near-term and long-term, I'll add in my own gripe: \"long-term\" implies that the effects will be felt only in the far future, even though many people focused on such effects are doing so because there's a significant probability of such effects being felt in only a few decades.\n\n[Exploring AI Futures Through Role Play](https://www.cser.ac.uk/resources/ai-futures-role-play/) *(Shahar Avin et al)* (summarized by Rohin): This paper argues that role playing (akin to the \"wargames\" used in the military) is a good way to explore possible AI futures, especially to discover unusual edge cases, in a 10-30 year time horizon. Each player is assigned a role (e.g. director of AI at Tencent, or president of the US) and asked to play out their role faithfully. Each game turn covers 2 simulated years, in which players can negotiate and take public and private actions. The game facilitator determines what happens in the simulated world based on these actions. While early games were unstructured, recent games have had an AI \"tech tree\", that determines what AI applications can be developed.\n\nFrom the games played so far, the authors have found a few patterns:\n\n- Cooperation between actors on AI safety and (some) restriction on destabilizing uses of AI seem to both be robustly beneficial.\n\n- Even when earlier advances are risky, or when current advances are of unclear value, players tend to pursue AI R&D quite strongly.\n\n- Many kinds of coalitions are possible, e.g. between governments, between corporations, between governments and corporations, and between sub-roles within a corporation.\n\n**Rohin's opinion:** It makes sense that role playing can help find extreme, edge case scenarios. I'm not sure how likely I should find such scenarios -- are they plausible but unlikely (because forecasting is hard but not impossible), or are they implausible (because it would be very hard to model an *entire government*, and no one person is going to do it justice)? Note that according to the paper, the prior literature on role playing is quite positive (though of course it's talking about role playing in other contexts, e.g. business and military contexts). Still, this seems like quite an important question that strongly impacts how seriously I take the results of these role playing scenarios.\n\n**Other progress in AI**\n========================\n\n### **Deep learning**\n\n[Speeding Up Transformer Training and Inference By Increasing Model Size](https://bair.berkeley.edu/blog/2020/03/05/compress/) *(Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin et al)* (summarized by Rohin): This blog post and associated paper confirm the findings from [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) ([AN #87](https://mailchi.mp/c29b3247da6f/4da2bu7tjd)) that the most efficient way to train Transformer-based language models is to train very large models and stop before convergence, rather than training smaller models to convergence.\n\n**Read more:** [Paper: Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers](https://arxiv.org/abs/2002.11794)", "url": "https://www.alignmentforum.org/posts/7d2PsdHXrJnbofrvF/an-90-how-search-landscapes-can-contain-self-reinforcing", "date_published": "2020-03-11T17:30:02Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.725505+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "8175e731f73e0e6d91e23e3716410952", "source": "alignmentforum", "title": "What are some exercises for building/generating intuitions about key disagreements in AI alignment?", "text": "I am interested in having my own opinion about more of the [key disagreements](https://www.lesswrong.com/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment) within the AI alignment field, such as whether there is a basin of attraction for corrigibility, whether there is [a theory of rationality that is sufficiently precise to build hierarchies of abstraction](https://www.greaterwrong.com/posts/suxvE2ddnYMPJN9HD/realism-about-rationality/comment/YMNwHcPNPd4pDK7MR), and to what extent there will be a [competence gap](https://agentfoundations.org/item?id=64).\n\n\nIn [\"Is That Your True Rejection?\"](https://www.lesswrong.com/posts/TGux5Fhcd7GmTfNGC/is-that-your-true-rejection), Eliezer Yudkowsky wrote:\n\n\n\n> \n> I suspect that, in general, if two rationalists set out to resolve a disagreement that persisted past the first exchange, they should expect to find that the true sources of the disagreement are either hard to communicate, or hard to expose. E.g.:\n> \n> \n> * Uncommon, but well-supported, scientific knowledge or math;\n> * Long inferential distances;\n> * Hard-to-verbalize intuitions, perhaps stemming from specific visualizations;\n> * Zeitgeists inherited from a profession (that may have good reason for it);\n> * Patterns perceptually recognized from experience;\n> * Sheer habits of thought;\n> * Emotional commitments to believing in a particular outcome;\n> * Fear that a past mistake could be disproved;\n> * Deep self-deception for the sake of pride or other personal benefits.\n> \n> \n> \n\n\nI am assuming that something like this is happening in the key disagreements in AI alignment. The last three bullet points are somewhat uncharitable to proponents of a particular view, and also seem less likely to me. Summarizing the first six bullet points, I want to say something like: some combination of \"innate intuitions\" and \"life experiences\" led e.g. Eliezer and Paul Christiano to arrive at different opinions. I want to go through a useful subset of the \"life experiences\" part, so that I can share some of the same intuitions.\n\n\nTo that end, my question is something like: What fields should I learn? What textbooks/textbook chapters/papers/articles should I read? What historical examples (from history of AI/ML or from the world at large) should I spend time thinking about? (The more specific the resource, the better.) What intuitions should I expect to build by going through this resource? In the question title I am using the word \"exercise\" pretty broadly.\n\n\nIf you believe one just needs to be born with one set of intuitions rather than another, and that there are no resources I can consume to refine my intuitions, then my question is instead more like: How can I better introspect so as to find out which side I am on :)?\n\n\nSome ideas I am aware of:\n\n\n* Reading discussions between Eliezer/Paul/other people: I've already done a lot of this; it just feels like now I am no longer making much progress.\n* Learn more theoretical computer science to learn the Search For Solutions And Fundamental Obstructions intuition, as mentioned in [this post](http://johnsalvatier.org/blog/2017/the-i-already-get-it-slide).", "url": "https://www.alignmentforum.org/posts/bDwQddhqaTiMhbpPF/what-are-some-exercises-for-building-generating-intuitions", "date_published": "2020-03-16T07:41:59Z", "authors": ["riceissa"], "tags": ["Intuition"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.726015+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "11fe38781ba058c11631aab6f9effbe4", "source": "alignmentforum", "title": "AI Alignment Podcast: On Lethal Autonomous Weapons with Paul Scharre", "text": "Most relevant to AI alignment, and a pertinent question to focus on for interested readers/listeners is: if we are are unable to establish a governance mechanism as a global community on the concept that we should not let AI make the decision to kill humans, then what effects will this have on and can we still deal with more subtle short-term alignment considerations and long-term AI x-risk? \n\n  \n**Podcast page and audio:**  <https://futureoflife.org/2020/03/16/on-lethal-autonomous-weapons-with-paul-scharre/> \n\n**Transcript:**\n---------------\n\n**Lucas Perry:** Welcome to the AI Alignment Podcast. I’m Lucas Perry. Today’s conversation is with Paul Scharre and explores the issue of lethal autonomous weapons. And so just what is the relation of lethal autonomous weapons and the related policy and governance issues to AI alignment and long-term AI risk? Well there’s a key question to keep in mind throughout this entire conversation and it’s that: if we cannot establish a governance mechanism as a global community on the concept that we should not let AI make the decision to kill, then how can we deal with more subtle near term issues and eventual long term safety issues about AI systems? This question is aimed at exploring the idea that autonomous weapons and their related governance represent a possibly critical first step on the international cooperation and coordination of global AI issues. If we’re committed to developing beneficial AI and eventually beneficial AGI then how important is this first step in AI governance and what precedents and foundations will it lay for future AI efforts and issues? So it’s this perspective that I suggest keeping in mind throughout the conversation. And many thanks to FLI’s Emilia Javorsky for much help on developing the questions for this podcast. \n\nPaul Scharre is a Senior Fellow and Director of the Technology and National Security Program at the Center for a New American Security. He is the award-winning author of [Army of None: Autonomous Weapons and the Future of War](https://www.amazon.com/Army-None-Autonomous-Weapons-Future/dp/0393608980/ref=mt_other?_encoding=UTF8&me), which won the 2019 Colby Award and was named one of Bill Gates’ top five books of 2018.\n\nMr. Scharre worked in the Office of the Secretary of Defense (OSD) where he played a leading role in establishing policies on unmanned and autonomous systems and emerging weapons technologies. Mr. Scharre led the DoD working group that drafted DoD Directive 3000.09, establishing the Department’s policies on autonomy in weapon systems. Mr. Scharre also led DoD efforts to establish policies on intelligence, surveillance, and reconnaissance (ISR) programs and directed energy technologies. He was involved in the drafting of policy guidance in the 2012 Defense Strategic Guidance, 2010 Quadrennial Defense Review, and Secretary-level planning guidance. His most recent position was Special Assistant to the Under Secretary of Defense for Policy. Prior to joining the Office of the Secretary of Defense, Mr. Scharre served as a special operations reconnaissance team leader in the Army’s 3rd Ranger Battalion and completed multiple tours to Iraq and Afghanistan.\n\nThe Future of Life Institute is a non-profit and this podcast is funded and supported by listeners like you. So if you find what we do on this podcast to be important and beneficial, please consider supporting the podcast by donating at [futureoflife.org/donate](http://futureoflife.org/donate). If you support any other content creators via services like Patreon, consider viewing a regular subscription to FLI in the same light. You can also follow us on your preferred listening platform, like on Apple Podcasts or Spotify, by searching for us directly or following the links on the page for this podcast found in the description.\n\nAnd with that, here’s my conversion with Paul Scharre. \n\nAll right. So we’re here today to discuss your book, Army of None, and issues related to autonomous weapons in the 21st century. To start things off here, I think we can develop a little bit of the motivations for why this matters. Why should the average person care about the development and deployment of lethal autonomous weapons?\n\n**Paul Scharre:** I think the most basic reason is because we all are going to live in the world that militaries are going to be deploying future weapons. Even if you don’t serve in the military, even if you don’t work on issues surrounding say, conflict, this kind of technology could affect all of us. And so I think we all have a stake in what this future looks like.\n\n**Lucas Perry:** Let’s clarify a little bit more about what this technology actually looks like then. Often in common media, and for most people who don’t know about lethal autonomous weapons or killer robots, the media often portrays it as a terminator like scenario. So could you explain why this is wrong, and what are more accurate ways of communicating with the public about what these weapons are and the unique concerns that they pose?\n\n**Paul Scharre:** Yes, I mean, the Terminator is like the first thing that comes up because it’s such a common pop culture reference. It’s right there in people’s minds. So I think go ahead and for the listeners, imagine that humanoid robot in the Terminator, and then just throw that away, because that’s not what we’re talking about. Let me make a different comparison. Self-driving cars. We are seeing right now the evolution of automobiles that with each generation of car incorporate more autonomous features: parking, intelligent cruise control, automatic braking. These increasingly autonomous features in cars that are added every single year, a little more autonomy, a little more autonomy, are taking us down at some point in time to a road of having fully autonomous cars that would drive themselves. We have something like the Google car where there’s no steering wheel at all. People are just passengers along for the ride. We’re seeing something very similar happen in the military with each generation of robotic systems and we now have air and ground and undersea robots deployed all around the world in over 100 countries and non state groups around the globe with some form of drones or robotic systems, and with each generation they’re becoming increasingly autonomous.\n\nNow, the issue surrounding autonomous weapons is, what happens when a predator drone has as much autonomy as a self-driving car? What happens when you have a weapon that’s out in the battlefield, and it’s making its own decisions about whom to kill? Is that something that we’re comfortable with? What are the legal and moral and ethical ramifications of this? And the strategic implications? What might they do for the balance of power between nations, or stability among countries? These are really the issues surrounding autonomous weapons, and it’s really about this idea that we might have, at some point of time and perhaps the not very distant future, machines making their own decisions about whom to kill on the battlefield.\n\n**Lucas Perry:** Could you unpack a little bit more about what autonomy really is or means because it seems to me that it’s more like an aggregation of a bunch of different technologies like computer vision and image recognition, and other kinds of machine learning that are aggregated together. So could you just develop a little bit more about where we are in terms of the various technologies required for autonomy?\n\n**Paul Scharre:** Yes, so autonomy is not really a technology, it’s an attribute of a machine or of a person. And autonomy is about freedom. It’s the freedom that a machine or a person is given to perform some tasks in some environment for some period of time. As people, we have very little autonomy as children and more autonomy as we grow up, we have different autonomy in different settings. In some work environments, there might be more constraints put on you; what things you can and cannot do. And it’s also environment-specific and task-specific. You might have autonomy to do certain things, but not other things. It’s the same with machines. We’re ultimately talking about giving freedom to machines to perform certain actions under certain conditions in certain environments.\n\nThere are lots of simple forms of autonomy that we interact with all the time that we sort of take for granted. A thermostat is a very simple autonomous system, it’s a machine that’s given a freedom to decide… decide, let’s put that in air quotes, because we come back to what it means for machines to decide. But basically, the thermostat is given the ability to turn on and off the heat and air conditioning based on certain parameters that a human sets, a desired temperature, or if you have a programmable thermostat, maybe the desired temperature at certain times a day or days of the week, is a very bounded kind of autonomy. And that’s what we’re talking about for any of these machines. We’re not talking about freewill, or whether the machine develops consciousness. That’s not a problem today, maybe someday, but certainly not with the machines we’re talking about today. It’s a question really of, how much freedom do we want to give machines, or in this case, weapons operating on the battlefield to make certain kinds of choices?\n\nNow we’re still talking about weapons that are designed by people, built by people, launched by people, and put into the battlefields to perform some mission, but there might be a little bit less human control than there is today. And then there are a whole bunch of questions that come along with that, like, is it going to work? Would it be effective? What happens if there are accidents? Are we comfortable with seeding that degree of control over to the machine?\n\n**Lucas Perry:** You mentioned the application of this kind of technology in the context of battlefields. Is there also consideration and interest in the use of lethal autonomous weapons in civilian contexts?\n\n**Paul Scharre:** Yes, I mean, I think there’s less energy on that topic. You certainly see less of a poll from the police community. I mean, I don’t really run into people in a police or Homeland Security context, saying we should be building autonomous weapons. Well, you will hear that from militaries. Oftentimes, groups that are concerned about the humanitarian consequences of autonomous weapons will raise that as a concern. There’s both what might militaries do in the battlefield, but then there’s a concern about proliferation. What happens when the technology proliferates, and it’s being used for internal security issues, could be a dictator, using these kinds of weapons to repress the population. That’s one concern. And that’s, I think, a very, very valid one. We’ve often seen one of the last checks against dictators, is when they tell their internal security forces to fire on civilians, on their own citizens. There have been instances where the security forces say, “No, we won’t.” That doesn’t always happen. Of course, tragically, sometimes security forces do attack their citizens. We saw in the massacre in Tiananmen Square that Chinese military troops are willing to murder Chinese citizens. But we’ve seen other instances, certainly in the fall of the Eastern Bloc at the end of the Cold War, that security forces… these are our friends, these are our family. We’re not going to kill them.\n\nAnd autonomous weapons could take away one of those checks on dictators. So I think that’s a very valid concern. And that is a more general concern about the proliferation of military technology into policing even here in America. We’ve seen this in the last 20 years, is a lot of military tech ends up being used by police forces in ways that maybe isn’t appropriate. And so that’s, I think, a very valid and legitimate sort of concern about… even if this isn’t kind of the intended use, what would that look like and what are the risks that could come with that, and how should we think about those kinds of issues as well?\n\n**Lucas Perry:** All right. So we’re developing autonomy in systems and there’s concern about how this autonomy will be deployed in context where lethal force or force may be used. So the question then arises and is sort of the question at the heart of lethal autonomous weapons: Where is it that we will draw a line between acceptable and unacceptable uses of artificial intelligence in autonomous weapons or in the military, or in civilian policing? So I’m curious to know how you think about where to draw those lines or that line in particular, and how you would suggest to any possible regulators who might be listening, how to think about and construct lines of acceptable and unacceptable uses of AI.\n\n**Paul Scharre:** That’s a great question. So I think let’s take a step back first and sort of talk about, what would be the kinds of things that would make uses acceptable or unacceptable. Let’s just talk about the military context just to kind of bound the problem for a second. So in the military context, you have a couple reasons for drawing lines, if you will. One is legal issues, legal concerns. We have a legal framework to think about right and wrong in war. It’s called the laws of war or international humanitarian law. And it lays out a set of parameters for what is acceptable and what… And so that’s one of the places where there has been consensus internationally, among countries that come together at the United Nations through the Convention on Certain Conventional Weapons, the CCW, the process, we’ve had conversations going on about autonomous weapons.\n\nOne of the points of consensus among nations is that existing international humanitarian law or the laws of war would apply to autonomous weapons. And that any uses of autonomy in weapons, those weapons have to be used in a manner that complies with the laws of war. Now, that may sound trivial, but it’s a pretty significant point of agreement and it’s one that places some bounds on things that you can or cannot do. So, for example, one of the baseline principles of the laws of war is the principle of distinction. Military forces cannot intentionally target civilians. They can only intentionally target other military forces. And so any use of force these people to comply with this distinction, so right off the bat, that’s a very important and significant one when it comes to autonomous weapons. So if you have to use a weapon that could not be used in a way to comply with this principle of distinction, it would be illegal under the laws war and you wouldn’t be able to build it.\n\nAnd there are other principles as well, principles about proportionality, and ensuring that any collateral damage that affects civilians or civilian infrastructure is not disproportionate to the military necessity of the target that is being attacked. There are principles about avoiding unnecessary suffering of combatants. Respecting anyone who’s rendered out of combat or the appropriate term is “hors de combat,” who surrendered have been incapacitated and not targeting them. So these are like very significant rules that any weapon system, autonomous weapon or not, has to comply with. And any use of any weapon has to comply with, any use of force. And so that is something that constrains considerably what nations are permitted to do in a lawful fashion. Now do people break the laws of war? Well, sure, that happens. We’re seeing that happen in Syria today, Bashar al-Assad is murdering civilians, there are examples of Rogue actors and non state terrorist groups and others that don’t care about respecting the laws of war. But those are very significant bounds.\n\nNow, one could also say that there are more bounds that we should put on autonomous weapons that might be moral or ethical considerations that exist outside the laws of war, that aren’t written down in a formal way in the laws of war, but they’re still important and I think those often come to the fore with this topic. And there are other ones that might apply in terms of reasons why we might be concerned about stability among nations. But the laws of war, at least a very valuable starting point for this conversation about what is acceptable and not acceptable. I want to make clear, I’m not saying that the laws of war are insufficient, and we need to go beyond them and add in additional constraints. I’m actually not saying that. There are people that make that argument, and I want to give credit to their argument, and not pretend it doesn’t exist. I want the listeners to sort of understand the full scope of arguments about this technology. But I’m not saying myself that’s the case necessarily. But I do think that there are concerns that people raise.\n\nFor example, people might say it’s wrong for a machine to decide whom to kill, it’s wrong for a machine to make the decision about life and death. Now I think that’s an interesting argument. Why? Why is it wrong? Is it because we think the machine might get the answer wrong, it might perform not as well as the humans because I think that there’s something intrinsic about weighing the value of life and death that we want humans to do, and appreciating the value of another person’s life before making one of these decisions. Those are all very valid counter arguments that exist in this space.\n\n**Lucas Perry:** Yes. So thanks for clarifying that. For listeners, it’s important here to clarify the difference where some people you’re saying would find the laws of war to be sufficient in the case of autonomous weapons, and some would not.\n\n**Paul Scharre:** Yes, I mean, this is a hotly debated issue. I mean, this is in many ways, the crux of the issue surrounding autonomous weapons. I’m going to oversimplify a bit because you have a variety of different views on this, but you certainly have some people whose views are, look, we have a set of structures called the laws of war that tell us what right and wrong looks like and more. And most of the things that people are worried about are already prohibited under the laws of war. So for example, if what you’re worried about is autonomous weapons, running amok murdering civilians, that’s illegal under the laws of war. And so one of the points of pushback that you’ll sometimes get from governments or others to the idea of creating like an ad hoc treaty that would ban autonomous weapons or some class of autonomous weapons, is look, some of the things people worry about like they’re already prohibited under the laws of war, passing another law to say the thing that’s already illegal is now illegal again doesn’t add any value.\n\nThere’s group of arguments that says the laws of war dictate effects in the battlefield. So they dictate sort of what the end effect is, they don’t really affect the process. And there’s a line of reasoning that says, that’s fine. The process doesn’t matter. If someday we could use autonomous weapons in a way that was more humane and more precise than people, then we should use them. And just the same way that self-driving cars will someday save lives on roads by avoiding accidents, maybe we could build autonomous weapons that would avoid mistakes in war and accidentally targeting civilians, and therefore we should use them. And let’s just focus on complying better with the laws of war. That’s one school of thought.\n\nThen there’s a whole bunch of reasons why you might say, well, that’s not enough. One reason might be, well, militaries’ compliance with the laws of war. Isn’t that great? Actually, like people talk a good game, but when you look at military practice, especially if the rules for using weapon are kind of convoluted, you have to take a bunch of additional steps in order to use it in a way that’s lawful, that kind of goes out the window in conflict. Real world and tragic historical example of this was experienced throughout the 20th century with landmines where land mines were permitted to be used lawfully, and still are, if you’re not a signatory to the Ottawa Convention, they’re permitted to be used lawfully provided you put in a whole bunch of procedures to make sure that minefields are marked and we know the location of minefields, so they can be demined after conflict.\n\nNow, in practice, countries weren’t doing this. I mean, many of them were just scattering mines from the air. And so we had this horrific problem of millions of mines around the globe persisting after a conflict. The response was basically this global movement to ban mines entirely to say, look, it’s not that it’s inconceivable to use mines in a way that you mean, but it requires a whole bunch of additional efforts, that countries aren’t doing, and so we have to take this weapon away from countries because they are not actually using it in a way that’s responsible. That’s a school of thought with autonomous weapons. Is look, maybe you can conjure up thought experiments about how you can use autonomous weapons in these very specific instances, and it’s acceptable, but once you start any use, it’s a slippery slope, and next thing you know, it’ll be just like landmines all over again, and they’ll be everywhere and civilians will be being killed. And so the better thing to do is to just not let this process even start, and not letting militaries have access to the technology because they won’t use it responsibly, regardless of whether it’s theoretically possible. That’s a pretty reasonable and defensible argument. And there are other arguments too.\n\nOne could say, actually, it’s not just about avoiding civilian harm, but there’s something intrinsic about weighing the value of an enemy soldier’s life, that we want humans involved in that process. And that if we took humans away from that process, we’ll be losing something that sure maybe it’s not written down in the laws of war, but maybe it’s not written down because it was always implicit that humans will always be making these choices. And now that it’s decision in front of us, we should write this down, that humans should be involved in these decisions and should be weighing the value of the human life, even an enemy soldier. Because if we give that up, we might give up something that is a constraint on violence and war that holds back some of the worst excesses of violence, we might even can make something about ourselves. And this is, I think, a really tricky issue because there’s a cost to humans making these decisions. It’s a very real cost. It’s a cost in post traumatic stress that soldiers face and moral injury. It’s a cost in lives that are ruined, not just the people that are killed in a battlefield, but the people have to live with that violence afterwards, and the ramifications and even the choices that they themselves make. It’s a cost in suicides of veterans, and substance abuse and destroyed families and lives.\n\nAnd so to say that we want humans to stay still evolved to be more than responsible for killing, is to say I’m choosing that cost. I’m choosing to absorb and acknowledge and take on the cost of post traumatic stress and moral injury, and also the burdens that come with war. And I think it’s worth reflecting on the fact that the burdens of war are distributed very unequally, not just between combatants, but also on the societies that fight. As a democratic nation in the United States, we make a decision as a country to go to war, through our elected representatives. And yet, it’s a very tiny slice of the population that bears the burden for that war, not just putting themselves at risk, but also carrying the moral burden of that afterwards.\n\nAnd so if you say, well, I want there to be someone who’s going to live with that trauma for the rest of your life. I think that’s an argument that one can make, but you need to acknowledge that that’s real. And that’s not a burden that we all share equally, it’s a burden we’re placing on young women and men that we send off to fight on our behalf. The flip side is if we didn’t do that, if we fought a war and no one felt the moral burden of killing, no one slept uneasy at night afterwards, what would they say about us as a society? I think these are difficult questions. I don’t have easy answers to that. But I think these are challenging things for us to wrestle with.\n\n**Lucas Perry:** Yes, I mean, there’s a lot there. I think that was a really good illustration of the different points of views on this. I hadn’t heard or considered much the implications of post traumatic stress. And I think moral burden, you called it that would be a factor in what autonomous weapons would relieve in countries which have the power to develop them. Speaking personally, I think I find the arguments most compelling about the necessity of having human beings integrated in the process of decision making with regards to killing, because if you remove that, then you’re removing the deep aspect of humanity, which sometimes does not follow the laws of war, which we currently don’t have complex enough preference learning techniques and machine learning techniques to actually train autonomous weapon systems in everything that human beings value and care about, and that there are situations where deviating from following the laws of war may be the best thing to do. I’m not sure if you have any thoughts about this, but I think you did a good job of illustrating all the different positions, and that’s just my initial reaction to it.\n\n**Paul Scharre:** Yes, these are tricky issues. And so I think one of the things I want to try to do for listeners is try to lay out the landscape of what these arguments are, and some of the pros and cons of them because I think sometimes they will often oversimplify on all sides. The other people will be like, well, we should have humans involved in making these decisions. Well, humans involved where? If I get into a self-driving car that has no steering wheel, it’s not true that there’s no human involvement. The type of human involvement has just changed in terms of where it exists. So now, instead of manually driving the car, I’m still choosing the car’s destination, I’m still telling the car where I want to go. You’re going to get into the car and car take me wherever you want to go. So the type of human involvement is changed.\n\nSo what kind of human relationship do we want with decisions about life and death in the battlefield? What type of human involvement is right or necessary or appropriate and for what reason? For a legal reason, for a moral reason. These are interesting challenges. We haven’t had to confront anymore. These arguments I think unfairly get simplified on all sides. Conversely, you hear people say things like, it doesn’t matter, because these weapons are going to get built anyway. It’s a little bit overly simplistic in the sense that there are examples of successes in arms control. It’s hard to pull off. There are many examples of failures as well, but there are places where civilized nations have walked back from some technologies to varying degrees of success, whether it’s chemical weapons or biological weapons or other things. So what is success look like in constraining a weapon? Is it no one ever uses the weapon? Is it most nations don’t use it? It’s not used in certain ways. These are complicated issues.\n\n**Lucas Perry:** Right. So let’s talk a little bit here about integrating human emotion and human reasoning and humanity itself into the autonomous weapon systems and the life or death decisions that they will be making. So hitting on a few concepts here, if you could help explain what people mean when they say human in the loop, and human on the loop, and how this relates to the integration of human control and human responsibility and human accountability in the use of autonomous weapons.\n\n**Paul Scharre:** Let’s unpack some of this terminology. Broadly speaking, people tend to use the terms human in the loop, on the loop, or out of the loop to refer to semi autonomous weapons human is in the loop, which means that for any really semi autonomous process or system, the machine is taking an action and then it pauses and waits for humans to take a positive action before proceeding. A good example of a human in the loop system is the automated backups on your computer when they require you to push a button to say okay to do the backup now. They’re waiting music in action before proceeding. In a human on the loop system, or one where the supervisor control is one of the human doesn’t have to take any positive action for the system to proceed. The human can intervene, so the human can sit back, and if you want to, you can jump in.\n\nExample of this might be your thermostat. When you’re in a house, you’ve already set the parameters, it’ll turn on the heat and air conditioning on its own, but if you’re not happy with the outcome, you could change it. Now, when you’re out of the house, your thermostat is operating in a fully autonomous fashion in this respect where humans out of the loop. You don’t have any ability to intervene for some period of time. It’s really all about time duration. For supervisory control, how much time does the human have to identify something is wrong and then intervene? So for example, things like the Tesla autopilots. That’s one where the human is in a supervisory control capacity. So the autopilot function in a car, the human doesn’t have to do anything, car’s driving itself, but they can intervene.\n\nThe problem with some of those control architectures is the time that you are permitting people to identify that there’s a problem, figure out what’s going on, decide to take action, intervene, really realistic before harm happens. Is it realistic that a human can be not paying attention, and then all of a sudden, identify that the car is in trouble and leap into action to avoid an accident when you’re speeding on the highway 70 miles an hour? And then you can see quite clearly in a number of fatal accidents with these autopilots, that that’s not feasible. People actually aren’t capable of doing that. So you’ve got to think about sort of what is the role of the human in this process? This is not just a semi autonomous or supervised autonomous or fully autonomous process. It’s one where the human is involved in some varying capacity.\n\nAnd what are we expecting the human to do? Same thing with something that’s fully autonomous. We’re talking about a system that’s operating on its own for some period of time. How long before it checks back in with a person? What information is that person given? And what is their capacity to intervene or how bad could things go wrong when the person is not involved? And when we talk about weapons specifically. There are lots of weapons that operate in a semi autonomous fashion today where the human is choosing the target, but there’s a lot of automation in IDing targets presenting information to people in actually carrying out an attack, once the human has chosen a target, there are many, many weapons that are what the military calls fire and forget weapon, so once it’s launched, it’s not coming back. Those have been widely used for 70 years since World War Two. So that’s not new.\n\nThere are a whole bunch of weapons that operate in a supervisory autonomy mode, where humans on the loop. These are generally used in a more limited fashion for immediate localized defense of air bases or ships or ground vehicles defending against air or missile or rocket attack, particularly when the speed of these attacks might overwhelm people’s ability to respond. For humans to be in the loop, for humans to push a button, every time there’s a missile coming in, you could have so many missiles coming in so fast that you have to just simply activate an automatic defensive mode that will shoot down all have the missiles based on some pre-programmed parameters that humans put into the system. This exists today. The systems have been around for decades since the 1980s. And there were widespread use with at least 30 countries around the globe. So this is a type of weapon system that’s already in operation. These supervisory autonomous weapons. What really would be new would be fully autonomous weapons that operate on their own, whereas humans are still building them and launching them, but humans put them into operation, and then there’s some period of time where they were able to search a target area for targets and they were able to find these targets, and then based on some programming that was designed by people, identify the targets and attack them on their own.\n\n**Lucas Perry:** Would you consider that out of the loop for that period of time?\n\n**Paul Scharre:** Exactly. So over that period of time, humans are out of the loop on that decision over which targets they’re attacking. That would be potentially largely a new development in war. There are some isolated cases of some weapon systems that cross this line, by in large that would be new. That’s at least the starting point of what people might be concerned about. Now, you might envision things that are more advanced beyond that, but that’s sort of the near term development that could be on the horizon in the next five to 15 years, telling the weapon system, go into this area, fly around or search around underwater and find any ships of this type and attack them for some period of time in space. And that changes the human’s relationship with the use of force a little bit. It doesn’t mean the humans not involved at all, but the humans not quite as involved as they used to be. And is that something we’re comfortable with? And what are the implications of that kind of shift in warfare.\n\n**Lucas Perry:** So the relevant things here are how this helps to integrate human control and human responsibility and human accountability into autonomous weapons systems. And just hearing you speak about all of that, it also seems like very relevant questions have to do with human psychology, about what human beings are actually likely to be able to do. And then also, I think you articulately put the practical question of whether or not people will be able to react to certain threats given certain situations. So in terms of trying to understand acceptable and unacceptable uses of autonomous weapons, that seems to supervene upon a lot of these facets of benefits and disadvantages of in the loop, on the loop, and out of the loop for different situations and different risks, plus how much we’re willing to automate killing and death and remove human decision making from some of these situations or not.\n\n**Paul Scharre:** Yes, I mean, I think what’s challenging in this space is that it would be nice, it would be ideal if we could sort of reach agreement among nations for sort of a lethal laws of robotics, and Isaac Asimov’s books about robots you think of these three laws of robotics. Well, those laws aren’t going to work because one of them is not harming a human being and it’s not going to work in the military context, but could there be some agreement among countries for lethal laws of robots that would govern the behavior of autonomous systems in war, and it might sort of say, these are the things that are acceptable or not? Maybe. Maybe that’s possible someday. I think we’re not there yet at least, there are certainly not agreement as widespread disagreement among nations about what approach to take. But the good starting position of trying to understand what are the goals we want to achieve. And I think you’re right that we need to keep the human sort of front and center. But I this this is like a really important asymmetry between humans and machines that’s worth highlighting, which is to say that the laws of war government effects in the battlefield, and then in that sentence, the laws of war, don’t say the human has to pick every target, the laws of war say that the use of force must be executed according to certain principles of distinction and proportionality and other things.\n\nOne important asymmetry in the laws of war, however, is that machines are not legal agents. Only humans have legal agents. And so it’s ultimately humans that are responsible for complying with the laws of war. You can’t put a machine on trial for a war crime. It doesn’t make sense. It doesn’t have intentionality. So it’s ultimately a human responsibility to ensure this kind of compliance with the laws of war. It’s a good starting point then for conversation to try to understand if we start from that proposition that it’s a human responsibility to ensure compliance with the laws of war, then what follows from that? What balances that place on human involvement? One of the early parts of the conversations on autonomous weapons internationally came from this very technological based conversation. To say, well, based on the technology, draw these lines, you should put these limits in place. The problem with that approach is not that you can’t do it.\n\nThe problem is the state of the technology when? 2014 when discussions on autonomous weapons started at the very beginning of the deep learning revolution, today, in 2020, our estimate of whether technology might be in five years or 10 years or 50 years? The technology moving so quickly than any technologically based set of rules about how we should approach this problem and what is the appropriate use of machines versus human decision making in the use of force. Any technologically based answer is one that we may look back in 10 years or 20 years and say is wrong. We could get it wrong in the sense that we might be leaving valuable technological opportunities on the table and we’re banning technology that if we used it actually might make war more humane and reduce civilian casualties, or we might be permitting technologies that turned out in retrospect to be problematic, and we shouldn’t have done that.\n\nAnd one of the things we’ve seen historically when you look at attempts to ban weapons is that ones that are technologically based don’t always fare very well over time. So for example, the early bans on poison gas banned the use of poison gas that are launched from artillery shells. It allowed actually poison gas administered via canisters, and so the first use of poison gas in World War One by the Germans was canister based, they actually just laid out little canisters and then open the valves. Now that turns out to be not very practical way of using poison gas in war, because you have someone basically on your side standing over this canister, opening a valve and then getting gassed. And so it’s a little bit tricky, but technically permissible.\n\nOne of the things that can be challenging is it’s hard to foresee how the technology is going to evolve. A better approach and one that we’ve seen the dialogue internationally sort of shift towards is our human-centered approach. To start from the position of the human and say, look, if we had all the technology in the world and war, what decisions would we want humans to make and why? Not because the technology cannot make decisions, but because it should not. I think it’s actually a very valuable starting place to understand a conversation, because the technology is moving so quickly.\n\nWhat role do we want humans to play in warfare, and why do we think this is the case? Are there some tasks in war, or some decisions that we think are fundamentally human that should be decisions that only humans should make and we shouldn’t hand off to machines? I think that’s a really valuable starting position then to try to better interrogate how do we want to use this technology going forward? Because the landscape of technological opportunity is going to keep expanding. And so what do we want to do with this technology? How do we want to use it? And are there ways that we can use this technology that keeps humans in control of the use of force in the battlefield? Keep humans legally and morally and ethically responsible, but may make war more humane in the process, that may make war more precise, that may reduce civilian casualties without losing our humanity in the process.\n\n**Lucas Perry:** So I guess the thought experiment, there would be like, if we had weapons that let us just delete people instantly without consequences, how would we want human decision making to be integrated with that? Reflecting on that also makes me consider this other point that I think is also important for my considerations around lethal autonomous weapons, which is the necessity of integrating human experience in the consequences of war, the pain and the suffering and the carnage and the PTSD as being almost necessary vehicles to some extent to make us tired of it to integrate how horrible it is. So I guess I would just be interested in integrating that perspective into it not just being about humans making decisions and the decisions being integrated in the execution process, but also about the experiential ramifications of being in relation to what actually happens in war and what violence is like and what happens in violence.\n\n**Paul Scharre:** Well, I think that we want to unpack a little bit some of the things you’re talking about. Are we talking about ensuring that there is an accurate representation to the people carrying out the violence about what’s happening on the other end, that we’re not sanitizing things. And I think that’s a fair point. When we begin to put more psychological barriers between the person making the decision and the effects, it might be easier for them to carry out larger scale attacks, versus actually making war and more horrible. Now that’s a line of reasoning, I suppose, to say we should make war more horrible, so there’ll be less of it. I’m not sure we might get the outcome that there is less of it. We just might have more horrible war, but that’s a different issue. Those are more difficult questions.\n\nI will say that I often hear philosophers raising things about skin in the game. I rarely hear them being raised by people who have had skin in the game, who have experienced up close in a personal way the horrors of war. And I’m less convinced that there’s a lot of good that comes from the tragedy of war. I think there’s value in us trying to think about how do we make war less terrible? How do we reduce civilian casualties? How do we have less war? But this often comes up in the context of technologies like we should somehow put ourselves at risk. No military does that, no military has ever done that in human history. The whole purpose of militaries getting technology in training is to get an advantage on the adversary. It’s not a fair fight. It’s not supposed to be, it’s not a boxing match. So these are things worth exploring. We need to come from the standpoint of the reality of what war is and not from a philosophical exercise about war might be, but deal with the realities of what actually occurs in the battlefield.\n\n**Lucas Perry:** So I think that’s a really interesting point. And as someone with a background and interest in philosophy, it’s quite funny. So you do have experience in war, right?\n\n**Paul Scharre:** Yes, I’ve fought in Iraq and Afghanistan.\n\n**Lucas Perry:** Then it’s interesting for me, if you see this distinction between people who are actually veterans, who have experienced violence and carnage and tragedies of war, and the perspective here is that PTSD and associated trauma with these kinds of experiences, you find that they’re less salient for decreasing people’s willingness or decision to engage in further war. Is that your claim?\n\n**Paul Scharre:** I don’t know. No, I don’t know. I don’t know the answer to that. I don’t know. That’s some difficult question for political scientists to figure out about voting preferences of veterans. All I’m saying is that I hear a lot of claims in this space that I think are often not very well interrogated or not very well explored. And there’s a real price that people pay for being involved. Now, people want to say that we’re willing to bear that price for some reason, like okay, but I think we should acknowledge it.\n\n**Lucas Perry:** Yeah, that make sense. I guess the thing that I was just pointing at was it would be psychologically interesting to know if philosophers are detached from the experience, maybe they don’t actually know about the psychological implications of being involved in horrible war. And if people who are actually veterans disagree with philosophers about the importance of there being skin in the game, if philosophers say that skin in the game reduces willingness to be in war, if the claim is that that wouldn’t actually decrease their willingness to go to war. I think that seems psychologically very important and relevant, because there is this concern about how autonomous weapons and integrating human decision making to lethal autonomous weapons would potentially sanitize war. And so there’s the trade off between the potential mitigating effects of being involved in war, and then also the negative effects which are incurred by veterans who would actually have to be exposed by it and bring the trauma back for communities to have deeper experiential relation with.\n\n**Paul Scharre:** Yes, and look, we don’t do that, right? We had a whole generation of veterans come back from Vietnam and we as society listen to the stories and understand them and understand, no. I have heard over the years people raise this issue whether it’s drones, autonomous weapons, this issue of having skin in the game either physically being at risk or psychologically. And I’ve rarely heard it raised by people who it’s been them who’s on the line. People often have very gut emotional reactions to this topic. And I think that’s valuable because it’s speaking to something that resonates with people, whether it’s an emotional reaction opposed to autonomous weapons, and that you often get that from many people that go, there’s something about this. It doesn’t feel right. I don’t like this idea. Or people saying, the opposite reaction. Other people that say that “wouldn’t this make war great, it’s more precise and more humane,” and which my reaction is often a little bit like… have you ever interacted with a computer? They break all the time. What are you talking about?\n\nBut all of these things I think they’re speaking to instincts that people have about this technology, but it’s worth asking questions to better understand, what is it that we’re reacting to? Is it an assumption about the technologies, is it an assumption about the nature of war? One of the concerns I’ve heard raised is like this will impersonalize war and create more distance between people killing. If you sort of buy that argument, that impersonal war is a bad thing, then you would say the greatest thing would be deeply personal war, like hand to hand combat. It appears to harken back to some glorious age of war when people looked each other in the eye and hacked each other to bits with swords, like real humans. That’s not that that war never occurred in human history. In fact, we’ve had conflicts like that, even in recent memory that involve hand to hand weapons. They tend not to be very humane conflicts. When we see civil violence, when people are murdering each other with machetes or garden tools or other things, it tends to be horrific communal violence, mass atrocities in Rwanda or Cambodia or other places. So I think it’s important to deal with the reality of what war is and not some fantasy.\n\n**Lucas Perry:** Yes, I think that that makes a lot of sense. It’s really tricky. And the psychology around this I think is difficult and probably not studied enough.\n\n**Paul Scharre:** There’s real war that occurs in the world, and then there’s the fantasy of war that we, as a society, tell ourselves when we go to movie theaters, and we watch stories about soldiers who are heroes, who conquer the bad guys. We’re told a fantasy, and it’s a fantasy as a society that allows society to perpetuate wars, that allows us to send young men and women off to die. And it’s not to say that there are no circumstances in which a nation might need to go to war to defend itself or its interest, but we sort of dress war up in these pretty clothes, and let’s not confuse that with the reality of what actually occurs. People said, well, through autonomous weapons, then we won’t have people sort of weighing the value of life and death. I mean, it happens sometimes, but it’s not like every time someone dies in war, that there was this thoughtful exercise where a committee sat around and said, “Do we really need to kill this person? Is it really appropriate?” There’s a lot of dehumanization that goes on on the battlefield. So I think this is what makes this issue very challenging. Many of the objections to autonomous weapons are objections to war. That’s what people are actually objecting to.\n\nThe question isn’t, is war bad? Of course war’s terrible? The question is sort of, how do we find ways going forward to use technology that may make war more precise and more humane without losing our humanity in the process, and are ways to do that? It’s a challenging question. I think the answer is probably yes, but it’s one that’s going to require a lot of interrogation to try to get there. It’s a difficult issue because it’s also a dynamic process where there’s an interplay between competitors. If we get this wrong, we can easily end up in a situation where there’s less human control, there’s more violence and war. There are lots of opportunities to make things worse as well.\n\nIf we could make war perfect, that would be great, in terms of no civilian suffering and reduce the suffering of enemy combatants and the number of lives lost. If we could push a button and make war go away, that would be wonderful. Those things will all be great. The more practical question really is, can we improve upon the status quo and how can we do so in a thoughtful way, or at least not make things worse than today? And I think those are hard enough problems to try to address.\n\n**Lucas Perry:** I appreciate that you bring a very holistic, well-weighed perspective to the varying sides of this issue. So these are all very big and difficult. Are you aware of people actually studying whether some of these effects exist or not, and whether they would actually sanitize things or not? Or is this basically all just coming down to people’s intuitions and simulations in their head?\n\n**Paul Scharre:** Some of both. There’s really great scholarship that’s being done on autonomous weapons, certainly there’s a robust array of legal based scholarship, people trying to understand how the law of war might interface with autonomous weapons. But there’s also been worked on by thinking about some of these human psychological interactions, Missy Cummings, who’s at Duke who runs the humans and automation lab down has done some work on human machine interfaces on weapon systems to think through some of these concerns. I think probably less attention paid to the human machine interface dimension of this and the human psychological dimension of it. But there’s been a lot of work done by people like Heather Roth, people at Article 36, and others thinking about concepts of meaningful human control and what might look like in weapon systems.\n\nI think one of the things that’s challenging across the board in this issue is that it is a politically contentious topic. You have kind of levels of this debate going on, you have scholars trying to sort of understand the issue maybe, and then you also have a whole array of politically motivated groups, international organizations, civil society organizations, countries, duking it out basically, at the UN and in the media about where we should go with this technology. As you get a lot of motivated reasoning on all sides about what should the answer be. So for example, one of the things that fascinates me is i’ll often hear people say, autonomous weapons are terrible, and they’ll have a terrible outcome, and we need to ban them now. And if we just pass a treaty and we have enough political will we could ban them. I’ll also hear people say a ban would be pointless, it wouldn’t work. And anyways, wouldn’t autonomous weapons be great? There are other possible beliefs. One could say that a ban is feasible, but the weapons aren’t that big of a deal. So it just seems to me like there’s a lot of politically motivated reasoning that goes on this debate, which makes it very challenging.\n\n**Lucas Perry:** So one of the concerns around autonomous weapons has to do with accidental escalation of warfare and conflict. Could you explore this point and explain what some strategies might be to prevent accidental escalation of warfare as AI is increasingly being used in the military?\n\n**Paul Scharre:** Yes, so I think in general, you could bucket maybe concerns about autonomous weapons into two categories. One is a concern that they may not function very well and could have accidents, those accidents could lead to civilian casualties, that could lead to accidental escalation among nations and a crisis, military force forces operating in close proximity to one another and there could be accidents. This happens with people. And you might worry about actions with autonomous systems and maybe one shoots down an enemy aircraft and there’s an escalation and people are killed. And then how do you unwind that? How do you communicate to your adversary? We didn’t mean to do that. We’re sorry. How do you do that in a period of tension? That’s a particular challenge.\n\nThere’s a whole other set of challenges that come from the weapons might work. And that might get to some of these deeper questions about the role of humans in decision making about life and death. But this issue of accidental escalation kind of comes into the category of they don’t work very well, then they’re not reliable. And this is the case for a lot of AI and autonomous technology today, which isn’t to say it doesn’t work at all, if it didn’t work at all, it would be much easier. There’d be no debates about bias and facial recognition systems if they never identify faces. There’d be no debates about safety with self-driving cars if the car couldn’t go anywhere. The problem is that a lot of these AI based systems work very well in some settings, and then if the settings change ever so slightly, they don’t work very well at all anymore. And the performance can drop off very dramatically, and they’re not very robust to changes in environmental conditions. So this is a huge problem for the military, because in particular, the military doesn’t get to test its systems in its actual operating environment.\n\nSo you can take a car, and you can take it on the roads, and you can test it in an actual driving environment. And we’ve seen car companies rack up 10 million miles or more of driving data. And then they can go back and they can run simulations. So Waymo has said that they run 10 million miles of simulated driving every single day. And they can simulate in different lighting conditions, in different environmental conditions. Well, the military can build simulations too, but simulations of what? What will the next war look like? Well we don’t know because we haven’t fought it yet. The good news is that war’s very rare, which is great. But that also means that for these kinds of systems, we don’t necessarily know the operating conditions that they’ll be in, and so there is this real problem of this risk of accidents. And it’s exacerbated in the fact that this is also a very adversarial environment. So you actually have an enemy who’s trying to trick your system and manipulate it. That’s adds another layer of complications.\n\nDriving is a little bit competitive, maybe somebody doesn’t want to let you into the lane, but the pedestrians aren’t generally trying to get hit by cars. That’s a whole other complication in the military space. So all of that leads to concerns that the systems may do okay in training, and then we take them out in the real world, and they fail and they fail a pretty bad way. If it’s a weapon system that is making its own decisions about whom to kill, it could be that it fails in a benign way, then it targets nothing. And that’s a problem for the military who built it, or fails in a more hazardous way, in a dangerous way and attacks the wrong targets. And when we’re talking about an autonomous weapon, the essence of this autonomous weapon is making its own decisions about which targets to attack and then carrying out those attacks. If you get that wrong, those could be pretty significant consequences with that. One of those things could be civilian harm. And that’s a major concern. There are processes in place for printing that operationally and test and evaluation, are those sufficient? I think they’re good reasons to say that maybe they’re not sufficient or not completely sufficient, and they need to be revised or improved.\n\nAnd I’ll point out, we can come back to this that the US Defense Department actually has a more stringent procedure in place for reviewing autonomous weapons more than other weapons, beyond what the laws of war have, the US is one of the few countries that has this. But then there’s also question about accidental escalation, which also could be the case. Would that lead to like an entire war? Probably not. But it could make things a lot harder to defuse tensions in a crisis, and that could be problematic. So we just had an incident not too long ago, where the United States carried out an attack against the very senior Iranian General, General Soleimani, who’s the head of the Iranian Quds Force and killed him in a drone strike. And that was an intentional decision made by a person somewhere in the US government.\n\nNow, did they fully think that through? I don’t know, that’s a different question. But a human made that decision in any case. Well, that’s a huge escalation of hostilities between the US and Iraq. And there was a lot of uncertainty afterwards about what would happen and Iran launched some ballistic missiles against US troops in Iraq. And whether that’s it, or there’s more retaliation to come, I think we’ll see. But it could be a much more challenging situation, if you had a situation in the future where an autonomous weapon malfunctioned and took some action. And now the other side might feel compelled to respond. They might say, well, we have to, we can’t let this go. Because humans emotions are on the line and national pride and prestige, and they feel like they need to maintain a principle of deterrence and they need to retaliate it. So these could all be very complicated things if you had an accident with an autonomous weapon.\n\n**Lucas Perry:** Right. And so an adjacent issue that I’d like to explore now is how a potential arms race can have interplay with issues around accidental escalation of conflict. So is there already an arms race brewing for autonomous weapons? If so, why and what could potentially be done to deescalate such a situation?\n\n**Paul Scharre:** If there’s an arms race, it’s a very strange one because no one is building the weapons. We see militaries advancing in robotics and autonomy, but we don’t really see sort of this rush to build autonomous weapons. I struggle to point to any programs that I’m aware of in militaries around the globe that are clearly oriented to build fully autonomous weapons. I think there are lots of places where much like these incremental advancements of autonomy in cars, you can see more autonomous features in military vehicles and drones and robotic systems and missiles. They’re adding more autonomy. And one might be violently concerned about where that’s going. But it’s just simply not the case that militaries have declared their intention. We’re going to build autonomous weapons, and here they are, and here’s our program to build them. I would struggle to use the term arms race. It could happen, maybe worth a starting line of an arms race. But I don’t think we’re in one today by any means.\n\nIt’s worth also asking, when we say arms race, what do we mean and why do we care? This is again, one of these terms, it’s often thrown around. You’ll hear about this, the concept of autonomous weapons or AI, people say we shouldn’t have an arms race. Okay. Why? Why is an arms race a bad thing? Militaries normally invest in new technologies to improve their national defense. That’s a normal activity. So if you say arms race, what do you mean by that? Is it beyond normal activity? And why would that be problematic? In the political science world, the specific definitions vary, but generally, an arms race is viewed as an increase in defense spending overall, or in a particular technology area above normal levels of modernizing militaries. Now, usually, this is problematic for a couple of reasons. One could be that it ends up just in a massive national expenditure, like during the case of the Cold War, nuclear weapons, that doesn’t really yield any military value or increase anyone’s defense or security, it just ends up net flushing a lot of money down the drain. That’s money that could be spent elsewhere for pre K education or healthcare or something else that might be societally beneficial instead of building all of these weapons. So that’s one concern.\n\nAnother one might be that we end up in a world that the large number of these weapons or the type of their weapons makes it worse off. Are we really better off in a world where there are 10s of thousands of nuclear weapons on hair-trigger versus a few thousand weapons or a few hundred weapons? Well, if we ever have zero, all things being equal, probably fewer nuclear weapons is better than more of them. So that’s another kind of concern whether in terms of violence and destructiveness of war, if a war breakout or the likelihood of war and the stability of war. This is an A in an area where certainly we’re not in any way from a spending standpoint, in an arms race for autonomous weapons or AI today, when you look at actual expenditures, they’re a small fraction of what militaries are spending on, if you look at, say AI or autonomous features at large.\n\nAnd again for autonomous weapons, there really aren’t at least openly declared programs to say go build a fully autonomous weapon today. But even if that were the case, why is that bad? Why would a world where militaries are racing to build lots of atomic weapons be a bad thing? I think it would be a bad thing, but I think it’s also worth just answering that question, because it’s not obvious to everyone. This is something that’s often missing in a lot of these debates and dialogues about autonomous weapons, people may not share some of the underlying assumptions. It’s better to bring out these assumptions and explain, I think this would be bad for these reasons, because maybe it’s not intuitive to other people that they don’t share those reasons and articulating them could increase understanding.\n\nFor example, the FLI letter on autonomous weapons from a few years ago said, “the key question for humanity today is whether to start a global AI arms race or prevent it from starting. If any major military power pushes ahead with AI weapon development, the global arms race is virtually inevitable. And the endpoint of this technological trajectory is obvious. Autonomous weapons will become the Kalashnikovs of tomorrow.” I like the language, it’s very literary, “the Kalashnikovs of tomorrow.” Like it’s a very concrete image. But there’s a whole bunch of assumptions packed into those few sentences that maybe don’t work in the letter that’s intended to like sort of galvanize public interest and attention, but are worth really unpacking. What do we mean when we say autonomous weapons are the Kalashnikovs of tomorrow and why is that bad? And what does that mean? Those are, I think, important things to draw out and better understand.\n\nIt’s particularly hard for this issue because the weapons don’t exist yet. And so it’s not actually like debates around something like landlines. We could point to the mines and say like “this is a landmine, we all agree this is a landmine. This is what it’s doing to people.” And everyone could agree on what the harm is being caused. The people might disagree on what to do about it, but there’s agreement on what the weapon is and what the effect is. But for autonomous weapons, all these things are up to debate. Even the term itself is not clearly defined. And when I hear people describe it, people can be describing a whole range of things. Some people when they say the word autonomous weapon, they’re envisioning a Roomba with a gun on it. And other people are envisioning the Terminator. Now, both of those things are probably bad ideas, but for very different reasons. And that is important to draw out in these conversations. When you say autonomous weapon, what do you mean? What are you envisioning? What are you worried about? Worried about certain types of scenarios or certain types of effects?\n\nIf we want to get to the place where we really as a society come together and grapple with this challenge, I think first and foremost, a better communication is needed and people may still disagree, but it’s much more helpful. Stuart Russell from Berkeley has talked a lot about dangers of small anti-personnel autonomous weapons that would widely be the proliferated. He made the Slaughterbots video that’s been seen millions of times on YouTube. That’s a very specific image. It’s an image that’s very concrete. So then you can say, when Stuart Russell is worried about autonomous weapons, this is what he’s worried about. And then you can start to try to better understand the assumptions that go into that.\n\nNow, I don’t share Stuart’s concerns, and we’ve written about it and talked about before, but it’s not actually because we disagree about the technology, I would agree that that’s very doable with existing technology. We disagree about the social responses to that technology, and how people respond, and what are the countermeasures and what are ways to prevent proliferation. So we, I think, disagree on some of the political or social factors that surround kind of how people approach this technology and use it. Sometimes people actually totally agree on the risks and even maybe the potential futures, they just have different values. And there might be some people who their primary value is trying to have fewer weapons in the world. Now that’s a noble goal. And they’re like, hey, anyway that we can have fewer weapons, fewer advanced technologies, that’s better. That’s very different from someone who’s coming from a position of saying, my goal is to improve my own nation’s defense. That’s a totally different value system. A total different preference. And they might be like, I also value what you say, but I don’t value it as much. And I’m going to take actions that advance these preferences. It’s important to really sort of try to better draw them out and understand them in this debate, if we’re going to get to a place where we can, as a society come up with some helpful solutions to this problem.\n\n**Lucas Perry:** Wonderful. I’m totally on board with that. Two questions and confusions on my end. The first is, I feel a bit confused when you say these weapons don’t exist already. It seems to me more like autonomy exists on a spectrum and is the integration of many different technologies and decision making in systems. It seems to me there is already a certain degree of autonomy, there isn’t Terminator level autonomy, or specify an objective and the autonomous system can just basically go execute that, that seems to require very high level of generality, but there seems to already exist a level of autonomy today.\n\nAnd so in that video, Stuart says that slaughterbots in particular represent a miniaturization and integration of many technologies, which already exist today. And the second thing that I’m confused about is when you say that it’s unclear to you that militaries are very interested in this or that there currently is an arms race. It seems like yes, there isn’t an arms race, like there was with nuclear weapons where it’s very clear, and they’re like Manhattan projects around this kind of technology, but given the strategic advantage conferred by this technology now and likely soon, it seems to me like game theoretically, from the position of militaries around the world that have the capacity to invest in these things, that it is inevitable given their battlefield importance that there would be massive ramping up or investments, or that there already is great interest in developing the autonomy and the subtechnologies required for developing fully autonomous systems.\n\n**Paul Scharre:** Those are great questions and right on point. And I think the central issues in both of your questions are when we say these weapons or when I say these things, I should be more precise. When we say autonomous weapons, what do we mean exactly? And this is one of the things that can be tricky in this space, because there are not these universally agreed upon definitions. There are certainly many weapons systems used widely around the globe today that incorporate some autonomous features. Many of these are fire and forget weapons. When someone launches them, they’re not coming back. They have in that sense, autonomy to carry out their mission. But autonomy is relatively limited and narrowly bounded, and humans, for the most part are choosing the targets. So you can think of kind of maybe these three classes of weapons, these semi autonomous weapons, where humans are choosing the targets, but there’s lots of autonomy surrounding that decision, queuing information to people, flying the munition once the person launches it. That’s one type of weapon, widely used today by really every advanced military.\n\nAnother one is the supervised autonomous weapons that are used in these relatively limited settings for defensive purposes, where there is kind of this automatic mode that people can turn them on and activate them to defend the ship or the ground base or the vehicle. And these are really needed for these situations where the incoming threats are too fast for humans to respond. And these again are widely used around the globe and have been in place for decades. And then there are what we could call fully autonomous weapons, where the human’s launching them and human programs in the parameters, but they have some freedom to fly a search pattern over some area and then once they find a target, attack it on their own. For the most part, with some exceptions, those weapons are not widely used today. There have been some experimental systems that have been designed. There have been some put into operation in the past. The Israeli harpy drone is an example of this that is still in operation today. It’s been around since the ’90s, so it’s not really very new. And it’s been sold to a handful of countries, India, Turkey, South Korea, China, and the Chinese have reportedly reverse engineered their own version of this.\n\nBut it’s not like when widespread. So it’s not like a major component of militaries order of that. I think you see militaries investing in robotic systems, but the bulk of their fleets are still human occupied platforms, robotics are largely an adjunct to them. And in terms of spending, while there is increased spending on robotics, most of the spending is still going towards more traditional military platforms. The same is also true about the degree of autonomy, most of these robotic systems are just remote controlled, and they have very limited autonomy today. Now we’re seeing more autonomy over time in both robotic vehicles and in missiles. But militaries have a strong incentive to keep humans involved.\n\nIt is absolutely the case that militaries want technologies that will give them an advantage on the battlefield. But part of achieving an advantage means your systems work, they do what you want them to do, the enemy doesn’t hack them and take them over, you have control over them. All of those things point to more human control. So I think that’s the thing where you actually see militaries trying to figure out where’s the right place on the spectrum of autonomy? How much autonomy is right, and that line is going to shift over time. But it’s not the case that they necessarily want just full autonomy because what does that mean, then they do want weapon systems to sort of operate under some degree of human direction and involvement. It’s just that what that looks like may evolve over time as the technology advances.\n\nAnd there are also, I should add, other bureaucratic factors that come into play that militaries investments are not entirely strategic. There’s bureaucratic politics within organizations. There’s politics more broadly with the domestic defense industry interfacing with the political system in that country. They might drive resources in certain directions. There’s some degree of inertia of course in any system that are also factors in play.\n\n**Lucas Perry:** So I want to hit here a little bit on longer term perspectives. So the Future of Life Institute in particular is interested in mitigating existential risks. We’re interested in the advanced risks from powerful AI technologies where AI not aligned with human values and goals and preferences and intentions can potentially lead us to suboptimal equilibria that were trapped in permanently or could lead to human extinction. And so other technologies we care about are nuclear weapons and synthetic-bio enabled by AI technologies, etc. So there is this view here that if we cannot establish a governance mechanism as a global community on the concept that we should not let AI make the decision to kill then how can we deal with more subtle near term issues and eventual long term safety issues around the powerful AI technologies? So there’s this view of ensuring beneficial outcomes around lethal autonomous weapons or at least beneficial regulation or development of that technology, and the necessity of that for longer term AI risk and value alignment with AI systems as they become increasingly intelligent. I’m curious to know if you have a view or perspective on this.\n\n**Paul Scharre:** This is the fun part of the podcast with the Future of Life because this rarely comes up in a lot of the conversations because I think in a lot of the debates, people are focused on just much more near term issues surrounding autonomous weapons or AI. I think that if you’re inclined to see that there are longer term risks for more advanced developments in AI, then I think it’s very logical to say that there’s some value in humanity coming together to come up with some set of rules about autonomous weapons today, even if the specific rules don’t really matter that much, because the level of risk is maybe not as significant, but the process of coming together and agreeing on some set of norms and limits on particularly military applications in AI is probably beneficial and may begin to create the foundations for future cooperation. The stakes for autonomous weapons might be big, but are certainly not existential. I think in any reasonable interpretation of autonomous weapons might do really, unless you start thinking about autonomy wired into, like nuclear launch decisions which is basically nuts. And I don’t think it’s really what’s on the table for realistically what people might be worried about.\n\nWhen we try to come together as a human society to grapple with problems, we’re basically forced to deal with the institutions that we have in place. So for example, for autonomous weapons, we’re having debates in the UN Convention on Certain Conventional Weapons to CCW. Is that the best form for talking about autonomous weapons? Well, it’s kind of the form that exists for this kind of problem set. It’s not bad. It’s not perfect in some respects, but it’s the one that exists. And so if you’re worried about future AI risk, creating the institutional muscle memory among the relevant actors in society, whether it’s nation states, AI scientists, members of civil society, militaries, if you’re worried about military applications, whoever it is, to come together, to have these conversations, and to come up with some answer, and maybe set some agreements, some limits is probably really valuable actually because it begins to establish the right human networks for collaboration and cooperation, because it’s ultimately people, it’s people who know each other.\n\nSo oh, “I worked with this person on this last thing.” If you look at, for example, the international movement that The Campaign to Stop Killer Robots is spearheading, that institution or framework, those people, those relationships are born out of past successful efforts to ban landmines and then cluster munitions. So there’s a path dependency, and human relationships and bureaucracies, institutions that really matters. Coming together and reaching any kind of agreement, actually, to set some kind of limits is probably really vital to start exercising those muscles today.\n\n**Lucas Perry:** All right, wonderful. And a final fun FLI question for you. What are your views on long term AI safety considerations? Do you view AI eventually as an existential risk and do you integrate that into your decision making and thinking around the integration of AI and military technology?\n\n**Paul Scharre:** Yes, it’s a great question. It’s not something that comes up a lot in the world that I live in, in Washington in the policy world, people don’t tend to think about that kind of risk. I think it’s a concern. It’s a hard problem because we don’t really know how the technology is evolving. And I think that one of the things is challenging with AI is our frame for future more advanced AI. Often the default frame is sort of thinking about human like intelligence. When people talk about future AI, people talk about terms like AGI, or high level machine intelligence or human like intelligence, we don’t really know how the technology is evolving.\n\nI think one of the things that we’re seeing with AI machine learning that’s quite interesting is that it often is evolving in ways that are very different from human intelligence, in fact, very quite alien and quite unusual. And I’m not the first person to say this, but I think that this is valid that we are, I think, on the verge of a Copernican revolution in how we think about intelligence, that rather than thinking of human intelligence as the center of the universe, that we’re realizing that humans are simply one type of intelligence among a whole vast array and space of possible forms of intelligence, and we’re creating different kinds, they may have very different intelligence profiles, they may just look very different, they may be much smarter than humans in some ways and dumber in other ways. I don’t know where things are going. I think it’s entirely possible that we move forward into a future where we see many more forms of advanced intelligent systems. And because they don’t have the same intelligence profile as human beings, we continue to kick the can down the road into being true intelligence because it doesn’t look like us. It doesn’t think like us. It thinks differently. But these systems may yet be very powerful in very interesting ways.\n\nWe’ve already seen lots of AI systems, even very simple ones exhibit a lot of creativity, a lot of interesting and surprising behavior. And as we begin to see the sort of scope of their intelligence widen over time, I think there are going to be risks that come with that. They may not be the risks that we were expecting, but I think over time, there going to be significant risks, and in some ways that our anthropocentric view is, I think, a real hindrance here. And I think it may lead us to then underestimate risk from things that don’t look quite like humans, and maybe miss some things that are very real. I’m not at all worried about some AI system one day becoming self aware, and having human level sentience, that does not keep me up at night. I am deeply concerned about advanced forms of malware. We’re not there today yet. But you could envision things over time that are adapting and learning and begin to populate the web, like there are people doing interesting ways of thinking about systems that have misaligned goals. It’s also possible to envision systems that don’t have any human directed goals at all. Viruses don’t. They replicate. They’re effective at replicating, but they don’t necessarily have a goal in the way that we think of it other than self replication.\n\nIf you have systems that are capable of replicating, of accumulating resources, of adapting, over time, you might have all of the right boxes to check to begin to have systems that could be problematic. They could accumulate resources that could cause problems. Even if they’re not trying to pursue either a goal that’s misaligned with human interest or even any goal that we might recognize. They simply could get out in the wild, if they’re effective at replication and acquiring resources and adapting, then they might survive. I think we’re likely to be surprised and continue to be surprised by how AI systems evolve, and where that might take us. And it might surprise us in ways that are humbling for how we think about human intelligence. So one question I guess is, is human intelligence a convergence point for more intelligent systems? As AI systems become more advanced, and they become more human like, or less human like and more alien.\n\n**Lucas Perry:** Unless we train them very specifically on human preference hierarchies and structures.\n\n**Paul Scharre:** Right. Exactly. Right. And so I’m not actually worried about a system that has the intelligence profile of humans, when you think about capacity in different tasks.\n\n**Lucas Perry:** I see what you mean. You’re not worried about an anthropomorphic AI, you’re worried about a very powerful, intelligent, capable AI, that is alien and that we don’t understand.\n\n**Paul Scharre:** Right. They might have cross domain functionality, it might have the ability to do continuous learning. It might be adaptive in some interesting ways. I mean, one of the interesting things we’ve seen about the field of AI is that people are able to tackle a whole variety of problems with some very simple methods and algorithms. And this seems for some reason offensive to some people in the AI community, I don’t know why, but people have been able to use some relatively simple methods, with just huge amounts of data and compute, it’s like a variety of different kinds of problems, some of which seem very complex.\n\nNow, they’re simple compared to the real world, when you look at things like strategy games like StarCraft and Dota 2, like the world looks way more complex, but these are still really complicated kind of problems. And systems are basically able to learn totally on their own. That’s not general intelligence, but it starts to point towards the capacity to have systems that are capable of learning a whole variety of different tasks. They can’t do this today, continuously without suffering the problem of catastrophic forgetting that people are working on these things as well. The problems today are the systems aren’t very robust. They don’t handle perturbations in the environment very well. People are working on these things. I think it’s really hard to see how this evolves. But yes, in general, I think that our fixation on human intelligence as the pinnacle of intelligence, or even the goal of what we’re trying to build, and the sort of this anthropocentric view is, I think, probably one that’s likely to lead us to maybe underestimate some kinds of risks.\n\n**Lucas Perry:** I think those are excellent points and I hope that mindfulness about that is able to proliferate in government and in actors who have power to help mitigate some of these future and short term AI risks. I really appreciate your perspective and I think you bring a wholesomeness and a deep authentic entertaining of all the different positions and arguments here on the question of autonomous weapons and I find that valuable. So thank you so much for your time and for helping to share information about autonomous weapons with us.\n\n**Paul Scharre:** Thank you and thanks everyone for listening. Take care.", "url": "https://www.alignmentforum.org/posts/xEzudcydk7APZbnai/ai-alignment-podcast-on-lethal-autonomous-weapons-with-paul", "date_published": "2020-03-16T23:00:18Z", "authors": ["Palus Astra"], "tags": ["Interviews", "Transcripts", "Autonomous Weapons"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.726927+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "feb0ab58247cedb8406dcb2cb52787b4", "source": "alignmentforum", "title": "What is Interpretability?", "text": "In this post we lay out some ideas around framing interpretability research which we have found quite useful. Our framing is goal-oriented, which we believe is important for making sure interpretability research is meaningful. We also go over a variety of dimensions which we think are useful to consider when thinking about interpretability research. We wanted to have a shared vocabulary when talking about this kind of research, and found that these ideas helped us communicate effectively.\n\n\nOne of our motivations for having these thoughts and discussions is so we can understand the relevance of interpretability to alignment, and to help us think about which categories or dimensions of interpretability research are important for alignment of strong AI. In a coming post we discuss interpretability and alignment, using the ideas from this post and other previous writing on the subject.\n\n\nInterpretability, transparency, explainability?\n-----------------------------------------------\n\n\nA quick note on terminology. Many terms are used to mean many similar things in the interpretability research literature. We think of *Interpretability* as the broadest possible category, which is why it can be hard to define and operationalise. Two specific terms are used with frequency apart from interpretability - *explainability* and *transparency* - and following the definitions in *The Mythos Of Model Interpretability* (summarised in more detail in the appendix), we’ll use them as follows:\n\n\n* *Explainability* is usually a post-hoc action, where we want to understand and explain why a model made a specific prediction (or enacted certain behaviour) after the fact.\n* *Transparency* is normally seen as a more intrinsic property of a model, and is about understanding it’s internal workings and mechanisms. This is often a harder property to achieve. This can often mean a human would be able to “Run an approximation of the algorithm in their head”.\n\n\nDimensions for interpretability\n===============================\n\n\nWe found two dimensions most useful to consider when thinking about interpretability. The first dimension (*Goals*) is concerned with which goals interpretability research aims to fulfil. The second (*Enabling Humans*) refers to the idea that interpretability is human-centred, and we feel that interpretability methods normally must have a human involved in interpreting the model for them to be meaningful. We could also consider agents acting on behalf of humans (i.e. in amplification or recursive reward modelling). These aren’t the only axes on which to consider interpretability research (see Other Dimensions), but they are the two we felt were the most useful.\n\n\nGoals\n-----\n\n\nPossibly the most important dimension to consider is the Goals of interpretability methods. We describe five generic goals (not specific to certain domains). We found that this helps us the most when thinking about what kinds of interpretability methods we should pursue developing and what a specific interpretability method is hoping to achieve. This is descriptive, in that these are (generic) goals researchers have put forward for using interpretability methods, or that interpretability methods have been used to achieve. There’s overlap between these goals, and especially the first three are interrelated, as are the last two.\n\n\nThe goals are:\n\n\n* *Predicting behaviour* - This is about being able to predict a model’s behaviour in novel scenarios. This normally will be without having a full formal definition of the scenario (otherwise we should just run it through the model and see what it does). We might care about predicting large classes of behaviour (“The model never acts against human interest”) which is overlapping with the *Assurance of properties* goal below; we might also care about more specific classes of behaviour (“Will the agent smash the red vase in this gridworld?”).\n\n\n\t+ Some current methods which might help with this are [Counterfactual States for Atari Agents via Generative Deep Learning](https://arxiv.org/abs/1909.12969) and [Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents](https://arxiv.org/abs/1904.01318), as they both visualise what states would produce certain behaviours, and hence enable us to predict behaviours in these states.\n\t+ One way in which this goal can be validated is to test whether it enables users to construct inputs which produce a certain behaviour, rather than predicting the input given the behaviour. This would have to be done without access to the model (otherwise you could just optimise for that behaviour). The two methods above are examples of this kind of approach: Having seen several visualisations of counterfactual states, you could be able to produce counterfactual states on your own without the method, which demonstrates you’ve gained some understanding of the model enough to predict it’s behaviour.\n* *Assurance of properties* - This is related to the *Predicting Behaviour* goal, but where the goal is to give broader assurances that are better described as properties rather than specific behavioural predictions. In the strongest form, this might take the form of formal verification, but in weaker forms we might be satisfied with visualisations or explanations of behaviour which imply that the model has a certain property.\n\n\n\t+ [Understanding Black-box Predictions via Influence Functions](https://arxiv.org/abs/1703.04730) and [Estimating Training Data Influence by Tracking Gradient Descent](https://arxiv.org/abs/2002.08484) are both methods designed to find training data which is influential for specific model decisions. If a model’s influential training points for a specific action are unrelated to this action, we might suppose that the model has learned spurious correlations rather than a robust approach. These methods can hence give us a small amount of assurance that the model is reasoning correctly about it’s inputs.\n\t+ Visualising adversarial examples might give us some assurance of the robustness (or not) of the network to adversarial attacks.\n\t+ We could also consider value learning as an interpretability method. In the case of perfect IRL, we can find the revealed preferences of an agent, and having these preferences would allow you to examine some properties of the agent. This would also help with *Predicting Behaviour*.\n* *Persuasion of properties* - This is similar to the *Assurance of properties* goal, but with the difference that the aim is purely persuasion of a person, rather than a truthful representation of the model’s properties. Often interpretability methods might be used to convince a non-technical person (perhaps an auditor or a companies machine learning models) that the model has certain properties. This goal can often be problematic, as the incentives come apart from truthfully presenting the behaviour or properties of the model. That is, we could create methods that produce pretty interesting visualisations which seem to show the model is doing the right thing, when the visualisations aren’t actually dependent on the model’s parameters.\n\n\n\t+ [Sanity Checks for Saliency Maps](https://arxiv.org/abs/1810.03292) and [The (Un)reliability of Saliency methods](https://research.google/pubs/pub46494/) both seem to point at the possibility that saliency maps are doing this implicitly. Obviously no researcher is explicitly thinking *“Let's find the prettiest picture to convince people or some interesting property”*, but without better forms of validation this seems like it could happen by accident for some methods. This was one of our motivations for coming up with a goal-oriented framing in the first place\n\t+ Note that we’re not saying this a goal one *should* aim for, and it’s rarely if ever explicitly mentioned as a goal in literature; however, it likely to have been (at least implicitly) optimised for in certain methods, as the above bullet point alludes to.\n* *Improving model performance* - Many methods are designed to give us understanding which we can use to improve the performance of the model. We might find weaknesses in how the model currently performs, or unbalanced training data, which we can use to improve the model’s performance.\n\n\n\t+ For example, saliency maps ([Visualizing and Understanding Atari Agents](https://arxiv.org/abs/1711.00138), [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391), many others) tell us what the model is attending to in the input, which can tell us whether it’s learning the correct signals and information to attend to. This information might help us improve the model behaviour.\n\t+ When trying to improve the robustness of our models, especially by adversarial training, we might use interpretability methods to find uninterpretable or seemingly un-robust behaviours, and bias our adversarial training towards these behaviours. If the behaviours are reliant on spurious correlations, then we can improve the robustness of our model by training on examples where these correlations break down, and if they in fact are robust, then we may have learned something new about the environment or task which we didn’t realise beforehand.\n* *Debugging model* - Similar to the *Improving model performance*, and especially in deep learning, this is about finding incorrect model behaviour and fixing it.\n\n\n\t+ To clarify the difference from *Improving model performance*, we might say a model is implemented and executing correctly if it’s implementation is fully faithful to the conception in your head, or to some pseudocode, mathematical formula or design document. In this case, we might use interpretability to *Improve model performance* by giving us insights into how it behaves.\n\t+ However, we might say a model is implemented (and hence executing) incorrectly if it’s implementation is unfaithful to the pseudocode, mathematical formulation or design document, most likely due to an error in implementation, or a possibly hidden assumption being violated in the training process. For example, we might assume that all the training data is labelled correctly, but it might not be, and we may be able to find this using interpretability methods. This would fall under *Debugging models* rather than *Improving model performance*.\n\t+ This might take the form of an interactive debugger or some logging view such as tensorboard, or visualisations of the flow of gradients through the network, to make sure they’re flowing the way you expect.\n\n\nA key caveat is that many methods might not target a specific goal in the above list, but instead aim to produce in the end-user of the interpretability method some *Generic Understanding* or a *Gearful Model* of the object being interpreted. This understanding could then serve in the fulfilment of a variety of goals, such as aiding the user in predicting behaviour, or understanding why the model fails and hence improving its performance. A key example of these kinds of *Generic Understanding* methods are the series of methods based on [Feature Visualisation](https://distill.pub/2017/feature-visualization/), especially the [Activation Atlas](https://distill.pub/2019/activation-atlas/). The Activation Atlas paper is a good example of a method giving *Generic Understanding* which is then cashed out in the ability to produce adversarial examples aimed at specific things using only overlapping natural images, which is a kind of *Predicting Behaviour.*\n\n\nWe want to stress, however, that validating whether a interpretability method produces genuine understanding seems very difficult (or perhaps impossible) to do without seeing whether the understanding can be cashed out to achieve one of the goals above. The aim when trying to produce understanding should not be for the user to just *feel* like they understand the method, but rather for the user’s understanding to enable them to better achieve one of the goals above. If a method produces some fuzzy feeling of understanding but doesn’t seem useful for any downstream task, then it’s worth considering what it’s useful for.\n\n\nEnabling Humans\n---------------\n\n\nThe second dimension, *Enabling Humans*, is about whether the method enables a *human* to achieve one of the goals above, as opposed to achieving the goal directly. That is, there are many methods or algorithms in machine learning which *Improve model performance*, such as the Adam optimiser or adding more layers to your neural network. These techniques are obviously not interpretability techniques, but it’s useful to think about the difference between them and a technique we would call interpretability which does enable an improvement in model performance.\n\n\nWe think the key difference is the existence of a human in a chain between interpretability method and in this case *Improvement of model performance*. An interpretability method will *Enable Humans* to achieve the goals above. For a method such as automatic verification (which could be considered interpretability) the verifier is doing the vast majority of the work, but the human still needs to decide what to verify, i.e. they need to ask the right questions. On the other hand, methods such as feature visualisation and saliency maps only provide some visualisation of the model behaviour or internals, and hence still require human effort to achieve one of the goals above.\n\n\nFor some of the goals above this is basically guaranteed: When trying to gain *Assurance of Properties*, we’re the human in the loop, and the method could give us these assurances or enable us to find them for ourselves. However as mentioned above, for *Improving model performance* the distinction becomes more necessary.\n\n\nThis dimension is more difficult to reason about than the *Goals* dimension, and is hence less fleshed out. However, we think it’s just as important to consider what the role of the human is in an interpretability method’s use.\n\n\nA Map of Methods\n----------------\n\n\nThese dimensions aren’t an operationalisation of interpretability, and don't present necessary and sufficient conditions, and we’re uncertain whether it’d be useful to have a strict dividing line. We do think that the framing is useful even if it’s not a formal definition of what interpretability is, as it helped us to discuss what goals interpretability methods are trying to achieve, and whether they’re enabling humans to achieve those goals or not. To test out our ideas with a variety of current methods, we produced a map of current interpretability methods on these two dimensions. This helped us in earlier formulations to spot problems with the description of the framing, relations between various parts of it, and similar patterns of goal behaviour from different methods. This is just an illustration of applying some of our ideas to current methods. Bold lines indicate that method helping more with that goal. We don't want to stake a lot on exactly where the bold lines are drawn.\n\n\n![Map of methods](https://i.imgur.com/Wnto7sT.png)\n\n\nOther dimensions\n----------------\n\n\nThe two dimensions described above were the ones we felt were the most useful at a higher level when thinking about generating ideas for interpretability methods. However, there’s a few other dimensions which we think are also important to think about. This can also be useful in a generative sense. If we draw out the matrix of all dimensions, we can consider whether methods in certain points of the matrix would be useful, and how they might look. These other important dimensions are:\n\n\n* Interpreting the *training process vs finished model* (vs both). A method might aim to interpret how training happens (such as [LCA: Loss Change Allocation for Neural Network Training](https://arxiv.org/abs/1909.01440) or [Understanding Black-box Predictions via Influence Functions](https://arxiv.org/abs/1703.04730)), or it might be interpreting the finished model (such as [Feature Visualization](https://distill.pub/2017/feature-visualization/)).\n\n\n\t+ We think this dimension can still be orthogonal to the *Goals* dimension, as we might want *Assurances of properties* or *Predicting Behaviour* of the training process as opposed to the finished model. The distinction is more blurry when it comes to *Improving model performance* or *Debugging models*, but we can still say a method looks at the training process of the finished model (or both) to aid in achieving this goal. In an online/meta-learning setup this also not be a crisp definition, as these paradigms have training as part of the “finished model”.\n* *Level of access* to components of training and model. In some settings, we might have access to all the training data, the whole training process (i.e. the mini batches and loss and model checkpoints at every iteration), the final model (i.e. it’s architecture and weights), and a copy of the environment/simulation (especially relevant in reinforcement learning). In this full information setting many more interpretability methods are possible than in a setting where we only have access to the final model (architecture and weights), or even just examples of it’s behaviour.\n\n\n\t+ We find it useful to think about what resources the interpretability method requires to produce its results. This is especially pertinent in reinforcement learning, as some methods such as in [Understanding DQNs](https://arxiv.org/abs/1602.02658) require running the model for many time steps in the environment to gain insight from a behavioural standpoint, but this can be problematic depending on the type and simulatability of the environment.\n* *Ground Truth Access*. Some interpretability methods may rely on access to the ground truth, either from a human’s evaluation or otherwise. In some settings (such as natural image classification) this is the case, but in reinforcement learning we don’t know what the optimal policy in most settings is. This will become more pertinent as we get stronger AIs and they start to overtake humans on domains other than board games.\n\n\n\t+ Methods like [NetDisect](http://netdissect.csail.mit.edu/) require dense pixel level labellings of images, an even stronger notion of ground truth than just class labels for inputs, and gradient-based saliency methods require the ground truth label to take the gradient with, whereas perturbation based saliency methods and methods such as feature visualisation have no such requirement.\n\n\nWhat’s next?\n============\n\n\nWhile the ideas in this post helped us deconfuse and discuss interpretability research in general, we think an ideal use-case for the framing (which also motivates us) is talking about interpretability in AI alignment. This problem setting is different from those tackled in the standard machine learning literature: We’ll care about a subset of the goals described above, and the way in which a method enables humans will also deserve more scrutiny. A second post coming soon will focus on this issue, using the ideas in this post and other previous writing on the subject. To give an (obvious) sneak preview, *Persuasion of Properties* is a goal we would want to avoid explicitly optimising for when thinking about aligning strong AI.\n\n\nAppendix: Previous Literature\n=============================\n\n\nThere have been previous works addressing the problem of defining and discussing interpretability research. We here summarise two works which seemed to be the most discussed.\n\n\n[The Mythos of Model Interpretability](https://arxiv.org/abs/1606.03490) tackles the problem that interpretability is an under-defined term. The author first presents five desiderata which cover most interpretability researchers’ motivations for doing interpretability research: *Trust*, *Causality*, *Informativeness*, *Transferability*, and *Fair and ethical decision making*. In many problems, the problem specification used to train the model (i.e. the loss function, which normally just optimises accuracy) does not capture all of the desired properties of the model (i.e. not discriminating different races, having an understanding of causality, being transferable to other similar problems). Hence, optimising to solve this problem specification will not produce these properties. In these cases, Interpretability can be used to validate that these extra properties hold. This point is made both in this paper and following one.\n\n\nThe paper describes a taxonomy of interpretability methods, with the top level topics of *transparency* and *post-hoc interpretability*:\n\n\n* *Transparency* is about understanding what procedure the algorithm is implementing, and in its strongest form about being to simulate the algorithm yourself. This definition has problems when we consider superhuman AI, as a human presumably won’t be able to simulate the AI’s algorithm. (See [One Way to Think About ML Transparency](https://www.lesswrong.com/posts/jg6ZJLE5eHkfuxk67/one-way-to-think-about-ml-transparency) for an attempt to solve this specific problem by extending the definition from human-simulatable in practice to human-simulatable in theory without constant access to the model being simulated).\n* *Post-hoc Explanations* refers to methods which aim to understand or explain why a machine learning model has made a certain decision, either through examples, visualisation or explicit textual explanation produced by the model itself.\n\n\n[Towards A Rigorous Science of Interpretable Machine Learning](https://arxiv.org/abs/1702.08608) echoes the point made in the above paper that interpretability research is aimed at addressing the case where the loss function a model is optimised for doesn’t capture all of the desired properties of the trained model. To the list of possible use-cases or problem underspecification cases it adds having a *Multi-objective trade-off* or only having trained with a *Proxy reward function*. It provides a semi-formal description of the variety of ways in which interpretability methods or models can be validated, and the links between these methods and the desiderata or methods described above.\n\n\nSome papers we haven’t looked into but also seem to tackle a similar topic:\n\n\n* [*The Challenge of Crafting Intelligible Intelligence*](https://arxiv.org/abs/1803.04263)\n* [*Techniques for Interpretable Machine Learning*](https://arxiv.org/abs/1808.00033)\n* [*Explicability? Legibility? Predictability? Transparency? Privacy? Security? The Emerging Landscape of Interpretable Agent Behavior*](https://arxiv.org/abs/1811.09722)\n* [*The What, the Why, and the How of Artificial Explanations in Automated Decision-Making*](https://arxiv.org/abs/1808.07074)\n* [*Explaining Explanations: An Overview of Interpretability of Machine Learning*](https://arxiv.org/abs/1806.00069)\n\n\n*This work was done as part of the AISRP 2019 <https://aisrp.org/>*", "url": "https://www.alignmentforum.org/posts/rSMbGFfsLMB3GWZtX/what-is-interpretability", "date_published": "2020-03-17T20:23:33Z", "authors": ["RobertKirk", "Tomáš Gavenčiak", "Ada Böhm"], "tags": ["Interpretability (ML & AI)", "AI"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.727797+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "5b1612aa19d40c732bb1be2525eb1748", "source": "alignmentforum", "title": "[AN #91]: Concepts, implementations, problems, and a benchmark for impact measurement", "text": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.\n\nAudio version **[here](http://alignment-newsletter.libsyn.com/alignment-newsletter-91)** (may not be up yet). \n\nHIGHLIGHTS\n==========\n\n**[Reframing Impact - Part 2](https://www.alignmentforum.org/s/7CdoznhJaLEKHwvJW)** *(Alex Turner)* (summarized by Rohin): In **[part 1](https://www.alignmentforum.org/s/7CdoznhJaLEKHwvJW)** (**[AN #68](https://mailchi.mp/87c0d944c461/an-68-the-attainable-utility-theory-of-impact)**) of this sequence, we saw that an event is *impactful* if it *changes our ability to get what we want*. This part takes this understanding and applies it to AI alignment.\n\nIn the real world, there are many events that cause *objective* negative impacts: they reduce your ability to pursue nearly any goal. An asteroid impact that destroys the Earth is going to be pretty bad for you, whether you want to promote human flourishing or to make paperclips. Conversely, there are many plans that produce objective positive impacts: for many potential goals, it's probably a good idea to earn a bunch of money, or to learn a lot about the world, or to command a perfectly loyal army. This is particularly exacerbated when the environment contains multiple agents: for goals that benefit from having more resources, it is objectively bad for you if a different agent seizes your resources, and objectively good for you if you seize other agents' resources.\n\nBased on this intuitive (but certainly not ironclad) argument, we get the **Catastrophic Convergence Conjecture (CCC)**: \"Unaligned goals tend to have catastrophe-inducing optimal policies because of power-seeking incentives\".\n\nLet's now consider a *conceptual* version of **[Attainable Utility Preservation (AUP)](https://www.alignmentforum.org/posts/yEa7kwoMpsBgaBCgb/towards-a-new-impact-measure)** (**[AN #25](https://mailchi.mp/0c5eeec28f75/alignment-newsletter-25)**): the agent optimizes a primary (possibly unaligned) goal, but is penalized for changing its \"power\" (in the intuitive sense). Intuitively, such an agent no longer has power-seeking incentives, and so (by the **[contrapositive](https://en.wikipedia.org/wiki/Contraposition)** of the CCC) it will not have a catastrophe-inducing optimal policy -- exactly what we want! This conceptual version of AUP also avoids thorny problems such as ontology identification and butterfly effects, because the agent need only reason about its own beliefs, rather than having to reason directly about the external world.\n\n**Rohin's opinion:** This was my favorite part of the sequence, as it explains the conceptual case for AUP clearly and concisely. I especially liked the CCC: I believe that we should be primarily aiming to prevent an AI system \"intentionally\" causing catastrophe, while not attempting to guarantee an absence of \"accidental\" mistakes (**[1](https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment)** (**[AN #33](https://mailchi.mp/b6dc636f6a1b/alignment-newsletter-33)**), **[2](https://www.alignmentforum.org/posts/E2aZ9Xwdz3i2ghPtn/techniques-for-optimizing-worst-case-performance)** (**[AN #43](https://mailchi.mp/768a8130013f/alignment-newsletter-43)**)), and the CCC is one way of cashing out this intuition. It's a more crisp version of the idea that **[convergent instrumental subgoals](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)** are in some sense the \"source\" of AI accident risk, and if we can avoid instrumental subgoals we will probably have solved AI safety.\n\n**[Reframing Impact - Part 3](https://www.alignmentforum.org/s/7CdoznhJaLEKHwvJW)** *(Alex Turner)* (summarized by Rohin): The final section of the sequence turns to an actual implementation of AUP, and deals with problems in how the implementation deviates from the conceptual version of AUP. We measure power by considering a set of auxiliary rewards, and measuring the change in attainable utilities of this auxiliary set as impact, and penalizing the agent for that. The first post presents some empirical results, many of which **[we've covered before](https://www.alignmentforum.org/posts/mDTded2Dn7BKRBEPX/penalizing-impact-via-attainable-utility-preservation)** (**[AN #39](https://mailchi.mp/036ba834bcaf/alignment-newsletter-39)**), but I wanted to note the new results on **[SafeLife](https://www.partnershiponai.org/safelife/)** (summarized below). On the high-dimensional world of SafeLife, the authors train a VAE to find a good latent representation, and choose a single linear reward function on the latent representation as their auxiliary reward function: it turns out this is enough to avoid side effects in at least some cases of SafeLife.\n\nWe then look at some improvements that can be made to the original AUP implementation. First, according to CCC, we only need to penalize *power*, not *impact*: as a result we can just penalize *increases* in attainable utilities, rather than both increases and decreases as in the original version. Second, the auxiliary set of rewards only provides a *proxy* for impact / power, which an optimal agent could game (for example, by **[creating subagents](https://www.alignmentforum.org/posts/mdQEraEZQLg7jtozn/subagents-and-impact-measures-full-and-fully-illustrated)**, summarized below). So instead, we can penalize increases in attainable utility for the *primary* goal, rather than using auxiliary rewards. There are some other improvements that I won't go into here.\n\n**Rohin's opinion:** I think the plan \"ensure that the AI systems we build don't seek power\" is pretty reasonable and plausibly will be an important part of AI alignment. However, the implementation of AUP is trying to do this under the threat model of optimal agents with potentially unaligned primary goals. I think this is probably going to do something quite different from the conceptual version of AUP, because impact (as defined in this sequence) occurs only when the agent's beliefs *change*, which doesn't happen for optimal agents in deterministic environments. The current implementation of AUP tries to get around this using proxies for power (but these can be gamed) or by defining \"dumber\" beliefs against which power is measured (but this fails to leverage the AI system's understanding of the world). See **[this comment](https://www.alignmentforum.org/posts/wAAvP8RG6EwzCvHJy/reasons-for-excitement-about-impact-of-impact-measure?commentId=s48grPhMbuBEXNtyc)** for more details.\n\nNote that the author himself is more **[excited](https://www.alignmentforum.org/s/7CdoznhJaLEKHwvJW/p/wAAvP8RG6EwzCvHJy)** about AUP as deconfusion, rather than as a solution to AI alignment, though he is more optimistic about the implementation of AUP than I am.\n\n**[Subagents and impact measures, full and fully illustrated](https://www.alignmentforum.org/posts/mdQEraEZQLg7jtozn/subagents-and-impact-measures-full-and-fully-illustrated)** *(Stuart Armstrong)* (summarized by Flo): Impact measures reduce the reward an agent **A** receives for reaching a state **s** by the distance between **s** and some baseline state **s'**. This article clarifies the interaction of **[impact measures and subagents](https://www.alignmentforum.org/posts/sYjCeZTwA84pHkhBJ/attainable-utility-has-a-subagent-problem)** (**[AN #86](https://mailchi.mp/598f425b1533/an-86-improving-debate-and-factored-cognition-through-human-experiments)**).\n\nIn the **[attainable utility framework](https://www.alignmentforum.org/posts/mDTded2Dn7BKRBEPX/penalizing-impact-via-attainable-utility-preservation)** (**[AN #39](https://mailchi.mp/036ba834bcaf/alignment-newsletter-39)**) the distance metric measures changes in **A**'s power, its ability to optimize a set of auxiliary rewards **R**, relative to doing nothing. This means that **A** is never penalized for doing nothing. Thus, if **A** manages to construct a subagent **SA** without getting penalized for that, **A** can just stop doing anything afterwards without getting a penalty, no matter what **SA** does. \n\nFor the *stepwise inaction baseline*, where **s'** is the state that **A** would have reached had it done nothing in the last time step, **A** can completely avoid the penalty by building an **SA** that commits to balance out the change in **A**'s ability to optimize the auxiliary reward **R** if **A** decided to optimize **R**, but instead becomes an optimizer for **A**'s real reward when **A** does nothing in the next step. \n\nFor the *inaction baseline*, where **s'** is the state reached if **A** had not done anything for the whole episode, it gets more complicated. When only gains in power are penalized, **SA** has to be built such that it does not cause a significant increase in **R**, even if **A** decided to optimize for **R** later on. If **SA** is constructed in a way such that **A** cannot control its behaviour after some time has passed, the penalty **A** receives for the construction only depends on the *actual* effect of **SA** on **R**. This is a lot weaker than the original penalty, which was about the *possible* effects of **A**'s actions on **R**. \n\nIn the case where decreases in power are also penalized, **A** needs to make sure that building **SA** does not reduce its ability to optimize **R**. This is not always possible without **A** retaining some control over **SA**, but giving up control was only necessary to prevent the construction of **SA** from increasing **A**'s potential to optimize **R**. It seems like it is often possible for **A** to finetune the amount of control it retains over **SA** such that **A**'s ability to optimize **R** barely changes while **SA** is able to do things **A** could not do unpenalized. \n\nIn the most extreme example, **A** boxes itself into **SA** and does nothing in reality, but is counterfactually able to intervene on **SA**'s policy. If **A** is able to cleverly put constraints on its own future actions in other ways, no subagents are needed to avoid the penalty. While other impact penalties like **[relative reachability](https://arxiv.org/abs/1806.01186)** (**[AN #10](https://mailchi.mp/d1a19c140226/alignment-newsletter-10)**) do allow for subagents to be constructed, they do not undermine the penalty for these.\n\n**Flo's opinion:** The article contains many useful examples in a grid world and I recommend reading it for a deeper understanding of the problem. The main problem with the attainable utility penalty seems to be the definition of power. While this has proven to be quite useful in simple examples, it seems like agents in more complex environments would be able to disentangle their ability to optimize their own reward and the auxiliary rewards, for example by essentially committing to never optimize the auxiliary rewards beyond a certain threshold. Such an agent would still look very powerful from the outside and I don't see why power-reducing commitments would diminish the agent's incentive to take away power from others. So while the **[catastrophic convergence conjecture](https://www.lesswrong.com/posts/w6BtMqKRLxG9bNLMr/the-catastrophic-convergence-conjecture)**, which states that unaligned goals tend to lead to catastrophic optimal policies because of power-seeking incentives, still rings true, it seems like we need to look at power from our perspective instead of the agent's.\n\n**Rohin's opinion:** I agree with Flo above: the issue is that AUP is measuring a proxy for our intuitive notion of power that falls apart under adversarial optimization. In particular, while it is normally reasonable to measure power by looking at the ability to optimize a set of auxiliary reward functions, this characterization no longer works when the agent can ensure that it won't be able to optimize those specific rewards, while still being able to optimize its primary reward. Subagents are a particularly clean way of demonstrating the problem.\n\n**[Introducing SafeLife: Safety Benchmarks for Reinforcement Learning](https://www.partnershiponai.org/safelife/)** *(Carroll Wainwright et al)* (summarized by Rohin): So far, techniques to avoid negative side effects have only been tested on **[simple](https://arxiv.org/abs/1806.01186)** (**[AN #10](https://mailchi.mp/d1a19c140226/alignment-newsletter-10)**) **[gridworlds](https://www.alignmentforum.org/posts/mDTded2Dn7BKRBEPX/penalizing-impact-via-attainable-utility-preservation)** (**[AN #39](https://mailchi.mp/036ba834bcaf/alignment-newsletter-39)**) **[or](https://bair.berkeley.edu/blog/2019/02/11/learning_preferences/)** (**[AN #45](https://mailchi.mp/35b451cb4d70/alignment-newsletter-45)**) **[hypotheticals](https://www.alignmentforum.org/posts/wzPzPmAsG3BwrBrwy/test-cases-for-impact-regularisation-methods)** (**[AN #45](https://mailchi.mp/35b451cb4d70/alignment-newsletter-45)**). SafeLife aims to provide a high-dimensional environment in which negative side effects are likely. It is based on Conway's Game of Life, which allows for complex effects arising out of relatively simple rules. An agent is given the ability to move, create life in an adjacent cell, or destroy life in an adjacent cell. With the specified reward function, the agent must build desired patterns, remove undesired patterns, and navigate to the exit.\n\nThe challenge comes when there are additional \"neutral\" patterns in the environment. In this case, we want the agent to leave those patterns alone, and not disrupt them, even if doing so would allow it to complete the main task faster. The post shows several examples of agents attempting these levels. Vanilla RL agents don't avoid side effects at all, and so unsurprisingly they do quite badly. An agent with a naive impact measure that simply says to preserve the initial state can correctly solve levels where all of the \"neutral\" patterns are static, but has much more trouble when the existing patterns are dynamic (i.e. they oscillate over time).\n\n**Read more:** **[Paper: SafeLife 1.0: Exploring Side Effects in Complex Environments](http://arxiv.org/abs/1912.01217)**\n\n**Rohin's opinion:** I am a big fan of benchmarks; they seem to be a prerequisite to making a lot of quantitative progress (as opposed to more conceptual progress, which seems more possible to do without benchmarks). This benchmark seems particularly nice to me because the \"side effects\" which need to be avoided haven't been handcoded into the benchmark, but instead arise from some simple rules that produce complex effects.\n\nTECHNICAL AI ALIGNMENT\n======================\n\nHANDLING GROUPS OF AGENTS\n-------------------------\n\n**[TanksWorld: A Multi-Agent Environment for AI Safety Research](http://arxiv.org/abs/2002.11174)** *(Corban G. Rivera et al)* (summarized by Asya): This paper presents TanksWorld, a simulation environment that attempts to illustrate three important aspects of real-world AI safety challenges: competing performance objectives, human-machine learning, and multi-agent competition. TanksWorld consists of two teams of N vs. N tanks. Tanks move and shoot while navigating in a closed arena with obstacles. Tanks are rewarded for killing opponent tanks and penalized for killing neutral and allied tanks according to a specified reward function. Each tank is controlled by either its own AI or a special policy meant to mimic a 'human' teammate. Each individual tank can only see a small portion of its environment, and must communicate with other teammates to gain more information. The following parameters can be varied to emphasize different research challenges:\n\n- The communication range between tanks -- meant to represent environmental uncertainty.\n\n- The number of neutral tanks and obstacles -- meant to represent the extent to which tanks must care about 'safety', i.e. avoid collateral damage.\n\n- The control policies of teammates -- meant to represent the variability of human-machine teams.\n\n**Asya's opinion:** I am generally excited about more work on demonstrating safety challenges; I think it helps to seed and grow the field in concrete directions. I am particularly excited about the possibility for TanksWorld to demonstrate multi-agent safety problems with agents in direct competition. I feel unsure about whether TanksWorld will be a good demonstration of general problems with human-machine interaction-- intuitively, that seems to me like it would be very difficult to capture and require more complex real-world modeling.\n\nFORECASTING\n-----------\n\n**[Distinguishing definitions of takeoff](https://www.alignmentforum.org/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff)** *(Matthew Barnett)* (summarized by Rohin): This post lists and explains several different \"types\" of AI takeoff that people talk about. Rather than summarize all the definitions (which would only be slightly shorter than the post itself), I'll try to name the main axes that definitions vary on (but as a result this is less of a summary and more of an analysis):\n\n1. *Locality*. It could be the case that a single AI project far outpaces the rest of the world (e.g. via recursive self-improvement), or that there will never be extreme variations amongst AI projects across all tasks, in which case the \"cognitive effort\" will be distributed across multiple actors. This roughly corresponds to the Yudkowsky-Hanson FOOM debate, and the latter position also seems to be that taken by **[CAIS](https://www.fhi.ox.ac.uk/reframing/)** (**[AN #40](https://mailchi.mp/b649f32b07da/alignment-newsletter-40)**).\n\n2. *Wall clock time*. In **[Superintelligence](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/1501227742)**, takeoffs are defined based on how long it takes for a human-level AI system to become strongly superintelligent, with \"slow\" being decades to centuries, and \"fast\" being minutes to days.\n\n3. *GDP trend extrapolation*. Here, a continuation of an exponential trend would mean there is no takeoff (even if we some day get superintelligent AI), a hyperbolic trend where the doubling time of GDP decreases in a relatively continuous / gradual manner counts as continuous / gradual / slow takeoff, and a curve which shows a discontinuity would be a discontinuous / hard takeoff.\n\n**Rohin's opinion:** I found this post useful for clarifying exactly which axes of takeoff people disagree about, and also for introducing me to some notions of takeoff I hadn't seen before (though I haven't summarized them here).\n\n**[Will AI undergo discontinuous progress?](https://www.alignmentforum.org/posts/5WECpYABCT62TJrhY/will-ai-undergo-discontinuous-progress)** *(Sammy Martin)* (summarized by Rohin): This post argues that the debate over takeoff speeds is over a smaller issue than you might otherwise think: people seem to be arguing for either discontinuous progress, or continuous but fast progress. Both camps agree that once AI reaches human-level intelligence, progress will be extremely rapid; the disagreement is primarily about whether there is already quite a lot of progress *before* that point. As a result, these differences don't constitute a \"shift in arguments on AI safety\", as some have claimed.\n\nThe post also goes through some of the arguments and claims that people have made in the past, which I'm not going to summarize here.\n\n**Rohin's opinion:** While I agree that the debate about takeoff speeds is primarily about the path by which we get to powerful AI systems, that seems like a pretty important question to me with **[many ramifications](https://alignmentforum.org/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment)** (**[AN #62](https://mailchi.mp/4a1b3c4249ae/an-62are-adversarial-examples-caused-by-real-but-imperceptible-features)**).\n\nOTHER PROGRESS IN AI\n====================\n\nREINFORCEMENT LEARNING\n----------------------\n\n**[On Catastrophic Interference in Atari 2600 Games](http://arxiv.org/abs/2002.12499)** *(William Fedus, Dibya Ghosh et al)* (summarized by Rohin): One common worry with deep learning is the possibility of *catastrophic interference*: as the model uses gradients to learn a new behaviour, those same gradients cause it to forget past behaviours. In model-free deep RL, this would be particularly harmful in long, sequential tasks as in hard exploration problems like Montezuma’s Revenge: after the model learns how to do the first few subtasks, as it is trying to learn the next subtask, it would “forget” the first subtasks, degrading performance. The authors set out to test this hypothesis.\n\nIf this hypothesis were true, there would be an easy way to improve performance: once you have learned to perform the first subtask, just create a brand new neural net for the next subtask, so that training for this next subtask doesn’t interfere with past learning. Since the new agent has no information about what happened in the past, and must just “pick up” from wherever the previous agent left off, it is called the Memento agent (a reference to the movie of the same name). One can then solve the entire task by executing each agent in sequence.\n\nIn practice, they train an agent until its reward plateaus. They train a new Memento agent starting from the states that the previous agent reached, and note that it reliably makes further progress in hard exploration games like Montezuma’s Revenge, and not in “steady-state” games like Pong (where you wouldn’t expect as much catastrophic interference). Of course, with the Memento agent, you get both twice the training time and twice the model size, which could explain the improvement. They compare against giving the original agent twice the compute and model capacity, and find that Memento still does significantly better. They also present some fine-grained experiments which show that for a typical agent, training on specific contexts adversely affects performance on other contexts that are qualitatively different.\n\n**Rohin's opinion:** I think this is pretty strong evidence that catastrophic interference is in fact a problem with the Atari games. On the other hand, **[OpenAI Five](https://blog.openai.com/openai-five/)** (**[AN #13](https://mailchi.mp/8234356e4b7f/alignment-newsletter-13)**) also has many, many subtasks, that in theory should interfere with each other, and it still seems to train well. Some guesses at how to reconcile these facts:\n\n1) the tasks in Dota are more correlated than in (say) Montezuma’s Revenge, and so interference is less of a problem (seems plausible)\n\n2) the policy in OpenAI Five was large enough that it could easily allocate separate capacity for various subtasks (seems unlikely, I believe the policy was relatively small), or\n\n3) with sufficiently large-scale training, there is more “exploration” in weight-space until a configuration is found where interference doesn’t happen (seems unlikely given that large batch sizes help, since they tend to reduce weight-space exploration).\n\nDEEP LEARNING\n-------------\n\n**[A new model and dataset for long-range memory](https://deepmind.com/blog/article/A_new_model_and_dataset_for_long-range_memory)** *(Jack W. Rae et al)* (summarized by Nicholas): A central challenge in language modeling is capturing long-range dependencies. For example, a model needs to be able to identify the antecedent of a pronoun even if it is much earlier in the text. Existing datasets consist of news and Wikipedia articles, where articles have average lengths ranging from 27 to 3,600 words. This paper introduces a dataset of Project Gutenberg books, PG-19, where each book has a much longer average length of 69,000 words. This benchmark enables comparison of how well algorithms can make use of information that is spread out across a much larger context.\n\nThey then introduce the *Compressive Transformer*, which builds on the ***[TransformerXL](http://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html)*** (**[AN #44](https://mailchi.mp/6bfac400a0c3/alignment-newsletter-44)**). The *TransformerXL* saves old activations into a FIFO queue, discarding them when the queue is full. The *Compressive Transformer* instead has two FIFO queues: the first stores the activations just like *TransformerXL*, but when activations are ejected, they are compressed and added to the second queue. This functions as a sort of long-term memory, storing information from a longer period of time but in a compressed format. \n\nThey try a number of types of compression function and find that it is best to use a 1D convolutional compression function with an auxiliary loss that leads to lossy compression, where information that is not attended to can be removed. The compression network and the Transformer optimize independent losses without any mixing. \n\nThey find that the *Compressive Transformer* improves on *TransformerXL* on their new PG-19 dataset and is state of the art on the already existing WikiText-103 and Enwik8 benchmarks. They also inspect where the network attends to and find that more attention is paid to the compressed memory than the oldest activations in regular memory, showing that the network is preserving some valuable information.\n\n**Read more:** **[Paper: Compressive Transformers for Long-Range Sequence Modelling](https://arxiv.org/abs/1911.05507)**\n\n**Nicholas's opinion:** I like the idea of saving long-term memory in a more efficient but lower-dimensional format than short-term memory. The current **[trend](https://arxiv.org/abs/2001.08361)** (**[AN #87](https://mailchi.mp/c29b3247da6f/4da2bu7tjd)**) in language modelling is that more computation leads to better results, so I think that algorithms that target computation on the most relevant information are promising. I’d be interested to see (and curious if the authors tried) more continuous variants of this where older information is compressed at a higher rate than newer information, since it seems rather arbitrary to split into two FIFO queues where one has a fixed compression rate. \n\nI’m not well calibrated on the meaning of the evaluation metrics for NLP, so I don’t have a sense of how much of an improvement this is over the *TransformerXL*. I looked through some of the example text they gave in the blog post and thought it was impressive but has clear room for improvement.\n\nMACHINE LEARNING\n----------------\n\n**[Quantifying Independently Reproducible Machine Learning](https://thegradient.pub/independently-reproducible-machine-learning/)** *(Edward Raff)* (summarized by Flo): While reproducibility refers to our ability to obtain results that are similar to the results presented in a paper, **independent reproducibility** requires us to be able to reproduce similar results using *only* what is written in the paper. Crucially, this excludes using the author's code. This is important, as a paper should distill insights rather than just report results. If minor technical details in a reimplementation can lead to vastly different results, this suggests that the paper did not accurately capture all important aspects. The distinction between reproducibility and independent reproducibility is similar to the previously suggested distinctions between **[reproducibility of methods and reproducibility of conclusions](http://proceedings.mlr.press/v97/bouthillier19a.html)** (**[AN #66](https://mailchi.mp/c8ea4a5e842f/an-66-decomposing-robustness-into-capability-robustness-and-alignment-robustness)**) and **[replicability and reproducibility](http://cogprints.org/7691/7/ICMLws09.pdf)**. \n\nThe author attempted to replicate 255 machine learning papers, of which 162 were successfully replicated and ran a statistical analysis on the results. Factors that helped with independent reproduction included specified hyperparameters, ease of reading and authors answering emails. Meanwhile, neither shared code nor the inclusion of pseudo-code robustly increased the rate of reproduction. Interestingly, papers with a strong focus on theory performed worse than mostly empirical or mixed ones. While more rigour can certainly be valuable in the long term, including learning bounds or complicated math just for the sake of it should thus be avoided. Most of the data is **[publically available](https://github.com/EdwardRaff/Quantifying-Independently-Reproducible-ML)** and the author encourages further analysis.\n\n**Read more:** **[Paper: A Step Toward Quantifying Independently Reproducible Machine Learning Research](https://arxiv.org/abs/1909.06674)**\n\n**Flo's opinion:** I appreciate this hands-on approach to evaluating reproducibility and think that independent reproducibility is important if we want to draw robust conclusions about the general properties of different ML systems. I am a bit confused about the bad reproducibility of theory-heavy papers: One hypothesis would be that there is little incentive to provide theoretical justification for approaches that work robustly, as empirical evidence for their merits is generated more easily than theoretical results. This relationship might then flip, as results get more brittle.\n\n**Rohin's opinion:** My explanation for the theoretical results is different: most theory tends to make at least a few assumptions that don't actually hold in order to obtain interesting guarantees. A paper will typically only include empirical results that confirm the theory, which will tend to select for environments in which the assumptions are minimally violated. If you then try to reproduce the paper in a new setting, it is more likely that the assumption is violated more strongly, and so the theoretical results don't show up any more.\n\n#### **FEEDBACK**\n\nI'm always happy to hear feedback; you can send it to me, **[Rohin Shah](https://rohinshah.com/)**, by **replying to this email**. \n\n#### **PODCAST**\n\nAn audio podcast version of the **Alignment Newsletter** is available. This podcast is an audio version of the newsletter, recorded by **[Robert Miles](http://robertskmiles.com/)**.", "url": "https://www.alignmentforum.org/posts/dJanptWZnZx5omwBz/an-91-concepts-implementations-problems-and-a-benchmark-for", "date_published": "2020-03-18T17:10:02Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.728273+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "fed24f043ccbd4d583bf13a99e64c8d9", "source": "alignmentforum", "title": "Alignment as Translation", "text": "[Technology Changes Constraints](https://www.lesswrong.com/s/xEFeCwk3pdYdeG2rL/p/JJv8jmLYzYzdYkS3c) argues that economic constraints are usually modular with respect to technology changes - so for reasoning about technology changes, it’s useful to cast them in terms of economic constraints. Two constraints we’ll talk about here:\n\n* Compute - flops, memory, etc.\n* Information - sensors, data, etc.\n\nThanks to ongoing technology changes, both of these constraints are becoming more and more slack over time - compute and information are both increasingly abundant and cheap.\n\nImmediate question: what happens in the limit as the prices of both compute and information go to zero?\n\nEssentially, we get omniscience: our software has access to a perfect, microscopically-detailed model of the real world. Computers have the memory and processing capability to run arbitrary queries on that model, and predictions are near-perfectly accurate (modulo quantum noise). This limit applies even without AGI - as compute and information become more abundant, our software approaches omniscience, even limiting ourselves to special-purpose reasoning algorithms.\n\nOf course, AGI would presumably be closer to omniscience than non-AGI algorithms, at the same level of compute/information. It would be able to more accurately predict more things which aren’t directly observable via available sensors, and it would be able to run larger queries with the same amount of compute. (*How much* closer to omniscience an AGI would get is an open question, but it would at least not be any worse in a big-O sense.)\n\nNext question: as compute and information constraints slacken, which constraints become taut? What new bottlenecks appear, for problems which were previously bottlenecked on compute/information?\n\nTo put it differently: if our software can run arbitrary queries on an accurate, arbitrarily precise low-level model of the physical world, what else do we need in order to get value out of that capability?\n\nWell, mainly we need some way to specify what it is that we want. [We need an interface](https://www.lesswrong.com/s/xEFeCwk3pdYdeG2rL/p/hyShz2ABiKX56j5tJ).\n\nOur highly accurate low-level world model can tell us anything about the physical world, but the things-we-want are generally more abstract than molecules/atoms/fields. Our software can have arbitrarily precise knowledge and predictive power on physical observables, but it still won’t have any notion that air-pressure-oscillations which sound like the word “cat” have something to do with the organs/cells/biomolecules which comprise a cat. It won’t have built-in any notion of “tree” or “rock” or “human” - using such high-level abstractions would only impede predictive power, when we could instead model the individual components of such high-level objects.\n\nIt’s the [prototypical interface problem](https://www.lesswrong.com/s/xEFeCwk3pdYdeG2rL/p/hyShz2ABiKX56j5tJ): the structure of a high-precision world-model generally does not match the structure of what-humans-want, or the structure of human abstractions in general. Someone/something has to translate between the structures in order to produce anything useful.\n\nAs I see it, this is the central problem of alignment.\n\nSome Approaches\n---------------\n\n**Default: Humans Translate**\n\nWithout some scalable way to build high-level world models out of low-level world models, we constantly need to manually translate things-humans-want into low-level specifications. It’s an intellectual-labor-intensive and error-prone process; writing programs in assembly code is not just an analogy but an example. Even today’s “high-level programming languages” are much more structurally similar to assembly code than to human world-models - Python has no notion of “oak tree”.\n\nAn analogy: translating high-level structure into low-level specification the way we do today is like translating English into Korean by hand.\n\n**Humans Translate Using Better Tools**\n\nIt’s plausible (though I find it unlikely) that we could tackle the problem by building better tools to help humans translate from high-level to low-level - something like much-higher-level programming languages. I find it unlikely because we’d probably need major theoretical breakthroughs - for instance, how do I formally define “tree” in terms of low-level observables? Even if we had ways to do that, they’d probably enable easier strategies than building better programming languages.\n\nAnalogy: it’s like translating by hand from English to Korean, but with the assistance of a dictionary, spell-checker, grammar-checker, etc. But if we had an English-Korean dictionary, we'd probably be most of the way to automated translation anyway (in this respect, the analogy is imperfect).\n\n**Examples + Interpolation**\n\nAnother path which is plausible (though I find it unlikely) is something like programming-by-example - not unlike today’s ML. This seems unlikely to work from both an inside and outside view:\n\n* Inside view: the whole problem in the first place is that low-level structure doesn’t match high-level structure, so there’s no reason to expect software systems to interpolate along human-intuitive dimensions.\n* Outside view: programming-by-example (and today’s ML with it) is notoriously unreliable.\n\nExamples alone aren’t enough to make software reliably carve reality at the same joints as humans. There probably are *some* architectures which would reliably carve at the same joints as humans - different humans tend to chunk the world into similar objects, after all. But figuring out such an architecture would take more than just throwing lots of data at the problem.\n\nTo put it differently: the way-in-which-we-want-things-translated is itself something which needs to be translated. A human’s idea-of-what-constitutes-a-“good”-low-level-specification-of-“oak tree” is itself pretty high-level and abstract; that idea itself needs to be translated into a low-level specification before it can be used. If we’re trying to use examples+interpolation, then the interpolation algorithm is our “specification” of how-to-translate… and it probably isn’t a very good translation of our actual high-level idea of how-to-translate.\n\nAnalogy: it’s like teaching English to Korean speakers by pointing to trees and saying “tree”, pointing to cars and saying “car”, etc… *except* that none of them actually realize they’re supposed to be learning another language. The Korean-language instructions they received were not actually a translation of the English explanation “learn the language that person is speaking”.\n\n**Incentives**\n\nA small tweak to the previous approach: train a reinforcement learner.\n\nThe analogy: rather than giving our Korean-speakers some random Korean-language instructions, we don't give them any instructions - we just let them try things, and then pay them when they happen to translate things from English to Korean.\n\nProblem: this requires some way to check that the translation was correct. Knowing what to incentivize is not any easier than specifying what-we-want to begin with. Rather than translating English-to-Korean, we’re translating English-to-incentives.\n\nNow, there is a lot of room here for clever tricks. What if we verify the translation by having one group translate English-to-Korean, another group translate back, and reward both when the result matches the original? Or taking the Korean translation, giving it to some other Korean speaker, and seeing what they do? Etc. These are possible approaches to translating English into incentives, within the context of the analogy.\n\nIt’s possible in principle that translating what-humans-want into incentives is easier than translating into low-level specifications directly. However, if that’s the case, I have yet to see compelling evidence - attempts to specify incentives seem plagued by the same surprising corner cases and long tail of difficult translations as other strategies.\n\n**AI Translates**\n\nThis brings us to the obvious general answer: have the AI handle the translation from high-level structure to low-level structure. This is probably what will happen eventually, but the previous examples should make it clear why it’s hard: an explanation of how-to-translate must itself be translated. In order to make an AI which translates high-level things-humans-want into low-level specifications, we first need a low-level specification of the high-level concept “translate high-level things-humans-want into low-level specifications”.\n\nContinuing the earlier analogy: we’re trying to teach English to a Korean speaker, but that Korean speaker doesn’t have any idea that they’re supposed to be learning another language. In order to get them to learn English, we first need to somehow translate something like “please learn this language”.\n\nThis is a significant reduction of the problem: rather than translating everything by hand all the time, we just need to translate the one phrase “please learn this language”, and then the hard part is done and we can just use lots of examples for the rest.\n\nBut we do have a chicken-and-egg problem: somehow, we need to properly translate that first phrase. Screw up that first translation, and nothing else will work. That part cannot be outsourced; the AI cannot handle the translation because it has no idea that that’s what we want it to do.", "url": "https://www.alignmentforum.org/posts/42YykiTqtGMyJAjDM/alignment-as-translation", "date_published": "2020-03-19T21:40:01Z", "authors": ["johnswentworth"], "tags": ["AI"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.728942+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "f61bfa282f241f7deaaa2314f1aa5465", "source": "alignmentforum", "title": "Thinking About Filtered Evidence Is (Very!) Hard", "text": "*The content of this post would not exist if not for conversations with Zack Davis, and owes something to conversations with Sam Eisenstat.*\n\nThere's been some [talk](https://www.lesswrong.com/posts/G5TwJ9BGxcgh5DsmQ/yes-requires-the-possibility-of-no) [about](https://www.lesswrong.com/posts/zTfSXQracE7TW8x4w/mistakes-with-conservation-of-expected-evidence-1) [filtered](https://www.lesswrong.com/posts/DoPo4PDjgSySquHX8/heads-i-win-tails-never-heard-of-her-or-selective-reporting) [evidence](https://www.lesswrong.com/posts/bSmgPNS6MTJsunTzS/maybe-lying-doesn-t-exist) [recently](https://www.lesswrong.com/posts/fmA2GJwZzYtkrAKYJ/algorithms-of-deception). I want to make a mathematical observation which causes some trouble for the Bayesian treatment of filtered evidence. [OK, when I *started* writing this post, it was \"recently\". It's been on the back burner for a while.]\n\nThis is also a continuation of the [line of research](https://www.lesswrong.com/posts/5bd75cc58225bf067037518c/all-mathematicians-are-trollable-divergence-of-naturalistic-logical-updates) [about](https://www.lesswrong.com/posts/5bd75cc58225bf06703751a0/you-can-t-beat-a-troll-by-predicting-it) [trolling](https://www.lesswrong.com/posts/5bd75cc58225bf0670375533/an-untrollable-mathematician) [mathematicians](https://www.lesswrong.com/posts/CvKnhXTu9BPcdKE4W/an-untrollable-mathematician-illustrated), and hence, relevant to logical uncertainty.\n\nI'm going to be making a mathematical argument, but, I'm going to keep things rather informal. I think this increases the clarity of the argument for *most* readers. I'll make some comments on proper formalization at the end.\n\n\n\n---\n\nAlright, here's my argument.\n\nAccording to the Bayesian treatment of filtered evidence, you need to update on *the fact that the fact was presented to you*, rather than the raw fact. This involves [reasoning about the algorithm which decided which facts to show you](https://www.lesswrong.com/posts/kJiPnaQPiy4p9Eqki/what-evidence-filtered-evidence). The point I want to make is that this can be incredibly computationally difficult, *even if the algorithm is so simple that you can predict what it will say next.* IE, I don't need to rely on anything like \"humans are too complex for humans to really treat as well-specified evidence-filtering algorithms\".\n\nFor my result, we imagine that a Bayesian reasoner (the \"listener\") is listening to a series of statements made by another agent (the \"speaker\").\n\nFirst, I need to establish some terminology:\n\n**Assumption 1.** *A listener will be said to have a* **rich hypothesis space***if the listener assigns some probability to the speaker enumerating any computably enumerable set of statements.*\n\nThe intuition behind this assumption is supposed to be: due to computational limitations, the listener may need to restrict to some set .mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\nH of easily computed hypotheses; for example, the hypotheses might be poly-time or even log-poly. This prevents hypotheses such as \"the speaker is giving us the bits of a halting oracle in order\", as well as \"the speaker has a little more processing power than the listener\". However, the hypothesis space is not *so* restricted as to limit the world to being a finite-state machine. The listener can imagine the speaker proving complicated theorems, so long as it is done *sufficiently slowly for the listener to keep up*. In such a model, the listener might imagine the speaker staying quiet for quite a long time (observing the null string over and over, or some simple sentence such as 1=1) while a long computation completes; and only then making a complicated claim.\n\nThis is also not to say that I *assume* my listener considers *only* hypotheses in which it can 100% keep up with the speaker's reasoning. The listener can also have probabilistic hypotheses which recognize its inability to perfectly anticipate the speaker. I'm only pointing out that my result *does not rely on* a speaker which the listener can't keep up with.\n\nWhat it *does* rely on is that there are not too many restrictions on what the speaker *eventually* says.\n\n**Assumption 2.** *A listener* **believes a speaker to be honest** *if the listener distinguishes between \"X\" and \"the speaker claims X at time t\" (aka \"claims*t*-X\"), and also has beliefs such that* P(X| claimst-X)=1 *when* P(claimst-X) > 0*.*\n\nThis assumption is, basically, saying that the agent trusts its observations; the speaker can filter evidence, but the speaker cannot falsify evidence.\n\nMaybe this assumption seems quite strong. I'll talk about relaxing it after I sketch the central result.\n\n**Assumption 3.** *A listener is said to have* **minimally consistent beliefs** *if each proposition* X *has a negation* X\\**, and* P(X)+P(X\\*)≤1. \n\nThe idea behind minimally consistent beliefs is that the listener need not be logically omniscient, but does avoid outright contradictions. This is important, since assuming logical omniscience would throw out computability from the start, making any computational-difficulty result rather boring; but totally throwing out logic would make my result impossible. Minimal consistency keeps an extremely small amount of logic, but, it is enough to prove my result.\n\n**Theorem(/Conjecture).** *It is not possible for a Bayesian reasoner, observing a sequence of remarks made by a speaker, to simultaneously:*\n\n* *Have a rich hypothesis space.*\n* *Believe the speaker to be honest.*\n* *Have minimally consistent beliefs.*\n* *Have computable beliefs.*\n\n**Proof sketch.** Suppose assumptions 1-3. Thanks to the rich hypothesis space assumption, the listener will assign some probability to the speaker enumerating theorems of PA (Peano Arithmetic). Since this hypothesis makes distinct predictions, it is possible for the confidence to rise above 50% after finitely many observations. At that point, since the listener expects each theorem of PA to *eventually* be listed, with probability > 50%, and the listener believes the speaker, the listener *must* assign > 50% probability to each theorem of PA! But this implies that the listener's beliefs are not computable, since if we had access to them we could *separate theorems of PA from contradictions* by checking whether a sentence's probability is > 50%. □ \n\nSo goes my argument.\n\n\n\n---\n\nWhat does the argument basically establish?\n\nThe argument is supposed to be surprising, because *minimally consistent beliefs* are compatible with computable beliefs; and *rich hypothesis space* is compatible with beliefs which are computable on observations alone; yet, when combined with a belief that the speaker is honest, we get an incomputability result.\n\nMy take-away from this result is that we cannot *simultaneously* use our unrestricted ability to predict sensory observations accurately *and* have completely coherent beliefs about the world which produces those sensory observations, at least if our \"bridge\" between the sensory observations and the world includes something like language (whereby sensory observations contain complex \"claims\" about the world).\n\nThis is because using the *full force* of our ability to predict sensory experiences includes some hypotheses which *eventually* make surprising claims about the world, by incrementally computing increasingly complicated information (like a theorem prover which slowly but inevitably produces all theorems of PA). In other words, a rich sensory model contains *implicit information about the world* which we cannot immediately compute the consequences of (in terms of probabilities about the hidden variables out there in the world). This \"implicit\" information can be *necessarily* implicit, in the same way that PA is *necessarily* incomplete.\n\nTo give a non-logical example: suppose that your moment-to-moment anticipations of your relationship with a friend are pretty accurate. It might be that if you roll those anticipations forward, you inevitably become closer and closer until the friendship becomes a romance. *However,* you can't necessarily predict that right now; even though the anticipation of each next moment is relatively easy, you face a halting-problem-like difficulty if you try to anticipate what the *eventual* behavior of your relationship is. Because our ability to look ahead is bounded, each new consequence can be predictable without the overall outcome being predictable.\n\nThus, in order for an agent to use the full force of its computational power on predicting sensory observations, it must have *partial* hypotheses -- similar to the way [logical induction](https://intelligence.org/2016/09/12/new-paper-logical-induction/) contains traders which focus only on special classes of sentences, or Vanessa's [incomplete Bayesianism](https://www.alignmentforum.org/posts/5bd75cc58225bf067037530a/towards-learning-incomplete-models-using-inner-prediction-markets) contains incomplete hypotheses which do not try to predict everything.\n\nSo, this is an argument against strict Bayesianism. In particular, it is an argument against strict Bayesianism *as a model of updating on filtered evidence!* I'll say more about this, but first, let's talk about possible holes in my argument.\n\n\n\n---\n\nHere are some concerns you might have with the argument.\n\nOne might possibly object that the perfect honesty requirement is unrealistic, and therefore conclude that the result does not apply to realistic agents.\n\n* I would point out that the assumption is not so important, so long as the listener *can conceive of the possibility of perfect honesty,* and assigns it nonzero probability*.* In that case, we can consider P(X|honesty) rather than P(X). Establishing that some conditional beliefs are not computable seems similarly damning.\n* Furthermore, because the \"speaker\" is serving the role of our *observations*, the perfect honesty assumption is just a version of P(X|observe-X)=1. IE, *observing X gives us X.* This is true in typical filtered-evidence setups; IE, filtered evidence can be misleading, but it can't be false.\n* However, one might further object that *agents need not be able to conceive of \"perfect honesty\",* because this assumption has an unrealistically aphysical, \"perfectly logical\" character. One might say that *all* observations are imperfect; none are perfect evidence of what is observed. In doing so, we can get around my result. This has some similarity to the assertion that zero is not a valid probability. I don't find this response particularly appealing, but I also don't have a strong argument against it.\n\nAlong similar lines, one might object that the result depends on an example (\"the speaker is enumerating theorems\") which comes from logic, as opposed to any realistic physical world-model. The example does have a \"logical\" character -- we're not explicitly reasoning about evidence-filtering algorithms interfacing with an empirical world and selectively telling us some things about it. However, I want to point out that I've assumed extremely little \"logic\" -- the only thing I use is that you don't expect a sentence and its negation to both be true. Observations corresponding to theorems of PA are just an example used to prove the result. The fact that P(X) can be very hard to compute even when we restrict to easily computed P(claimst-X) is very general; even if we do restrict attention to finite-state-machine hypotheses, we are in P-vs-NP territory.\n\n\n\n---\n\nWhat does this result say about logical uncertainty?\n\nSam's [untrollable prior](https://www.lesswrong.com/posts/5bd75cc58225bf0670375533/an-untrollable-mathematician) beat the [trollable-mathematician problem](https://www.lesswrong.com/posts/5bd75cc58225bf067037518c/all-mathematicians-are-trollable-divergence-of-naturalistic-logical-updates) by the usual Bayesian trick of explicitly modeling the sequence of observations -- updating on I-observed-X-at-this-time rather than only X. (See also [the illustrated explanation](https://www.lesswrong.com/posts/CvKnhXTu9BPcdKE4W/an-untrollable-mathematician-illustrated).)\n\nHowever, it did so at a high cost: Sam's prior is *dumb.* It isn't able to perform rich Occam-style induction to divine the hidden rules of the universe. It doesn't *believe in* hidden rules; it believes \"if there's a law of nature constraining everything to fit into a pattern, *I will eventually observe that law directly*.\" It shifts its probabilities when it makes observations, but, in some sense, it doesn't shift them *very much*; and indeed, that property seems key to the computability of that prior.\n\nSo, a natural question arises: is this an *essential* property of an untrollable prior? Or can we construct a \"rich\" prior which entertains hypotheses about the deep structure of the universe, learning about them in an Occam-like way, which is nonetheless still untrollable?\n\nThe present result is a first attempt at an answer: given my (admittedly a bit odd) notion of rich hypothesis space, it is indeed impossible to craft a computable prior over logic with some minimal good properties (like believing what's proved to it). I don't directly address a trollability-type property, unfortunately; but I do think I get close to the heart of the difficulty: a \"deep\" ability to adapt in order to predict data better stands in contradiction with computability of the latent probability-of-a-sentence.\n\n\n\n---\n\nSo, how should we think about filtered evidence?\n\nOrthodox Bayesian (OB): We can always resolve the problem by distinguishing between X and \"I observe X\", and conditioning on **all** the evidence available. Look how nicely it works out in [the Monty Hall problem and other simple examples we can write down](https://www.lesswrong.com/posts/kJiPnaQPiy4p9Eqki/what-evidence-filtered-evidence). \n\nSkeptical Critique (SC): You're ignoring the argument. You can't handle cases where running your model forward is easier than answering questions about what happens eventually; in those cases, many of your beliefs will either be uncomputable or incoherent.\n\nOB: That's not a problem for me. Bayesian ideals of rationality apply to the logically omniscient case. What they give you is an idealized notion of rationality, which defines the **best** an agent **could** do.\n\nSC: Really? Surely your Bayesian perspective is supposed to have some solid implications for finite beings who are not logically omniscient. I see you giving out all this advice to machine learning programmers, statisticians, doctors, and so on.\n\nOB: Sure. We might not be able to achieve perfect Bayesian rationality, but whenever we see something less Bayesian than it could be, we can correct it. That's how we get closer to the Bayesian ideal!\n\nSC: That sounds like cargo-cult Bayesianism to me. If you spot an inconsistency, **it matters how you correct it**; you don't want to go around correcting for the planning fallacy by trying to do everything faster, right? Similarly, if your rule-of-thumb for the frequency of primes is a little off, you don't want to add composite numbers to your list of primes to fudge the numbers.\n\nOB: No one would make those mistakes.\n\nSC: That's because there are, in fact, rationality principles which apply. You **don't** just cargo-cult Bayesianism by correcting inconsistencies any old way. A boundedly rational agent has rationality constraints which apply, guiding it to better approximate \"ideal\" rationality. And those rationality constraints don't actually need to refer to the \"ideal\" rationality. **The rationality constraints are about the update, not in the ideal which the update limits to.**\n\nOB: Maybe we can imagine some sort of finite Bayesian reasoner, who treats logical uncertainty as a black box, and follows the evidence toward unbounded-Bayes-optimality in a bounded-Bayes-optimal way...\n\nSC: Maybe, but I don't know of a good picture which looks like that. The picture we **do** have is given by logical induction: we learn to avoid Dutch books by noticing lots of Dutch books against ourselves, and gradually becoming less exploitable.\n\nOB: That sounds a lot like the picture I gave.\n\nSC: Sure, but it's more precise. And more importantly, it's **not** a Bayesian update -- there is a kind of family resemblance in the math, but it isn't learning through a Bayesian update in a strict sense.\n\nOB: Ok, so what does all this have to do with filtered evidence? I still don't see why the way I handle that is wrong.\n\nSC: Well, isn't the standard Bayesian answer a little suspicious? The numbers conditioning on X don't come out to what you want, so you introduce something new to condition on, observe-X, which can have different conditional probabilities. Can't you get whatever answer you want, that way?\n\nOB: I don't think so? The numbers are dictated by the scenario. The Monty Hall problem has a right answer, which determines how you should play the game if you want to win. You can't fudge it without changing the game.\n\nSC: Fair enough. But I still feel funny about something. Isn't there an infinite regress? We jump to updating on observe-X when X is filtered. What if observe-X is filtered? Do we jump to observe-observe-X? What if we can construct a \"meta Monty-Hall problem\" where it isn't sufficient to condition on observe-X?\n\nOB: If you observe, you observe that you observe. And if you observe that you observe, then you must observe. So there's no difference.\n\nSC: If you're logically perfect, sure. But a boundedly rational agent need not realize immediately that it observed X. And certainly it need not realize and update on the entire sequence \"X\", \"I observed X\", \"I observed that I observed X\", and so on.\n\nOB: Ok...\n\nSC: To give a simple example: call a sensory impression \"subliminal\" when it is weak enough that only X is registered. A stronger impression also registers \"observe-X\", making the sensory impression more \"consciously available\". Then, we cannot properly track the effects of filtered evidence for subliminal impressions. Subliminal impressions would always register as if they were unfiltered evidence.\n\nOB: ...no. \n\nSC: What's wrong?\n\nOB: An agent should come with a basic notion of sensory observation. If you're a human, that could be activation in the nerves running to sensory cortex. If you're a machine, it might be RBG pixel values coming from a camera. That's the only thing you ever have to condition on; all your evidence has that form. Observing a rabbit means *getting pixel values corresponding to a rabbit.* We don't start by conditioning on \"rabbit\" and then patch things by adding \"observe-rabbit\" as an additional fact. We condition on *the complicated observation corresponding to the rabbit*, which happens to, *by inference,* tell us that there is a rabbit.\n\nSC: That's... a bit frustrating.\n\nOB: How so?\n\nSC: The core Bayesian doctrine is the Kolmogorov axioms, together with the rule that we update beliefs via Bayesian conditioning. A common extension of Bayesian doctrine *grafts on a distinction between observations and hypotheses,* naming some special events as observable, and others as non-observable hypotheses. I want you to *notice when you're using the extension rather than the core.*\n\nOB: How is that even an extension? It just sounds like a special case, which happens to apply to just about any organism.\n\nSC: But you're restricting the rule \"update beliefs by Bayesian conditioning\" -- you're saying that it only works for *observations*, not for other kinds of events.\n\nOB: Sure, but you could never update on those other kinds of events anyway.\n\nSC: Really, though? Can't you? Some information you update on comes from sensory observations, but other information comes from *reasoning*. Something like a feedforward neural network just computes one big function on sense-data, and can probably be modeled in the way you're suggesting. But something like a [memory network](https://arxiv.org/abs/1410.3916) has a nontrivial reasoning component. A Bayesian can't handle \"updating\" on internal calculations it's completed; at best they're treated as if they're black boxes whose outputs are \"observations\" again.\n\nOB: Ok, I see you're backing me into a corner with logical uncertainty stuff again. I still feel like there should be a Bayesian way to handle it. But what does this have to do with filtered evidence?\n\nSC: *The whole point of the argument we started out discussing* is that if you have this kind of observation/hypothesis divide, and have sufficiently rich ways of predicting sensory experiences, *and remain a classical Bayesian*, then your beliefs about the hidden information are not going to be computable, even if your hypotheses themselves are easy to compute. So we can't realistically reason about the hidden information just by Bayes-conditioning on the observables. The only way to maintain both computability and a rich hypothesis space under these conditions is to be less Bayesian, allowing for more inconsistencies in your beliefs. *Which means,* **reasoning about filtered evidence doesn't reduce to applying Bayes' Law**.\n\nOB: That... seems wrong.\n\nSC: Now we're getting somewhere!\n\n\n\n---\n\nAll that being said, reasoning about filtered evidence via Bayes' Law in the orthodox way still seems quite practically compelling. The perspective SC puts forward in the above dialogue would be much more compelling if I had more practical/interesting \"failure-cases\" for Bayes' Law, and more to say about alternative ways of reasoning which work better for those cases. A real \"meta Monty-Hall problem\".\n\nArguably, logical induction *doesn't* use the \"condition on the fact that X was observed\" solution:\n\n* Rather than the usual sequential prediction model, logical induction accommodates information coming in for any sentence, in any order. So, like the \"core of Bayesianism\" mentioned by SC, it maintains its good properties without special assumptions about what is being conditioned on. This is in contrast to, e.g., Solomonoff induction, which uses the sequential prediction model.\n* In particular, in Monty Hall, although there is a distinction between the sentence \"there is a goat behind door 3\" and \"the LI discovers, at time *t*, that there is a goat behind door 3\" (or suitable arithmetizations of these sentences), we can condition on the first rather than the second. A logical inductor would learn to react to this in the appropriate way, since doing otherwise would leave it Dutch-bookable.\n\nOne might argue that the traders are implicitly using the standard Bayesian \"condition on the fact that X was observed\" solution in order to accomplish this. Or that the update an LI performs upon seeing X *is always* that it saw X. But to me, this feels like stretching things. The core of the Bayesian method for handling filtered evidence is to distinguish between X and the observation of X, and update on the latter. A logical inductor doesn't explicitly follow this, and indeed appears to violate it. Part of the usual idea seems to be that a Bayesian needs to \"update on all the evidence\" -- but a logical inductor just gets a black-box report of X, without any information on how X was concluded or where it came from. So information can be arbitrarily excluded, and the logical inductor will still do its best (which, in the case of Monty Hall, appears to be sufficient to learn the correct result).\n\n\n\n---\n\nA notable thing about the standard sort of cases, where the Bayesian way of reasoning about filtered evidence is entirely adequate, is that you have a gears-level model of what is going on -- a causal model, which you can turn the crank on. If you run such a model \"forward\" -- in causal order -- you compute the hidden causes *before* you compute the filtered evidence about them. This makes it sound as if predicting the hidden variables should be *easier* than predicting the sensory observations; and, certainly makes it hard to visualize the situation where it is much much harder.\n\nHowever, even in cases where we have a nice causal model like that, inferring the hidden variables from what is observed can be intractably computationally difficult, since it requires *reverse-engineering* the computation from its outputs. [Forward-sampling](https://ermongroup.github.io/cs228-notes/inference/sampling/) causal models is always efficient; running them backwards, not so.\n\nSo even with causal models, there can be good reason to engage more directly with logical uncertainty rather than use pure Bayesian methods.\n\nHowever, I suspect that one could construct a much more convincing example if one were to use partial models explicitly in the construction of the example. Perhaps something involving an \"outside view\" with strong empirical support, but lacking a known \"inside view\" (lacking a single consistent causal story).\n\nUnfortunately, such an example escapes me at the moment.\n\n\n\n---\n\nFinally, some notes on further formalisation of my main argument.\n\nThe listener is supposed to have probabilistic beliefs of the standard variety -- an event space which is a sigma-algebra, and which has a P(event) obeying the Kolmogorov axioms. In particular, the beliefs are supposed to be perfectly logically consistent in the usual way.\n\nHowever, in order to allow logical uncertainty, I'm assuming that there is *some embedding of arithmetic;* call it E[]. So, for each arithmetic sentence S, there is an event E[S]. Negation gets mapped to the \"star\" of an event: E[¬S] = (E[S])\\*. This need not be the compliment of the event E[S]. Similarly, the embedding E[A∨B] need not be E[A]∪E[B]; E[A∧B] need not be E[A]∩E[B]; and so on. That's what allows for logical non-omniscience -- the probability distribution doesn't necessarily *know* that E[A∧B] should act like E[A]∩E[B], and so on.\n\nThe more we impose requirements which force the embedding to *act like it should,* the more logical structure we are forcing onto the beliefs. If we impose very much consistency, however, then that would already imply uncomputability and the central result would not be interesting. So, the \"minimal consistency\" assumption requires very little of our embedding. Still, it is enough for the embedding of PA to cause trouble in connection with the other assumptions.\n\nIn addition to all this, we have a distinguished set of events which count as observations. A first pass on this is that for any event A, there is an associated event obs(A) which is the observation of A. But I do worry that this includes more observation events than we want to require. Some events A do not correspond to sentences; sigma-algebras are closed under countable unions. If we think of the observation events as *claims made by the speaker*, it doesn't make sense to imagine the speaker claiming a countable union of sentences (particularly not the union of an uncomputable collection).\n\nSo, more conservatively, we might say that for events E[S], that is *events in the image of the embedding*, we also have an event obs(E[S]). In any case, this is closer to the minimal thing we need to establish the result.\n\nI don't know if the argument works out exactly as I sketched; it's possible that the rich hypothesis assumption needs to be \"and also positive weight on a particular enumeration\". Given that, we can argue: take one such enumeration; as we continue getting observations consistent with that observation, the hypothesis which predicts it loses no weight, and hypotheses which (eventually) predict other things must (eventually) lose weight; so, the updated probability eventually believes that particular enumeration will continue with probability > 1/2.\n\nOn the other hand, that patched definition is certainly less nice. Perhaps there is a better route.", "url": "https://www.alignmentforum.org/posts/fhJkQo34cYw6KqpH3/thinking-about-filtered-evidence-is-very-hard", "date_published": "2020-03-19T23:20:06Z", "authors": ["abramdemski"], "tags": ["Rationality", "Filtered Evidence", "Epistemic Hygiene"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.729490+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "00afed09943bf665600df641c409aa99", "source": "alignmentforum", "title": "[Meta] Do you want AIS Webinars?", "text": "Ever now and then I talk to someone who tell me that they can not get good feedback on their research (e.g. the they don't get much responses on their alignment forum post), and been thinking about how to solve this? Also, right now is a good time to try out various online solutions.\n\nI am offering to run webinars if enough people are interested. \n\nThe format I suggest is that each webinar is 1h 40min and centers around one person presenting their idea or result.\n\n* 5 min introduction\n* 30 min presentation\n* 30 min of questions and discussion centered around trying to understand and improve idea.\n* 30 min of attacking the idea to find out all the ways it can fail.\n* 5 min rounding off but the presenter summering what their take away is.\n\nThe reason I suggest this structure is\n\n1. I think a webinar needs a structure to keep on track\n2. It is important to look for weaknesses in an idea, but it is even more important to fist understand it and to make sure you attack the strongest version of that idea.\n\nI am very much open to different format, and would like to experiment a bit if there is enough interest.\n\n**Edit:** Not sure if I should interpret up votes as interest. If this is something you would like to participate in, pleas leave a comment.", "url": "https://www.alignmentforum.org/posts/BbrsgHPJmGxeg7nXG/meta-do-you-want-ais-webinars", "date_published": "2020-03-21T16:01:03Z", "authors": ["Linda Linsefors"], "tags": ["Community", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.729939+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "28c9f38b43807c65faacb81d91c7af64", "source": "alignmentforum", "title": "Deconfusing Human Values Research Agenda v1", "text": "On Friday I attended the [2020 Foresight AGI Strategy Meeting](https://foresight.org/event/2020-foresight-agi-strategy-meeting/). Eventually a report will come out summarizing some of what was talked about, but for now I want to focus on what I talked about in my session on deconfusing human values. For that session I wrote up some notes summarizing what I've been working on and thinking about. None of it is new, but it is newly condensed in one place and in convenient list form, and it provides a decent summary of the current state of my research agenda for building beneficial superintelligent AI; a version 1 of my agenda, if you will. Thus, I hope this will be helpful in making it a bit clearer what it is I'm working on, why I'm working on it, and what direction my thinking is moving in. As always, if you're interesting in collaborating on things, whether that be discussing ideas or something more, please reach out.\n\nProblem overview\n================\n\n* I think we're confused about what we really mean when we talk about human values.\n* This is a problem because:\n+ building aligned AI likely requires a [mathematically precise understanding of the structure of human values](https://www.lesswrong.com/posts/7dvDgqvqqziSKweRs/formally-stating-the-ai-alignment-problem-1), though not necessarily the content of human values;\n+ we can't trust AI to discover that structure for us because we would need to understand it enough to verify the result, and I think we're so confused about what human values are we couldn't do that without [high risk of error](https://www.lesswrong.com/posts/JYdGCrD55FhS4iHvY/robustness-to-fundamental-uncertainty-in-agi-alignment-1).\n\n* What are values?\n+ We don't have an agreed upon precise definition, but loosely it's \"stuff people care about\".\n- When I talk about \"values\" I mean the cluster we sometimes also point at with words like value, preference, affinity, taste, aesthetic, intention, and axiology.\n\n+ Importantly, what people care about is used to make decisions, and this has had implications for existing approaches to understanding values.\n\n* Much research on values tries to understand the content of human values or why humans value what they value, but not what the structure of human values is such that we could use it to model arbitrary values. This research unfortunately does not appear very useful to this project.\n* The best attempts we have right now are based on the theory of preferences.\n+ In this model a preference is a statement located within a (weak, partial, total, etc.)-order. Often written like A > B > C to mean A is preferred to B is preferred to C.\n+ Problems:\n- [Goodhart effects are robust](https://www.lesswrong.com/posts/NqQxTn5MKEYhSnbuB/goodhart-s-curse-and-limitations-on-ai-alignment) and preferences in formal models are measures that is not the thing we care about itself\n- Stated vs. revealed preferences: we generally favor revealed preferences, this approach has some problems:\n* can only infer preferences from behaviors observed; latent preferences\n* inferring preferences from observation [requires making normative assumptions](https://www.lesswrong.com/posts/LRYwpq8i9ym7Wuyoc/other-versions-of-no-free-lunch-in-value-learning), and if we don't make normative assumptions there are [too many free variables](https://www.lesswrong.com/posts/rtphbZbMHTLCepd6d/humans-can-be-assigned-any-values-whatsoever)\n\n- General vs. specific preferences: do we look for context-independent preferences (\"essential\" values) or context-dependent preferences\n* generalized preferences, e.g. \"I like cake better than cookies\", can lead to irrational preferences (e.g. non-transitive preferences)\n* contextualized preferences, e.g. \"I like cake better than cookies at this precise moment\", limit our ability to reason about what someone would prefer in new situations\n\n+ See [Stuart Armstrong's work](https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into-1) for an attempt to address these issues so we can turn preferences into utility functions.\n\n* Preference based models look to me to be trying to specify human values at the wrong level of abstraction. But what would the right level of abstraction be?\n\nSolution overview\n=================\n\n* What follows is a summary of what I so far think moves us closer to less confusion about human values. I hope to either think some of this is wrong or insufficient by the end of the discussion!\n* Assumptions:\n+ [Humans are embedded agents.](https://www.lesswrong.com/posts/WJzsTmsDctYCCyMfy/humans-are-embedded-agents-too)\n+ Agents have fuzzy but definable boundaries.\n- Everything in every moment causes everything in every next moment up to the limit of the speed of light, but we can find clusters of stuff that interact with themselves in ways that are \"aligned\" such that the stuff in a cluster makes sense to model as an agent separate from the stuff not in an agent.\n\n* [Basic model](https://www.lesswrong.com/posts/WAqG5BQMzAs34mpc2/towards-deconfusing-values):\n+ Humans (and other agents) cause events. We call this **acting**.\n+ The process that leads to taking one action rather than another possible action is **deciding**.\n+ Decisions are made by some **decision generation process**.\n+ **Values** are the inputs to the decision generation process that determine its decisions and hence actions.\n+ **Preferences** and **meta-preferences** are statistical regularities we can observe over the actions of an agent.\n\n![](https://docs.google.com/drawings/d/e/2PACX-1vTLiZGRahzO8ojiZ8d_JqQ1d0cHTgbyQQSyCz_vur1C0roIYbznrESS26-eGluQvJ2ULIceTcW6ullR/pub?w=1440&h=1080)* Important differences from preference models:\n+ Preferences are causally after, not causally before, decisions, contrary to the standard preference model.\n- This is not 100% true. Preferences can be observed by self-aware agents, like humans, and influence the decision generation process.\n\n* So then what are values? The inputs to the decision generation process?\n+ My best guess: [valence](https://www.lesswrong.com/posts/ALvnz3DrjHwmLG29F/values-valence-and-alignment)\n- My best best guess: [valence as modeled by minimization of prediction error](https://www.lesswrong.com/posts/Cu7yv4eM6dCeA67Af/minimization-of-prediction-error-as-a-foundation-for-human)\n\n+ This leaves us with new problems. Now rather than trying to infer preferences from observations of behavior, we need to understand the decision generation process and valence in humans, i.e. this is now a neuroscience problem.\n\nDiscussion\n==========\n\n* underdetermination due to noise; many models are consistent with the same data\n+ this makes it easy for us to get confused, even when we're trying to deconfuse ourselves\n+ this makes it hard to know if our model is right since we're often in the situation of explaining rather than predicting\n\n* is this a descriptive or causal model?\n+ both. descriptive of what we see, but trying to find the causal mechanism of what we reify as \"values\" at the human level in terms of \"gears\" at the neuron level\n\n* what is valence?\n+ [see here](https://www.lesswrong.com/posts/ALvnz3DrjHwmLG29F/values-valence-and-alignment)\n\n* complexities of going from neurons to human level notions of values\n+ there's a lot of layers of different systems interacting on the way from neurons to values and we don't understand enough about almost any of them or even for sure what systems there are in the causal chain\n\n* [Valence in human computer interaction research](https://medium.com/human-computer-interaction-and-games-research/biosignal-datasets-for-emotion-recognition-d3a8c61ef781)\n\nAcknowledgements\n================\n\nThanks to Dan Elton, De Kai, Sai Joseph, and several other anonymous participants of the session for their attention, comments, questions, and insights.", "url": "https://www.alignmentforum.org/posts/k8F8TBzuZtLheJt47/deconfusing-human-values-research-agenda-v1", "date_published": "2020-03-23T16:25:28Z", "authors": ["Gordon Seidoh Worley"], "tags": ["Perceptual Control Theory", "Value Learning", "Research Agendas", "Metaethics", "AI"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.730028+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "e695f16ba4a318059f6b28a079386c2d", "source": "alignmentforum", "title": "[AN #92]: Learning good representations with contrastive predictive coding", "text": "**Newsletter #92**\n\nAlignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.\n\nAudio version **[here](http://alignment-newsletter.libsyn.com/alignment-newsletter-92)** (may not be up yet). \n\nHIGHLIGHTS\n==========\n\n**[Representation Learning with Contrastive Predictive Coding](https://arxiv.org/abs/1807.03748)** *(Aaron van den Oord et al)* (summarized by Rohin): This paper from 2018 proposed Contrastive Predictive Coding (CPC): a method of unsupervised learning that has been quite successful. At its core it is quite simple: it simply combines the ideas of predictive coding and contrastive losses, both of which have been significantly studied in the past.\n\nThe simplest form of unsupervised learning would be data compression via generative models (as in e.g. VAEs), in which, to model the data **p(x)**, you attempt to encode **x** into a latent (hidden) state **z** in such a way that you can then recover the original data point **x** from **z**. Intuitively, we want **z** to have high mutual information with **x**.\n\nFor sequential data in a partially observed setting, you need to deal with the full sequence. Consider natural language: in this setting, each x would be a single word. Consider the sentence \"I sat on the chair\". If the **z** corresponding to the word \"the\" only has to reconstruct the word \"the\", it's not going to \"remember\" that the past context involved sitting, and so that **z** would be terrible at predicting that the next word will be chair. To fix this, we can use predictive coding, where we instead require that we can *predict* future words using **z**. This now incentivizes **z\\_t** to have high mutual information with **x\\_{t+k}**.\n\nThere is still a problem: reconstructing the entire input **x** would require a lot of irrelevant information, such as e.g. the background color of the environment in RL, even if that never changes. How can we get rid of these irrelevant features? Contrastive losses allow us to do this: intuitively, since the irrelevant features are the ones that are common across all the **x**s (and so are fully captured by **p(x)** ), if we train the neural net to *distinguish* between various **x**s, we can incentivize only the relevant features. In particular, given a latent state **z\\_t**, we take the true **x\\_{t+k}**, and throw in a bunch of other **x**s sampled from **p(x)** (known as *negative samples*), and train the network to correctly classify **x\\_{t+k}**. The authors show that the optimum of this loss function is indeed for the neural net to compute **p(x | z) / p(x)**, which implies that it is maximizing a lower bound on the mutual information between X and Z.\n\nThis gives us a pretty simple overall algorithm. Take a sequence **x\\_1 ... x\\_T**, compute **z\\_t** using a recurrent model on **x\\_1 ... x\\_t**, put **x\\_{t+k}** and some negative samples into a set, and train a classifier to correctly predict which of the samples is the true **x\\_{t+k}**. In practice, we do batches of these at the same time, and for every data point in the batch we use all of the other data points as our negative examples. The features you learn are then the ones that help *distinguish* between **x\\_{t+k}** and the negative samples, and you'll ignore any features that are common across all the samples. This means that the results depend quite a lot on how you choose your samples (this effectively determines what **p(x)** you are using).\n\nThe authors evaluate their algorithm on several domains and show that it achieves or surpasses state of the art on them.\n\n**Rohin's opinion:** I like this paper: the intuition makes sense, the math is straightforward, and the empirical results are strong, and have continued to be strong when looking at later work that builds on it.\n\n**[On Variational Bounds of Mutual Information](https://arxiv.org/abs/1905.06922)** *(Ben Poole et al)* (summarized by Rohin): This paper is a pretty dense and technical explanation of various ways in which we can estimate and/or optimize the mutual information between two variables. I specifically want to highlight that it provides a proof that the Contrastive Predictive Coding objective (summarized above) is a lower bound on the mutual information between the input and the representation, and compares it to other lower bounds on mutual information.\n\nTECHNICAL AI ALIGNMENT\n======================\n\nTECHNICAL AGENDAS AND PRIORITIZATION\n------------------------------------\n\n**[An Analytic Perspective on AI Alignment](https://www.alignmentforum.org/posts/8GdPargak863xaebm/an-analytic-perspective-on-ai-alignment)** *(Daniel Filan)* (summarized by Asya): In this post, Daniel Filan presents an analytic perspective on how to do useful AI alignment research. His take is that in a world with powerful AGI systems similar to neural networks, it may be sufficient to be able to detect whether a system would cause bad outcomes before you deploy it on real-world systems with unknown distributions. To this end, he advocates for work on transparency that gives **[mechanistic understandings](https://www.lesswrong.com/posts/3kwR2dufdJyJamHQq/mechanistic-transparency-for-machine-learning)** (**[AN #15](https://mailchi.mp/4920e52dd61b/alignment-newsletter-15)**) of the systems in question, combined with foundational research that allows us to reason about the safety of the produced understandings.\n\n**Rohin's opinion:** My broad take is that I agree that analyzing neural nets is useful and more work should go into it, but I broadly disagree that this leads to reduced x-risk by increasing the likelihood that developers can look at their trained model, determine whether it is dangerous by understanding it mechanistically, and decide whether to deploy it, in a \"zero-shot\" way. The key difficulty here is the mechanistic transparency, which seems like far too strong a property for us to aim for: I would expect the cost of making a neural network mechanistically transparent to far exceed the cost of training that neural network in the first place, and so it would be hard to get developers to mechanistically understand trained models to detect danger.\n\nRight now for e.g. image classifiers, some people on OpenAI's Clarity team have spent multiple years understanding a single image classifier, which is orders of magnitude more expensive than training the classifier. My guess is that this will become superlinearly harder as models get bigger (and especially as models become superhuman), and so it seems quite unlikely that we could have mechanistic transparency for very complex AGI systems built out of neural nets. More details in **[this comment](https://www.alignmentforum.org/posts/8GdPargak863xaebm/an-analytic-perspective-on-ai-alignment?commentId=c8vogpyjgoXEspX8z)**. Note that Daniel agrees that it is an open question whether this sort of mechanistic transparency is possible, and thinks that we don't have much evidence yet that it isn't.\n\nROBUSTNESS\n----------\n\n**[The Conditional Entropy Bottleneck](http://arxiv.org/abs/2002.05379)** *(Ian Fischer)* (summarized by Rohin): While I've categorized this paper under robustness because it can apply to most forms of training, I'll talk about it specifically in the context of unsupervised learning (and in particular its relation to Contrastive Predictive Coding (CPC), summarized in the highlights).\n\nOne potential problem with deep learning is that there might be too *much* information in the input, causing the model to learn spurious correlations that do not actually generalize well (see **[Causal Confusion in Imitation Learning](https://arxiv.org/abs/1905.11979)** (**[AN #79](https://mailchi.mp/8d9e3703fbde/an-79-recursive-reward-modeling-as-an-alignment-technique-integrated-with-deep-rl)**) as an example). The idea with the Conditional Entropy Bottleneck (CEB) is to penalize the model for learning irrelevant information, using a form of *information bottleneck*.\n\nWe consider a setting where we want to learn a representation **Z** of some input data **X** in order to predict some downstream data **Y**. In CPC, **X** would be the inputs from time 1 to t, **Z** would be the latent representation **z\\_t**, and **Y** would be the future data **x\\_{t+k}**. Then, we want **Z** to capture the **minimum necessary information** needed for **Z** to predict **Y** as best as possible. The *necessary* information is **I(Y; Z)**, that is, the mutual information between Z and Y: we want to maximize this to maximize our accuracy at predicting **Y**. Since **Y** depends on **X**, and **Z** is computed from **X**, any information about **Y** must come through mutual information between **X** and **Z**. Maximizing just this **I(Y; Z)** term gives us Contrastive Predictive Coding.\n\nHowever, we don't want to capture any extra irrelevant information (the minimality criterion), which means that **Z** shouldn't capture any *more* information about **X** beyond what it captured to maximize **I(Y; Z)**. In information-theoretic terms, we want to *minimize* **I(X; Z | Y)**. Thus, we have the CEB objective: minimizing **I(X; Z | Y) - γ I(Y; Z)**, where **γ** is a hyperparameter controlling the tradeoff between the two terms. The authors then use some fairly straightforward math to reduce the objective to simpler terms which can be bounded using variational approximations, leading to an algorithm that can work in practice. \n\nThe authors perform experiments on Fashion MNIST and CIFAR10 (where Y corresponds to the labels for the images, so we're in the supervised learning setting). Since the main benefit of CEB is to remove unnecessary information from the model, they evaluate adversarial robustness and out-of-distribution detection in addition to standard performance checks. They find that models trained with CEB perform better than ones trained with a variational information bottleneck, or ones trained with vanilla SGD.\n\n**Rohin's opinion:** While I'm not sure to what extent models learn truly irrelevant information (see **[Adversarial Examples Are Not Bugs, They Are Features](http://gradientscience.org/adv/)** (**[AN #62](https://mailchi.mp/4a1b3c4249ae/an-62are-adversarial-examples-caused-by-real-but-imperceptible-features)**)), it seems good to add an incentive against learning information that won't be useful for a downstream task, and the empirical results (especially of the next paper) suggest that it is providing some benefit.\n\n**[CEB Improves Model Robustness](http://arxiv.org/abs/2002.05380)** *(Ian Fischer et al)* (summarized by Rohin): This empirical paper finds that ImageNet classifiers trained with the CEB objective (summarized above) are already somewhat adversarially robust, without having any decrease in accuracy, and without any adversarial training. Notably, since CEB does not rely on knowing the attack method ahead of time, its adversarial robustness generalizes to multiple kinds of attacks, whereas models that were adversarially trained tend to be fragile in the face of previously unseen attacks.\n\nOTHER PROGRESS IN AI\n====================\n\nREINFORCEMENT LEARNING\n----------------------\n\n**[Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation](https://arxiv.org/abs/1806.10729)** *(Niels Justesen et al)* (summarized by Zach): Deep reinforcement learning has been able to use high-dimensional input, such as images, to learn optimal policies. However, when neural networks are trained in a fixed environment, such as on a single level in a video game, they will usually over-fit and fail to generalize to new levels. This paper uses procedurally generated levels during training in an attempt to increase the generality of deep RL. They make use of the General Video Game AI framework (GVG-AI) which allows rapid design of video games through the specification of rewards, objects, etc. Moreover, they introduce Progressive PCG (PPCG) to smoothly control the difficulty of generated levels to build a curriculum for the agent. The authors show that for some games procedural level generation enables generalization to new levels within the same distribution.\n\n**Zach's opinion:** The GVG-AI framework seems like a useful tool to explore learning videogames. Setting up curriculum learning by using PPCG is also a clever idea. However, the results are a bit mixed. On two of the games they tested, training on a single difficult level works better than training on a variety of levels for generalization. Having said this, the method can learn the game Frogs (57% win rate) while DQN/A2C make zero progress even after 40 million steps. It seems as though certain conditions make PPCG a good method to use. It'd be interesting to investigate what those conditions are in a future publication.\n\nDEEP LEARNING\n-------------\n\n**[SLIDE : In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems](http://arxiv.org/abs/1903.03129)** *(Beidi Chen et al)* (summarized by Asya): This paper presents an algorithmic technique called SLIDE (Sub-LInear Deep learning Engine) which takes advantage of sparsity in inputs and activations to speed up the training of large neural networks.\n\nSuppose that activations at layer k are a\\_k. Then, the ith element of a\\_{k+1} is given by the dot product of a\\_k and w\\_i for some weight vector w\\_i. Call w\\_i the ith neuron of layer k + 1. The largest activations in a\\_{k+1} are the ones for whom w\\_i has high magnitude and points in the same direction as a\\_k. The core proposal of SLIDE is to only compute the largest elements of a\\_{k+1}, which they call the “activated neurons”, and approximate all of the others are zero, allowing us to avoid a lot of computation.\n\nIn order to do this, we maintain a data structure called a *locality-sensitive hash table*, which when given an activation a\\_k can tell us which neurons (w\\_is) are most similar. We can then compute the outputs for just those neurons to get a\\_{k+1}. In this way, we can effectively ‘sparsify’ the network, calculating the activations and updating the weights of only a small subset of the neurons. This is what gives us our computational gains.\n\nSLIDE randomly initializes weights in the network and generates the locality-sensitive hash table that maps activations to activated neurons. To take a gradient step on an input, it calculates the activated neurons in a forward pass, then backpropagates through the activated neurons, and then updates the locality-sensitive hash table. The hash table update is computationally expensive, and SLIDE uses several mechanisms to make it less costly, such as updating hash tables less frequently later in the training process since gradients are likely to change less then. Due to the sparsity, the gradients for different inputs are often changing different neurons, and so SLIDE asynchronously parallelizes gradient updates without worrying about race conditions, allowing for much better scaling with additional cores.\n\nThe paper evaluates SLIDE on large multi-label classification tasks, which must run on neural networks with extremely wide final layers. It finds that the CPUs running SLIDE are 1.8 times faster in clock-time than the GPU on the Delicious 200k dataset, and 2.7 times faster than the GPU on the Amazon-670K dataset, with an additional ~1.3x speed-up after performing cache optimization on SLIDE. Scalability tests suggest that the SLIDE CPUs beat GPU performance even when using only 8 cores. The paper claims that SLIDE’s computational benefits come because the number of neurons sampled in the wide final layer is extremely small-- fewer than 0.5% of active neurons.\n\n**Asya's opinion:** The tasks they test on are *extremely* sparse: since there are hundreds of thousands of possible labels, even if you take the top ~thousand predictions in the final layer (which corresponds to most of the computation), that’s only 1% of the total number of predictions, saving you 99% of the arithmetic you would have had to do. The input features are also very sparse: in both datasets, less than 0.06% (yes, percent) of features are non-zero. It’s cool that under such conditions you can design an algorithm that is ~an order of magnitude better on cost, but it’s not going to be “the death of NVIDIA” or anything like that — without further optimizations, SLIDE will be worse than regular Tensorflow on GPU for something like ImageNet.\n\nI'm also not sure I agree with the 'thesis' of the paper that smart algorithms beat hardware acceleration-- it seems to me like there are large gains from investing in the combination of the two. Even if GPUs aren't optimized to run SLIDE, I can imagine specialized hardware optimized for SLIDE creating even bigger performance gains.\n\n**[Linear Mode Connectivity and the Lottery Ticket Hypothesis](https://arxiv.org/abs/1912.05671)** *(Jonathan Frankle et al)* (summarized by Flo): Instability analysis looks at how sensitive neural network training is to noise in SGD. A network is called stable if the test error remains approximately constant along the line connecting network weights obtained by training on differently ordered data. \n\nThe authors find that most popular networks in image classification are unstable at initialization for more challenging tasks but become stable long before convergence. They also find that **[winning tickets](https://ai.facebook.com/blog/understanding-the-generalization-of-lottery-tickets-in-neural-networks)** (**[AN #77](https://mailchi.mp/d2f2d15b7114/an-77-double-descent-a-unification-of-statistical-theory-and-modern-ml-practice)**) found by iterative magnitude pruning are usually stable, while unstable subnetworks don't manage to match the original network's performance after training. As the original network, pruned subnetworks become more stable when they are initialized with weights from later stages of the training process. This is consistent with previous results showing that resetting subnetwork weights to states in early training leads to increased performance after retraining, compared to resetting to the initial state. While stability seems to correspond to better accuracy for subnetworks, very sparse subnetworks perform worse than the unpruned network, even if they are stable.\n\n**Flo's opinion:** The correspondence between subnetwork stability and performance after retraining might just be an artefact of both (somewhat obviously) improving with more training. What is interesting is that small amounts of training seem to have disproportionate effects for both factors, although one should keep in mind that the same is true for the loss, at least in absolute terms.\n\nNEWS\n====\n\n**[Careers at the Joint AI Center](https://www.ai.mil/careers.html)** (summarized by Rohin) (H/T Jon Rodriguez): The Joint AI Center is searching for ML experts for a variety of roles.\n\n**Rohin's opinion:** You might be wondering why I've included these jobs in the newsletter, given that I don't do very many promotions. I think that it is reasonably likely that the US government (and the military in particular) will be a key player in the future of AI, and that there could be a lot to learn from their testing, evaluation, validation & verification (TEV&V) framework (which often seems more risk-averse to me than many alignment schemes are). As a result, I would be excited if readers of this newsletter interested in how the military thinks about AI filled these positions: it seems great to have a flow of ideas between the two communities (so that the government learns about alignment concerns, and so that we learn about TEV&V).\n\n#### **FEEDBACK**\n\nI'm always happy to hear feedback; you can send it to me, **[Rohin Shah](https://rohinshah.com/)**, by **replying to this email**. \n\n#### **PODCAST**\n\nAn audio podcast version of the **Alignment Newsletter** is available. This podcast is an audio version of the newsletter, recorded by **[Robert Miles](http://robertskmiles.com/)**.", "url": "https://www.alignmentforum.org/posts/XE6LD2c9NtB7gMdEm/an-92-learning-good-representations-with-contrastive", "date_published": "2020-03-25T17:20:02Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.730308+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "86e4991caf1e071a19d116701d99e706", "source": "alignmentforum", "title": "How important are MDPs for AGI (Safety)?", "text": "I don't think finite-state MDPs are a particularly powerful conceptual tool for designing strong RL algorithms. I'll consider the case of no function approximation first.\n\nIt is certainly easier to do RL in a finite-state MDP. The benefit of modeling an environment as a finite-state MDP, and then using an MDP-inspired RL algorithm, is that when the agent searches for plans to follow, it doesn't evaluate the same plans twice.\n\nInstead, it caches the (approximate) \"value\" for each possible \"state\", and then if a plan would take it to a state that it has already evaluated, it doesn't have to re-evaluate what the plan would be from that point on. It already knows, more or less, how much utility it could get thereafter. Compare that to naïve approach of using a world-model to do full expectimax search at each timestep.\n\nThe model-the-environment-as-finite-state-MDP-then-do-dynamic-programming approach, or just \"the MDP approach\" for short, is, I think, *all about* not searching the same region of the planning search space twice. This is clearly a good thing, but I don't think the MDP approach in RL contains much more conceptual progress toward AGI than that. If I were to try to do a pre-natum of a fairly advanced RL agent, that is, if I tried to anticipate a response to \"things went well; why did that happen?\", my guess would be that a big part of the answer would be:\n\nIt avoids searching much of the planning search space even once, certainly not twice.\n\nThe MDP approach with function approximation is more powerful, depending on how good the function approximation is. There's no upper bound on how good the MDP approach with function approximation could be, because buried inside the function approximation (whether that's approximation of the value, or the optimal policy, or both) could be some clever RL algorithm that does most of the work on its own. A good function approximator that is able to generate accurate predictions of the value and/or the optimal policy might appear to us to \"generalize\" well across \"similar states\". But it's not clear to me to what extent it is a useful abstraction to say that the function approximator thinks in terms of the agent bouncing around a set of states that it classifies as more or less similar to each other.\n\nI don't mean to say that the MDP approach is useless. I'm certainly not against using a TD-style update instead of a full Monte Carlo rollout for training a function approximator; it's better than not using one and effectively searching parts of planning search space many times over. I just don't think it's a hugely big deal conceptually.\n\nI think this is one small, very disputable argument against defaulting to a finite-state MDP formalism in AGI safety work. A natural alternative is to consider the agent's entire interaction history as the state, and suppose that the agent is still somehow using clever, efficient heuristics for approximating expectimax planning, with or without built-in methods for caching plans that have already been evaluated. None of this says that there's any cost to using a finite-state MDP formalism for AGI safety work, only that the benefits don't seem so great as to make it a \"natural choice\".", "url": "https://www.alignmentforum.org/posts/6gL83HMF6tvPHKQxW/how-important-are-mdps-for-agi-safety", "date_published": "2020-03-26T20:32:59Z", "authors": ["michaelcohen"], "tags": ["AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.730525+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "9fbfa00d56e32f5f8177ca539b457e2b", "source": "alignmentforum", "title": "What are the most plausible \"AI Safety warning shot\" scenarios?", "text": "A \"AI safety warning shot\" is some event that causes a substantial fraction of the relevant human actors (governments, AI researchers, etc.) to become substantially more supportive of AI research and worried about existential risks posed by AI.\n\nFor example, suppose we build an unaligned AI system which is \"only\" about as smart as a very smart human politician, and it escapes and tries to take over the world, but only succeeds in taking over North Korea before it is stopped. This would presumably have the \"warning shot\" effect.\n\nI currently think that scenarios like this are not very plausible, because there is a very narrow range of AI capability between \"too stupid to do significant damage of the sort that would scare people\" and \"too smart to fail at takeover if it tried.\" Moreover, within that narrow range, systems would probably realize that they are in that range, and thus bide their time rather than attempt something risky.\n\nEDIT: To make more precise what I mean by \"substantial:\" I'm looking for events that cause >50% of the relevant people who are at the time skeptical or dismissive of existential risk from AI to change their minds.", "url": "https://www.alignmentforum.org/posts/hLKKH9CM6NDiJBabC/what-are-the-most-plausible-ai-safety-warning-shot-scenarios", "date_published": "2020-03-26T20:59:58Z", "authors": ["Daniel Kokotajlo"], "tags": ["AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.730584+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "37c4653c6f89b70db20d6a5e4112aa31", "source": "alignmentforum", "title": "My current framework for thinking about AGI timelines", "text": "At the beginning of 2017, someone I deeply trusted said they thought AGI would come in 10 years, with 50% probability.\n\nI didn't take their opinion at face value, especially since so many experts seemed confident that AGI was decades away. But the possibility of imminent apocalypse seemed plausible enough and important enough that I decided to prioritize investigating AGI timelines over trying to strike gold. I left the VC-backed startup I'd cofounded, and went around talking to every smart and sensible person I could find who seemed to have opinions about when humanity would develop AGI.\n\nMy biggest takeaways after 3 years might be disappointing -- I don't think the considerations currently available to us point to any decisive conclusion one way or another, and I don't think anybody really knows when AGI is coming. At the very least, the fields of knowledge that I think bear on AGI forecasting (including deep learning, predictive coding, and comparative neuroanatomy) are disparate, and I don't know of any careful and measured thinkers with all the relevant expertise.\n\nThat being said, I did manage to identify a handful of background variables that consistently play significant roles in informing people's intuitive estimates of when we'll get to AGI. In other words, people would often tell me that their estimates of AGI timelines would significantly change if their views on one of these background variables changed.\n\nI've put together a framework for understanding AGI timelines based on these background variables. Among all the frameworks for AGI timelines I've encountered, it's the framework that most comprehensively enumerates crucial considerations for AGI timelines, and it's the framework that best explains how smart and sensible people might arrive at vastly different views on AGI timelines.\n\nOver the course of the next few weeks, I'll publish a series of posts about these background variables and some considerations that shed light on what their values are. I'll conclude by describing my framework for how they come together to explain various overall viewpoints on AGI timelines, depending on different prior assumptions on the values of these variables.\n\nBy trade, I'm a math competition junkie, an entrepreneur, and a hippie. I am not an expert on any of the topics I'll be writing about -- my analyses will not be comprehensive, and they might contain mistakes. I'm sharing them with you anyway in the hopes that you might contribute your own expertise, correct for my epistemic shortcomings, and perhaps find them interesting.\n\nI'd like to thank Paul Christiano, Jessica Taylor, Carl Shulman, Anna Salamon, Katja Grace, Tegan McCaslin, Eric Drexler, Vlad Firiou, Janos Kramar, Victoria Krakovna, Jan Leike, Richard Ngo, Rohin Shah, Jacob Steinhardt, David Dalrymple, Catherine Olsson, Jelena Luketina, Alex Ray, Jack Gallagher, Ben Hoffman, Tsvi BT, Sam Eisenstat, Matthew Graves, Ryan Carey, Gary Basin, Eliana Lorch, Anand Srinivasan, Michael Webb, Ashwin Sah, Yi Sun, Mark Sellke, Alex Gunning, Paul Kreiner, David Girardo, Danit Gal, Oliver Habryka, Sarah Constantin, Alex Flint, Stag Lynn, Andis Draguns, Tristan Hume, Holden Lee, David Dohan, and Daniel Kang for enlightening conversations about AGI timelines, and I'd like to apologize to anyone whose name I ought to have included, but forgot to include.\n\nTable of contents\n=================\n\nAs I post over the coming weeks, I'll update this table of contents with links to the posts, and I might update some of the titles and descriptions.\n\n**[How special are human brains among animal brains?](https://www.alignmentforum.org/posts/d2jgBurQygbXzhPxc/how-special-are-human-brains-among-animal-brains)**\n\nHumans can perform intellectual feats that appear qualitatively different from those of other animals, but are our brains really doing anything so different?\n\n**[How uniform is the neocortex?](https://www.alignmentforum.org/posts/WFopenhCXyHX3ukw3/how-uniform-is-the-neocortex#fn-5zsuJgkrWJs5Hot98-3)**\n\nTo what extent is the part of our brain responsible for higher-order functions like sensory perception, cognition, and language[[1]](about:blank#fn-J2SaFb2tN7jEDZoyB-1), uniformly composed of general-purpose data-processing modules?\n\n**How much are our innate cognitive capacities just shortcuts for learning?**\n\nTo what extent are our innate cognitive capacities (for example, a [pre-wired ability to learn language](https://en.wikipedia.org/wiki/Innateness_hypothesis)) crutches provided by evolution to help us learn more quickly what we otherwise would have been able to learn anyway?\n\n**Are mammalian brains all doing the same thing at different levels of scale?**\n\nAre the brains of smarter mammals, like humans, doing essentially the same things as the brains of less intelligent mammals, like mice, except at a larger scale?\n\n**How simple is the simplest brain that can be scaled?**\n\nIf mammalian brains can be scaled, what's the simplest brain that could? A turtle's? A spider's?\n\n**How close are we to simple biological brains?**\n\nGiven how little we understand about how brains work, do we have any reason to think we can recapitulate the algorithmic function of even simple biological brains?\n\n**What's the smallest set of principles that can explain human cognition?**\n\nIs there a small set of principles that underlies the breadth of cognitive processes we've observed (e.g. language, perception, memory, attention, and reasoning)[[2]](about:blank#fn-J2SaFb2tN7jEDZoyB-2), similarly to how Newton’s laws of motion underlie a breadth of seemingly-disparate physical phenomena? Or is our cognition more like a big mess of irreducible complexity?\n\n**How well can humans compete against evolution in designing general intelligences?**\n\nHumans can design some things much better than evolution (like rockets), and evolution can design some things much better than humans (like immune systems). Where does general intelligence lie on this spectrum?\n\n**Tying it all together, part I**\n\nMy framework for what these variables tell us about AGI timelines\n\n**Tying it all together, part II**\n\nMy personal views on AGI timelines\n\n\n\n---\n\n1. <https://en.wikipedia.org/wiki/Neocortex> [↩︎](about:blank#fnref-J2SaFb2tN7jEDZoyB-1)\n2. <https://en.wikipedia.org/wiki/Cognitive_science> [↩︎](about:blank#fnref-J2SaFb2tN7jEDZoyB-2)", "url": "https://www.alignmentforum.org/posts/w4jjwDPa853m9P4ag/my-current-framework-for-thinking-about-agi-timelines", "date_published": "2020-03-30T01:23:57Z", "authors": ["zhukeepa"], "tags": ["AI Takeoff", "AI", "AI Timelines", "Crucial Considerations"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.730698+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "9843ed3236fb5278a20910ae63e586c5", "source": "alignmentforum", "title": "Three Kinds of Competitiveness", "text": "*Crossposted from [AI Impacts](https://aiimpacts.org/three-kinds-of-competitiveness/)*\n\n*[Epistemic status: I wrote this for [Blog Post Day II.](https://www.lesswrong.com/posts/FredSMMXc77kMAquA/blog-post-day-ii) Sorry it's late.]*\n\nIn this post, I distinguish between three different kinds of competitiveness -- Performance, Cost, and Date -- and explain why I think these distinctions are worth the brainspace they occupy. For example, they help me introduce and discuss a problem for AI safety proposals having to do with aligned AIs being outcompeted by unaligned AIs. \n\nDistinguishing three kinds of competitiveness and competition\n-------------------------------------------------------------\n\nA system is *performance-competitive* insofar as its ability to perform relevant tasks compares with competing systems. If it is better than any competing system at the relevant tasks, it is very performance-competitive. If it is almost as good as the best competing system, it is less performance-competitive.   \n\n\n(For AI in particular, “speed” “quality” and “collective” intelligence as [Bostrom defines them](https://www.lesswrong.com/posts/semvkn56ZFcXBNc2d/superintelligence-5-forms-of-superintelligence) all contribute to performance-competitiveness.)  \n\n\nA system is *cost-competitive* to the extent that it costs less to build and/or operate than its competitors. If it is more expensive, it is less cost-competitive, and if it is much more expensive, it is not at all cost-competitive.   \n\n\nA system is *date-competitive* to the extent that it can be created sooner (or not much later than) its competitors. If it can only be created after a prohibitive delay, it is not at all date-competitive.   \n\n\nA *performance competition* is a competition that performance-competitiveness helps you win. The more important performance-competitiveness is to winning, the more intense the performance competition is.  \n\n\nLikewise for cost and date competitions. Most competitions are all three types, to varying degrees. Some competitions are none of the types; e.g. a “competition” where the winner is chosen randomly.   \n\n\nI briefly searched the AI alignment forum for uses of the word “competitive.” It seems that when people talk about competitiveness of AI systems, they [usually](https://www.alignmentforum.org/posts/H5gXpFtg93qDMZ6Xn/aligning-a-toy-model-of-optimization#oGdcKrWwPfwGzXNjT) mean performance-competitiveness, but [sometimes](https://www.alignmentforum.org/posts/5Kv2qNfRyXXihNrx2/ai-safety-debate-and-its-applications) mean cost-competitiveness, and [sometimes](https://www.alignmentforum.org/posts/ZHXutm7KpoWEj9G2s/an-unaligned-benchmark) both at once. Meanwhile, I suspect that [this important post](https://ai-alignment.com/prosaic-ai-control-b959644d79c2) can be summarized as “We should do prosaic AI alignment in case only prosaic AI is date-competitive.”\n\nPutting these distinctions to work\n----------------------------------\n\nFirst, I’ll sketch some different future scenarios. Then I’ll sketch how different AI safety schemes might be more or less viable depending on which scenario occurs. For me at least, having these distinctions handy makes this stuff easier to think and talk about.  \n\n\n*Disclaimer: The three scenarios I sketch aren’t supposed to represent the scenarios I think most likely; similarly, my comments on the three safety proposals are mere hot takes. I’m just trying to illustrate how these distinctions can be used.*  \n\n\n**Scenario: FOOM:** There is a level of performance which leads to a localized FOOM, i.e. very rapid gains in performance combined with very rapid drops in cost, all within a single AI system (or family of systems in a single AI lab). Moreover, these gains & drops are enough to give decisive strategic advantage to the faction that benefits from them. Thus, in this scenario, *control over the future is mostly a date competition.* If there are two competing AI projects, and one project is building a system which is twice as capable and half the price but takes 100 days longer to build, *that project will lose*.  \n\n\n**Scenario: Gradual Economic Takeover:** The world economy gradually accelerates over several decades, and becomes increasingly dominated by billions of AGI agents. However, no one entity (AI or human, individual or group) has most of the power. In this scenario, *control over the future is mostly a cost and performance competition.* The values which shape the future will be the values of the bulk of the economy, and that in turn will be the values of the most popular and successful AGI designs, which in turn will be the designs that have the best combination of performance- and cost-competitiveness. Date-competitiveness is mostly irrelevant.  \n\n\n**Scenario:** **Final Conflict:** It’s just like the Gradual Economic Takeover scenario, except that several powerful factions are maneuvering and scheming against each other, in a Final Conflict to decide the fate of the world. This Final Conflict takes almost a decade, and mostly involves “cold” warfare, propaganda, coalition-building, alliance-breaking, and that sort of thing. Importantly, the victor in this conflict will be determined not so much by economic might as by clever strategy; a less well resourced faction that is nevertheless more far-sighted and strategic will gradually undermine and overtake a larger/richer but more dysfunctional faction. In this context, having the most *capable* AI advisors is of the utmost importance; having your AIs be cheap is much less important. In this scenario, *control of the future is mostly a performance competition.* (Meanwhile, in this same scenario, popularity in the wider economy is a moderately intense competition of all three kinds.)  \n\n\n**Proposal: [Value Learning](https://www.alignmentforum.org/posts/5eX8ko7GCxwR5N9mN/what-is-ambitious-value-learning):** By this I mean schemes that take state-of-the-art AIs and train them to have human values. I currently think of these schemes as not very date-competitive, but pretty cost-competitive and very performance-competitive. I say value learning isn’t date-competitive because my impression is that it is probably harder to get right, and thus slower to get working, than other alignment proposals. Value learning would be better for the gradual economic takeover scenario because the world will change slowly, so we can afford to spend the time necessary to get it right, and once we do it’ll be a nice add-on to the existing state-of-the-art systems that won’t sacrifice much cost or performance.  \n\n\n**Proposal: [Iterated Distillation and Amplification:](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616)** By this I mean… well, it’s hard to summarize. It involves training AIs to imitate humans, and then scaling them up until they are arbitrarily powerful while still human-aligned. I currently think of this scheme as decently date-competitive but not as cost-competitive or performance-competitive. But lack of performance-competitiveness isn’t a problem in the FOOM scenario because IDA is above the threshold needed to go FOOM; similarly, lack of cost-competitiveness is only a minor problem because if they don’t have enough money already, the first project to build FOOM-capable AI will probably be able to attract a ton of investment (e.g. via being nationalized) without even using their AI for anything, and then reinvest that investment into paying the extra cost of aligning it via IDA.  \n\n\n**Proposal: [Impact regularization:](https://medium.com/@deepmindsafetyresearch/designing-agent-incentives-to-avoid-side-effects-e1ac80ea6107)** By this I mean attempts to modify state-of-the-art AI designs so that they deliberately avoid having a big impact on the world. I think of this scheme as being cost-competitive and fairly date-competitive. I think of it as being performance-uncompetitive in some competitions, but performance-competitive in others. In particular, I suspect it would be very performance-uncompetitive in the Final Conflict scenario (because AI advisors of world leaders need to be impactful to do anything), yet nevertheless performance-competitive in the Gradual Economic Takeover scenario.  \n\n\nPutting these distinctions to work again\n----------------------------------------\n\nI came up with these distinctions because they helped me puzzle through the following problem:  \n\n\n\n> Lots of people worry that in a vastly multipolar, hypercompetitive AI economy (such as described in Hanson’s *Age of Em* or Bostrom’s “Disneyland without children” scenario) eventually pretty much everything of merely intrinsic value will be stripped away from the economy; the world will be dominated by hyper-efficient self-replicators various kinds, performing their roles in the economy very well and seeking out new roles to populate but not spending any time on art, philosophy, leisure, etc. Some value might remain, but the overall situation will be Malthusian.   \n> Well, why not apply this reasoning more broadly? Shouldn’t we be pessimistic about *any* AI alignment proposal that involves using aligned AI to compete with unaligned AIs? After all, at least one of the unaligned AIs will be willing to cut various ethical corners that the aligned AIs won’t, and this will give it an advantage.\n\nThis problem is more serious the more the competition is cost-intensive and performance-intensive. Sacrificing things humans value is likely to lead to cost- and performance-competitiveness gains, so the more intense the competition is in those ways, the worse our outlook is.  \n\n\nHowever, it’s plausible that the gains from such sacrifices are small. If so, we need only worry in scenarios of extremely intense cost and performance competition.  \n\n\nMoreover, the extent to which the competition is date-intensive seems relevant. Optimizing away things humans value, and gradually outcompeting systems which didn’t do that, takes time. And plausibly, scenarios which are not at all date competitions are also very intense performance and cost competitions. (Given enough time, lots of different designs will appear, and minor differences in performance and cost will have time to overcome differences in luck.) On the other hand, aligning AI systems might take time too, so if the competition is *too* date-intensive things look grim also. Perhaps we should hope for a scenario in between, where control of the future is a moderate date competition.  \n\n\nConcluding thoughts\n-------------------\n\nThese distinctions seem to have been useful for me. However, I could be overestimating their usefulness. Time will tell; we shall see if others make use of them.  \n\n\nIf you think they would be better if the definitions were rebranded or modified, now would be a good time to say so! I currently expect that a year from now my opinions on which phrasings and definitions are most useful will have evolved. If so, I'll come back and update this post.  \n\n\n  \n*Thanks to Katja Grace and Ben Pace for comments on a draft.*", "url": "https://www.alignmentforum.org/posts/sD6KuprcS3PFym2eM/three-kinds-of-competitiveness", "date_published": "2020-03-31T01:00:56Z", "authors": ["Daniel Kokotajlo"], "tags": ["AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.730900+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "98bb204c7519335b04fc9289bfba191e", "source": "alignmentforum", "title": "How special are human brains among animal brains?", "text": "Humans are capable of feats of cognition that appear qualitatively more sophisticated than those of any other animals. Is this appearance of a qualitative difference indicative of human brains being essentially more complex than the brains of any other animal? Or is this “qualitative difference” illusory, with the vast majority of human cognitive feats explainable as nothing more than a scaled-up version of the cognitive feats of lower animals? \n\n*“How special are human brains among animal brains?” is one of the background variables in my [framework for AGI timelines](https://www.alignmentforum.org/posts/w4jjwDPa853m9P4ag/my-current-framework-for-thinking-about-agi-timelines). My aim for this post is **not** to present a complete argument for some view on this variable, so much as it is to:* \n\n* *present some considerations I’ve encountered that shed light on this variable*\n* *invite a collaborative effort among readers to shed further light on this variable (e.g. by leaving comments about considerations I haven’t included, or pointing out mistakes in my analyses)*\n\nDoes mastery of language make humans unique?\n============================================\n\nHuman conscious experience may have emerged from language\n---------------------------------------------------------\n\nHumans seem to have much higher degrees of consciousness and agency than other animals, and this may have emerged from our capacities for language. [Helen Keller](https://en.wikipedia.org/wiki/Helen_Keller#Early_childhood_and_illness) (who was deaf and blind since infancy, and only started learning language when she was 6) gave an [autobiographical account](http://scentofdawn.blogspot.com/2011/07/before-soul-dawn-helen-keller-on-her.html) of how she was driven by blind impetuses until she learned the meanings of the words “I” and “me”: \n\n\n> Before my teacher came to me, I did not know that I am. I lived in a world that was a no-world. I cannot hope to describe adequately that unconscious, yet conscious time of nothingness. I did not know that I knew aught, or that I lived or acted or desired. I had neither will nor intellect. I was carried along to objects and acts by a certain blind natural impetus. I had a mind which caused me to feel anger, satisfaction, desire. These two facts led those about me to suppose that I willed and thought. I can remember all this, not because I knew that it was so, but because I have tactual memory. It enables me to remember that I never contracted my forehead in the act of thinking. I never viewed anything beforehand or chose it. I also recall tactually the fact that never in a start of the body or a heart-beat did I feel that I loved or cared for anything. My inner life, then, was a blank without past, present, or future, without hope or anticipation, without wonder or joy or faith.\n\n\n> [...] \n\n\n> … When I learned the meaning of \"I\" and \"me\" and found that I was something, I began to think. Then consciousness first existed for me. Thus it was not the sense of touch that brought me knowledge. It was the awakening of my soul that first rendered my senses their value, their cognizance of objects, names, qualities, and properties. Thought made me conscious of love, joy, and all the emotions. I was eager to know, then to understand, afterward to reflect on what I knew and understood, and the blind impetus, which had before driven me hither and thither at the dictates of my sensations, vanished forever.\n\n### Mastery of language may have conferred unique intellectual superpowers\n\nI think humans underwent a phase transition in their intellectual abilities when they came to master language, at which point their intellectual abilities jumped far beyond those of other animals on both an individual level and a species level. \n\nOn an individual level, our capacity for language enables us to entertain and express arbitrarily complex thoughts, which appears to be an ability unique to humans. In theoretical linguistics, this is referred to as “[digital infinity”, or “the infinite use of finite means”](https://en.wikipedia.org/wiki/Digital_infinity). \n\nOn a species level, our mastery of language enables intricate insights to accumulate over generations with high fidelity. Our ability to stand on the shoulders of giants is unique among animals, which is why our culture is unrivaled in its richness in sophistication. \n\nLanguage aside, how unique are humans?\n======================================\n\n### Humans ≈ Neanderthals + language?\n\nThe most quintessentially human intellectual accomplishments (e.g. proving theorems, composing symphonies, going into space) were only made possible by culture post-agricultural revolution. So, when evaluating humans’ innate intellectual capacities, a better reference point than modern humans like ourselves would be our hunter-gatherer ancestors. \n\nWe can reduce the question of how complex our hunter-gatherer ancestors’ brains are into two sub-questions: how complex is our capacity for mastering language, and how complex are brains that are similar to ours, but don’t have the capacity for mastering language? \n\nNeanderthal brains seem like plausible proxies for the latter. Neanderthals are similar enough to modern humans that [they’ve interbred](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4947341/), and the [currently available evidence](https://en.wikipedia.org/wiki/Neanderthal_behavior) suggests that they may not have mastered language in the same way that [behaviorally modern humans](https://en.wikipedia.org/wiki/Behavioral_modernity) have. (I don’t think this evidence is very strong, but this doesn’t matter for my purposes—I’m just using Neanderthals as a handy stand-in to gesture at what a human-like intelligence might look like if it didn’t have the capacity for language.)\n\n### Higher intelligence in animals\n\nChimpanzees, crows, and dolphins are capable of impressive feats of higher intelligence, and I don’t think there’s any particular reason to think that Neanderthals are capable of doing anything qualitatively more impressive. I’ll share some examples of these animals’ intellectual feats that I found particularly illustrative. \n\nChimpanzees have been observed to lie to each other under experimental conditions. [From Wikipedia](https://en.wikipedia.org/wiki/Deception_in_animals#Tactical_deception):\n\n\n> ...food was hidden and only one individual, named Belle, in a group of chimpanzees was informed of the location. Belle was eager to lead the group to the food but when one chimpanzee, named Rock, began to refuse to share the food, Belle changed her behaviour. She began to sit on the food until Rock was far away, then she would uncover it quickly and eat it. Rock figured this out though and began to push her out of the way and take the food from under her. Belle then sat farther and farther away waiting for Rock to look away before she moved towards the food. In an attempt to speed the process up, Rock looked away until Belle began to run for the food. On several occasions he would even walk away, acting disinterested, and then suddenly spin around and run towards Belle just as she uncovered the food.\n\nIn [Aesop’s fable of the crow and the pitcher](https://en.wikipedia.org/wiki/The_Crow_and_the_Pitcher), a thirsty crow figures out that it can drop pebbles into a pitcher, so that the water rises to a high enough level for it to drink from. This behavior has been [experimentally replicated](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0092895), indicating that crows have a “sophisticated, but incomplete, understanding of the causal properties of displacement, rivalling that of 5–7 year old children”. \n\nWhen [Kelly the dolphin](https://www.littlethings.com/brilliant-kelly-the-dolphin-fools-trainers/3) was given rewards of fish for picking up scraps of paper, “Kelly figured out that she received the same fish regardless of the size of the piece of trash she was delivering to her trainer. So she began hiding big pieces of trash under a rock. Kelly would then rip off small pieces from the trash and deliver them one at a time so that she could receive more fish.” Additionally, “when a bird landed in the pool, Kelly snatched it and delivered it to her trainers. She received a large amount of fish in return. Knowing this, she decided to start hiding fish each time she was fed. She would then use the fish to lure birds when none of her trainers were around. Kelly knew that by saving one or two fish now, she could get many more fish later by turning in a bird.“ (Also reported on [The Guardian](https://www.theguardian.com/science/2003/jul/03/research.science?awc=11152_1585688382_de186cf736339cc47a455fe5a0cfd7da&utm_source=afl&utm_medium=awin&utm_content=RockYou+Media); I don’t know how reputable these sources are, so take this anecdote with a grain of salt.)\n\nSee [these](https://en.wikipedia.org/wiki/Tool_use_by_animals) [Wikipedia pages](https://en.wikipedia.org/wiki/Deception_in_animals#Tactical_deception) for some more interesting examples, and see [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4650126/) for a more thorough review of the evidence of higher intelligence in animals. \n\n### “Qualitatively” more advanced cognition may emerge from scale\n\nMany aspects of human cognition that may appear qualitatively different from what other animals are capable of, such as long chains of abstract reasoning, also appear qualitatively different from what less intelligent humans are capable of. As a particularly extreme example, John von Neumann’s [cognitive abilities](https://en.wikipedia.org/wiki/John_von_Neumann#Cognitive_abilities) were so advanced that a Nobel Laureate, Hans Bethe, once remarked that \"[his] brain indicated a new species, an evolution beyond man\". \n\nAt the same time, the genes that code for different humans’ brains are virtually identical from an evolutionary perspective. This suggests that the seemingly qualitative differences between humans’ and animals’ cognition might not be so different from the seemingly qualitative differences between John von Neumann’s cognition and mine—our brains might be doing essentially the same thing as theirs, except at a higher scale. \n\nHow hard is mastery of language?\n================================\n\n### Could language capacity fall out from general capacities?\n\nMaybe it was extraordinarily difficult to evolve the cognitive mechanisms that allow us to learn language, above and beyond our cognitive machinery for learning other things. I think this is plausible, but I don’t think the case for this is very strong. \n\nAnimals ([Washoe](https://en.wikipedia.org/wiki/Washoe_(chimpanzee)), [Koko](https://en.wikipedia.org/wiki/Koko_(gorilla)), and [Alex the parrot](https://en.wikipedia.org/wiki/Alex_(parrot))) have demonstrated the ability to learn simple forms of symbolic communication, which they never evolved to do, indicating that their ability to learn things in general is good enough to learn very simple forms of language. It’s true that there are [aspects of human language that escape animals](https://en.wikipedia.org/wiki/Animal_language#Aspects_of_human_language), but they also escape [feral children](https://en.wikipedia.org/wiki/Language_deprivation#Feral_children), and might escape animals for mundane reasons, like their not having [critical periods](https://en.wikipedia.org/wiki/Critical_period#Linguistics) long enough to learn these aspects of language. \n\nAdditionally, [AI language models](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) provide evidence that simple and general learning mechanisms can capture many of the intricacies of human language that other animals miss, further suggesting that there’s nothing intrinsically difficult about learning language. Here’s an excerpt from [GPT-2](https://openai.com/blog/better-language-models/#sample1), a relatively recent language model: \n\n\n> SYSTEM PROMPT (HUMAN-WRITTEN)\n\n\n> In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\n> MODEL COMPLETION (MACHINE-WRITTEN, 10 TRIES)\n\n\n> The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science.\n\n\n> Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved.\n\n\n> Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow. \n\n\n> Pérez and the others then ventured further into the valley. “By the time we reached the top of one peak, the water looked blue, with some crystals on top,” said Pérez.\n\n### Why haven’t other species mastered language?\n\nIf language isn’t a particularly difficult cognitive capacity to acquire, why don’t we see more animal species with language? \n\nOne possibility is that the first species that masters language, by virtue of being able to access intellectual superpowers inaccessible to other animals, has a high probability of becoming the dominant species extremely quickly. (Humans underwent the agricultural revolution within 50,000 years of behavioral modernity—a blink of an eye on evolutionary timescales—after which their dominance as a species became unquestionable.) Since we shouldn’t expect to see more than one dominant species at a time, this would imply a simple anthropic argument for our unique capacities for language: we shouldn’t expect to see more than one species at a time with mastery of language, and we just happen to be the species that made it there first. \n\nIt may also turn out that language is hard to evolve not because it’s a particularly sophisticated cognitive mechanism, but because the environments that could have supported language and selected for it might have been very unique. For example, it may be that a threshold of general intelligence has to be crossed before it’s viable for a species to acquire language, and that humans are the only species to have crossed this threshold. (Humans do have the highest [cortical information processing capacity](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4685590/) among mammals.)\n\nIt might also turn out that the cultural contexts under which language could evolve [require a mysteriously high degree of trust](https://en.wikipedia.org/wiki/Origin_of_language#Problems_of_reliability_and_deception): “... language presupposes relatively high levels of mutual trust in order to become established over time as an [evolutionarily stable strategy](https://en.wikipedia.org/wiki/Evolutionarily_stable_strategy). This stability is born of a longstanding mutual trust and is what grants language its authority. A theory of the origins of language must therefore explain why humans could begin trusting cheap signals in ways that other animals apparently cannot (see [signalling theory](https://en.wikipedia.org/wiki/Signalling_theory)).” \n\nMy current take\n---------------\n\nAs we came to master language, I think we underwent a phase transition in our intellectual abilities that set us apart from other animals. Besides language, I don't see much that sets us apart from other animals—in particular, most other cognitive differences seem explainable as consequences of either language or scale, and I don’t think the cognitive mechanisms that allow us to master language are particularly unique or difficult to acquire. Overall, I don’t see much reason to believe that human brains have significantly more innate complexity than the brains of other animals. \n\n  \n*Thanks to Paul Kreiner and Stag Lynn for helpful commentary and feedback.*", "url": "https://www.alignmentforum.org/posts/d2jgBurQygbXzhPxc/how-special-are-human-brains-among-animal-brains", "date_published": "2020-04-01T01:35:37Z", "authors": ["zhukeepa"], "tags": ["Neuroscience", "Consciousness", "General Intelligence", "Biology", "AI Timelines"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.731244+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "80a73329506f56145c0f5a171e2e7a66", "source": "alignmentforum", "title": "[AN #93]: The Precipice we’re standing at, and how we can back away from it", "text": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.\n\nAudio version **[here](http://alignment-newsletter.libsyn.com/alignment-newsletter-93)** (may not be up yet). \n\nHIGHLIGHTS\n==========\n\n**[The Precipice: Existential Risk and the Future of Humanity](https://www.amazon.com/Precipice-Existential-Risk-Future-Humanity/dp/0316484911)** *(Toby Ord)* (summarized by Rohin): This book argues that humanity is in a special stage of its development: it is on the *precipice*, a narrow time during which we have enough power to destroy ourselves, but not enough wisdom to have mitigated such risks. It first argues that existential risk would be very important to reduce (for all the standard reasons), and then considers many different kinds of existential risks, finding that natural ones (asteroids, supervolcanoes, stellar explosions) are small relative to anthropogenic risks, both current (nuclear war, climate change, environmental destruction) and future (engineered pandemics, unaligned AI, dystopian scenarios). I'll focus primarily on the part about AI risk, as well as some of the comments on existential risk in general.\n\nThe AI risk presentation in the book was similar to that in **[Superintelligence](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/1501227742)**: it argues for risk from goal-directed AI systems (though the terminology used in the book is different). It first demonstrates the strong progress in deep learning, and then notes that expert surveys estimate that AGI is more likely than not to arrive in the next century. It then notes that we don't know how to specify a reward function for an AI system (even with e.g. inverse reinforcement learning), and to the extent that we get it wrong, it pits us in competition against a superintelligent adversary. Ideas like switching off the AI system wouldn't work, due to convergent instrumental subgoals like survival.\n\nIt also considers some obvious objections, including the very reasonable objection that \"AI researchers won't build something that will kill them\". However, Toby is still worried, citing that due to the unilateralist curse unaligned AGI might still be built by the most optimistic researchers, and in any case the personal benefits to the researchers might justify the risk of misalignment to them personally (though it would not be justified for the world as a whole).\n\nThe book then spends some time discussing *risk factors*, which are things that do not *directly* lead to existential risks, but indirectly *exacerbate* other existential risks, making them more likely. For example, great power war seems like a risk factor: it isn't going to cause an existential catastrophe by itself, but it increases the likelihood that we use risky technologies like bioweapons and AI that could then cause an existential catastrophe.\n\nThe book also has lots of useful insights about existential risks in general, which then also apply to AI risk: for example, risks that strike sooner should be prioritized (since the later risks can be dealt with later), risks that are more sudden will be more important to focus on (since we won't be able to build support as the risk gradually comes in), and risks that are \"sharper\" will be more neglected since there won't be as many \"warning shots\".\n\n**Read more:** **[FLI Podcast: The Precipice: Existential Risk and the Future of Humanity with Toby Ord](https://futureoflife.org/2020/03/31/he-precipice-existential-risk-and-the-future-of-humanity-with-toby-ord/)**\n\n**Rohin's opinion:** I enjoyed this book more than I thought I would: it had a lot of novel content for me, and I liked the explanations and comparisons across different kinds of existential risks (something that I hadn't really seen a single unified perspective on), and I especially liked the constant focus on what we do and don't know -- it felt more like a research paper (albeit in a conversational style) than a popular book, and was similarly information-dense.\n\nOn the AI part specifically, I liked that one of the endnotes cashed out powerful AI systems using model-based RL: this indeed seems like the thing that is closest to the classic expected utility maximizer, so the conclusions make a bit more sense. You still have to wonder how exactly the model is learned, and how exactly the AI system becomes good at using the model to find good actions, but at least under those two assumptions you would have all the standard convergent instrumental subgoals. In contrast, with model-free RL, the default expectation is that the RL agent needs to try things multiple times before it can learn to do them again, so it's less clear how it starts doing novel things. It seems that model-based and model-free RL are pretty similar so the distinction doesn't matter in practice, but at least conceptually it's a lot easier to reason about the model-based system (at least in the context of AI risk).\n\nToby gives a 1 in 10 chance of existential catastrophe from AI in the next century (more than half of his total of 1 in 6), which decomposes into a 1 in 2 chance of AGI this century, and 1 in 5 of it leading to existential catastrophe. This is a bit more pessimistic than Paul's **[estimate](https://aiimpacts.org/conversation-with-paul-christiano/)** (**[AN #80](https://mailchi.mp/b3dc916ac7e2/an-80-why-ai-risk-might-be-solved-without-additional-intervention-from-longtermists)**) of 10% EV loss (which was over all time, not just this century), which is in turn a bit more pessimistic than the 1 in 10 chance that I **[estimated](https://aiimpacts.org/conversation-with-rohin-shah/)** (**[AN #80](https://mailchi.mp/b3dc916ac7e2/an-80-why-ai-risk-might-be-solved-without-additional-intervention-from-longtermists)**) (and am now forever anchored on), which was over all time *and* conditional on no additional effort from longtermists. But I wouldn't read too much into this -- 10 is a nice round number, and that probably played a big role in why I chose it. I certainly don't feel calibrated enough to easily tell the difference between 1 in 5 and 1 in 20 on a question of this complexity.\n\nI am very happy about this trend of people actually stating numbers: it's a lot easier to narrow down on the important disagreements when people put down numbers, even if they're completely made up. I'd really like to see numbers from people who have larger disagreements (as I expect would be the case with e.g. MIRI researchers).\n\nTECHNICAL AI ALIGNMENT\n======================\n\nLEARNING HUMAN INTENT\n---------------------\n\n**[Deconfusing Human Values Research Agenda v1](https://www.alignmentforum.org/posts/k8F8TBzuZtLheJt47/deconfusing-human-values-research-agenda-v1)** *(G Gordon Worley III)* (summarized by Rohin): This post argues that since 1. human values are necessary for alignment, 2. we are confused about human values, and 3. we couldn't verify it if an AI system discovered the structure of human values, we need to do research to become less confused about human values. This research agenda aims to deconfuse human values by modeling them as the input to a decision process which produces behavior and preferences. The author's best guess is that human values are captured by valence, as modeled by **[minimization of prediction error](https://www.alignmentforum.org/posts/Cu7yv4eM6dCeA67Af/minimization-of-prediction-error-as-a-foundation-for-human)**.\n\n**Rohin's opinion:** This is similar to the argument in **[Why we need a *theory* of human values](https://www.alignmentforum.org/posts/zvrZi95EHqJPxdgps/why-we-need-a-theory-of-human-values)** (**[AN #36](https://mailchi.mp/6751e45fbb48/alignment-newsletter-36)**), and my opinion remains roughly the same: I strongly agree that we are confused about human values, but I don't see an understanding of human values as necessary for value alignment. We could hope to build AI systems in a way where we don't need to specify the ultimate human values (or even a framework for learning them) before running the AI system. As an analogy, my friends and I are all confused about human values, but nonetheless I think they are more or less aligned with me (in the sense that if AI systems were like my friends but superintelligent, that sounds broadly fine).\n\nINTERPRETABILITY\n----------------\n\n**[What is Interpretability?](https://www.alignmentforum.org/posts/rSMbGFfsLMB3GWZtX/what-is-interpretability)** *(Robert Kirk et al)* (summarized by Rohin): This post categorizes several interpretability methods based on their *goal* and how they *enable humans* to achieve the goal.\n\n**Rohin's opinion:** It's striking to me how different this is from other work, e.g. **[Explicability? Legibility? Predictability? Transparency? Privacy? Security? The Emerging Landscape of Interpretable Agent Behavior](http://arxiv.org/abs/1811.09722)** (**[AN #36](https://mailchi.mp/6751e45fbb48/alignment-newsletter-36)**). It seems like interpretability is a really vague, nebulous term that has so far (to my limited knowledge) not been made precise.\n\nADVERSARIAL EXAMPLES\n--------------------\n\n**[Physically Realistic Attacks on Deep Reinforcement Learning](https://bair.berkeley.edu/blog/2020/03/27/attacks/)** *(Adam Gleave)* (summarized by Rohin): This is a blog post for a previously summarized paper, **[Adversarial Policies: Attacking Deep Reinforcement Learning](https://arxiv.org/abs/1905.10615)** (**[AN #70](https://mailchi.mp/732eaa192df0/an-70-agents-that-help-humans-who-are-still-learning-about-their-own-preferences)**).\n\nFORECASTING\n-----------\n\n**[2019 trends in GPU price per FLOPS](https://aiimpacts.org/2019-recent-trends-in-gpu-price-per-flops/)** *(Asya Bergal)* (summarized by Rohin): This post analyzes the the trends in cost per FLOP for GPUs. There are a bunch of details in how to do this analysis, but they end up finding that this cost goes down by an order of magnitude over 17 years for single-precision FLOPS (halving time: 5 years), 10 years for half-precision FLOPS (halving time: 3 years), and 5 years for half-precision fused multiply-add FLOPS (halving time: 1.5 years). However, the latter two categories have become more popular in recent years with the rise of deep learning, so their low halving times might be because some of the single-precision hardware was converted to half-precision hardware, rather than fundamental technological improvements.\n\nMISCELLANEOUS (ALIGNMENT)\n-------------------------\n\n**[If I were a well-intentioned AI](https://www.alignmentforum.org/s/knbhjv252HshMSwpt/p/gzWb5kWwzhdaqmyTt)** *(Stuart Armstrong)* (summarized by Rohin): This sequence takes on the perspective of an AI system that is well-intentioned, but lacking information about what humans want. The hope is to find what good AI reasoning might look like, and hopefully use this to derive insights for safety. The sequence considers Goodhart problems, adversarial examples, distribution shift, subagent problems, etc.\n\n**Rohin's opinion:** I liked this sequence. Often when presented with a potential problem in AI safety, I ask myself why the problem doesn't also apply to humans, and how humans have managed to solve the problem. This sequence was primarily this sort of reasoning, and I think it did a good job of highlighting how with sufficient conservatism it seems plausible that many problems are not that bad if the AI is well-intentioned, even if it has very little information, or finds it hard to communicate with humans, or has the wrong abstractions.\n\nOTHER PROGRESS IN AI\n====================\n\nDEEP LEARNING\n-------------\n\n**[Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization](http://arxiv.org/abs/2002.10657)** *(Satrajit Chatterjee)* (summarized by Nicholas): Deep neural networks trained with gradient descent do well at generalizing from their training set, but the field currently has relatively little understanding of why that is. Large networks have enough parameters to fully memorize the training set and can do so even if trained on data with entirely random labels. This allows for many functions that would fit the training set well, but not generalize, a phenomenon known as overfitting. The question is how gradient descent picks out one of a small subset of functions that will generalize well. \n\nThe *Coherent Gradients* hypothesis, introduced here and tested further in **[this paper](http://arxiv.org/abs/2003.07422)**, is that this results from per-example gradients being averaged during gradient descent. For each example, some of the gradient points in a direction that is idiosyncratic to that example, but some of it points towards a more general solution. When the average is taken across these gradients, the more general directions reinforce each other while the example-specific directions cancel out. As a result, the training process moves faster towards more general directions.\n\nIn order to test this hypothesis, they run two experiments. First they use varying amounts of label noise (corrupting a fraction of the dataset to have random labels). They predict and find that:\n\n1. More label noise leads to slower learning.\n\n2. The uncorrupted examples will be learned faster.\n\nThe next experiment tests a novel form of regularization, called winsorization, where they clip the gradients on a per-example and per-parameter basis to prevent a single example from dominating the gradient, effectively curtailing the component of the gradient that is example-specific. Since the computation of per-example gradients is expensive, when scaling this up to larger networks, they instead use the median of 3 mini-batches to address outliers. The experiments suggest that winsorization reduces overfitting and in particular prevents neural nets from learning randomly labeled data.\n\n**Read more:** **[Explaining Memorization and Generalization: A Large-Scale Study with Coherent Gradients](http://arxiv.org/abs/2003.07422)**\n\n**Nicholas's opinion:** The hypothesis makes sense to me and the experiments do seem to bear out their conclusions. However, none of the results of the experiments were surprising to me and seem to me like they could be consistent with other explanations for generalization. I would be more convinced if the Coherent Gradients hypothesis made predictions that were different from other leading theories and then those turned out to be true.\n\n#### **FEEDBACK**\n\nI'm always happy to hear feedback; you can send it to me, **[Rohin Shah](https://rohinshah.com/)**, by **replying to this email**. \n\n#### **PODCAST**\n\nAn audio podcast version of the **Alignment Newsletter** is available. This podcast is an audio version of the newsletter, recorded by **[Robert Miles](http://robertskmiles.com/)**.", "url": "https://www.alignmentforum.org/posts/rPC9Y9b5vkTqakywC/an-93-the-precipice-we-re-standing-at-and-how-we-can-back", "date_published": "2020-04-01T17:10:02Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "Existential Risk", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.731641+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "f1aec2e3cc5a4b07321c2a45c735f5ff", "source": "alignmentforum", "title": "Equilibrium and prior selection problems in multipolar deployment", "text": "To [avoid catastrophic conflict in multipolar AI scenarios](https://www.alignmentforum.org/posts/DbuCdEbkh4wL5cjJ5/preface-to-eaf-s-research-agenda-on-cooperation-conflict-and), we would like to design AI systems such that AI-enabled actors will tend to cooperate. This post is about some problems facing this effort and some possible solutions. To explain these problems, I'll take the view that the agents deployed by AI developers (the ''principals'') in a multipolar scenario are *moves in a game*. The payoffs to a principal in this game depend on how the agents behave over time. We can talk about the equilibria of this game, and so on. Ideally, we would be able to make guarantees like this:\n\n\n1. The payoffs resulting from the deployed agents' actions are optimal with respect to some appropriate \"welfare function''. This welfare function would encode some combination of total utility, fairness, and other social desiderata;\n2. The agents are in equilibrium --- that is, no principal has an incentive to deploy an agent with a different design, given the agents deployed by the other principals.\n\n\nThe motivation for item 1 is clear: we want outcomes which are fair by each of the principals' lights. In particular, we want an outcome that the principals will all agree to. And item 2 is desirable because an equilibrium constitutes a self-enforcing contract; each agent wants to play their equilibrium strategy, if they believe that the other agents are playing the same equilibrium. Thus, given that the principals all say that they will deploy agents that satisfy 1 and 2, we could have some confidence that a welfare-optimal outcome will in fact obtain.\n\n\nTwo simple but critical problems need to be addressed in order to make such\nguarantees: the equilibrium and prior selection problems. The **equilibrium selection problem** is that this deployment game will have many equilibria. Even if the principals agree on a welfare function, it is possible that many different profiles of agents optimize the same welfare function. So the principals\nneed to coordinate on the profile of agents deployed in order to make guarantees\nlike 1 and 2. Moreover, the agents will probably have private information, such as information about their payoffs, technological capabilities, and so on. As I will explain below, conflicting priors about private information can lead to suboptimal outcomes. And we can’t expect agents to arrive at the same priors by default.\nSo a **prior selection problem** also has to be solved.\n\n\nThe equilibrium selection problem is [well-known](https://en.wikipedia.org/wiki/Equilibrium_selection). The prior selection problem is discussed less. In games where agents have uncertainty about some aspect of their counterparts (like their utility functions), the standard solution concept ---\n[Bayesian Nash equilibrium](https://en.wikipedia.org/wiki/Bayesian_game#Bayesian_Nash_equilibrium) ---\nrequires the agents to have a common prior over the possible values of the players' private information. This assumption might be very useful for some kinds of economic modeling, say. But we cannot expect that AI agents deployed by different principals will have the same priors over private information --- or even common knowledge of each others' priors --- in all of their interactions, in the absence of coordination[[1]](#fn-ejmYasfmcQsXntpgX-1).\n\n\nIt might be unnatural to think about coordinating on a prior; aren't your priors your beliefs? How can you change your beliefs without additional evidence?\nBut there may be many reasonable priors to have, especially for a boundedly\nrational agent whose \"beliefs'' are (say) some complicated property of a neural network. This may be especially true when it comes to beliefs about other agents' private information, which is something that's particularly difficult to\nlearn about from observation (see [here](https://papers.nips.cc/paper/7803-occams-razor-is-insufficient-to-infer-the-preferences-of-irrational-agents.pdf) for example). And while there may be many reasonable priors to have, incorrect beliefs about others' priors could nonetheless have large downsides[[2]](#fn-ejmYasfmcQsXntpgX-2). I give an example of the risks associated with disagreeing priors later in the post.\n\n\nPossible solutions to these problems include:\n\n\n* Coordination by the principals to build a single agent;\n* Coordination by the principals on a profile of agents which are in a welfare-optimal equilibrium;\n* Coordination by the principals on procedures for choosing among equilibria and specifying a common prior at least in certain high-stakes interactions between their agents (e.g., interactions which might escalate to destructive conflict).\n\n\nFinally, a simple but important takeaway of the game-theoretic perspective on multipolar AI deployment is that *it is not enough to evaluate the safety of an agent's behavior in isolation from the other agents that will be deployed*. Whether an agent will behave safely depends on how other agents are designed to interact, including their notions of fairness and how they form beliefs about their counterparts. This is more reason to promote coordination by AI developers, not just on single-agent safety measures but on the game theoretically-relevant aspects of their agents' architectures and training.\n\n\n### A learning game model of multipolar AI deployment\n\n\nIn this idealized model, n principals simultaneously deploy their agents. The agents take actions on the principals' behalf for the rest of time. Principal i has reward function ri, which their agent is trying (in some sense) to maximize. I'll assume that ri perfectly captures what principal i values, in order to separate alignment problems from coordination problems. The agent deployed by principal i is described by a learning algorithm σi.\n\n\nAt each time point t, learning algorithms σi map histories of observations Hti to actions At. For example, these algorithms might choose their actions by planning according to an estimated model. Let γ be a discount factor and Xv the (partially observed) world-state at time v. Denote policies for agent i by πi. Write the world-model estimated from data Hti (which might include models of other agents) as ˆM(Hti). Let Eπi,ˆM(Hti) be the expectation over trajectories generated udner policy πi and model ˆM(Hti). Then this model-based learning algorithm might look like:\nπti=arg maxπiEπi,ˆM(Hti)[∞∑v=tγv−tri(Xv)]σi(Hti)=πti(Hti).\nIn a multiagent setting, each agent’s payoffs depend on the learning algorithms of the other agents. Write the profile of learning algorithms as σ=(σ1,…,σn). Then we write the expected cumulative payoffs for agent i when the agents described by σ are deployed as Vi(σ).\n\n\nThe *learning game* is the game in which strategies are learning algorithms σi and payoffs are long-run rewards Vi(σ). We will say that σ is a *learning equilibrium* if it is a Nash equilibrium of the learning game (cf. [Brafman and Tennenholtz](https://papers.nips.cc/paper/2147-efficient-learning-equilibrium.pdf)). Indexing all players except i by −i, this means that for each i\nVi(σ)≥supσ′iVi(σ′i,σ−i).\n\n\nLet w be a *welfare function* measuring the quality of payoff profile\ngenerated by learning algorithm profile σ. For instance, w might simply be the sum of the individual payoffs: w(σ)=∑iVi(σ). Another candidate for w is the [Nash welfare](https://en.wikipedia.org/wiki/Bargaining_problem#Nash_bargaining_solution).\nIdeally we would have guarantees like 1 and 2 above with respect to an appropriately-chosen welfare function. Weaker, more realistic guarantees might look like:\n\n\n* σ is a w-optimal equilibrium with respect to the agents' world-models at each time-step (thus not necessarily an equilibrium with respect to the *true* payoffs), or\n* The actions recommended by σ constitute a w-optimal equilibrium in\nsufficiently high-stakes interactions, according to the agents' current world-models.\n\n\nThe equilibrium and prior selection problems need to be solved to make such guarantees. I'll talk about these in the next two subsections.\n\n\n#### The equilibrium selection problem\n\n\nFor a moment, consider the reward functions for a different game: an asymmetric version of Chicken (Table 1)[[3]](#fn-ejmYasfmcQsXntpgX-3).\nSuppose players 1 and 2 play this game infinitely many times.\nThe [folk theorems](https://en.wikipedia.org/wiki/Folk_theorem_(game_theory)) tell us that there a Nash equilibrium of this repeated game for every profile of long-run average payoffs in which each player gets at least as much as they can guarantee themselves unilaterally (−1.5 for player 1 and −1 for player 2). Any such payoff profile can be attained in equilibrium by finding a sequence of action profiles that generates the desired payoffs, and then threatening long strings of punishments for players who deviate from this plan. This is a problem, because it means that if a player wants to know what to do, it's not sufficient to play a Nash equilibrium strategy. They could do arbitrarily badly if their counterpart is playing a strategy from a *different* Nash equilibrium.\n\n\nSo, if we want to guarantee that the players don't end up playing lots of (D,D)'s, it is not enough to look at the properties of a *single* player. For instance, in the case of AI, suppose there are two separate AI teams independently training populations of agents. Each AI team wants to teach their agents to behave \"fairly\" in some sense, so they train them until they converge to an [evolutionary stable strategy](https://en.wikipedia.org/wiki/Evolutionarily_stable_strategy)\nin which some \"reasonable'' welfare function is maximized. But, these populations will likely be playing different equilibria. So disaster could still arise if agents from the two populations are played against\neach other[[4]](#fn-ejmYasfmcQsXntpgX-4).\n\n\nThen how do players choose among these equilibria, to make sure that they're playing strategies from the same one? It helps a lot if the players have an opportunity to coordinate on an equilibrium before the game starts, as the principals do in our multipolar AI deployment model.\n\n\n![](https://imgur.com/AvvSE5N.png)\n\n\nOne intuitively fair solution would be alternating between (C,D) and (D,C) at each step. This would lead to player 1 getting an average payoff of 1.75 and player 2 getting an average payoff of 0.5. Another solution might be arranging moves such that the players get the same payoff (equal to about 0.9), which in this case would mean playing twelve (C,D)'s for every seven (D,C)'s. Or,\nplayer 2 might think they can demand more because they can make player 1 worse-off than player 1 can make them. But, though they may come to the bargaining table with differing notions of fairness, both players have an interest in\navoiding coordination failures. So there is hope that the players would reach some agreement, given a chance to coordinate before the game.\n\n\nLikewise, the learning game introduced above is a complex sequential game --- its payoffs are not known at the outset, but can be learned over time.\nAnd this game will also have different equilibria that correspond to different notions of fairness. One solution is for the principals to coordinate on a set of learning algorithms which jointly maximize a welfare function w and punish deviations from this optimization plan, in order to incentivize cooperation. I discuss this approach [in example 5.1.1 here](https://www.alignmentforum.org/posts/4GuKi9wKYnthr8QP9/sections-5-and-6-contemporary-architectures-humans-in-the) and [in this draft](https://longtermrisk.org/files/toward_cooperation_learning_games_oct_2020.pdf).\n\n\nThe problem of enforcement is avoided if the principals coordinate to build a single agent, of course. But it's not clear how likely this is to happen, so it seems important to have solutions which require different degrees of cooperation by\nthe principals. On the other hand, what if the principals are not even able to fully coordinate on the choice of learning algorithms? The principals could at least coordinate on bargaining procedures that their agents will use in the highest-stakes encounters. Such an arrangement could be modeled as specifying a welfare function for measuring the fairness of different proposals in high-stakes interactions, and specifying punishment mechanisms for not following the deal that is maximally fair according to this function. Ensuring that this kind of procedure leads to efficient outcomes also requires agreement on how to form credences in cases where agents possess private information. I address this next.\n\n\n#### The prior selection problem\n\n\nIn this section, I'll give an example of the risks of noncommon priors. In this example, agents having different beliefs about the credibility of a coercive threat leads to the threat being carried out.\n\n\nBayesian Nash equilibrium (BNE) is the standard solution concept for games of\n[incomplete information](https://en.wikipedia.org/wiki/Complete_information), i.e., games in which the players have some private information. (An agent's private information often corresponds to their utility function. However, in my example below it's more intuitive to specify the private information differently.) In this formalism, each player i has a set of possible \"types'' τi encoding their private information. A strategy si maps the set of types τi to the set of mixtures over actions (which we'll denote by Δ(Ai). Finally, assume that the players have a common\nprior P(τ1,…,τn) over the set of types. Let Vi{s(τ)} be\nthe expected payoff to player i when the (possibly mixed) action profile s(τ)=(s1(τ1),…,sn(τn)) is played. Thus, a BNE is a strategy profile s such that, for each i and each τi,\n∑τ−iVi{si(τi),s−i(τ−i)}P(τ−i∣τi)≥sups′i∈Δ(Ai)∑τ−iVi{s′i,s−i(τ−i)}P(τ−i∣τi).\n\n\nTo illustrate the importance of coordination on a common prior, suppose that two agents find themselves in a high-stakes interaction under incomplete information. Suppose that at time t,\nagent 2 (Threatener) tells agent 1 (Target) that they will carry out some dire threat\nif Target doesn't transfer some amount of resources to them. However, it is uncertain whether the Threatener has actually committed to carrying out such a threat.\n\n\nSay that Threatener is a Commitment type if they can commit to carrying out the threat, and a Non-Commitment type otherwise. To compute a BNE, the agents need to specify a common prior for the probability that Threatener is a Commitment type. But, without coordination,\nthey may in fact specify different values for this prior. More precisely, define\n\n\n* pTh: The probability Threatener thinks Target assigns to being a Commitment type;\n* pTa: The Target’s credence that Threatener is a Commitment type;\n* VTh(Carry out)<0: The utility to Threatener if they carry out;\n* VTh(Give in)>0: The utility to Threatener if Target gives in;\n* VTa(Give in)<0: The utility to Target if they give in;\n* VTa(Carry out)<VTa(Give in): The utility to Target if the threat is carried out.\n\n\nA threat being carried out is the worst outcome for everyone. In BNE, Commitment types threaten (and thus commit to carry out a threat) if and only if they think that Target will give in, i.e., VTa(Give in)>pThVTa(Carry out). But Targets give in only if VTa(Give in)>pTaVTa(Carry out). Thus threats will be carried out by Commitment types if and only if\npThpTa>VTa(Give in)VTa(Carry out)pTa.(1)\nOn the other hand, suppose the agents agree on the common prior probability that Threatener is a Commitment type (so pTh=pTa). Then the execution of threats is always avoided.\n\n\nHow might the agents agree on a common prior? In the extreme case, the principals could try to coordinate to design their agents so that they always form the same credences from public information. Remember that the learning algorithms σi introduced above fully specify the action of player i given an observation history. This includes specifying how agents form credences like pTa,pTh. Thus full coordination on the profile of learning algorithms chosen, as suggested in the previous subsection, could in principle solve the problem of specifying a common prior. For instance, write the set of mutually observed data as Ht=n⋂i=1Hti. Let\np be a function mapping Ht to common prior probabilities\nthat Threatener is a Commitment type, pt=p(Ht). The learning algorithms then could be chosen to satisfy  \n\nσTa(HtTa)=Give in ⟺ptVTa(Carry out)<VTa(Give in);σTh(HtTh)=Commit ⟺ptVTa(Carry out)<VTa(Give in).\n\n\nAgain, full coordination on a pair of learning algorithms might be unrealistic. But it still might be possible to agree beforehand on a method for specifying a common prior in certain high-stakes situations. Because of incentives to misrepresent one's credences, it might not be enough to agree to have agents just report their credences and (say) average them (in this case e.g. Target would want to understate\ntheir credence that Threatener is a Commitment type). One direction is to have an agreed-upon standard for measuring the fit of different credences to mutually observed data. A simple model of this would for the principals to agree on a loss function L which measures the fit of credences to data. Then the common credence at the time of a high-stakes interaction t, given the history of mutually observed data Ht, would be p(Ht)=arg minpL(p,Ht). This can be arranged without full coordination on the learning algorithms σi.\n\n\nI won't try to answer the question of how agents decide, in a particular\ninteraction, whether they should use some \"prior commonification''\nmechanism. To speculate a bit, the decision might involve higher-order priors. For instance, if Threatener has a higher-order prior over pTa and thinks that there's a sufficiently high chance that inequality (1) holds, then they\nmight think they're better off coordinating on a prior. But, developing a\nprincipled answer to this question is a direction for future work.\n\n\n### Acknowledgements\n\n\nThanks to Tobi Baumann, Alex Cloud, Nico Feil, Lukas Gloor, and Johannes Treutlein for helpful comments.\n\n\n\n\n---\n\n\n\n1. Actually, the problem is more general than that. The agents might not only have disagreeing priors, but model their strategic interaction using different games entirely. I hope to address this in a later post. For simplicity I'll focus on the special case of priors here. Also, see the literature on \"hypergames\" (e.g. Bennett, P.G., 1980. Hypergames: developing a model of conflict), which describe agents who have different models of the game they're playing. [↩︎](#fnref-ejmYasfmcQsXntpgX-1)\n2. Compare with the literature on [misperception in international relations](https://www.amazon.com/Perception-Misperception-International-Politics-University/dp/0691175853), and how misperceptions can lead to disaster in human interaction. Many instances of misperception might be modeled as \"incorrect beliefs about others' priors''. Compare also with the discussion of crisis bargaining under incomplete information in [Section 4.1 here](https://www.alignmentforum.org/posts/8xKhCbNrdP4gaA8c3/sections-3-and-4-credibility-peaceful-bargaining-mechanisms). [↩︎](#fnref-ejmYasfmcQsXntpgX-2)\n3. I set aside the problem of truthfully eliciting each player's utility function. [↩︎](#fnref-ejmYasfmcQsXntpgX-3)\n4. Cf. [this CHAI paper](https://arxiv.org/pdf/1910.05789.pdf),\nwhich makes a related point in the context of human-AI interaction. However,\nthey say that we can't expect an AI trained to play an equilibrium\nstrategy in self-play to perform well against a human, because humans\nmight play off-equilibrium (seeing as humans are \"suboptimal''). But the problem is not just that one of the players might play off-equilibrium. It's that even if they are both playing an equilibrium strategy, they may have selected different equilibria. [↩︎](#fnref-ejmYasfmcQsXntpgX-4)", "url": "https://www.alignmentforum.org/posts/Tdu3tGT4i24qcLESh/equilibrium-and-prior-selection-problems-in-multipolar-1", "date_published": "2020-04-02T20:06:14Z", "authors": ["JesseClifton"], "tags": ["Multipolar Scenarios", "AI Governance", "AI"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.732041+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "488aa02e04bf12da25338c4c2309559e", "source": "alignmentforum", "title": "Announcing Web-TAISU, May 13-17", "text": "**I am excited to announce Web-TAISU!**  \n**May 13-17, 2020**\n\nI was going to run a [Technical AI Safety Unconference](https://www.lesswrong.com/events/BPTzfeQeZZ6chHvtr/taisu-technical-ai-safety-unconference-1) (TAISU) at [CEEALAR](https://ceealar.org/) (formerly EA Hotel), Blackpool, UK. Then there was a pandemic. *So instead there will be a Web-TAISU on the Internet.*\n\nThis is an unconference, which means that the program is participant driven. I do have some backup ideas to put in if there are not enough suggestions from you, but I don’t expect those to be necessary.\n\nI thought a lot about how to adapt this event to be run online, and I am grateful to the participants who took time to discuss this with me. In the end I concluded that I think it would work very well to stick to more or less the same format as the in-person TAISU I organised last year. The main adjustment I made is that the days are shorter, both because online calls tend to be more tiring, and for the event to work well in more timezones.\n\nWeb-TAISU is scheduled to happen during daytime for everyone in the Americas and Europe and Africa. I’m sorry everyone else, hopefully you find some hours to join, anyway.\n\nSignup\n======\n\nPlease do this as soon as possible, to give me an idea of who is coming.\n\n1. Use [this Omnipotent](https://omnipointment.com/EAM55oo7Vsekm5Rv4el9/respond) to tell me your best guess of when you will be attending Web-TAISU. ***Go there right now and give me your best guess!*** You can always change it later.\n2. [Create a QiqoChat user](https://qiqochat.com/login)\n3. Join the [AI Safety Circle](https://ais.qiqochat.com/members)\n4. Let others know who you are by filling in your profile\n1. Click on “Profiles”\n2. Click on yourself\n3. Click “Edit”\n4. Add a profile picture - ***Important!***\n5. Fill in the rest of your profile\n\n[Schedule and Other Information](https://docs.google.com/document/d/1AJ67N78A60njFmJPx3UN6ghwP3IRproPeh04DIvtgck/edit#heading=h.3ur1js4vza6m)\n=============================================================================================================================================", "url": "https://www.alignmentforum.org/posts/CMnMaTxNAhXfcEtgm/announcing-web-taisu-may-13-17", "date_published": "2020-04-04T11:48:14Z", "authors": ["Linda Linsefors"], "tags": ["Community Page"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.732260+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "69f2bdac8e378f7331cff97f6421dd17", "source": "alignmentforum", "title": "Resources for AI Alignment Cartography", "text": "**I want to make an *actionable* map of AI alignment.**\n\nAfter years of reading papers, blog posts, online exchanges, books, and occasionally hidden documents about AI alignment and AI risk, and having extremely interesting conversations about it, most arguments I encounter now feel familiar at best, rehashed at worst. This should mean I have a good *map* of the field being discussed.\n\nI have been, however, frustrated by how little [actual advice](https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences) I could derive from this map. The message I understood from most agendas was \"here are the tractable sub-problems we will work on and why they should be solved\". I didn't find much justification for why they are critically important, or why one particular set of problems should be explored instead of the next research group's set.\n\nSo I looked harder. I found useful mapping work, yet nothing quite exactly like what I was looking for. I also found related concerns in [this post](https://www.lesswrong.com/posts/AE9yM7ZaPiZ662BF8/thoughts-on-ben-garfinkel-s-how-sure-are-we-about-this-ai) and [this comment thread](https://forum.effectivealtruism.org/posts/tDk57GhrdK54TWzPY/i-m-buck-shlegeris-i-do-research-and-outreach-at-miri-ama?commentId=Ys9aysftw28wXoeRQ).\n\n**You'll find, in the following sections**, my (current) selection of:\n\n* \"cartography work\", to draw a map of relevant arguments and concepts;\n* research agendas, from research groups or individuals;\n* points of entry for newcomers.\n\nHere are the caveats. The list is not exhaustive. I *did* try to cover as many visible ideas as possible, and there will be significant overlap and cross-references between the items listed here. Some references I consider useful (e.g. [this](https://intelligence.org/2017/02/28/using-machine-learning/)) have not made the cut. I attempted to categorize the resources by focus, but a handful could have ended up in a different category. Please don’t rely on it too much.\n\nMy comments aren't summaries, rather justifications for why I included the reference. They also reuse liberally the original formulations. Please tell me if I left strong misrepresentations of the ideas in there.\n\nAll these references, and countless comments scattered all across LessWrong, the [Alignment forum](https://www.alignmentforum.org/), and the [Effective Altruism forum](https://forum.effectivealtruism.org/), will hopefully help me build *something actionable*, something that would let newcomers and experts explore the field with more clarity and make better decisions.\n\n**My short-term plan is to create minimal interactive explanations** for the relevance of various propositions in AI alignment, with the option to question and expand their premises. I want to do this for a first few high-level ideas, and if it goes well, expand to a first full scenario.\n\nThe long-term plan is to map as many propositions and available scenarios as possible, to have a common framework in which to compare research directions. My intuition (to be challenged) is that there’s broad agreement in the field on most premises I could describe, and that we would benefit a lot from locating cruxes (e.g. [here](https://forum.effectivealtruism.org/posts/Ayu5im98u8FeMWoBZ/my-personal-cruxes-for-working-on-ai-safety)). My overarching motivation is to reduce [research debt](https://distill.pub/2017/research-debt/).\n\nThe references here will be my first source of information. The second one would be **discussions**. If you are the author of one of the resources below and/or if you had more conversations about alignment-related arguments than you can remember, and want to share your insights, **please reach out to me**. I will do my best to answer in a timely manner.\n\n*Thanks to Adam Shimi , Alexis Carlier and Maxime Riché for reviewing drafts of this post and suggesting resources!*\n\n\n\n---\n\nArgument mapping & reviews\n--------------------------\n\n**[Disentangling arguments for the importance of AI safety](https://www.alignmentforum.org/posts/JbcWQCxKWn3y49bNB/disentangling-arguments-for-the-importance-of-ai-safety)**\n\n*Richard Ngo - January 2019*\n\nSplits the core motivating arguments for AI safety into six rough categories: maximizers being dangerous, target loading, prosaic alignment, human safety, misuses/vulnerabilities, and large impact.\n\nMakes the case for more clarity around the fundamental ideas, analysis of the arguments, description of deployment scenarios, as well as making more explicit the assumptions behind research agendas.\n\n**[Clarifying some key hypotheses in AI alignment](https://www.lesswrong.com/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment)**\n\n*Ben Cottier, Rohin Shah - August 2019*\n\nCreates a diagram linking hypotheses, scenarios, agendas, and catastrophic problems. Selects for debated and important arguments, does not claim to be comprehensive, links ideas through diverse relationships (support, conditional support, entailment, etc.)\n\nThe post itself goes into more details on the hypotheses, with resources listed for each one.\n\n**[My personal cruxes for working on AI safety](https://forum.effectivealtruism.org/posts/Ayu5im98u8FeMWoBZ/my-personal-cruxes-for-working-on-ai-safety)**\n\n*Buck Shlegeris - January 2020*\n\nThe first section of the talk highlights the limits of heuristic arguments, the usefulness of spelling out premises and making a deliberate effort to build compelling arguments for your personal stance.\n\nThe talk then proceeds to detail the speaker’s own argument for AI alignment work. Many commenters express their gratitude for all this exposition.\n\n**[How sure are we about this AI stuff?](https://www.effectivealtruism.org/articles/ea-global-2018-how-sure-are-we-about-this-ai-stuff/)**\n\n*Ben Garfinkel - February 2019*\n\nRuns through the intuitive arguments being AI risk prioritization: “AI as a big deal”, instability, lock-in, and accidents. Expands why each of them aren’t forceful, or with missing pieces/details.\n\nCalls for the arguments being fleshed out further as a neglected issue, with potential high value.\n\n**[A shift in arguments for AI risk](https://fragile-credences.github.io/prioritising-ai/)**\n\n*Tom Adamczewski - February 2019*\n\nDescribes the evolution of AI risk arguments, from early descriptions of the alignment problem, to discontinuities as a premise for Bostrom’s *Superintelligence*, to alignment issues without discontinuity. Also describes non-alignment catastrophes, such as misuse risks.\n\nCalls for clarification of arguments related to AI risk, especially on the subject of discontinuities, for better prioritization, and reduction of costly misunderstandings.\n\nScenarios, forecasting & strategy\n---------------------------------\n\n**[AI Impacts (selected references)](https://aiimpacts.org/)**\n\n*[AI Impacts contributors](https://aiimpacts.org/about/) - Since 2014*\n\nThe website in general is dedicated to building AI forecasting resources, to inform arguments and decisions. Some of their content most closely related to AI risk arguments:\n\n* [Takeaways from safety by default interviews](https://aiimpacts.org/takeaways-from-safety-by-default-interviews/) - April 2020\n* [Evidence against current methods leading to human level artificial intelligence](https://aiimpacts.org/evidence-against-current-methods-leading-to-human-level-artificial-intelligence/) - August 2019\n* [Likelihood of discontinuous progress around the development of AGI](https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/) - February 2018\n\n**[What failure looks like](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like)**\n\n*Paul Christiano - March 2019*\n\nDescribes two scenarios for AI catastrophe which don’t depend on a fast surprise takeover by a powerful AI system. Also notable for the level of engagement in the comments.\n\n**[Disjunctive Scenarios of Catastrophic AI Risk](https://www.lesswrong.com/posts/8uJ3n3hu8pLXC4YNE/some-conceptual-highlights-from-disjunctive-scenarios-of)**\n\n*Kaj Sotala - February 2018*\n\nBreaks down a wide range of scenarios leading to (at least) catastrophic risk, by decomposing them into a variety of factors: strategic advantage, takeoff speed, autonomy acquisition, plurality of agents, etc.\n\nExplores the idea of there being multiple combinations of factors which may be realized, each of them leading to a catastrophe (as opposed to a specific privileged scenario, which may receive too much focus).\n\n**[The Main Sources of AI Risk?](https://www.alignmentforum.org/posts/WXvt8bxYnwBYpy9oT/the-main-sources-of-ai-risk-1)**\n\n*Wei Dai, Daniel Kokotajlo - March 2019 (last updated March 2020)*\n\nThirty-two (and counting) high-level scenarios for AI catastrophe. Wei Dai emphasizes that they aren’t disjunctive, as some scenarios may subsume or cause others. Daniel Kokotajlo (who maintains and updates the list) suggests it could be refined, expanded and reorganized.\n\n**[Chris Olah’s views on AGI safety](https://www.lesswrong.com/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety)**\n\n*Evan Hubinger - November 2019*\n\nReports arguments on the importance of transparency and interpretability, and about how to improve the field of machine learning to make progress on these issues.\n\n**[Classification of global catastrophic risks connected with artificial intelligence](https://www.lesswrong.com/posts/f7Ewshjh2aSWXgdnX/paper-classification-of-global-catastrophic-risks-connected)**\n\n*Alexey Turchin, David Denkenberger - January 2018*\n\nLists and categorizes a wide range of catastrophic scenarios, from narrow or general AI, near-term or long-term, misuse or accidents, and many other factors, with references.\n\nAgendas & reports focused on problem framing\n--------------------------------------------\n\n**[Embedded agency](https://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version)**\n\n*Scott Garrabrant, Abram Demski - November 2018*\n\nClarifies and motivates technical research stemming from the idea of embedded agents, where AI systems are no longer logically separated from their environment, implying modeling and self-modification issues, among others.\n\nDescribes the subproblems associated with that hypothesis: decision theory, embedded world-models, robust delegation, and subsystem alignment.\n\n**[AI Governance: A Research Agenda](http://www.fhi.ox.ac.uk/govaiagenda)**\n\n*Allan Dafoe - August 2018*\n\nFrom the Center for the Governance of AI, Future of Humanity Institute. The agenda aims for superficial comprehensiveness, gathering as many questions relevant to AI Governance as possible in 53 pages, and providing extensive references for further details. It doesn’t focus on prioritization, nor tractability/impact estimates.\n\nThe questions are divided in three clusters: technical landscape (modeling and forecasting AI progress, mapping AI capabilities, and technical AI safety), AI politics (transformation of government, of the job market, and regulatory concerns), and ideal AI governance (desirable values, institutions and scenarios).\n\n**[Building safe artificial intelligence: specification, robustness, and assurance](https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1)**\n\n*Pedro A. Ortega, Vishal Maini, DeepMind - September 2018*\n\nMotivates DeepMind’s technical AI safety research, dividing it in three areas: specification (how to define the purpose of a system, whether explicitly designed or emergent), robustness (how to prevent, anticipate, defend against, and recover from perturbations), and assurance (understand, evaluate and actively control the behavior of a system).\n\nThe post defines a broad array of technical terms. The challenges are grounded in problems already present in current AI systems, and in simple environments (gridworlds).\n\n**[Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565)**\n\n*Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Mané - June 2016*\n\nDescribes and motivates five technical safety research problems in machine learning-based systems, tractable through direct experimentation, in toy environments and/or small-scale models. All problems, sub-problems, and proposed abstract solutions are grounded in the existing machine learning literature.\n\nThe authors also argue for the increasing relevance of these problems as AI capabilities progress.\n\n**[Reframing Superintelligence: Comprehensive AI Services as General Intelligence](https://www.fhi.ox.ac.uk/reframing/)**\n\n*K. Eric Drexler - January 2019*\n\nExpands on Bostrom’s *Superintelligence*, through a mesh of forty high-level statements addressing the possibility of an intelligence explosion, the nature of advanced machine intelligence, the relationship between goals and intelligence, the use and control of advanced AI, and near/long-term considerations in AI safety & strategy.\n\nThe common underlying framing is a service-centered model of general intelligence, suggesting the integration of a diversity of task-oriented systems built incrementally, rather than mostly independent, self-improving superintelligent agents.\n\nAI alignment reviews\n--------------------\n\n**[AI Alignment Research Overview](https://docs.google.com/document/d/1FbTuRvC4TFWzGYerTKpBU7FJlyvjeOvVYF2uYNFSlOc/edit#)**\n\n*Jacob Steinhardt - October 2019*\n\nOutlines four broad categories of technical work: technical alignment (how to create aligned AI), detecting failures (how to proactively check for alignment), methodological understanding (best practices), and system-building (how to do the previous three for large systems).\n\nAll problems (or sub-problems, for the first category) are explored through a high-level definition, motivation, solution desiderata, possible research avenues, personal takes, and references.\n\n**[The Landscape of AI Safety and Beneficence Research](https://futureoflife.org/landscape/ResearchLandscapeExtended.pdf)**\n\n*Richard Mallah - January 2017*\n\nMaps a large set of concepts and techniques in AI safety. The core content can be explored in [this interactive visualization](https://futureoflife.org/landscape/). The concepts are primarily organized through a hierarchical map, with secondary links for related ideas. All concepts are given high-level descriptions with references.\n\nThe stated purpose of the work is to provide a comprehensive map and a reference set of concepts for the field, to be extended through further research.\n\n**[AI Alignment 2018-19 Review](https://www.alignmentforum.org/posts/dKxX76SCfCvceJXHv/ai-alignment-2018-19-review)**\n\n*Rohin Shah - January 2020*\n\nThe first section of the post is dedicated to recent work in basic AI risk analysis: new explorations of goal-directedness and comprehensive AI services, as well as new write-ups for, or against AI risk (many of which are listed in this very document).\n\nThe rest of the post details recent work in the many sub-problems of AI alignment, noting that the over 300 references have been selected from a larger set of around 500 articles, clustered for readability (the reader shouldn’t take the chosen categorization as authoritative).\n\n**[2019 AI Alignment Literature Review and Charity Comparison](https://www.lesswrong.com/posts/SmDziGM9hBjW9DKmf/2019-ai-alignment-literature-review-and-charity-comparison)**\n\n*Larks - December 2019*\n\nSorts AI alignment work by origin, and not by topic. It highlights more specifically the agendas of the various research teams, and lists the collaborations between them. It also references a wide range of independent research.\n\nIn addition, the post details the funding of the various organizations involved in the field, as well as methodological comments on prioritization, funding, and research avenues.\n\n**[AGI Safety Literature Review](https://arxiv.org/abs/1805.01109)**\n\n*Tom Everitt, Gary Lea, Marcus Hutter - May 2018*\n\nFocuses specifically on powerful AI systems: plausible conceptual models; forecasting of capability increase and risks; technical safety problems; design ideas and concepts; and public policy.\n\nThe paper explores safety problems shared by multiple research agendas, and summarizes a wide range of publications in the domain.\n\nIntroductory material\n---------------------\n\n**[Benefits & Risks of Artificial Intelligence](https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/)**\n\n*Future of Life Institute - November 2015 (first version)*\n\nSummarizes in an accessible way the very high-level case for AI alignment research, the most common naive objections and misconceptions, with further reading references.\n\n**[Superintelligence: Paths, Dangers, Strategies](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies)**\n\n*Nick Bostrom - July 2014*\n\nMakes the case for the risk from superintelligent entities (not necessarily AI systems, though it is presented as its most probable origin). The book represents an early edited, long-form, philosophical introduction to numerous concepts such as the control problem, takeoff speeds, treacherous turn, instrumental convergence, decisive strategic advantage, value loading, and many more.\n\n**[Human Compatible: Artificial Intelligence and the Problem of Control](https://en.wikipedia.org/wiki/Human_Compatible)**\n\n*Stuart Russell - October 2019*\n\nMakes the case for the risk from advanced AI systems through failure of alignment. The book describes the continued progress in AI capabilities, reviews critically the major arguments around AI risk and forecasting, and argues for early safety research, showcasing significant hurdles to solve, and possible research avenues. \n\n**[Potential Risks from Advanced Artificial Intelligence: The Philanthropic Opportunity](https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity)**\n\n*Holden Karnofsky - May 2016*\n\nMakes the philanthropic case for AI risk research, describing three classes of risk: misuse risk (malevolent, or value-locking use of powerful technology), accident risk (stemming typically from alignment failure) and other risks (such as structural effects due to automation, or dissemination of increasingly capable tools). Also explains several principles for prioritization work.\n\n**[Current Work in AI Alignment](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment)**\n\n*Paul Christiano - June 2019*\n\nDecomposes the then-current main approaches in AI alignment research by building a tree diagram and giving friendly high-level explanations of the ideas. The exploration is itself biased towards iterated amplification, which is put in its broader context.\n\n**[Arbital - Explore AI Alignment](https://arbital.com/explore/ai_alignment/)**\n\n*Many authors - From 2014 to 2018*\n\nProvides detailed explanations for many concepts in AI Alignment, in an explorable way. Now in an [archived state](https://www.lesswrong.com/posts/kAgJJa3HLSZxsuSrf/arbital-postmortem).\n\n**[Robert Miles’s YouTube channel](https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg/videos)**\n\n*Robert Miles - Since 2014*\n\nClear and friendly explanations of many concepts in AI alignment. For introductory material, it is best to start with [his Computerphile videos](https://www.youtube.com/playlist?list=PLqL14ZxTTA4fRMts7Af2G8t4Rp17e8MdS), produced before the channel’s creation.\n\nTechnical agendas focused on possible solutions\n-----------------------------------------------\n\n**[Iterated Amplification](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd)**\n\n*Paul Christiano - October 2018*\n\nDescribes iterated amplification, an alignment technique for powerful ML-based systems. Spells out the core hypotheses behind the validity of the techniques. In the fourth section, details the associated research directions, and desiderata for AI alignment research.\n\n**[Value Learning](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc)**\n\n*Rohin Shah, Paul Christiano, Stuart Armstrong, Jacob Steinhardt, Owain Evans - October 2018*\n\nInvestigates and motivates value learning, discussing the arguments stemming from the idea of a powerful AI system pursuing a particular utility function, using human behavior as a data source. Clearly restates the core arguments in the [conclusion post](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/TE5nJ882s5dCMkBB8).\n\n**[Reframing Impact](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW)**\n\n*Alex Turner - July 2019*\n\nExplores and motivates new ways to work with impact measures, a common component of various approaches in AI safety research, and how to think about scenarios where a powerful AI system makes wide-ranging decisions and actions.\n\n**[Research Agenda v0.9: Synthesising a human's preferences into a utility function](https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into)**\n\n*Stuart Armstrong - June 2019*\n\nClarifies and motivates a technical agenda for building specific assumptions into AI systems that would let them infer human preferences, as an instrumental goal for aligning onto them.\n\n**[Deconfusing Human Values Research Agenda v1](https://www.alignmentforum.org/posts/k8F8TBzuZtLheJt47/deconfusing-human-values-research-agenda-v1)**\n\n*G Gordon Worley III - March 2020*\n\nDefines a technical agenda for building a formal expression of the structure of human values, modeling them as the input of their decision process.\n\n**[The Learning-Theoretic AI Alignment Research Agenda](https://www.alignmentforum.org/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda)**\n\n*Vanessa Kosoy - July 2018*\n\nDetails and motivates philosophically a technical agenda to ground AI alignment in statistical and computational learning theory, as well as algorithmic information theory.\n\n**[Scalable agent alignment via reward modeling: a research direction](https://arxiv.org/abs/1811.07871)**\n\n*Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, Shane Legg - November 2018*\n\nDeepMind paper, which defines a more specific agenda around the learning of a reward function through reinforcement learning, from interaction with a human user, in a way that scales to complex and general domains. \n\n**[AI Safety Needs Social Scientists](https://distill.pub/2019/safety-needs-social-scientists/)**\n\n*Geoffrey Irving, Amanda Askell - February 2019*\n\nExplores and motivates the debate approach to alignment, learning human values through experiments, asking questions and arbitrating between arguments. Tied to the [AI safety via debate OpenAI paper](https://arxiv.org/abs/1805.00899).\n\nSpecial mentions\n----------------\n\n**[Technical AGI safety research outside AI](https://forum.effectivealtruism.org/posts/2e9NDGiXt8PjjbTMC/technical-agi-safety-research-outside-ai)**\n\n*Richard Ngo - October 2019*\n\nThe first section *Studying and understanding safety problems* motivates this very project. The entire post is full of interesting problems to solve.\n\n**[AI safety resources](https://vkrakovna.wordpress.com/ai-safety-resources/)**\n\n*Victoria Krakovna - Regularly updated since August 2017*\n\nProvides a wealth of useful references, which significantly helped expand this list. Still receiving updates!\n\n\n\n---\n\nTo reiterate, just above the comment box : **I'm looking for insights**. If your favorite reference is missing; if you spot a glaring error; if you have a strong opinion on research directions; if you share my frustrations, or disagree: **do share!** (Yes, the post is long, please don't let that stop you from engaging).", "url": "https://www.alignmentforum.org/posts/4az2cFrJp3ya4y6Wx/resources-for-ai-alignment-cartography", "date_published": "2020-04-04T14:20:11Z", "authors": ["Gyrodiot"], "tags": ["Research Agendas", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.732527+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "611d65026a0d4b076083d092fa1c6746", "source": "alignmentforum", "title": "An Orthodox Case Against Utility Functions", "text": "*This post has benefitted from discussion with Sam Eisenstat, Scott Garrabrant, Tsvi Benson-Tilsen, Daniel Demski, Daniel Kokotajlo, and Stuart Armstrong. It started out as a thought about [Stuart Armstrong's research agenda](https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into-1).*\n\nIn this post, I hope to say something about what it means for a rational agent to have preferences. The view I am putting forward is relatively new to me, but it is not *very* radical. It is, dare I say, a conservative view -- I hold close to Bayesian expected utility theory. However, my impression is that it differs greatly from *common impressions* of Bayesian expected utility theory.\n\nI will argue against a particular view of expected utility theory -- a view which I'll call *reductive utility*. I do not recall seeing this view explicitly laid out and defended (except in in-person conversations). However, I expect at least a good chunk of the assumptions are commonly made.\n\nReductive Utility\n=================\n\nThe core tenets of reductive utility are as follows:\n\n* The [sample space](https://en.wikipedia.org/wiki/Probability_space) .mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\nΩ of a rational agent's beliefs is, more or less, the set of possible ways the world could be -- which is to say, the set of possible *physical configurations of the universe*. Hence, each world ω∈Ω is one such configuration.\n* The preferences of a rational agent are represented by a utility function U:Ω→R from worlds to real numbers.\n* Furthermore, the utility function should be a *computable* function of worlds.\n\nSince I'm setting up the view which I'm knocking down, there is a risk I'm striking at a straw man. However, I think there are some good reasons to find the view appealing. The following subsections will expand on the three tenets, and attempt to provide some motivation for them.\n\nIf the three points seem obvious to you, you might just skip to the next section.\n\nWorlds Are Basically Physical\n-----------------------------\n\nWhat I mean here resembles the standard physical-reductionist view. However, my emphasis is on certain features of this view:\n\n* There is some \"basic stuff\" -- like like quarks or vibrating strings or what-have-you.\n* What there is to know about the world is some set of statements about this basic stuff -- particle locations and momentums, or wave-form function values, or what-have-you.\n* These special atomic statements should be logically independent from each other (though they may of course be probabilistically related), and together, fully determine the world.\n* These should (more or less) be what beliefs are about, such that we can (more or less) talk about beliefs in terms of the sample space ω∈Ω as being the set of worlds understood in this way.\n\nThis is the so-called \"view from nowhere\", as [Thomas Nagel puts it](https://en.wikipedia.org/wiki/The_View_from_Nowhere).\n\nI don't intend to construe this position as ruling out certain non-physical facts which we may have beliefs about. For example, we may believe [indexical](https://www.lesswrong.com/posts/SFLCB5BgjzruJv9sp/logical-and-indexical-uncertainty) facts on top of the physical facts -- there might be (1) beliefs about the universe, and (2) [beliefs about where we are in the universe](https://www.lesswrong.com/posts/ethRJh2E7mSSjzCay/building-phenomenological-bridges). Exceptions like this [violate an extreme reductive view](https://www.lesswrong.com/posts/my5pbcmQPb6ASSHYM/puzzles-for-physicalists), but are still close enough to count as reductive thinking for my purposes.\n\nUtility Is a Function of Worlds\n-------------------------------\n\nSo we've got the \"basically physical\" ω∈Ω. Now we write down a utility function U(ω). In other words, utility is a [random variable](https://en.wikipedia.org/wiki/Random_variable) on our event space.\n\nWhat's the big deal?\n\nOne thing this is saying is that *preferences are a function of the world*. Specifically, *preferences need not only depend on what is observed.* This is [incompatible with standard RL in a way that matters](https://www.lesswrong.com/posts/aAzApjEpdYwAxnsAS/reinforcement-learning-with-imperceptible-rewards). \n\nBut, in addition to saying that utility can depend on more than just observations, we are *restricting* utility to *only* depend on things that are in the world. After we consider all the information in ω, there cannot be any extra uncertainty about utility -- no extra \"moral facts\" which we may be uncertain of. If there are such moral facts, they have to be present somewhere in the universe (at least, derivable from facts about the universe).\n\nOne implication of this: *if utility is about high-level entities, the utility function is responsible for deriving them from low-level stuff.* For example, if the universe is made of quarks, but utility is a function of beauty, consciousness, and such, then U() needs to contain the beauty-detector and consciousness-detector and so on -- otherwise how can it compute utility given all the information about the world?\n\nUtility Is Computable\n---------------------\n\nFinally, and most critically for the discussion here, U() should be a computable function.\n\nTo clarify what I mean by this: ω should have some sort of representation which allows us to feed it into a Turing machine -- let's say it's an infinite bit-string which assigns true or false to each of the \"atomic sentences\" which describe the world. U() should be a computable function; that is, there should be a Turing machine F which takes a rational number ϵ>0 and takes ω, prints a rational number within ϵ of U(ω), and halts. (In other words, we can compute U(ω) to any desired degree of approximation.)\n\nWhy should U() be computable?\n\nOne argument is that U() should be computable because the agent has to be able to use it in computations. This perspective is especially appealing if you think of U() as a black-box function which you can only optimize through search. If you can't evaluate U(), how are you supposed to use it? If U() exists as an actual module somewhere in the brain, how is it supposed to be implemented? (If you don't think this sounds very convincing, great!)\n\nRequiring U() to be computable may also seem easy. What is there to lose? Are there preference structures we really care about being able to represent, which are fundamentally not computable?\n\nAnd what would it even mean for a computable agent to have non-computable preferences?\n\nHowever, the computability requirement is more restrictive than it may seem.\n\nThere is a sort of [continuity implied by computability](https://cs.stackexchange.com/questions/80978/why-are-computable-functions-continuous): U() must not depend too much on \"small\" differences between worlds. The computation F(ϵ,ω) only accesses finitely many bits of ω before it halts. All the rest of the bits in ω must not make more than ϵ difference to the value of U(ω). \n\nThis means some seemingly simple utility functions are not computable.\n\nAs an example, consider the [procrastination paradox](https://intelligence.org/files/ProcrastinationParadox.pdf). Your task is to push a button. You get 10 utility for pushing the button. You can push it any time you like. However, if you never press the button, you get -10. On any day, you are fine with putting the button-pressing off for one more day. Yet, if you put it off forever, you lose!\n\nWe can think of ω as a string like 000000100.., where the \"1\" is the day you push the button. To compute the utility, we might look for the \"1\", outputting 10 if we find it.\n\nBut what about the all-zero universe, 0000000...? The program must loop forever. We can't tell we're in the all-zero universe by examining any finite number of bits. You don't know whether you will eventually push the button. (Even if the universe also gives your source code, you can't necessarily tell from that -- the logical difficulty of determining this about yourself is, of course, the original point of the procrastination paradox.)\n\nHence, a preference structure like this is not computable, and is not allowed according to the reductive utility doctrine.\n\nThe advocate of reductive utility might take this as a victory. The procrastination paradox has been avoided, and other paradoxes with a similar structure. (The [St. Petersburg Paradox](https://plato.stanford.edu/entries/paradox-stpetersburg/) is another example.)\n\nOn the other hand, if you think this is a *legitimate preference structure*, dealing with such 'problematic' preferences motivates abandonment of reductive utility.\n\nSubjective Utility: The Real Thing\n==================================\n\nWe can strongly oppose all three points without leaving orthodox Bayesianism. Specifically, I'll sketch how the [Jeffrey-Bolker axioms](https://plato.stanford.edu/entries/decision-theory/#JefThe) enable non-reductive utility. (The title of this section is a reference to Jeffrey's book *Subjective Probability: The Real Thing.*)\n\nHowever, the *real* position I'm advocating is more grounded in logical induction rather than the Jeffrey-Bolker axioms; I'll sketch that version at the end.\n\nThe View From Somewhere\n-----------------------\n\nThe reductive-utility view approached things from the starting-point of the universe. Beliefs are for what is real, and what is real is basically physical.\n\nThe non-reductive view starts from the standpoint of the agent. Beliefs are *for things you can think about*. This doesn't rule out a physicalist approach. What it *does* do is give high-level objects like tables and chairs an equal footing with low-level objects like quarks: both are inferred from sensory experience by the agent.\n\nRather than assuming an underlying set of *worlds*, Jeffrey-Bolker assume only a set of events. For two events P and Q, the conjunction P∧Q exists, and the disjunction P∨Q, and the negations ¬P and ¬Q. However, unlike in the [Kolmogorov axioms](https://en.wikipedia.org/wiki/Probability_axioms), these are not assumed to be intersection, union, and complement of an underlying set of worlds.\n\nLet me emphasize that: *we need not assume there are \"worlds\" at all.*\n\nIn philosophy, this is called [situation semantics](https://plato.stanford.edu/entries/situations-semantics/) -- an alternative to the more common [possible-world semantics](https://plato.stanford.edu/entries/possible-worlds/). In mathematics, it brings to mind [pointless topology](https://en.wikipedia.org/wiki/Pointless_topology).\n\nIn the Jeffrey-Bolker treatment, a world is just a maximally specific event: an event which describes everything completely. But there is no requirement that maximally-specific events exist. Perhaps any event, no matter how detailed, can be further extended by specifying some yet-unmentioned stuff. (Indeed, the Jeffrey-Bolker axioms assume this! Although, Jeffrey does not seem philosophically committed to that assumption, from what I have read.)\n\nThus, there need not be any \"view from nowhere\" -- no semantic vantage point from which we see the whole universe.\n\nThis, of course, deprives us of the objects which utility was a function of, in the reductive view.\n\nUtility Is a Function of Events\n-------------------------------\n\nThe reductive-utility makes a distinction between utility -- the random variable itself -- and *expected* utility, which is the subjective estimate of the random variable which we use for making decisions.\n\nThe Jeffrey-Bolker framework does not make a distinction. Everything is a subjective preference evaluation. \n\nA reductive-utility advocate sees the expected utility of an event E⊆Ω as ***derived from*** the utility of the worlds within the event. They start by defining U(ω); then, we define the expected utility of an event as E[U|E]:=∑ωU(ω)P(ω) -- or, more generally, the corresponding integral.\n\nIn the Jeffrey-Bolker framework, we instead define U(E) *directly* on events. These preferences are required to be *coherent with* breaking things up into sums, so U(E) = U(E∧A)⋅P(E∧A)+U(E∧¬A)⋅P(E∧¬A)P(E) -- but we do not define one from the other.\n\nWe don't have to know how to evaluate entire worlds in order to evaluate events. All we have to know is how to evaluate events!\n\nI find it difficult to really believe \"humans have a utility function\", even approximately -- but I find it *much easier* to believe \"humans have expectations on propositions\". Something like that could even be true at the *neural* level (although of course we would not obey the Jeffrey-Bolker axioms in our neural expectations).\n\n*Updates* Are Computable\n------------------------\n\nJeffrey-Bolker doesn't say anything about computability. However, if we do want to address this sort of issue, it leaves us in a different position.\n\nBecause *subjective expectation is primary*, it is now more natural to require that the agent can evaluate events, without any requirement about a function on worlds. (Of course, we *could* do that in the Kolmogorov framework.)\n\nAgents don't need to be able to compute the utility of a whole world. All they need to know is how to update expected utilities as they go along.\n\nOf course, the subjective utility can't be just *any* way of updating as you go along. It needs to be ***coherent**,* in the sense of the Jeffrey-Bolker axioms. And, maintaining coherence can be very difficult. But it can be quite easy even in cases where the random-variable treatment of the utility function is not computable.\n\nLet's go back to the procrastination example. In this case, to evaluate the expected utility of each action at a given time-step, the agent does not need to figure out whether it ever pushes the button. It just needs to have some probability, which it updates over time.\n\nFor example, an agent might initially assign probability 2−(t+1) to pressing the button at time t, and 1/2 to never pressing the button. Its probability that it would ever press the button, and thus its utility estimate, would decrease with each observed time-step in which it didn't press the button. (Of course, such an agent would press the button immediately.)\n\nOf course, this \"solution\" doesn't touch on any of the tricky logical issues which the procrastination paradox was originally introduced to illustrate. This isn't meant as a solution to the procrastination paradox -- only as an illustration of how to coherently update discontinuous preferences. This simple U() is ***uncomputable*** by the definition of the previous section.\n\nIt also doesn't address computational tractability in a very real way, since if the prior is very complicated, computing the subjective expectations can get extremely difficult.\n\nWe can come closer to addressing logical issues and computational tractability by considering things in a logical induction framework. \n\nUtility Is Not a Function\n=========================\n\nIn a logical induction (LI) framework, the central idea becomes *\"update your subjective expectations in any way you like, so long as those expectations aren't (too easily) exploitable to Dutch-book.\"* This clarifies what it means for the updates to be \"coherent\" -- it is somewhat more elegant than saying \"... any way you like, so long as they follow the Jeffrey-Bolker axioms.\"\n\nThis replaces the idea of \"utility function\" entirely -- there isn't any need for a *function* any more, just a logically-uncertain-variable (LUV, in the terminology from the LI paper).\n\nActually, there are different ways one might want to set things up. I hope to get more technical in a later post. For now, here's some bullet points:\n\n* In the simple procrastination-paradox example, you push the button if you have any uncertainty at all. So things are not that interesting. But, at least we've solved the problem.\n* In more complicated examples -- where there is some real benefit to procrastinating -- a LI-based agent could totally procrastinate forever. This is because LI doesn't give any guarantee about converging to correct beliefs for uncomputable propositions like whether Turing machines halt or whether people stop procrastinating.\n* Believing you'll stop procrastinating even though you won't is *perfectly coherent* -- in the same way that believing in [nonstandard numbers](https://en.wikipedia.org/wiki/Non-standard_model_of_arithmetic) is perfectly logically consistent. Putting ourselves in the shoes of such an agent, this just means we've examined our own decision-making to the best of our ability, and have put significant probability on \"we don't procrastinate forever\". This kind of reasoning is necessarily fallible.\n* Yet, if a system we built were to do this, we might have strong objections. So, this can count as an alignment problem. How can we give feedback to a system to avoid this kind of mistake? I hope to work on this question in future posts.", "url": "https://www.alignmentforum.org/posts/A8iGaZ3uHNNGgJeaD/an-orthodox-case-against-utility-functions", "date_published": "2020-04-07T19:18:12Z", "authors": ["abramdemski"], "tags": ["Indexical Information", "Utility Functions", "Rationality", "Decision Theory", "AI"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.733083+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "2c39d75c7c6ecbd5e8582857b2ae772b", "source": "alignmentforum", "title": "[AN #94]: AI alignment as translation between humans and machines", "text": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.\n\nAudio version **[here](http://alignment-newsletter.libsyn.com/alignment-newsletter-94)** (may not be up yet). \n\nHIGHLIGHTS\n==========\n\n**[Alignment as Translation](https://www.alignmentforum.org/posts/42YykiTqtGMyJAjDM/alignment-as-translation)** *(John S Wentworth)* (summarized by Rohin): At a very high level, we can model powerful AI systems as moving closer and closer to omniscience. As we move in that direction, what becomes the new constraint on technology? This post argues that the constraint is *good interfaces*, that is, something that allows us to specify what the AI should do. As with most interfaces, the primary challenge is dealing with the discrepancy between the user's abstractions (how humans think about the world) and the AI system's abstractions, which could be very alien to us (e.g. perhaps the AI system uses detailed low-level simulations). The author believes that this is the central problem of AI alignment: how to translate between these abstractions in a way that accurately preserves meaning.\n\nThe post goes through a few ways in which we could attempt to do this translation, but all of them seem to only reduce the amount of translation that is necessary: none of them solve the chicken-and-egg problem of how you do the very first translation between the abstractions.\n\n**Rohin's opinion:** I like this view on alignment, but I don't know if I would call it the *central* problem of alignment. It sure seems important that the AI is *optimizing* something: this is what prevents solutions like \"make sure the AI has an undo button / off switch\", which would be my preferred line of attack if the main source of AI risk were bad translations between abstractions. There's a longer discussion on this point **[here](https://www.alignmentforum.org/posts/42YykiTqtGMyJAjDM/alignment-as-translation?commentId=zfv4qGzSmYnSqxKBK)**.\n\nTECHNICAL AI ALIGNMENT\n======================\n\nAGENT FOUNDATIONS\n-----------------\n\n**[Two Alternatives to Logical Counterfactuals](https://www.alignmentforum.org/posts/yBdDXXmLYejrcPPv2/two-alternatives-to-logical-counterfactuals)** *(Jessica Taylor)*\n\nLEARNING HUMAN INTENT\n---------------------\n\n**[State-only Imitation with Transition Dynamics Mismatch](http://arxiv.org/abs/2002.11879)** *(Tanmay Gangwani et al)* (summarized by Zach): Most existing imitation learning algorithms rely on the availability of expert demonstrations that come from the *same* MDP as the one the imitator will be evaluated in. With the advent of **[adversarial inverse reinforcement learning (AIRL)](https://arxiv.org/abs/1710.11248)** (**[AN #17](https://mailchi.mp/ad852629e45a/alignment-newsletter-17)**), it has become possible to learn general behaviors. However, algorithms such as **[GAIL](https://arxiv.org/abs/1606.03476)** (**[AN #17](https://mailchi.mp/ad852629e45a/alignment-newsletter-17)**) are capable of learning with just state-information, something that AIRL was not designed for. In this paper, the authors introduce indirect-imitation learning (I2L) to try and merge the benefits of both GAIL and AIRL. The basic sketch of the algorithm is to first use a generalization of AIRL to imitate demonstrations via a buffer distribution and then focus on moving that buffer closer to the expert's demonstration distribution using a Wasserstein critic, a smoother way to train GAN networks. By combining these two approaches, agents trained with I2L learn how to control Ant in regular gravity and can *generalize* to perform in simulations with differing parameters for gravity. For the suite of Gym continuous domains, they show consistent advantages for I2L over other algorithms such as GAIL, BCO, and AIRL when parameters such as friction, density, and gravity are changed. \n\n**Prerequisities:** **[Wasserstein GAN](https://arxiv.org/abs/1701.07875)**\n\n**Read more:** **[Paper: Learning Robust Rewards With Adversarial](https://arxiv.org/pdf/1710.11248.pdf)**  \n**[Inverse Reinforcement Learning](https://arxiv.org/pdf/1710.11248.pdf)**\n\n**Zach's opinion:** The main contribution in this paper seems to be deriving a new bound so that AIRL can handle state-only imitation learning. The use of indirection via a buffer is also interesting and seems to be a good idea to provide stability in training. However, they did not do an ablation. Overall, it's aesthetically interesting that this paper is borrowing tricks, such as buffering and Wasserstein critic. Finally, the results seem promising, particularly for the sim-to-real problem. It would be interesting to see a follow-up to gauge whether or not I2L can help bridge this gap.\n\n**[The MineRL Competition on Sample-Efficient Reinforcement Learning Using Human Priors: A Retrospective](http://arxiv.org/abs/2003.05012)** *(Stephanie Milani et al)* (summarized by Rohin): This paper reports on the results of the **[MineRL competition](https://arxiv.org/abs/1904.10079)** (**[AN #56](https://mailchi.mp/894417149126/an-56-should-ml-researchers-stop-running-experiments-before-making-hypotheses)**), in which participants had to train agents to obtain a diamond in Minecraft using a limited amount of compute, environment interactions, and human demonstrations. While no team achieved this task, one team did make it to the penultimate milestone: obtaining an iron pickaxe.\n\nThe top nine teams all used some form of action reduction: that is, they constrained their agents to only take a subset of all available actions, shaping the space in which the agent had to learn and explore. The top four teams all used some form of hierarchy in order to learn longer \"options\" that could then be selected from. The second place team used pure imitation learning (and so required *no* environment interactions), while the eighth and ninth place teams used pure reinforcement learning (and so required *no* human demonstrations).\n\n**Rohin's opinion:** I was surprised to see pure RL solutions rank in the leaderboard, given the limitations on compute and environment interactions. Notably though, while the second place team (pure imitation) got 42.41 points, the eighth place team (pure RL) only got 8.25 points.\n\nMore generally, I was excited to see an actual benchmark for techniques using human demonstrations: so far there hasn't been a good evaluation of such techniques. It does seem like Minecraft benefits a lot from hierarchy and action pruning, which we may not care about when evaluating algorithms.\n\n**[Sample Efficient Reinforcement Learning through Learning from Demonstrations in Minecraft](http://arxiv.org/abs/2003.06066)** *(Christian Scheller et al)* (summarized by Rohin): This paper explains the technique used by the 3rd place team in the MineRL competition (summarized above). They used behavior cloning to train their neural net on human demonstrations, and then used reinforcement learning (specifically, IMPALA) with experience replay and advantage clipping to improve. There are more details about their architecture and design choices in the paper.\n\nHANDLING GROUPS OF AGENTS\n-------------------------\n\n**[Equilibrium and prior selection problems in multipolar deployment](https://www.alignmentforum.org/posts/Tdu3tGT4i24qcLESh/equilibrium-and-prior-selection-problems-in-multipolar-1)** *(Jesse Clifton)* (summarized by Rohin): Consider the scenario in which two principals with different terminal goals will separately develop and deploy learning agents, that will then act on their behalf. Let us call this a *learning game*, in which the \"players\" are the principals, and the actions are the agents developed.\n\nOne strategy for this game is for the principals to first agree on a \"fair\" joint welfare function, such that they and their agents are then licensed to punish the other agent if they take actions that deviate from this welfare function. Ideally, this would lead to the agents jointly optimizing the welfare function (while being on the lookout for defection).\n\nThere still remain two coordination problems. First, there is an *equilibrium selection problem*: if the two deployed learning agents are Nash strategies from *different* equilibria, payoffs can be arbitrarily bad. Second, there is a *prior selection problem*: given that there are many reasonable priors that the learning agents could have, if they end up with different priors from each other, outcomes can again be quite bad, especially in the context of **[threats](https://www.lesswrong.com/s/p947tK8CoBbdpPtyK)** (**[AN #86](https://mailchi.mp/598f425b1533/an-86-improving-debate-and-factored-cognition-through-human-experiments)**).\n\n**Rohin's opinion:** These are indeed pretty hard problems in any non-competitive game. While this post takes the framing of considering optimal principals and/or agents (and so considers Bayesian strategies in which only the prior and choice of equilibrium are free variables), I prefer the framing taken in **[our paper](https://bair.berkeley.edu/blog/2019/10/21/coordination/)** (**[AN #70](https://mailchi.mp/732eaa192df0/an-70-agents-that-help-humans-who-are-still-learning-about-their-own-preferences)**): the issue is primarily that the optimal thing for you to do depends strongly on who your partner is, but you may not have a good understanding of who your partner is, and if you're wrong you can do arbitrarily poorly.\n\nFORECASTING\n-----------\n\n**[Openness Norms in AGI Development](https://www.alignmentforum.org/posts/RvrTZ3qKWpg9aiFqZ/openness-norms-in-agi-development)** *(Sublation)* (summarized by Rohin): This post summarizes two papers that provide models of why scientific research tends to be so open, and then applies it to the development of powerful AI systems. The **[first](http://www.strevens.org/research/scistruc/Communicans.pdf)** models science as a series of discoveries, in which the first academic group to reach a discovery gets all the credit for it. It shows that for a few different models of info-sharing, info-sharing helps everyone reach the discovery sooner, but doesn't change the probabilities for who makes the discovery first (called *race-clinching probabilities*): as a result, sharing all information is a better strategy than sharing none (and is easier to coordinate on than the possibly-better strategy of sharing just some information).\n\nHowever, this theorem doesn't apply when info sharing compresses the discovery probabilities *unequally* across actors: in this case, the race-clinching probabilities *do* change, and the group whose probability would go down is instead incentivized to keep information secret (which then causes everyone else to keep their information secret). This could be good news: it suggests that actors are incentivized to share safety research (which probably doesn't affect race-clinching probabilities) while keeping capabilities research secret (thereby leading to longer timelines).\n\nThe **[second paper](http://philsci-archive.pitt.edu/13452/1/Heesen%202017%20Communism%20and%20the%20Incentive%20to%20Share%20in%20Science%20preprint.pdf)** assumes that scientists are competing to complete a k-stage project, and whenever they publish, they get credit for all the stages they completed that were not yet published by anyone else. It also assumes that earlier stages have a higher credit-to-difficulty ratio (where difficulty can be different across scientists). It finds that under this setting scientists are incentivized to publish whenever possible. For AI development, this seems not to be too relevant: we should expect that with powerful AI systems, most of the \"credit\" (profit) comes from the last few stages, where it is possible to deploy the AI system to earn money.\n\n**Rohin's opinion:** I enjoyed this post a lot; the question of openness in AI research is an important one, that depends both on the scientific community and industry practice. The scientific community is extremely open, and the second paper especially seems to capture well the reason why. In contrast industry is often more secret (plausibly due to **[patents](https://www.fhi.ox.ac.uk/wp-content/uploads/Patents_-FHI-Working-Paper-Final-.pdf)** (**[AN #88](https://mailchi.mp/9d279b575b1a/an-88-how-the-principal-agent-literature-relates-to-ai-risk)**)). To the extent that we would like to change one community in the direction of the other, a good first step is to understand their incentives so that we can try to then change those incentives.\n\nMISCELLANEOUS (ALIGNMENT)\n-------------------------\n\n**[Takeaways from safety by default interviews](https://aiimpacts.org/takeaways-from-safety-by-default-interviews/)** *(Asya Bergal)* (summarized by Rohin): This post lists three key takeaways from AI Impacts' conversations with \"optimistic\" researchers (summarized mainly in **[AN #80](https://mailchi.mp/b3dc916ac7e2/an-80-why-ai-risk-might-be-solved-without-additional-intervention-from-longtermists)** with one in **[AN #63](https://mailchi.mp/533c646a4b21/an-63how-architecture-search-meta-learning-and-environment-design-could-lead-to-general-intelligence)**). I'll just name the takeaways here, see the post for more details:\n\n1. Relative optimism in AI often comes from the belief that AGI will be developed gradually, and problems will be fixed as they are found rather than neglected.\n\n2. Many of the arguments I heard around relative optimism weren’t based on inside-view technical arguments.\n\n3. There are lots of calls for individuals with views around AI risk to engage with each other and understand the reasoning behind fundamental disagreements.\n\n**Rohin's opinion:** As one of the people interviewed, these seem like the right high-level takeaways to me.\n\nOTHER PROGRESS IN AI\n====================\n\nREINFORCEMENT LEARNING\n----------------------\n\n**[Robots Learning to Move like Animals](https://bair.berkeley.edu/blog/2020/04/03/laikago/)** *(Xue Bin Peng et al)* (summarized by Rohin): **[Previous work](http://bair.berkeley.edu/blog/2018/10/09/sfv/)** (**[AN #28](https://mailchi.mp/df2e472140b6/alignment-newsletter-28)**) has suggested that we can get good policies by estimating and imitating poses. This work takes this idea and tries to make it work with sim-to-real transfer. Domain randomization would result in a policy that must be robust to all the possible values of the hidden parameters (such as friction). To make the problem easier, they do domain randomization, but give the agent access to (a latent representation of) the hidden parameters, so that its policy can depend on the hidden parameters. Then, to transfer to the real world, they simply need to search over the latent representation of the hidden parameters in order to find one where the policy actually works in the real world. In practice, they can adapt to the real world with just 8 minutes of real world data.\n\n**Read more:** **[Paper: Learning Agile Robotic Locomotion Skills by Imitating Animals](https://arxiv.org/abs/2004.00784)**\n\n**Rohin's opinion:** This is a cool improvement to domain randomization: it seems like it should be distinctly easier to learn a policy that is dependent on the hidden parameters, and that seems to come at the relatively low cost of needing just a little real world data.\n\n#### **FEEDBACK**\n\n I'm always happy to hear feedback; you can send it to me, **[Rohin Shah](https://rohinshah.com/)**, by **replying to this email**. \n\n#### **PODCAST**\n\nAn audio podcast version of the **Alignment Newsletter** is available. This podcast is an audio version of the newsletter, recorded by **[Robert Miles](http://robertskmiles.com/)**.", "url": "https://www.alignmentforum.org/posts/gor57NZtxG4bq5eej/an-94-ai-alignment-as-translation-between-humans-and", "date_published": "2020-04-08T17:10:03Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.733539+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "874c8d89116387687b98104a4bf32305", "source": "alignmentforum", "title": "Asymptotically Unambitious AGI", "text": "Original Post:\n\nWe [present an algorithm](http://www.tinyurl.com/bomai-anon) [[updated version](https://arxiv.org/abs/1905.12186)], then show (given four assumptions) that in the limit, it is human-level intelligent and benign.\n\nWill MacAskill has commented that in the seminar room, he is a consequentialist, but for decision-making, he takes seriously the lack of a philosophical consensus. I believe that what is here is correct, but in the absence of feedback from the Alignment Forum, I don't yet feel comfortable posting it to a place (like arXiv) where it can get cited and enter the academic record. We have submitted it to IJCAI, but we can edit or revoke it before it is printed.\n\nI will distribute at least min($365, number of comments \\* $15) in prizes by April 1st (via venmo if possible, or else Amazon gift cards, or a donation on their behalf if they prefer) to the authors of the comments here, according to the comments' quality. If one commenter finds an error, and another commenter tinkers with the setup or tinkers with the assumptions in order to correct it, then I expect both comments will receive a similar prize (if those comments are at the level of prize-winning, and neither person is me). If others would like to donate to the prize pool, I'll provide a comment that you can reply to.\n\nTo organize the conversation, I'll start some comment threads below:\n\n* Positive feedback\n* General Concerns/Confusions\n* Minor Concerns\n* Concerns with Assumption 1\n* Concerns with Assumption 2\n* Concerns with Assumption 3\n* Concerns with Assumption 4\n* Concerns with \"the box\"\n* Adding to the prize pool\n\nEdit 30/5/19: An [updated version](https://arxiv.org/abs/1905.12186) is on arXiv. I now feel comfortable with it being cited. The key changes:\n\n* The Title. I suspect the agent is unambitious for its entire lifetime, but the title says \"asymptotically\" because that's what I've shown formally. Indeed, I suspect the agent is benign for its entire lifetime, but the title says \"unambitious\" because that's what I've shown formally. (See the section \"Concerns with Task-Completion\" for an informal argument going from unambitious -> benign).\n* The Useless Computation Assumption. I've made it a slightly stronger assumption. The original version is technically correct, but setting .mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\nβ is tricky if the weak version of the assumption is true but the strong version isn't. This stronger assumption also simplifies the argument.\n* The Prior. Rather than having to do with the description length of the Turing machine simulating the environment, it has to do with the number of states in the Turing machine. This was in response to Paul's point that the finite-time behavior of the original version is really weird. This also makes the Natural Prior Assumption (now called the No Grue Assumption) a bit easier to assess.\n\nEdit 17/02/20: Published at AAAI. The prior over world-models is now totally different, and much better. There's no \"amnesia antechamber\" required. The Useless Computation Assumption and the No Grue Assumption are now obselete. The argument for unambitiousness now depends on the \"Space Requirements Assumption\", which we probed empirically. The ArXiv link is up-to-date.", "url": "https://www.alignmentforum.org/posts/pZhDWxDmwzuSwLjou/asymptotically-unambitious-agi", "date_published": "2020-04-10T12:31:54Z", "authors": ["michaelcohen"], "tags": ["Bounties (closed)", "AI", "Instrumental Convergence", "Impact Regularization"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.733891+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "eb1ad3f15a8260a93b2330ea1f1d30f4", "source": "alignmentforum", "title": "\"How conservative\" should the partial maximisers be?", "text": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\nDue to the problem of building a [strong V-enhancer when we want a U-enhancer](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy) - and the [great difficulty in defining U](https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes), the utility we truly want to maximise - many people have suggested reducing the V-increasing focus of the AI. The idea is that, as long as the AI doesn't devote too much [optimisation power](https://www.lesswrong.com/posts/iJNK7rE5jphMSJJCa/thoughts-and-problems-with-eliezer-s-measure-of-optimization) to V, the V and U [will stay connected with each other](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy#Extremal_Goodhart), and hence a moderate increase in V will in fact lead to a moderate increase in U.\n\n\nThis has lead to interest in such things as [satisficers](https://en.wikipedia.org/wiki/Satisficing) and [low-impact AIs](https://arxiv.org/abs/1705.10720), [both](https://www.lesswrong.com/posts/2qCxguXuZERZNKcNi/satisficers-want-to-become-maximisers) of [which](https://www.lesswrong.com/posts/mdQEraEZQLg7jtozn/subagents-and-impact-measures-full-and-fully-illustrated) have their problems. Those try and put an *absolute* limit on how much V is optimised. The AI is not supposed to optimise V above a certain limit (satisficer) or if optimising it changes too much about the world or the power of other agents (low-impact).\n\n\nAnother approach is to put a *relative* limit on how much an AI can push a utility function. For example, [quantilizers](https://intelligence.org/files/QuantilizersSaferAlternative.pdf) will choose randomly among the top 0<q≤1 proportion of actions/policies, rather than picking the top action/policy. Then there is the approach of using [pessimism](https://www.lesswrong.com/posts/RzAmPDNciirWKdtc7/pessimism-about-unknown-unknowns-inspires-conservatism) to [make the AI more conservative](https://mkcohen-hosted-files.s3-us-west-1.amazonaws.com/Pessimism_alignmentforum.pdf). This pessimism is defined by a parametre β∈(0,1), with β→1 being very pessimistic.\n\n\nIntermediate value uncertainty\n------------------------------\n\n\nThe behaviours of q and β are pretty clear around the extremes. As β and q tend to 0, the agent will behave like a V-maximiser. As they tend to 1, the agent will behave randomly (q) or totally conservatively (β).\n\n\nThus, we expect that moving away from the extremes will improve the true U-performance, **and** that the conservative, 1 end, will be less disastrous than the V-maximising, 0 end (though we only know that second fact, [because of implicit assumptions we have on U and V](https://www.lesswrong.com/posts/uL74oQv5PsnotGzt7/all-i-know-is-goodhart)).\n\n\nThe problem is in the middle, where the behaviour is unknown (and, since we lack a full formulation of U, generically unknowable). There is no principled way of setting the q or the β. Consider, for example, this plot of q versus U:\n\n\n![](https://www.dropbox.com/s/18712y2gtbwwlg4/quant1.png?raw=1)\n\n\nHere, the ideal q is around 0.45, but the critical thing is to keep q above 0.3: that's the point at which it falls precipitously.\n\n\nContrast now with this one:\n\n\n![](https://www.dropbox.com/s/m9ixjd325klxu1q/quant2.png?raw=1)\n\n\nHere, any value of q above 0.2 is essentially the same, and q can be lowered as low as 0.05 before there are any problems.\n\n\nSo, in the first case, we need q above 0.3, and, in the second, below 0.2. And, moreover, it might be that the first situation appears in one world and the second in another, and both worlds are currently possible. So there's no consistent good value of q we can set (and in the general case, the curve might be multi-modal, with many peaks). And note that we don't know any of these graphs (since we can't define U fully). So we don't know what values to set q at, have little practical guidance on what to do, but expect that some values will be disastrous.\n\n\nThe conservatism approach has similar problems: β is even harder to interpret than q, we don't have any guidance on how to set it, and the ideal β may vary considerably depending on the circumstance. For example, what would we want our AI to do when it finds an unexpected red button connected to nuclear weapons?\n\n\n![](https://www.dropbox.com/s/3rmp89gc4ark2ne/red_button_robot.png?raw=1)\n\n\nWell, that depends on whether the button starts a nuclear launch - or if it cancels one.\n\n\nA future post will explore how to resolve this issue, and how to choose the conservatism parameter in a suitable way.", "url": "https://www.alignmentforum.org/posts/jRHLCyRKsQv5u2Lph/how-conservative-should-the-partial-maximisers-be", "date_published": "2020-04-13T15:50:00Z", "authors": ["Stuart_Armstrong"], "tags": ["AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.734305+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "1b0f1c1c51b7f0cdd9e0508840ec0f47", "source": "alignmentforum", "title": "[AN #95]: A framework for thinking about how to make AI go well", "text": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.\n\nAudio version **[here](http://alignment-newsletter.libsyn.com/alignment-newsletter-95)** (may not be up yet). \n\nHIGHLIGHTS\n==========\n\n**[Current Work in AI Alignment](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment)** *(Paul Christiano)* (summarized by Rohin): In this talk (whose main slide we covered **[before](https://ai-alignment.com/ai-alignment-landscape-d3773c37ae38)** (**[AN #74](https://mailchi.mp/49c956f84771/an-74separating-beneficial-ai-into-competence-alignment-and-coping-with-impacts)**)), Paul Christiano explains how he decomposes the problem of beneficial AI:\n\n1. At the top level, \"make AI go well\" is decomposed into making AI competent, making AI aligned, and coping with the impacts of AI. Paul focuses on the alignment part, which he defines as building AI systems that are *trying* to do what we want. See **[Clarifying \"AI Alignment\"](https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment)** (**[AN #33](https://mailchi.mp/b6dc636f6a1b/alignment-newsletter-33)**) and **[my comment on it](https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment#3ECKoYzFNW2ZqS6km)**. Paul considers many problems of competence as separate from alignment, including understanding humans well, and most reliability / robustness work.\n\n2. Within alignment, we can consider the concept of an \"alignment tax\": the cost incurred by insisting that we only deploy aligned AI. One approach is to help pay the alignment tax, for example, by convincing important actors that they should care about alignment, or by adopting agreements that make it easier to coordinate to pay the tax, as with the **[OpenAI Charter](https://blog.openai.com/openai-charter/)** (**[AN #2](https://mailchi.mp/14782876a85d/alignment-newsletter-2)**)). Technical AI safety research on the other hand can help *reduce* the alignment tax, by creating better aligned AI systems (which consequently incur less cost than before).\n\n3. With alignment tax reduction, we could either try to advance current alignable algorithms (making them more competent, and so reducing their tax), or make existing algorithms alignable. It would be particularly nice to take some general class of algorithms (such as deep reinforcement learning) and figure out how to transform them to make them alignable, such that improvements to the algorithms automatically translate to improvements in the alignable version. This is what Paul works on.\n\n4. The next layer is simply a decomposition of possible algorithms we could try to align, e.g. planning, deduction, and learning. Paul focuses on learning.\n\n5. Within aligned learning, we can distinguish between outer alignment (finding an objective that incentivizes aligned behavior) and inner alignment (ensuring that the trained agent robustly pursues the aligned objective). Paul works primarily on outer alignment, but has **[written about inner alignment](https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d)** (**[AN #81](https://mailchi.mp/6078fe4f9928/an-81-universality-as-a-potential-solution-to-conceptual-difficulties-in-intent-alignment)**).\n\n6. Within outer alignment, we could either consider algorithms that learn from a teacher, such as imitiation learning or preference inference, or we could find algorithms that perform better than the teacher (as would be needed for superhuman performance). Paul focuses on the latter case.\n\n7. To go beyond the teacher, you could extrapolate beyond what you've seen (i.e. generalization), do some sort of **[ambitious value learning](https://www.alignmentforum.org/posts/5eX8ko7GCxwR5N9mN/what-is-ambitious-value-learning)** (**[AN #31](https://mailchi.mp/7d0e3916e3d9/alignment-newsletter-31)**), or build a better teacher. Paul focuses on the last case, and thinks of amplification as a way to achieve this.\n\n**Rohin's opinion:** I really like this decomposition. I already laid out most of my thoughts back when I summarized just the **[main slide](https://ai-alignment.com/ai-alignment-landscape-d3773c37ae38)** (**[AN #74](https://mailchi.mp/49c956f84771/an-74separating-beneficial-ai-into-competence-alignment-and-coping-with-impacts)**); I still endorse them.\n\nTECHNICAL AI ALIGNMENT\n======================\n\nITERATED AMPLIFICATION\n----------------------\n\n**[Unsupervised Question Decomposition for Question Answering](https://arxiv.org/abs/2002.09758)** *(Ethan Perez et al)* (summarized by Zach): Existing methods are proficient at simple question and answering (QA). These simple questions are called single-hop and can be answered with a single yes/no or underlined passage in the text. However, progress on the more difficult task of multi-hop QA lags behind. **This paper introduces a method that can decompose hard multi-hop questions into easier single-hop questions that existing QA systems can answer.** Since collecting labeled decompositions is hard, the authors introduce a pseudo-decomposition where multi-hop questions are matched with similar single-hop questions while making sure the single-hop questions are diverse. Following this, the model is trained to map multi-hop questions to simpler subquestions using *unsupervised* sequence-to-sequence learning (as they found the supervised version performed worse). They show large improvement on the popular HotPot QA baseline with large improvement on out-of-domain questions due to the ability of sub-questions to help gather supporting facts that can be used to answer questions. \n\n**Zach's opinion:** A core feature of this paper is the unsupervised approach to producing question decompositions. By doing this, it's possible to augment the data-set significantly by question-crawling the data-sets which helps explain why the model has performance on-par with supervised approaches. Moreover, looking at a few decomposition examples from the model seems to indicate that relevant sub-questions are being discovered. It's worth noting that decompositions with more than two questions are unlikely due to the specific loss used in the main paper. In the appendix, the authors experiment with a different loss for the pseudo-decomposition that allows more questions in the decomposition, but it performs slightly worse than the original loss. This makes me wonder whether or not such a procedure would be useful if used recursively to create sub-sub-questions. Overall, I think the decomposition is useful for both down-stream processing and interpretation.\n\n**Rohin's opinion:** The capabilities of methods like iterated amplification depend on the ability to solve hard questions by decomposing them into simpler questions that we already know how to answer, and then combining the results appropriately. This paper demonstrates that even a very basic unsupervised approach (\"decompose into the most similar simpler questions\") to decomposition can work quite well, at least for current AI systems.\n\nIn private correspondence, Ethan suggested that in the long term a semi-supervised approach would probably work best, which agrees with my intuitions.\n\nAGENT FOUNDATIONS\n-----------------\n\n**[An Orthodox Case Against Utility Functions](https://www.alignmentforum.org/posts/A8iGaZ3uHNNGgJeaD/an-orthodox-case-against-utility-functions)** *(Abram Demski)* (summarized by Rohin): How might we theoretically ground utility functions? One approach could be to view the possible environments as a set of universe histories (e.g. a list of the positions of all quarks, etc. at all times), and a utility function as a function that maps these universe histories to real numbers. We might want this utility function to be computable, but this eliminates some plausible preferences we might want to represent. For example, in the procrastination paradox, the subject prefers to push the button as late as possible, but disprefers never pressing the button. If the history is infinitely long, no computable function can know for sure that the button was never pressed: it's always possible that it was pressed at some later day.\n\nInstead, we could use *subjective utility functions*, which are defined over *events*, which is basically anything you can think about (i.e. it could be chairs and tables, or quarks and strings). This allows us to have utility functions over high level concepts. In the previous example, we can define an event \"never presses the button\", and reason about that event atomically, sidestepping the issues of computability.\n\nWe could go further and view *probabilities* as subjective (as in the Jeffrey-Bolkor axioms), and only require that our beliefs are updated in such a way that we cannot be Dutch-booked. This is the perspective taken in logical induction.\n\nINTERPRETABILITY\n----------------\n\n**[Neuron Shapley: Discovering the Responsible Neurons](https://arxiv.org/abs/2002.09815)** *(Amirata Ghorbani et al)* (summarized by Robert): This paper presents a novel method, Neuron Shapley, that uses the **[Shapley value framework](https://en.wikipedia.org/wiki/Shapley_value)** to measure the importance of different neurons in determining an arbitrary metric of the neural net output. (Shapley values have been applied to machine learning before to **[measure the importance of features to a model's output](https://christophm.github.io/interpretable-ml-book/shapley.html)**, but here the authors use them to calculate neuron importance.) Due to several novel approaches and optimisations in calculating these Shapley values, **the top k most responsible neurons (k ~ 30) can be feasibly found for large networks such as Inception-v3**.\n\nThe authors demonstrate that finding these neurons enables the performance of model surgery. Removing the top 30 neurons that contribute to accuracy completely destroys the accuracy, whereas in expectation removing 30 neurons at random from the network barely moves the accuracy at all. Since the method can be applied to an arbitrary metric, this kind of surgery can be performed for other metrics we care about. For example, removing the neurons which are most responsible for vulnerability to adversarial attacks makes the network more robust, and removing the neurons most responsible for the class-accuracy imbalance (a fairness metric) makes the classes much more even, while only reducing the overall accuracy by a small amount.\n\n**Robert's opinion:** It's nice to see an interpretability method with demonstrable and measurable use cases. Many methods aim at improving insight, but often don't demonstrate this aim; I think this paper does this well in showing how its method can be used for model surgery. I think methods that allow us to investigate and understand individual neurons and their contributions are useful in building up a fine grained picture of how neural networks work. This links to previous work such as **[Network Dissection](http://netdissect.csail.mit.edu/)** as well as the recent **[Circuits Thread](https://distill.pub/2020/circuits/)** on Distill, and I'd love to see how these methods interact. They all give different kinds of understanding, and I think it would be interesting to see if given the results of the circuits tools we were able to predict which neurons where most responsible for different metrics (Neuron Shapley) or aligned to which relevant features (Network Dissection).\n\n**[Visualizing Neural Networks with the Grand Tour](https://distill.pub/2020/grand-tour/)** *(Mingwei Li et al)* (summarized by Flo): Visualizing a complete dataset instead of single input examples is helpful when we want to analyze the relationships between different input examples and how their classification changes during training, as we can do so by looking at a single video. \n\nThe authors use an example on MNIST in which the network learns to classify the numbers 1 and 7 in an almost discrete fashion during particular epochs to compare different methods for visualizing how the dataset is classified. They find that one problem with nonlinear dimensionality reduction like t-SNE and UMAPs is that changes to a subset of the dataset can strongly affect how unchanged data points are represented. Then they compare this to the Grand Tour, a classical technique that projects the data into two dimensions from varying points of view. As projections are linear in the input variables, it is rather easy to reason about how changes in the data affect this visualization and the times the classes 1 and 7 are learnt are indeed quite salient in their example. Another advantage of this method is that confusion between two specific classes can be identified more easily, as the corresponding data points will be projected onto the line connecting the clusters for these classes. A similar approach can be taken on a network's hidden layers to identify the layer in which different classes become clearly distinguishable. They find that they can identify adversarial examples generated by FGSM by looking at the second to last layer, where the adversarial examples form a cluster distinct from the real images. \n\nAs the Grand Tour involves varying rotations, it is basically unaffected by rotations of the data. The authors argue that this is a feature, as rotations are small changes to the data and should not have a large effect on the visualization.\n\n**Flo's opinion:** The dataset perspective on visualization seems pretty useful as a quick diagnostic tool for practitioners, but less useful than feature visualization for a detailed understanding of a model. While I think that it is good to highlight invariances, I am not convinced that rotational invariance is actually desirable for visualizing intermediate layers of a neural network, as most nonlinearities are strongly affected by rotations. \n\nFORECASTING\n-----------\n\n**[Atari early](https://aiimpacts.org/atari-early/)** *(Katja Grace)* (summarized by Rohin): With DeepMind's Agent57 (summarized below), it seems that it is feasible to outperform professional game testers on all Atari games using no game-specific knowledge. Interestingly, in a 2016 survey, the median response put a small chance (10%) on this being feasible by 2021, and a medium chance (50%) of being feasible by 2026.\n\nOTHER PROGRESS IN AI\n====================\n\nREINFORCEMENT LEARNING\n----------------------\n\n**[Agent57: Outperforming the human Atari benchmark](https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark)** *(Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann et al)* (summarized by Sudhanshu): This blogpost and its associated **[arxiv publication](https://arxiv.org/abs/2003.13350)** present Agent57, DeepMind's latest RL agent created for the purpose of achieving human-level performance in a suite of 57 Atari games. Notably, Agent57 is the first agent that is able to surpass average human performance, as measured by Human Normalized Score or HNS, on every individual game in the suite, with the same set of hyperparameters. The blogpost details the evolution of DeepMind's Atari agents from DQN up to Agent57, and the paper elaborates on the improvements made in Agent57.\n\nSpecifically, Agent57 builds on a recent agent 'Never Give Up' (NGU), which itself augments R2D2 with episodic memory for curiosity-driven exploration. Agent57 introduces (i) a new parameterization of state-action value function that decomposes into intrinsic and extrinsic rewards, and (ii) a meta-controller which selects which of its numerous distributed policies to prioritize during learning, allowing the agent to control the exploration/exploitation trade-off.\n\n**Read more:** **[Paper: Agent57: Outperforming the Atari Human Benchmark](https://arxiv.org/abs/2003.13350)**\n\n**Sudhanshu's opinion:** On the one hand, this work feels like the achievement of an important milestone in DeepMind's ongoing research agenda towards building more general agents. On the other hand, it has the flavour of engineered sophistry: a remarkable collection of building blocks arranged together to patch specific known weaknesses, but lacking in core insights about how to make agents more general, without, say, making them more complex.\n\nThe work is well presented and accessible, especially the blogpost that contains a snapshot of the functional development of deep reinforcement learning capabilities over time. There are several open questions from here on out; personally, I hope this progresses to a single instance of an agent that is proficient at multiple games, and to the design of agents that do not require extensive hyperparameter tuning. The scale of DeepMind's experiments continues to grow, with 256 actors, and 10s of billions of frames, suggesting that, for now, this work is only suitable for simulated environments.\n\n**[Massively Scaling Reinforcement Learning with SEED RL](https://ai.googleblog.com/2020/03/massively-scaling-reinforcement.html)** *(Lasse Espeholt, Raphaël Marinier, Piotr Stanczyk et al)* (summarized by Nicholas): Deep learning has **[historically](https://blog.openai.com/ai-and-compute/)** (**[AN #7](https://mailchi.mp/3e550712419a/alignment-newsletter-7)**) seen many improvements as a result of scaling to larger models with larger amounts of computation, as with the months-long training of **[OpenAI Five](http://arxiv.org/abs/1912.06680)** (**[AN #82](https://mailchi.mp/7ba40faa7eed/an-82-how-openai-five-distributed-their-training-computation)**) and **[AlphaStar](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/)** (**[AN #43](https://mailchi.mp/768a8130013f/alignment-newsletter-43)**). SEED RL redesigns the architecture of distributed RL to enable better machine utilization and communication and achieves an order of magnitude improvement in training speed. \n\nCurrent distributed architectures typically separate machines into *actors* and *learners*. *Actors* are typically CPUs that simulate the environment, and run inference to predict agent actions. They then send *trajectories* to the *learners*. *Learners* are typically accelerators (GPUs or TPUs), which are responsible for training the model. They then send the updated model parameters to the *actors*. \n\nSEED RL addresses 3 main issues in this setup:\n\n1. Inference could benefit from specialized accelerators\n\n2. Sending model parameters and states requires high bandwidth.\n\n3. Environment simulation and inference are very different tasks and having them on the same machine makes it hard to utilize the resource efficiently. \n\nThe solution is to instead have actors **only** simulate the environment. After each step, they send the resulting observation to the *learner*, which is responsible for both training and inference, possibly split on separate hardware. It then sends back just the actions to the environment. This enables each piece of hardware to be used for its designed purpose. Since they now need to communicate at each step, they use gRPC to minimize latency.\n\n**Read more:** **[Paper: SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference](https://arxiv.org/abs/1910.06591)**\n\n**Nicholas' opinion:** Given how compute-intensive deep RL is, I think it is quite useful to enable cheaper and faster training before these algorithms can be broadly useful. Their claimed speedup is quite impressive, and I like how well they can separate the training and inference from the simulation. I expect that specialized hardware for both training and inference will soon become the norm and SEED RL seems like it will scale well as those accelerators become faster. One thing to note is that this architecture seems very specifically tuned to the problem of games where CPUs can efficiently simulate the environment and it does not improve the sample efficiency for situations where we can’t run lots of simulations.\n\n**Rohin's opinion:** It was quite surprising to me that this worked as well as it did: this model requires communication across machines *at every timestep of the environment*, which intuitively means that latency should be a major bottleneck, while the standard model only requires communication once per batch of trajectories.\n\nDEEP LEARNING\n-------------\n\n**[AutoML-Zero: Evolving Machine Learning Algorithms From Scratch](http://arxiv.org/abs/2003.03384)** *(Esteban Real, Chen Liang et al)* (summarized by Sudhanshu): Most previous work in the area of automated machine learning, or AutoML, has focussed on narrow search spaces that are restricted to specific parts of the machine learning pipeline, e.g. the architecture of a neural network, or the optimizer in meta-learning. These spaces are often so constrained by the hand-engineered components around them that architectures and algorithms discovered, say, by evolutionary search (ES), are only slightly better than random search (RS). This work aims to set up the problem with very weak constraints and a wide search space: a) a machine learning program has three component functions, *Setup*, *Predict*, and *Learn*, which start out empty, and b) are populated by RS or ES with procedural operations from over 50 arithmetic, trigonometric, linear algebra, probability, and pre-calculus operators.\n\nThey demonstrate that with such a vast search space, RS fares very poorly in comparison to ES. They also report that ES finds several procedures that are recognizable as useful for machine learning, such as a simple neural network, gradient descent, gradient normalization, multiplicative interactions, noise augmentation, noisy dropout and learning rate decay.\n\n**Sudhanshu's opinion:** This work empirically demonstrates that we now have sufficient methods and tricks in our ES toolkit that enable us to evolve machine learning algorithms from scratch. Additionally, this process produces computer code, which itself may yield to theoretical analysis furthering our knowledge of learning algorithms. I think that powerful AI systems of the future may employ such techniques to discover solutions.\n\nNEWS\n====\n\n**[Announcing Web-TAISU, May 13-17](https://www.alignmentforum.org/posts/CMnMaTxNAhXfcEtgm/announcing-web-taisu-may-13-17)** *(Linda Linsefors)* (summarized by Rohin): The **[Technical AI Safety Unconference](https://www.lesswrong.com/events/yuMuDGnJ8omGhMx9y/taisu-technical-ai-safety-unconference)** (**[AN #57](https://mailchi.mp/392d2043e782/an-57why-we-should-focus-on-robustness-in-ai-safety-and-the-analogous-problems-in-programming)**) will be held online from May 13-17.\n\n#### **FEEDBACK**\n\nI'm always happy to hear feedback; you can send it to me, **[Rohin Shah](https://rohinshah.com/)**, by **replying to this email**. \n\n#### **PODCAST**\n\nAn audio podcast version of the **Alignment Newsletter** is available. This podcast is an audio version of the newsletter, recorded by **[Robert Miles](http://robertskmiles.com/)**.", "url": "https://www.alignmentforum.org/posts/9et86yPRk6RinJNt3/an-95-a-framework-for-thinking-about-how-to-make-ai-go-well", "date_published": "2020-04-15T17:10:03Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.734710+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "f470eb8b93f5a70cc394dc7aa1eca638", "source": "alignmentforum", "title": "AI Alignment Podcast: An Overview of Technical AI Alignment in 2018 and 2019 with Buck Shlegeris and Rohin Shah", "text": "Just a year ago we released a two part episode titled [An Overview of Technical AI Alignment with Rohin Shah](https://futureoflife.org/2019/04/11/an-overview-of-technical-ai-alignment-with-rohin-shah-part-1/). That conversation provided details on the views of central AI alignment research organizations and many of the ongoing research efforts for designing safe and aligned systems. Much has happened in the past twelve months, so we've invited Rohin — along with fellow researcher Buck Shlegeris — back for a follow-up conversation. Today's episode focuses especially on the state of current research efforts for beneficial AI, as well as Buck's and Rohin's thoughts about the varying approaches and the difficulties we still face. This podcast thus serves as a non-exhaustive overview of how the field of AI alignment has updated and how thinking is progressing.\n\n**Topics discussed in this episode include:**\n\n* Rohin's and Buck's optimisms and pessimism about different approaches to aligned AI\n* Traditional arguments for AI as an x-risk\n* Modeling agents as expected utility maximizers\n* Ambitious value learning and specification learning/narrow value learning\n* Agency and optimization\n* Robustness\n* Scaling to superhuman abilities\n* Universality\n* Impact regularization\n* Causal models, oracles, and decision theory\n* Discontinuous and continuous takeoff scenarios\n* Probability of AI-induced existential risk\n* Timelines for AGI\n* Information hazards\n\nYou can find the page for this podcast here: <https://futureoflife.org/2020/04/15/an-overview-of-technical-ai-alignment-in-2018-and-2019-with-buck-shlegeris-and-rohin-shah/> \n\n **Transcript**\n===============\n\n**Note:**The following transcript has been edited for style and clarity.\n\n  \n**Lucas Perry:** Welcome to the AI Alignment Podcast. I’m Lucas Perry. Today we have a special episode with Buck Shlegeris and Rohin Shah that serves as a review of progress in technical AI alignment over 2018 and 2019. This episode serves as an awesome birds eye view of the varying focus areas of technical AI alignment research and also helps to develop a sense of the field. I found this conversation to be super valuable for helping me to better understand the state and current trajectory of technical AI alignment research. This podcast covers traditional arguments for AI as an x-risk, what AI alignment is, the modeling of agents as expected utility maximizers, iterated distillation and amplification, AI safety via debate, agency and optimization, value learning, robustness, scaling to superhuman abilities, and more. The structure of this podcast is based on Rohin’s AI Alignment Forum post titled AI Alignment 2018-19 Review. That post is an excellent resource to take a look at in addition to this podcast. Rohin also had a conversation with us about just a year ago titled An Overview of Technical AI Alignment with Rohin shah. This episode serves as a follow up to that overview and as an update to what's been going on in the field. You can find a link for it on the page for this episode.  \n\nBuck Shlegeris is a researcher at the Machine Intelligence Research Institute. He tries to work to make the future good for sentient beings and currently believes that working on existential risk from artificial intelligence is the best way of doing this. Buck worked as a software engineer at PayPal before joining MIRI, and was the first employee at Triplebyte. He previously studied at the Australian National University, majoring in CS and minoring in math and physics, and he has presented work on data structure synthesis at industry conferences.\n\nRohin Shah is a 6th year PhD student in Computer Science at the Center for Human-Compatible AI at UC Berkeley. He is involved in Effective Altruism and was the co-president of EA UC Berkeley for 2015-16 and ran EA UW during 2016-2017. Out of concern for animal welfare, Rohin is almost vegan because of the intense suffering on factory farms. He is interested in AI, machine learning, programming languages, complexity theory, algorithms, security, and quantum computing to name a few. Rohin’s research focuses on building safe and aligned AI systems that pursue the objectives their users intend them to pursue, rather than the objectives that were literally specified. He also publishes the Alignment Newsletter, which summarizes work relevant to AI alignment. The Alignment Newsletter is something I highly recommend that you follow in addition to this podcast.  \n\nAnd with that, let’s get into our review of AI alignment with Rohin Shah and Buck Shlegeris.\n\nTo get things started here, the plan is to go through Rohin's post on the Alignment Forum about AI Alignment 2018 and 2019 In Review. We'll be using this as a way of structuring this conversation and as a way of moving methodically through things that have changed or updated in 2018 and 2019, and to use those as a place for conversation. So then, Rohin, you can start us off by going through this document. Let's start at the beginning, and we'll move through sequentially and jump in where necessary or where there is interest.\n\n**Rohin Shah:** Sure, that sounds good. I think I started this out by talking about this basic analysis of AI risk that's been happening for the last couple of years. In particular, you have these traditional arguments, so maybe I'll just talk about the traditional argument first, which basically says that the AI systems that we're going to build are going to be powerful optimizers. When you optimize something, you tend to get these sort of edge case outcomes, these extreme outcomes that are a little hard to predict ahead of time.\n\nYou can't just rely on tests with less powerful systems in order to predict what will happen, and so you can't rely on your normal common sense reasoning in order to deal with this. In particular, powerful AI systems are probably going to look like expected utility maximizers due to various coherence arguments, like the Von Neumann–Morgenstern rationality theorem, and these expected utility maximizers have convergent instrumental sub-goals, like not wanting to be switched off because then they can't achieve their goal, and wanting to accumulate a lot of power and resources.\n\nThe standard argument goes, because AI systems are going to be built this way, they will have these convergent instrumental sub-goals. This makes them dangerous because they will be pursuing goals that we don't want.\n\n**Lucas Perry:** Before we continue too much deeper into this, I'd want to actually start off with a really simple question for both of you. What is AI alignment?\n\n**Rohin Shah:** Different people mean different things by it. When I use the word alignment, I'm usually talking about what has been more specifically called intent alignment, which is basically aiming for the property that the AI system is trying to do what you want. It's trying to help you. Possibly it doesn't know exactly how to best help you, and it might make some mistakes in the process of trying to help you, but really what it's trying to do is to help you.\n\n**Buck Shlegeris:** The way I would say what I mean by AI alignment, I guess I would step back a little bit, and think about why it is that I care about this question at all. I think that the fundamental fact which has me interested in anything about powerful AI systems of the future is that I think they'll be a big deal in some way or another. And when I ask myself the question “what are the kinds of things that could be problems about how these really powerful AI systems work or affect the world”, one of the things which feels like a problem is that, we might not  know how to apply these systems reliably to the kinds of problems which we care about, and so by default humanity will end up applying them in ways that lead to really bad outcomes. And so I guess, from that perspective, when I think about AI alignment, I think about trying to make ways of building AI systems such that we can apply them to tasks that are valuable, such that that they'll reliably pursue those tasks instead of doing something else which is really dangerous and bad.\n\nI’m fine with intent alignment as the focus. I kind of agree with, for instance, Paul Christiano, that it's not my problem if my AI system incompetently kills everyone, that's the capability's people's problem. I just want to make the system so it's trying to cause good outcomes.\n\n**Lucas Perry:** Both of these understandings of what it means to build beneficial AI or aligned AI systems can take us back to what Rohin was just talking about, where there's this basic analysis of AI risk, about AI as powerful optimizers and the associated risks there. With that framing and those definitions, Rohin, can you take us back into this basic analysis of AI risk?\n\n**Rohin Shah:** Sure. The traditional argument looks like AI systems are going to be goal-directed. If you expect that your AI system is going to be goal-directed, and that goal is not the one that humans care about, then it's going to be dangerous because it's going to try to gain power and resources with which to achieve its goal.\n\nIf the humans tried to turn it off, it's going to say, \"No, don't do that,\" and it's going to try to take actions that avoid that. So it pits the AI and the humans in an adversarial game with each other, and you ideally don't want to be fighting against a superintelligent AI system. That seems bad.\n\n**Buck Shlegeris:** I feel like Rohin is to some extent setting this up in a way that he's then going to argue is wrong, which I think is kind of unfair. In particular, Rohin, I think you're making these points about VNM theorems and stuff to set up the fact that it seems like these arguments don't actually work. I feel that this makes it kind of unfairly sound like the earlier AI alignment arguments are wrong. I think this is an incredibly important question, of whether early arguments about the importance of AI safety were quite flawed. My impression is that overall the early arguments about AI safety were pretty good. And I think it's a very interesting question whether this is in fact true. And I'd be interested in arguing about it, but I think it’s the kind of thing that ought to be argued about explicitly.\n\n**Rohin Shah:** Yeah, sure.\n\n**Buck Shlegeris:** And I get that you were kind of saying it narratively, so this is only a minor complaint. It's a thing I wanted to note.\n\n**Rohin Shah:** I think my position on that question of “how good were the early AI risk arguments,” probably people's internal beliefs were good as to why AI was supposed to be risky, and the things they wrote down were not very good. Some things were good and some things weren't. I think [Intelligence Explosion Microeconomics](https://intelligence.org/files/IEM.pdf) was good. I think [AI Alignment: Why It's Hard and Where to Start](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/), was misleading.\n\n**Buck Shlegeris:** I think I agree with your sense that people probably had a lot of reasonable beliefs but that the written arguments seem flawed. I think another thing that's true is that random people like me who were on LessWrong in 2012 or something, ended up having a lot of really stupid beliefs about AI alignment, which I think isn't really the fault of the people who were thinking about it the best, but is maybe sociologically interesting.\n\n**Rohin Shah:** Yes, that seems plausible to me. Don't have a strong opinion on it.\n\n**Lucas Perry:** To provide a little bit of framing here and better analysis of basic AI x-risk arguments, can you list what the starting arguments for AI risk were?\n\n**Rohin Shah:** I think I am reasonably well portraying what the written arguments were. Underlying arguments that people probably had would be something more like, \"Well, it sure seems like if you want to do useful things in the world, you need to have AI systems that are pursuing goals.\" If you have something that's more like tool AI, like Google Maps, that system is going to be good at the one thing it was designed to do, but it's not going to be able to learn and then apply its knowledge to new tasks autonomously. It sure seems like if you want to do really powerful things in the world, like run companies or make policies, you probably do need AI systems that are constantly learning about their world and applying their knowledge in order to come up with new ways to do things.\n\nIn the history of human thought, we just don't seem to know of a way to cause that to happen except by putting goals in systems, and so probably AI systems are going to be goal-directed. And one way you can formalize goal-directedness is by thinking about expected utility maximizers, and people did a bunch of formal analysis of that. Mostly going to ignore it because I think you can just say all the same thing with the idea of pursuing goals and it's all fine.\n\n**Buck Shlegeris:** I think one important clarification to that, is you were saying the reason that tool AIs aren't just the whole story of what happens with AI is that you can't apply it to all problems. I think another important element is that people back then, and I now, believe that if you want to build a really good tool, you're probably going to end up wanting to structure that as an agent internally. And even if you aren't trying to structure it as an agent, if you're just searching over lots of different programs implicitly, perhaps by training a really large recurrent policy, you're going to end up finding something agent shaped.\n\n**Rohin Shah:** I don't disagree with any of that. I think we were using the words tool AI differently.\n\n**Buck Shlegeris:** Okay.\n\n**Rohin Shah:** In my mind, if we're talking about tool AI, we're imagining a pretty restricted action space where no matter what actions in this action space are taken, with high probability, nothing bad is going to happen. And you'll search within that action space, but you don't go to arbitrary action in the real world or something like that. This is what makes tool AI hard to apply to all problems.\n\n**Buck Shlegeris:** I would have thought that's a pretty non-standard use of the term tool AI.\n\n**Rohin Shah:** Possibly.\n\n**Buck Shlegeris:** In particular, I would have thought that restricting the action space enough that you're safe, regardless of how much it wants to hurt you, seems kind of non-standard.\n\n**Rohin Shah:** Yes. I have never really liked the concept of tool AI very much, so I kind of just want to move on.\n\n**Lucas Perry:** Hey, It’s post-podcast Lucas here. I just want to highlight here a little bit of clarification that Rohin was interested in adding, which is that he thinks that “tool AI evokes a sense of many different properties that he doesn't know which properties most people are  usually thinking about and as a result he prefers not to use the phrase tool AI. And instead would like to use more precise terminology. He doesn’t necessarily feel though that the concepts underlying tool AI are useless.” So let's tie things a bit back to these basic arguments for x-risk that many people are familiar with, that have to do with convergent instrumental sub-goals and the difficulty of specifying and aligning systems with our goals and what we actually care about in our preference hierarchies.\n\nOne of the things here that Buck was seeming to bring up, he was saying that you may have been narratively setting up the Von Neumann–Morgenstern theorem, which sets up AIs as expected utility maximizers, and that you are going to argue that that argument, which is sort of the formalization of these earlier AI risk arguments, that that is less convincing to you now than it was before, but Buck still thinks that these arguments are strong. Could you unpack this a little bit more or am I getting this right?\n\n**Rohin Shah:** To be clear, I also agree with Buck, that the spirit of the original arguments does seem correct, though, there are people who disagree with both of us about that. Basically, the VNM theorem roughly says, if you have preferences over a set of outcomes, and you satisfy some pretty intuitive axioms about how you make decisions, then you can represent your preferences using a utility function such that your decisions will always be, choose the action that maximizes the expected utility. This is, at least in writing, given as a reason to expect that AI systems would be maximizing expected utility. The thing is, when you talk about AI systems that are acting in the real world, they're just selecting a universe history, if you will. Any observed behavior is compatible with the maximization of some utility function. Utility functions are a really, really broad class of things when you apply it to choosing from universe histories.\n\n**Buck Shlegeris:** An intuitive example of this: suppose that you see that every day I walk home from work in a really inefficient way. It's impossible to know whether I'm doing that because I happened to really like that path. For any sequence of actions that I take, there’s some utility functions such that that was the optimal sequence of actions. And so we don't actually learn anything about how my policy is constrained based on the fact that I'm an expected utility maximizer.\n\n**Lucas Perry:** Right. If I only had access to your behavior and not your insides.\n\n**Rohin Shah:** Yeah, exactly. If you have a robot twitching forever, that's all it does, there is a utility function over a universe history that says that is the optimal thing to do. Every time the robot twitches to the right, it's like, yeah, the thing that was optimal to do at that moment in time was twitching to the right. If at some point somebody takes a hammer and smashes the robot and it breaks, then the utility function that corresponds to that being optimal is like, yeah, that was the exact right moment to break down.\n\nIf you have these pathologically complex utility functions as possibilities, every behavior is compatible with maximizing expected utility, you might want to say something like, probably we'll have the simple utility maximizers, but that's a pretty strong assumption, and you'd need to justify it somehow. And the VNM theorem wouldn't let you do that.\n\n**Lucas Perry:** So is the problem here that you're unable to fully extract human preference hierarchies from human behavior?\n\n**Rohin Shah:** Well, you're unable to extract agent preferences from agent behavior. You can see any agent behavior and you can rationalize it as expected utility maximization, but it's not very useful. Doesn't give you predictive power.\n\n**Buck Shlegeris:** I just want to have my go at saying this argument in three sentences. Once upon a time, people said that because all rational systems act like they're maximizing an expected utility function, we should expect them to have various behaviors like trying to maximize the amount of power they have. But every set of actions that you could take is consistent with being an expected utility maximizer, therefore you can't use the fact that something is an expected utility maximizer in order to argue that it will have a particular set of behaviors, without making a bunch of additional arguments. And I basically think that I was wrong to be persuaded by the naive argument that Rohin was describing, which just goes directly from rational things are expected utility maximizers, to therefore rational things are power maximizing.\n\n**Rohin Shah:** To be clear, this was the thing I also believed. The main reason I wrote the post that argued against it was because I spent half a year under the delusion that this was a valid argument.\n\n**Lucas Perry:** Just for my understanding here, the view is that because any behavior, any agent from the outside can be understood as being an expected utility maximizer, that there are behaviors that clearly do not do instrumental sub-goal things, like maximize power and resources, yet those things can still be viewed as expected utility maximizers from the outside. So additional arguments are required for why expected utility maximizers do instrumental sub-goal things, which are AI risky.\n\n**Rohin Shah:** Yeah, that's exactly right.\n\n**Lucas Perry:** Okay. What else is on offer other than expected utility maximizers? You guys talked about comprehensive AI services might be one. Are there other formal agentive classes of 'thing that is not an expected utility maximizer but still has goals?'\n\n**Rohin Shah:** A formalism for that? I think some people like John Wentworth is for example, thinking about markets as a model of agency. Some people like to think of multi-agent groups together leading to an emergent agency and want to model human minds this way. How formal are these? Not that formal yet.\n\n**Buck Shlegeris:** I don't think there's anything which is competitively popular with expected utility maximization as the framework for thinking about this stuff.\n\n**Rohin Shah:** Oh yes, certainly not. Expected utility maximization is used everywhere. Nothing else comes anywhere close.\n\n**Lucas Perry:** So there's been this complete focus on utility functions and representing the human utility function, whatever that means. Do you guys think that this is going to continue to be the primary way of thinking about and modeling human preference hierarchies? How much does it actually relate to human preference hierarchies? I'm wondering if it might just be substantially different in some way.\n\n**Buck Shlegeris:** Me and Rohin are going to disagree about this. I think that trying to model human preferences as a utility function is really dumb and bad and will not help you do things that are useful. I don't know; If I want to make an AI that's incredibly good at recommending me movies that I'm going to like, some kind of value learning thing where it tries to learn my utility function over movies is plausibly a good idea. Even things where I'm trying to use an AI system as a receptionist, I can imagine value learning being a good idea.\n\nBut I feel extremely pessimistic about more ambitious value learning kinds of things, where I try to, for example, have an AI system which learns human preferences and then acts in large scale ways in the world. I basically feel pretty pessimistic about every alignment strategy which goes via that kind of a route. I feel much better about either trying to not use AI systems for problems where you have to think about large scale human preferences, or having an AI system which does something more like modeling what humans would say in response to various questions and then using that directly instead of trying to get a value function out of it.\n\n**Rohin Shah: Y**eah. Funnily enough, I was going to start off by saying I think Buck and I are going to agree on this.\n\n**Buck Shlegeris:** Oh.\n\n**Rohin Shah:** And I think I mostly agree with the things that you said. The thing I was going to say was I feel pretty pessimistic about trying to model the normative underlying human values, where you have to get things like population ethics right, and what to do with the possibility of infinite value. How do you deal with fanaticism? What's up with moral uncertainty? I feel pretty pessimistic about any sort of scheme that involves figuring that out before developing human-level AI systems.\n\nThere's a related concept which is also called value learning, which I would prefer to be called something else, but I feel like the name's locked in now. In my sequence, I called it narrow value learning, but even that feels bad. Maybe at least for this podcast we could call it specification learning, which is sort of more like the tasks Buck mentioned, like if you want to learn preferences over movies, representing that using a utility function seems fine.\n\n**Lucas Perry:** Like superficial preferences?\n\n**Rohin Shah:** Sure. I usually think of it as you have in mind a task that you want your AI system to do, and now you have to get your AI system to reliably do it. It's unclear whether this should even be called a value learning at this point. Maybe it's just the entire alignment problem. But techniques like inverse reinforcement learning, preference learning, learning from corrections, inverse reward design where you learn from a proxy reward, all of these are more trying to do the thing where you have a set of behaviors in mind, and you want to communicate that to the agent.\n\n**Buck Shlegeris:** The way that I've been thinking about how optimistic I should be about value learning or specification learning recently has been that I suspect that at the point where AI is human level, by default we'll have value learning which is about at human level. We're about as good at giving AI systems information about our preferences that it can do stuff with as we are giving other humans information about our preferences that we can do stuff with. And when I imagine hiring someone to recommend music to me, I feel like there are probably music nerds who could do a pretty good job of looking at my Spotify history, and recommending bands that I'd like if they spent a week on it. I feel a lot more pessimistic about being able to talk to a philosopher for a week, and then them answer hard questions about my preferences, especially if they didn't have the advantage of already being humans themselves.\n\n**Rohin Shah:** Yep. That seems right.\n\n**Buck Shlegeris:** So maybe that's how I would separate out the specification learning stuff that I feel optimistic about from the more ambitious value learning stuff that I feel pretty pessimistic about.\n\n**Rohin Shah:** I do want to note that I collated a bunch of stuff arguing against ambitious value learning. If I had to make a case for optimism about even that approach, it would look more like, \"Under the value learning approach, it seems possible with uncertainty over rewards, values, preferences, whatever you want to call them to get an AI system such that you actually are able to change it, because it would reason that if you're trying to change it, well then that means something about it is currently not good for helping you and so it would be better to let itself be changed. I'm not very convinced by this argument.\"\n\n**Buck Shlegeris:** I feel like if you try to write down four different utility functions that the agent is uncertain between, I think it's just actually really hard for me to imagine concrete scenarios where the AI is corrigible as a result of its uncertainty over utility functions. Imagine the AI system thinks that you're going to switch it off and replace it with an AI system which has a different method of inferring values from your actions and your words. It's not going to want to let you do that, because its utility function is to have the world be the way that is expressed by your utility function as estimated the way that it approximates utility functions. And so being replaced by a thing which estimates utility functions or infers utility functions some other way means that it's very unlikely to get what it actually wants, and other arguments like this. I'm not sure if these are super old arguments that you're five levels of counter-arguments to.\n\n**Rohin Shah:** I definitely know this argument. I think the problem of fully updated deference is what I would normally point to as representing this general class of claims and I think it's a good counter argument. When I actually think about this, I sort of start getting confused about what it means for an AI system to terminally value the final output of what its value learning system would do. It feels like some additional notion of how the AI chooses actions has been posited, that hasn't actually been captured in the model and so I feel fairly uncertain about all of these arguments and kind of want to defer to the future. \n\n**Buck Shlegeris:** I think the thing that I'm describing is just what happens if you read the algorithm literally. Like, if you read the value learning algorithm literally, it has this notion of the AI system wants to maximize the human's actual utility function.\n\n**Rohin Shah:** For an optimal agent playing a CIRL (cooperative inverse reinforcement learning) game, I agree with your argument. If you take optimality as defined in the cooperative inverse reinforcement learning paper and it's playing over a long period of time, then yes, it's definitely going to prefer to keep itself in charge rather than a different AI system that would infer values in a different way.\n\n**Lucas Perry:** It seems like so far utility functions are the best way of trying to get an understanding of what human beings care about and value and have preferences over, you guys are bringing up all of the difficult intricacies with trying to understand and model human preferences as utility functions. One of the things that you also bring up here, Rohin, in your review, is the risk of lock-in, which may require us to solve hard philosophical problems before the development of AGI. That has something to do with ambitious value learning, which would be like learning the one true human utility function which probably just doesn't exist.\n\n**Buck Shlegeris:** I think I want to object to a little bit of your framing there. My stance on utility functions of humans isn't that there are a bunch of complicated subtleties on top, it's that modeling humans with utility functions is just a really sad state to be in. If your alignment strategy involves positing that humans behave as expected utility maximizers, I am very pessimistic about it working in the short term, and I just think that we should be trying to completely avoid anything which does that. It's not like there's a bunch of complicated sub-problems that we need to work out about how to describe us as expected utility maximizers, my best guess is that we would just not end up doing that because it's not a good idea.\n\n**Lucas Perry:** For the ambitious value learning?\n\n**Buck Shlegeris:** Yeah, that's right.\n\n**Lucas Perry:** Okay, do you have something that's on offer?\n\n**Buck Shlegeris:** The two options instead of that, which seem attractive to me? As I said earlier, one is you just convince everyone to not use AI systems for things where you need to have an understanding of large scale human preferences. The other one is the kind of thing that Paul Christiano's iterated distillation and amplification, or a variety of his other ideas, the kind of thing that he's trying to get there is, I think, if you make a really powerful AI system, it's actually going to have an excellent model of human values in whatever representation is best for actually making predictions about  humans because a really excellent AGI, like a really excellent paperclip maximizer, it's really important for it to really get how humans work so that it can manipulate them into letting it build lots of paperclip factories or whatever.\n\nSo I think that if you think that we have AGI, then by assumption I think we have a system which is able to reason about human values if it wants. And so if we can apply these really powerful AI systems to tasks such that the things that they do display their good understanding of human values, then we're fine and it's just okay that there was no way that we could represent a utility function directly. So for instance, the idea in IDA is that if we could have this system which is just trying to answer questions the same way that humans would, but enormously more cheaply because it can run faster than humans and a few other tricks, then we don't have to worry about writing down a utility functions of humans directly because we can just make the system do things that are kind of similar to the things humans would have done, and so it implicitly has this human utility function built into it. That's option two. Option one is don't use anything that requires a complex human utility function, option two is have your systems learn human values implicitly, by giving them a task such that this is beneficial for them and such that their good understanding of human values comes out in their actions.\n\n**Rohin Shah:** One way I might condense that point, is that you're asking for a nice formalism for human preferences and I just point to all the humans out there in the world who don't know anything about utility functions, which is 99% of them and nonetheless still seem pretty good at inferring human preferences.\n\n**Lucas Perry:** On this part about AGI, if it is AGI it should be able to reason about human preferences, then why would it not be able to construct something that was more explicit and thus was able to do more ambitious value learning?\n\n**Buck Shlegeris:** So it can totally do that, itself. But we can't force that structure from the outside with our own algorithms.\n\n**Rohin Shah:** Image classification is a good analogy. Like, in the past we were using hand engineered features, namely SIFT and HOG and then training classifiers over these hand engineered features in order to do image classification. And then we came to the era of deep learning and we just said, yeah, throw away all those features and just do everything end to end with a convolutional neural net and it worked way better. The point was that, in fact there are good representations for most tasks and humans trying to write them down ahead of time just doesn't work very well at that. It tends to work better if you let the AI system discover its own representations that best capture the thing you wanted to capture.\n\n**Lucas Perry:** Can you unpack this point a little bit more? I'm not sure that I'm completely understanding it. Buck is rejecting this modeling human beings explicitly as expected utility maximizers and trying to explicitly come up with utility functions in our AI systems. The first was to convince people not to use these kinds of things. And the second is to make it so that the behavior and output of the AI systems has some implicit understanding of human behavior. Can you unpack this a bit more for me or give me another example?\n\n**Rohin Shah:** So here's another example. Let's say I was teaching my kid that I don't have, how to catch a ball. It seems that the formalism that's available to me for learning how to catch a ball is, well, you can go all the way down to look at our best models of physics, we could use Newtonian mechanics let's say, like here are these equations, estimate the velocity and the distance of the ball and the angle at which it's thrown plug that into these equations and then predict that the ball's going to come here and then just put your hand there and then magically catch it. We won't even talk about the catching part. That seems like a pretty shitty way to teach a kid how to catch a ball.\n\nProbably it's just a lot better to just play catch with the kid for a while and let the kid's brain figure out this is how to predict where the ball is going to go such that I can predict where it's going to be and then catch it.\n\nI'm basically 100% confident that the thing that the brain is doing is not Newtonian mechanics. It's doing something else that's just way more efficient at predicting where the ball is going to be so that I can catch it and if I forced the brain to use Newtonian mechanics, I bet it would not do very well at this task.\n\n**Buck Shlegeris:** I feel like that still isn't quite saying the key thing here. I don't know how to say this off the top of my head either, but I think there's this key point about: just because your neural net can learn a particular feature of the world doesn't mean that you can back out some other property of the world by forcing the neural net to have a particular shape. Does that make any sense, Rohin?\n\n**Rohin Shah:** Yeah, vaguely. I mean, well, no, maybe not.\n\n**Buck Shlegeris:** The problem isn't just the capabilities problem. There's this way you can try and infer a human utility function by asking, according to this model, what's the maximum likelihood utility function given all these things the human did. If you have a good enough model, you will in fact end up making very good predictions about the human, it's just that the decomposition into their planning function and their utility function is not going to result in a utility function which is anything like a thing that I would want maximized if this process was done on me. There is going to be some decomposition like this, which is totally fine, but the utility function part just isn't going to correspond to the thing that I want.\n\n**Rohin Shah:** Yeah, that is also a problem, but I agree that is not the thing I was describing.\n\n**Lucas Perry:** Is the point there that there's a lack of alignment between the utility function and the planning function. Given that the planning function imperfectly optimizes the utility function.\n\n**Rohin Shah:** It's more like there are just infinitely many possible pairs of planning functions and utility functions that exactly predict human behavior. Even if it were true that humans were expected utility maximizers, which Buck is arguing we're not, and I agree with him. There is a planning function that's like humans are perfectly anti-rational and if you're like what utility function works with that planner to predict human behavior. Well, the literal negative of the true utility function when combined with the anti-rational planner produces the same behavior as the true utility function with the perfect planner, there's no information that lets you distinguish between these two possibilities.\n\nYou have to build it in as an assumption. I think Buck's point is that building things in as assumptions is probably not going to work.\n\n**Buck Shlegeris:** Yeah.\n\n**Rohin Shah:** A point I agree with. In philosophy this is called the is-ought problem, right? What you can train your AI system on is a bunch of “is” facts and then you have to add in some assumptions in order to jump to “ought” facts, which is what the utility function is trying to do. The utility function is trying to tell you how you ought to behave in new situations and the point of the is-ought distinction is that you need some bridging assumptions in order to get from is to ought.\n\n**Buck Shlegeris:** And I guess an important part here is your system will do an amazing job of answering “is” questions about what humans would say about “ought” questions. And so I guess maybe you could phrase the second part as: to get your system to do things that match human preferences, use the fact that it knows how to make accurate “is” statements about humans' ought statements?\n\n**Lucas Perry:** It seems like we're strictly talking about inferring the human utility function or preferences via looking at behavior. What if you also had more access to the actual structure of the human's brain?\n\n**Rohin Shah:** This is like the approach that Stuart Armstrong likes to talk about. The same things still apply. You still have the is-ought problem where the facts about the brain are \"is\" facts and how you translate that into \"ought\" facts is going to involve some assumptions. Maybe you can break down such assumptions that everyone would agree with. Maybe it's like if this particular neuron in a human brain spikes, that's a good thing and we want more of it and if this other one spikes, that's a bad thing. We don't want it. Maybe that assumption is fine.\n\n**Lucas Perry:** I guess I'm just pointing out, if you could find the places in the human brain that generate the statements about Ought questions.\n\n**Rohin Shah:** As Buck said, that lets you predict what humans would say about ought statements, which your assumption could then be, whatever humans say about ought statements, that's what you ought to do. And that's still an assumption. Maybe it's a very reasonable assumption that we're happy to put it into our AI system.\n\n**Lucas Perry:** If we're not willing to accept some humans’ “is” statements about “ought” questions then we have to do some meta-ethical moral policing in our assumptions around getting “is” statements from “ought” questions.\n\n**Rohin Shah:** Yes, that seems right to me. I don't know how you would do such a thing, but you would have to do something along those lines.\n\n**Buck Shlegeris:** I would additionally say that I feel pretty great about trying to do things which use the fact that we can trust our AI to have good “is” answers to “ought” questions, but there's a bunch of problems with this. I think it's a good starting point but trying to use that to do arbitrarily complicated things in the world has a lot of problems. For instance, suppose I'm trying to decide whether we should design a city this way or that way. It's hard to know how to go from the ability to know how humans would answer questions about preferences to knowing what you should do to design the city. And this is for a bunch of reasons, one of them is that the human might not be able to figure out from your city building plans what the city's going to actually be like. And another is that the human might give inconsistent answers about what design is good, depending on how you phrase the question, such that if you try to figure out a good city plan by optimizing for the thing that the human is going to be most enthusiastic about, then you might end up with a bad city plan. Paul Christiano has written in a lot of detail about a lot of this.\n\n**Lucas Perry:** That also reminds me of what Stuart Armstrong wrote about the framing on the questions changing output on the preference.\n\n**Rohin Shah:** Yep.\n\n**Buck Shlegeris:** Sorry, to be clear other people than Paul Christiano have also written a lot about this stuff, (including Rohin). My favorite writing about this stuff is by Paul.\n\n**Lucas Perry:** Yeah, those do seem problematic but it would also seem that there would be further “is” statements that if you queried people's meta-preferences about those things, you would get more “is” statements about that, but then that just pushes the “ought” assumptions that you need to make further back. Getting into very philosophically weedy territory. Do you think that this kind of thing could be pushed to the long reflection as is talked about by William MacAskill and Toby Ord or how much of this do you actually think needs to be solved in order to have safe and aligned AGI?\n\n**Buck Shlegeris:** I think there are kind of two different ways that you could hope to have good outcomes from AGI. One is: set up a world such that you never needed to make an AGI which can make large scale decisions about the world. And two is: solve the full alignment problem.\n\nI'm currently pretty pessimistic about the second of those being technically feasible. And I'm kind of pretty pessimistic about the first of those being a plan that will work. But in the world where you can have everyone only apply powerful and dangerous AI systems in ways that don't require an understanding of human values, then you can push all of these problems onto the long reflection. In worlds where you can do arbitrarily complicated things in ways that humans would approve of, you don't really need to long reflect this stuff because of the fact that these powerful AI systems already have the capacity of doing portions of the long reflection work inside themselves as needed. ([Quotes about the long reflection](https://forum.effectivealtruism.org/posts/H2zno3ggRJaph9P6c/quotes-about-the-long-reflection)) \n\n**Rohin Shah:** Yeah, so I think my take, it's not exactly disagreeing with Buck. It's more like from a different frame as Buck's. If you just got AI systems that did the things that humans did now, this does not seem to me to obviously require solving hard problems in philosophy. That's the lower bound on what you can do before having to do long reflection type stuff. Eventually you do want to do a longer reflection. I feel relatively optimistic about having a technical solution to alignment that allows us to do the long reflection after building AI systems. So the long reflection would include both humans and AI systems thinking hard, reflecting on difficult problems and so on.\n\n**Buck Shlegeris:** To be clear, I'm super enthusiastic about there being a long reflection or something along those lines.\n\n**Lucas Perry:** I always find it useful reflecting on just how human beings do many of these things because I think that when thinking about things in the strict AI alignment sense, it can seem almost impossible, but human beings are able to do so many of these things without solving all of these difficult problems. It seems like in the very least, we'll be able to get AI systems that very, very approximately do what is good or what is approved of by human beings because we can already do that.\n\n**Buck Shlegeris:** That argument doesn't really make sense to me. It also didn't make sense when Rohin referred to it a minute ago.\n\n**Rohin Shah:** It's not an argument for we technically know how to do this. It is more an argument for this as at least within the space of possibilities.\n\n**Lucas Perry:** Yeah, I guess that's how I was also thinking of it. It is within the space of possibilities. So utility functions are good because they can be optimized for, and there seem to be risks with optimization. Is there anything here that you guys would like to say about better understanding agency? I know this is one of the things that is important within the MIRI agenda.\n\n**Buck Shlegeris:** I am a bad MIRI employee. I don't really get that part of the MIRI agenda, and so I'm not going to defend it. I have certainly learned some interesting things from talking to Scott Garrabrant and other MIRI people who have lots of interesting thoughts about this stuff. I don't quite see the path from there to good alignment strategies. But I also haven't spent a super long time thinking about it because I, in general, don't try to think about all of the different AI alignment things that I could possibly think about.\n\n**Rohin Shah:** Yeah. I also am not a good person to ask about this. Most of my knowledge comes from reading things and MIRI has stopped writing things very much recently, so I don't know what their ideas are. I, like Buck, don't really see a good alignment strategy that starts with, first we understand optimization and so that's the main reason why I haven't looked into it very much.\n\n**Buck Shlegeris:** I think I don't actually agree with the thing you said there, Rohin. I feel like understanding optimization could plausibly be really nice. Basically the story there is, it's a real bummer if we have to make really powerful AI systems via searching over large recurrent policies for things that implement optimizers. If it turned out that we could figure out some way of coding up optimizer stuffs directly, then this could maybe mean you didn't need to make mesa-optimizers. And maybe this means that your inner alignment problems go away, which could be really nice. The thing that I was saying I haven't thought that much about is, the relevance of thinking about, for instance, the various weirdnesses that happen when you consider embedded agency or decision theory, and things like that.\n\n**Rohin Shah:** Oh, got it. Yeah. I think I agree that understanding optimization would be great if we succeeded at it and I'm mostly pessimistic about us succeeding at it, but also there are people who are optimistic about it and I don't know why they're optimistic about it.\n\n**Lucas Perry:** Hey it’s post-podcast Lucas here again. So, I just want to add a little more detail here again on behalf of Rohin. Here he feels pessimistic about us understanding optimization well enough and in a short enough time period that we are able to create powerful optimizers that we understand that rival the performance of the AI systems we’re already building and will build in the near future. Back to the episode. \n\n**Buck Shlegeris:** The arguments that MIRI has made about this,... they think that there are a bunch of questions about what optimization is, that are plausibly just not that hard compared to other problems which small groups of people have occasionally solved, like coming up with foundations of mathematics, kind of a big conceptual deal but also a relatively small group of people. And before we had formalizations of math, I think it might've seemed as impossible to progress on as formalizing optimization or coming up with a better picture of that. So maybe that's my argument for some optimism.\n\n**Rohin Shah:** Yeah, I think pointing to some examples of great success does not imply... Like there are probably many similar things that didn't work out and we don't know about them cause nobody bothered to tell us about them because they failed. Seems plausible maybe.\n\n**Lucas Perry:** So, exploring more deeply this point of agency can either, or both of you, give us a little bit of a picture about the relevance or non relevance of decision theory here to AI alignment and I think, Buck, you mentioned the trickiness of embedded decision theory.\n\n**Rohin Shah:** If you go back to our traditional argument for AI risk, it's basically powerful AI systems will be very strong optimizers. They will possibly be misaligned with us and this is bad. And in particular one specific way that you might imagine this going wrong is this idea of mesa optimization where we don't know how to build optimizers right now. And so what we end up doing is basically search across a huge number of programs looking for ones that do well at optimization and use that as our AGI system. And in this world, if you buy that as a model of what's happening, then you'll basically have almost no control over what exactly that system is optimizing for. And that seems like a recipe for misalignment. It sure would be better if we could build the optimizer directly and know what it is optimizing for. And in order to do that, we need to know how to do optimization well.\n\n**Lucas Perry:** What are the kinds of places that we use mesa optimizers today?\n\n**Rohin Shah:** It's not used very much yet. The field of meta learning is the closest example. In the field of meta learning you have a distribution over tasks and you use gradient descent or some other AI technique in order to find an AI system that itself, once given a new task, learns how to perform that task well.\n\nExisting meta learning systems are more like learning how to do all the tasks well and then when they'll see a new task they just figure out ah, it's this task and then they roll out the policy that they already learned. But the eventual goal for meta learning is to get something that, online, learns how to do the task without having previously figured out how to do that task.\n\n**Lucas Perry:** Okay, so Rohin did what you say cover embedded decision theory?\n\n**Rohin Shah:** No, not really. I think embedded decision theory is just, we want to understand optimization. Our current notion of optimization, one way you could formalize it is to say my AI agent is going to have Bayesian belief over all the possible ways that the environment could be. It's going to update that belief over time as it gets observations and then it's going to act optimally with respect to that belief, by maximizing its expected utility. And embedded decision theory basically calls into question the idea that there's a separation between the agent and the environment. In particular I, as a human, couldn't possibly have a Bayesian belief about the entire earth because the entire Earth contains me. I can't have a Bayesian belief over myself so this means that our existing formalization of agency is flawed. It can't capture these things that affect real agents. And embedded decision theory, embedded agency, more broadly, is trying to deal with this fact and have a new formalization that works even in these situations.\n\n**Buck Shlegeris:** I want to give my understanding of the pitch for it. One part is that if you don't understand embedded agency, then if you try to make an AI system in a hard coded way, like making a hard coded optimizer, traditional phrasings of what an optimizer is, are just literally wrong in that, for example, they're assuming that you have these massive beliefs over world states that you can't really have. And plausibly, it is really bad to try to make systems by hardcoding assumptions that are just clearly false. And so if we want to hardcode agents with particular properties, it would be good if we knew a way of coding the agent that isn't implicitly making clearly false assumptions.\n\nAnd the second pitch for it is something like when you want to understand a topic, sometimes it's worth looking at something about the topic which you're definitely wrong about, and trying to think about that part until you are less confused about it. When I'm studying physics or something, a thing that I love doing is looking for the easiest question whose answer I don't know, and then trying to just dive in until I have satisfactorily answered that question, hoping that the practice that I get about thinking about physics from answering a question correctly will generalize to much harder questions. I think that's part of the pitch here. Here is a problem that we would need to answer, if we wanted to understand how superintelligent AI systems work, so we should try answering it because it seems easier than some of the other problems.\n\n**Lucas Perry:** Okay. I think I feel satisfied. The next thing here Rohin in your AI alignment 2018-19 review is value learning. I feel like we've talked a bunch about this already. Is there anything here that you want to say or do you want to skip this?\n\n**Rohin Shah:** One thing we didn't cover is, if you have uncertainty over what you're supposed to optimize, this turns into an interactive sort of game between the human and the AI agent, which seems pretty good. A priori you should expect that there's going to need to be a lot of interaction between the human and the AI system in order for the AI system to actually be able to do the things that the human wants it to do. And so having formalisms and ideas of where this interaction naturally falls out seems like a good thing.\n\n**Buck Shlegeris:** I've said a lot of things about how I am very pessimistic about value learning as a strategy. Nevertheless it seems like it might be really good for there to be people who are researching this, and trying to get as good as we can get at improving sample efficiency so that can have your AI systems understand your preferences over music with as little human interaction as possible, just in case it turns out to be possible to solve the hard version of value learning. Because a lot of the engineering effort required to make ambitious value learning work will plausibly be in common with the kinds of stuff you have to do to make these more simple specification learning tasks work out. That's a reason for me to be enthusiastic about people researching value learning even if I'm pessimistic about the overall thing working.\n\n**Lucas Perry:** All right, so what is robustness and why does it matter?\n\n**Rohin Shah:** Robustness is one of those words that doesn't super clearly have a definition and people use it differently. Robust agents don't fail catastrophically in situations slightly different from the ones that they were designed for. One example of a case where we see a failure of robustness currently, is in adversarial examples for image classifiers, where it is possible to take an image, make a slight perturbation to it, and then the resulting image is completely misclassified. You take a correctly classified image of a Panda, slightly perturb it such that a human can't tell what the difference is, and then it's classified as a gibbon with 99% confidence. Admittedly this was with an older image classifier. I think you need to make the perturbations a bit larger now in order to get them.\n\n**Lucas Perry:** This is because the relevant information that it uses are very local to infer panda-ness rather than global properties of the panda?\n\n**Rohin Shah:** It's more like they’re high frequency features or imperceptible features. There's a lot of controversy about this but there is a pretty popular recent paper that I believe, but not everyone believes, that claims that this was because they're picking up on real imperceptible features that do generalize to the test set, that humans can't detect. That's an example of robustness. Recently people have been applying this to reinforcement learning both by adversarially modifying the observations that agents get and also by training agents that act in the environment adversarially towards the original agent. One paper out of CHAI showed that there's this kick and defend environment where you've got two MuJoCo robots. One of them is kicking a soccer ball. The other one's a goalie, that's trying to prevent the kicker from successfully shooting a goal, and they showed that if you do self play in order to get kickers and defenders and then you take the kicker, you freeze it, you don't train it anymore and you retrain a new defender against this kicker.\n\nWhat is the strategy that this new defender learns? It just sort of falls to the ground and flaps about in a random looking way and the kicker just gets so confused that it usually fails to even touch the ball and so this is sort of an adversarial example for RL agents now, it's showing that even they're not very robust.\n\nThere was also a paper out of DeepMind that did the same sort of thing. For their adversarial attack they learned what sorts of mistakes the agent would make early on in training and then just tried to replicate those mistakes once the agent was fully trained and they found that this helped them uncover a lot of bad behaviors. Even at the end of training.\n\nFrom the perspective of alignment, it's clear that we want robustness. It's not exactly clear what we want robustness to. This robustness to adversarial perturbations was kind of a bit weird as a threat model. If there is an adversary in the environment they’re probably not going to be restricted to small perturbations. They're probably not going to get white box access to your AI system; even if they did, this doesn't seem to really connect with the AI system as adversarially optimizing against humans story, which is how we get to the x-risk part, so it's not totally clear.\n\nI think on the intent alignment case, which is the thing that I usually think about, you mostly want to ensure that whatever is driving the \"motivation\" of the AI system, you want that to be very robust. You want it to agree with what humans would want in all situations or at least all situations that are going to come up or something like that. Paul Christiano has written a few blog posts about this that talk about what techniques he's excited about solving that problem, which boil down to interpretability, adversarial training, and improving adversarial training through relaxations of the problem.\n\n**Buck Shlegeris:** I'm pretty confused about this, and so it's possible what I'm going to say is dumb. When I look at problems with robustness or problems that Rohin put in this robustness category here, I want to divide it into two parts. One of the parts is, things that I think of as capability problems, which I kind of expect the rest of the world will need to solve on its own. For instance, things about safe exploration, how do I get my system to learn to do good things without ever doing really bad things, this just doesn't seem very related to the AI alignment problem to me. And I also feel reasonably optimistic that you can solve it by doing dumb techniques which don't have anything too difficult to them, like you can have your system so that it has a good model of the world that it got from unsupervised learning somehow and then it never does dumb enough things. And also I don't really see that kind of robustness problem leading to existential catastrophes. And the other half of robustness is the half that I care about a lot, which in my mind, is mostly trying to make sure that you succeeded at inner alignment. That is, that the mesa optimizers you've found through gradient descent have goals that actually match your goals.\n\nThis is like robustness in the sense that you're trying to guarantee that in every situation, your AI system, as Rohin was saying, is intent aligned with you. It's trying to do the kind of thing that you want. And I worry that, by default, we're going to end up with AI systems not intent aligned, so there exist a bunch of situations they can be put in such that they do things that are very much not what you'd want, and therefore they fail at robustness. I think this is a really important problem, it's like half of the AI safety problem or more, in my mind, and I'm not very optimistic about being able to solve it with prosaic techniques.\n\n**Rohin Shah:** That sounds roughly similar to what I was saying. Yes.\n\n**Buck Shlegeris:** I don't think we disagree about this super much except for the fact that I think you seem to care more about safe exploration and similar stuff than I think I do.\n\n**Rohin Shah:** I think safe exploration's a bad example. I don't know what safe exploration is even trying to solve but I think other stuff, I agree. I do care about it more. One place where I somewhat disagree with you is, you sort of have this point about all these robustness problems are the things that the rest of the world has incentives to figure out, and will probably figure out. That seems true for alignment too, it sure seems like you want your system to be aligned in order to do the things that you actually want. Everyone that has an incentive for this to happen. I totally expect people who aren’t EAs or rationalists or weird longtermists to be working on AI alignment in the future and to some extent even now. I think that's one thing.\n\n**Buck Shlegeris:** You should say your other thing, but then I want to get back to that point.\n\n**Rohin Shah:** The other thing is I think I agree with you that it's not clear to me how failures of the robustness of things other than motivation lead to x-risk, but I'm more optimistic than you are that our solutions to those kinds of robustness will help with the solutions to \"motivation robustness\" or how to make your mesa optimizer aligned.\n\n**Buck Shlegeris:** Yeah, sorry, I guess I actually do agree with that last point. I am very interested in trying to figure out how to have aligned to mesa optimizers, and I think that a reasonable strategy to pursue in order to get aligned mesa optimizers is trying to figure out how to make your image classifiers robust to adversarial examples. I think you probably won't succeed even if you succeed with the image classifiers, but it seems like the image classifiers are still probably where you should start. And I guess if we can't figure out how to make image classifiers robust to adversarial examples in like 10 years, I'm going to be super pessimistic about the harder robustness problem, and that would be great to know.\n\n**Rohin Shah:** For what it's worth, my take on the adversarial examples of image classifiers is, we're going to train image classifiers on more data with bigger nets, it's just going to mostly go away. Prediction. I'm laying my cards on the table.\n\n**Buck Shlegeris:** That's also something like my guess.\n\n**Rohin Shah:** Okay.\n\n**Buck Shlegeris:** My prediction is: to get image classifiers that are robust to epsilon ball perturbations or whatever, some combination of larger things and adversarial training and a couple other clever things, will probably mean that we have robust image classifiers in 5 or 10 years at the latest.\n\n**Rohin Shah:** Cool. And you wanted to return to the other point about the world having incentives to do alignment.\n\n**Buck Shlegeris:** So I don't quite know how to express this, but I think it's really important which is going to make this a really fun experience for everyone involved. You know how Airbnb... Or sorry, I guess a better example of this is actually Uber drivers. Where I give basically every Uber driver a five star rating, even though some Uber drivers are just clearly more pleasant for me than others, and Uber doesn't seem to try very hard to get around these problems, even though I think that if Uber caused there to be a 30% difference in pay between the drivers who I think of as 75th percentile and the drivers I think of as 25th percentile, this would make the service probably noticeably better for me. I guess it seems to me that a lot of the time the world just doesn't try do kind of complicated things to make systems actually aligned, and it just does hack jobs, and then everyone deals with the fact that everything is unaligned as a result.\n\nTo draw this analogy back, I think that we're likely to have the kind of alignment techniques that solve problems that are as simple and obvious as: we should have a way to have rate your hosts on Airbnb. But I'm worried that we won't ever get around to solving the problems that are like, but what if your hosts are incentivized to tell you sob stories such that you give them good ratings, even though actually they were worse than some other hosts. And this is never a big enough deal that people are unilaterally individually incentivized to solve the harder version of the alignment problem, and then everyone ends up using these systems that actually aren't aligned in the strong sense and then we end up in a doomy world. I'm curious if any of that made any sense.\n\n**Lucas Perry:** Is a simple way to put that we fall into inadequate or an unoptimal equilibrium and then there's tragedy of the commons and bad game theory stuff that happens that keeps us locked and that the same story could apply to alignment?\n\n**Buck Shlegeris:** Yeah, that's not quite what I mean.\n\n**Lucas Perry:** Okay.\n\n**Rohin Shah:** I think Buck's point is that actually Uber or Airbnb could unilaterally, no gains required, make their system better and this would be an improvement for them and everyone else, and they don't do it. There is nothing about equilibrium that is a failure of Uber to do this thing that seems so obviously good.\n\n**Buck Shlegeris:** I'm not actually claiming that it's better for Uber, I'm just claiming that there is a misalignment there. Plausibly, an Uber exec, if they were listening to this they'd just be like, \"LOL, that's a really stupid idea. People would hate it.\" And then they would say more complicated things like \"most riders are relatively price sensitive and so this doesn't matter.\" And plausibly they're completely right.\n\n**Rohin Shah:** That's what I was going to say.\n\n**Buck Shlegeris:** But the thing which feels important to me is something like a lot of the time it's not worth solving the alignment problems at any given moment because something else is a bigger problem to how things are going locally. And this can continue being the case for a long time, and then you end up with everyone being locked in to this system where they never solved the alignment problems. And it's really hard to make people understand this, and then you get locked into this bad world.\n\n**Rohin Shah:** So if I were to try and put that in the context of AI alignment, I think this is a legitimate reason for being more pessimistic. And the way that I would make that argument is: it sure seems like we are going to decide on what method or path we're going to use to build AGI. Maybe we'll do a bunch of research and decide we're just going to scale up language models or something like this. I don't know. And we will do that before we have any idea of which technique would be easiest to align and as a result, we will be forced to try to align this exogenously chosen AGI technique and that would be harder than if we got to design our alignment techniques and our AGI techniques simultaneously.\n\n**Buck Shlegeris:** I'm imagining some pretty slow take off here, and I don't imagine this as ever having a phase where we built this AGI and now we need to align it. It's more like we're continuously building and deploying these systems that are gradually more and more powerful, and every time we want to deploy a system, it has to be doing something which is useful to someone. And many of the things which are useful, require things that are kind of like alignment. \"I want to make a lot of money from my system that will give advice,\" and if it wants to give good generalist advice over email, it's going to need to have at least some implicit understanding of human preferences. Maybe we just use giant language models and everything's just totally fine here. A really good language model isn't able to give arbitrarily good aligned advice, but you can get advice that sounds really good from a language model, and I'm worried that the default path is going to involve the most popular AI advice services being kind of misaligned, and just never bothering to fix that. Does that make any more sense?\n\n**Rohin Shah:** Yeah, I think I totally buy that that will happen. But I think I'm more like as you get to AI systems doing more and more important things in the world, it becomes more and more important that they are really truly aligned and investment in alignment increases correspondingly.\n\n**Buck Shlegeris:** What's the mechanism by which people realize that they need to put more work into alignment here?\n\n**Rohin Shah:** I think there's multiple. One is I expect that people are aware, like even in the Uber case, I expect people are aware of the misalignment that exists, but decide that it's not worth their time to fix it. So the continuation of that, people will be aware of it and then they will decide that they should fix it.\n\n**Buck Shlegeris:** If I'm trying to sell to city governments this language model based system which will give them advice on city planning, it's not clear to me that at any point the city governments are going to start demanding better alignment features. Maybe that's the way that it goes but it doesn't seem obvious that city governments would think to ask that, and --\n\n**Rohin Shah:** I wasn't imagining this from the user side. I was imagining this from the engineers or designers side.\n\n**Buck Shlegeris:** Yeah.\n\n**Rohin Shah:** I think from the user side I would speak more to warning shots. You know, you have your cashier AI system or your waiter AIs and they were optimizing for tips more so than actually collecting money and so they like offer free meals in order to get more tips. At some point one of these AI systems passes all of the internal checks and makes it out into the world and only then does the problem arise and everyone's like, \"Oh my God, this is terrible. What the hell are you doing? Make this better.\"\n\n**Buck Shlegeris:** There's two mechanisms via which that alignment might be okay. One of them is that researchers might realize that they want to put more effort into alignment and then solve these problems. The other mechanism is that users might demand better alignment because of warning shots. I think that I don't buy that either of these is sufficient. I don't buy that it's sufficient for researchers to decide to do it because in a competitive world, the researchers who realize this is important, if they try to only make aligned products, they are not going to be able to sell them because their products will be much less good than the unaligned ones. So you have to argue that there is demand for the things which are actually aligned well. But for this to work, your users have to be able to distinguish between things that have good alignment properties and those which don't, and this seems really hard for users to do. And I guess, when I try to imagine analogies, I just don't see many examples of people successfully solving problems like this, like businesses making products that are different levels of dangerousness, and then users successfully buying the safe ones.\n\n**Rohin Shah:** I think usually what happens is you get regulation that forces everyone to be safe. I don't know if it was regulation, but like airplanes are incredibly safe. Cars are incredibly safe.\n\n**Buck Shlegeris:** Yeah but in this case what would happen is doing the unsafe thing allows you to make enormous amounts of money, and so the countries which don't put in the regulations are going to be massively advantaged compared to ones which don't.\n\n**Rohin Shah:** Why doesn't that apply for cars and airplanes?\n\n**Buck Shlegeris:** So to start with, cars in poor countries are a lot less safe. Another thing is that a lot of the effort in making safer cars and airplanes comes from designing them. Once you've done the work of designing it, it's that much more expensive to put your formally-verified 747 software into more planes, and because of weird features of the fact that there are only like two big plane manufacturers, everyone gets the safer planes.\n\n**Lucas Perry:** So tying this into robustness. The fundamental concern here is about the incentives to make aligned systems that are safety and alignment robust in the real world.\n\n**Rohin Shah:** I think that's basically right. I sort of see these incentives as existing and the world generally being reasonably good at dealing with high stakes problems.\n\n**Buck Shlegeris:** What's an example of the world being good at dealing with a high stakes problem?\n\n**Rohin Shah:** I feel like biotech seems reasonably well handled, relatively speaking,\n\n**Buck Shlegeris:** Like bio-security?\n\n**Rohin Shah:** Yeah.\n\n**Buck Shlegeris:** Okay, if the world handles AI as well as bio-security, there's no way we're okay.\n\n**Rohin Shah:** Really? I'm aware of ways in which we're not doing bio-security well, but there seem to be ways in which we're doing it well too.\n\n**Buck Shlegeris:** The nice thing about bio-security is that very few people are incentivized to kill everyone, and this means that it's okay if you're sloppier about your regulations, but my understanding is that lots of regulations are pretty weak.\n\n**Rohin Shah:** I guess I was more imagining the research community's coordination on this. Surprisingly good.\n\n**Buck Shlegeris:** I wouldn’t describe it that way.\n\n**Rohin Shah:** It seems like the vast majority of the research community is onboard with the right thing and like 1% isn’t. Yeah. Plausibly we need to have regulations for that last 1%.\n\n**Buck Shlegeris:** I think that 99% of the synthetic biology research community is on board with \"it would be bad if everyone died.\" I think that some very small proportion is onboard with things like \"we shouldn't do research if it's very dangerous and will make the world a lot worse.\" I would say like way less than half of synthetic biologists seem to agree with statements like \"it's bad to do really dangerous research.\" Or like, \"when you're considering doing research, you consider differential technological development.\" I think this is just not a thing biologists think about, from my experience talking to biologists.\n\n**Rohin Shah:** I'd be interested in betting with you on this afterwards.\n\n**Buck Shlegeris:** Me too.\n\n**Lucas Perry:** So it seems like it's going to be difficult to come down to a concrete understanding or agreement here on the incentive structures in the world and whether they lead to the proliferation of unaligned AI systems or semi aligned AI systems versus fully aligned AI systems and whether that poses a kind of lock-in, right? Would you say that that fairly summarizes your concern Buck?\n\n**Buck Shlegeris:** Yeah. I expect that Rohin and I agree mostly on the size of the coordination problem required, or the costs that would be required by trying to do things the safer way. And I think Rohin is just a lot more optimistic about those costs being paid.\n\n**Rohin Shah:** I think I'm optimistic both about people's ability to coordinate paying those costs and about incentives pointing towards paying those costs.\n\n**Buck Shlegeris:** I think that Rohin is right that I disagree with him about the second of those as well.\n\n**Lucas Perry:** Are you interested in unpacking this anymore? Are you happy to move on?\n\n**Buck Shlegeris:** I actually do want to talk about this for two more minutes. I am really surprised by the claim that humans have solved coordination problems as hard as this one. I think the example you gave is humans doing radically nowhere near well enough. What are examples of coordination problem type things... There was a bunch of stuff with nuclear weapons, where I feel like humans did badly enough that we definitely wouldn't have been okay in an AI situation. There are a bunch of examples of the US secretly threatening people with nuclear strikes, which I think is an example of some kind of coordination failure. I don't think that the world has successfully coordinated on never threaten first nuclear strikes. If we had successfully coordinated on that, I would consider nuclear weapons to be less of a failure, but as it is the US has actually according to Daniel Ellsberg threatened a bunch of people with first strikes.\n\n**Rohin Shah:** Yeah, I think I update less on specific scenarios and update quite a lot more on, \"it just never happened.\" The sheer amount of coincidence that would be required given the level of, Oh my God, there were close calls multiple times a year for many decades. That seems just totally implausible and it just means that our understanding of what's happening is wrong.\n\n**Buck Shlegeris:** Again, also the thing I'm imagining is this very gradual takeoff world where people, every year, they release their new most powerful AI systems. And if, in a particular year, AI Corp decided to not release its thing, then AI Corps two and three and four would rise to being one, two and three in total profits instead of two, three and four. In that kind of a world, I feel a lot more pessimistic.\n\n**Rohin Shah:** I'm definitely imagining more of the case where they coordinate to all not do things. Either by international regulation or via the companies themselves coordinating amongst each other. Even without that, it's plausible that AI Corp one does this. One example I'd give is, Waymo has just been very slow to deploy self driving cars relative to all the other self driving car companies, and my impression is that this is mostly because of safety concerns.\n\n**Buck Shlegeris:** Interesting and slightly persuasive example. I would love to talk through this more at some point. I think this is really important and I think I haven't heard a really good conversation about this.\n\nApologies for describing what I think is going wrong inside your mind or something, which is generally a bad way of saying things, but it sounds kind of to me like you're implicitly assuming more concentrated advantage and fewer actors than I think actually are implied by gradual takeoff scenarios.\n\n**Rohin Shah:** I'm usually imagining something like a 100+ companies trying to build the next best AI system, and 10 or 20 of them being clear front runners or something.\n\n**Buck Shlegeris:** That makes sense. I guess I don't quite see how the coordination successes you were describing arise in that kind of a world. But I am happy to move on.\n\n**Lucas Perry:** So before we move on on this point, is there anything which you would suggest as obvious solutions, should Buck's model of the risks here be the case. So it seemed like it would demand more centralized institutions which would help to mitigate some of the lock in here.\n\n**Rohin Shah:** Yeah. So there's a lot of work in policy and governance about this. Not much of which is public unfortunately. But I think the thing to say is that people are thinking about it and it does sort of look like trying to figure out how to get the world to actually coordinate on things. But as Buck has pointed out, we have tried to do this before and so there's probably a lot to learn from past cases as well. But I am not an expert on this and don't really want to talk as though I were one.\n\n**Lucas Perry:** All right. So there's lots of governance and coordination thought that kind of needs to go into solving many of these coordination issues around developing beneficial AI. So I think with that we can move along now to scaling to superhuman abilities. So Rohin, what do you have to say about this topic area?\n\n**Rohin Shah:** I think this is in some sense related to what we were talking about before, you can predict what a human would say, but it's hard to back out true underlying values beneath them. Here the problem is, suppose you are learning from some sort of human feedback about what you're supposed to be doing, the information contained in that tells you how to do whatever the human can do. It doesn't really tell you how to exceed what the human can do without having some additional assumptions.\n\nNow, depending on how the human feedback is structured, this might lead to different things like if the human is demonstrating how to do the task to you, then this would suggest that it would be hard to do the task any better than the human can, but if the human was evaluating how well you did the task, then you can do the task better in a way that the human wouldn't be able to tell was better. Ideally, at some point we would like to have AI systems that can actually do just really powerful, great things, that we are unable to understand all the details of and so we would neither be able to demonstrate or evaluate them.\n\nHow do we get to those sorts of AI systems? The main proposals in this bucket are iterated amplification, debate, and recursive reward modeling. So in iterated amplification, we started with an initial policy, and we alternate between amplification and distillation, which increases capabilities and efficiency respectively. This can encode a bunch of different algorithms, but usually amplification is done by decomposing questions into easier sub questions, and then using the agent to answer those sub questions. While distillation can be done using supervised learning or reinforcement learning, so you get these answers that are created by these amplified systems that take a long time to run, and you just train a neural net to very quickly predict the answers without having to do this whole big decomposition thing. In debate, we train an agent through self play in a zero sum game where the agent's goal is to win a question answering debate as evaluated by a human judge. The hope here is that since both sides of the debate can point out flaws in the other side's arguments -- they're both very powerful AI systems -- such a set up can use a human judge to train far more capable agents while still incentivizing the agents to provide honest true information. With recursive reward modeling, you can think of it as an instantiation of the general alternate between amplification and distillation framework, but it works sort of bottom up instead of top down. So you'll start by building AI systems that can help you evaluate simple, easy tasks. Then use those AI systems to help you evaluate more complex tasks and you keep iterating this process until eventually you have AI systems that help you with very complex tasks like how to design the city. And this lets you then train an AI agent that can design the city effectively even though you don't totally understand why it's doing the things it's doing or why they're even good.\n\n**Lucas Perry:** Do either of you guys have any high level thoughts on any of these approaches to scaling to superhuman abilities?\n\n**Buck Shlegeris:** I have some.\n\n**Lucas Perry:** Go for it.\n\n**Buck Shlegeris:** So to start with, I think it's worth noting that another approach would be ambitious value learning, in the sense that I would phrase these not as approaches for scaling to superhuman abilities, but they're like approaches for scaling to superhuman abilities while only doing tasks that relate to the actual behavior of humans rather than trying to back out their values explicitly. Does that match your thing Rohin?\n\n**Rohin Shah:** Yeah, I agree. I often phrase that as with ambitious value learning, there's not a clear ground truth to be focusing on, whereas with all three of these methods, the ground truth is what a human would do if they got a very, very long time to think or at least that is what they're trying to approximate. It's a little tricky to see why exactly they're approximating that, but there are some good posts about this. The key difference between these techniques and ambitious value learning is that there is in some sense a ground truth that you are trying to approximate.\n\n**Buck Shlegeris:** I think these are all kind of exciting ideas. I think they're all kind of better ideas than I expected to exist for this problem a few years ago. Which probably means we should update against my ability to correctly judge how hard AI safety problems are, which is great news, in as much as I think that a lot of these problems are really hard. Nevertheless, I don't feel super optimistic that any of them are actually going to work. One thing which isn't in the elevator pitch for IDA, which is iterated distillation and amplification (and debate), is that you get to hire the humans who are going to be providing the feedback, or the humans whose answers AI systems are going to be trained with. And this is actually really great. Because for instance, you could have this program where you hire a bunch of people and you put them through your one month long training an AGI course. And then you only take the top 50% of them. I feel a lot more optimistic about these proposals given you're allowed to think really hard about how to set it up such that the humans have the easiest time possible. And this is one reason why I'm optimistic about people doing research in factored cognition and stuff, which I'm sure Rohin's going to explain in a bit.\n\nOne comment about recursive reward modeling: it seems like it has a lot of things in common with IDA. The main downside that it seems to have to me is that the human is in charge of figuring out how to decompose the task into evaluations at a variety of levels. Whereas with IDA, your system itself is able to naturally decompose the task into a variety levels, and for this reason I feel a bit more optimistic about IDA.\n\n**Rohin Shah:** With recursive reward modeling, one agent that you can train is just an agent that's good at doing decompositions. That is a thing you can do with it. It's a thing that the people at DeepMind are thinking about. \n\n**Buck Shlegeris:** Yep, that’s a really good point. \n\n**Rohin Shah:** I also strongly like the fact that you can train your humans to be good at providing feedback. This is also true about specification learning. It's less clear if it's true about ambitious value learning. No one's really proposed how you could do ambitious value learning really. Maybe arguably Stuart Russell's book is kind of a proposal, but it doesn't have that many details.\n\n**Buck Shlegeris:** And, for example, it doesn't address any of my concerns in ways that I find persuasive.\n\n**Rohin Shah:** Right. But for specification learning also you definitely want to train the humans who are going to be providing feedback to the AI system. That is an important part of why you should expect this to work.\n\n**Buck Shlegeris:** I often give talks where I try to give an introduction to IDA and debate as a proposal for AI alignment. I'm giving these talks to people with computer science backgrounds, and they’re almost always incredibly skeptical that it's actually possible to decompose thought in this kind of a way. And with debate, they're very skeptical that truth wins, or that the nash equilibrium is accuracy. For this reason I'm super enthusiastic about research into the factored cognition hypothesis of the type that Ought is doing some of.\n\nI'm kind of interested in your overall take for how likely it is that the factored cognition hypothesis holds and that it's actually possible to do any of this stuff, Rohin. You could also explain what that is.\n\n**Rohin Shah:** I'll do that. So basically with both iterated amplification, debate, or recursive reward modeling, they all hinge on this idea of being able to decompose questions, maybe it's not so obvious why that's true for debate, but it's true. Go listen to the podcast about debate if you want to get more details on that.\n\nSo this hypothesis is basically for any tasks that we care about, it is possible to decompose this into a bunch of sub tasks that are all easier to do. Such that if you're able to do the sub tasks, then you can do the overall top level tasks and in particular you can iterate this down, building a tree of smaller and smaller tasks until you can get to the level of tasks that a human could do in a day. Or if you're trying to do it very far, maybe tasks that a human can do in a couple of minutes. Whether or not you can actually decompose the task \"be an effective CEO\" into a bunch of sub tasks that eventually bottom out into things humans can do in a few minutes is totally unclear. Some people are optimistic, some people are pessimistic. It's called the factored cognition hypothesis and Ought is an organization that's studying it.\n\nIt sounds very controversial at first and I, like many other people had the intuitive reaction of, 'Oh my God, this is never going to work and it's not true'. I think the thing that actually makes me optimistic about it is you don't have to do what you might call a direct decomposition. You can do things like if your task is to be an effective CEO, your first sub question could be, what are the important things to think about when being a CEO or something like this, as opposed to usually when I think of decompositions I would think of, first I need to deal with hiring. Maybe I need to understand HR, maybe I need to understand all of the metrics that the company is optimizing. Very object level concerns, but the decompositions are totally allowed to also be meta level where you'll spin off a bunch of computation that is just trying to answer the meta level of question of how should I best think about this question at all.\n\nAnother important reason for optimism is that based on the structure of iterated amplification, debate and recursive reward modeling, this tree can be gigantic. It can be exponentially large. Something that we couldn't run even if we had all of the humans on Earth collaborating to do this. That's okay. Given how the training process is structured, considering the fact that you can do the equivalent of millennia of person years of effort in this decomposed tree, I think that also gives me more of a, 'okay, maybe this is possible' and that's also why you're able to do all of this meta level thinking because you have a computational budget for it. When you take all of those together, I sort of come up with “seems possible. I don't really know.”\n\n**Buck Shlegeris:** I think I'm currently at 30-to-50% on the factored cognition thing basically working out. Which isn't nothing.\n\n**Rohin Shah:** Yeah, that seems like a perfectly reasonable thing. I think I could imagine putting a day of thought into it and coming up with numbers anywhere between 20 and 80.\n\n**Buck Shlegeris:** For what it's worth, in conversation at some point in the last few years, Paul Christiano gave numbers that were not wildly more optimistic than me. I don't think that the people who are working on this think it's obviously fine. And it would be great if this stuff works, so I'm really in favor of people looking into it.\n\n**Rohin Shah:** Yeah, I should mention another key intuition against it. We have all these examples of human geniuses like Ramanujan, who were posed very difficult math problems and just immediately get the answer and then you ask them how did they do it and they say, well, I asked myself what should the answer be? And I was like, the answer should be a continued fraction. And then I asked myself which continued fraction and then I got the answer. And you're like, that does not sound very decomposable. It seems like you need these magic flashes of intuition. Those would be the hard cases for factored cognition. It still seems possible that you could do it by both this exponential try a bunch of possibilities and also by being able to discover intuitions that work in practice and just believing them because they work in practice and then applying them to the problem at hand. You could imagine that with enough computation you'd be able to discover such intuitions.\n\n**Buck Shlegeris:** You can't answer a math problem by searching exponentially much through the search tree. The only exponential power you get from IDA is IDA is letting you specify the output of your cognitive process in such a way that's going to match some exponentially sized human process. As long as that exponentially sized human process was only exponentially sized because it's really inefficient, but is kind of fundamentally not an exponentially sized problem, then your machine learning should be able to speed it up a bunch. But the thing where you search over search strategy is not valid. If that's all you can do, that's not good enough.\n\n**Rohin Shah:** Searching over search strategies, I agree you can't do, but if you have an exponential search that could be implemented by humans. We know by hypothesis, if you can solve it with a flash of intuition, there is in fact some more efficient way to do it and so whether or not the distillation steps will actually be enough to get to the point where you can do those flashes of intuition. That's an open question.\n\n**Buck Shlegeris:** This is one of my favorite areas of AI safety research and I would love for there to be more of it. Something I have been floating for a little while is I kind of wish that there was another Ought. It just seems like it would be so good if we had definitive information about the factored cognition hypothesis. And it also it seems like the kind of thing which is potentially parallelizable. And I feel like I know a lot of people who love talking about how thinking works. A lot of rationalists are really into this. I would just be super excited for some of them to form teams of four and go off on their own and build an Ought competitor. I feel like this is the kind of thing where plausibly, a bunch of enthusiastic people could make progress on their own.\n\n**Rohin Shah:** Yeah, I agree with that. Definitely seems like one of the higher value things but I might be more excited about universality.\n\n**Lucas Perry:** All right, well let's get started with universality then. What is universality and why are you optimistic about it?\n\n**Rohin Shah:** So universality is hard to explain well, in a single sentence. For whatever supervisor is training our agent, you want that supervisor to “know everything the agent knows.” In particular if the agent comes up with some deceptive strategy to look like it's achieving the goal, but actually it hasn't. The supervisors should know that it was doing this deceptive strategy for the reason of trying to trick the supervisor and so the supervisor can then penalize it. The classic example of why this is important and hard also due to Paul Christiano is plagiarism. Suppose you are training on the AI system to produce novel works of literature and as part of its training data, the AI system gets to read this library of a million books.\n\nIt's possible that this AI system decides, Hey, you know the best way I can make a great novel seeming book is to just take these five books and take out plot points, passages from each of them and put them together and then this new book will look totally novel and will be very good because I used all of the best Shakespearean writing or whatever. If your supervisor doesn't know that the agent has done this, the only way the supervisor can really check is to go read the entire million books. Even if the agent only read 10 books and so then the supervision becomes a way more costly than running the agent, which is not a great state to be in, and so what you really want is that if the agent does this, the supervisor is able to say, I see that you just copied this stuff over from these other books in order to trick me into thinking that you had written something novel that was good.\n\nThat's bad. I'm penalizing you. Stop doing that in the future. Now, this sort of property, I mean it's very nice in the abstract, but who knows whether or not we can actually build it in practice. There's some reason for optimism that I don't think I can adequately convey, but I wrote a newsletter summarizing some of it sometime ago, but again, reading through the posts I became more optimistic that it was an achievable property, than when I first heard what the property was. The reason I'm optimistic about it is that it just sort of seems to capture the thing that we actually care about. It's not everything, like it doesn't solve the robustness problem. Universality only tells you what the agent's currently doing. You know all the facts about that. Whereas for robustness you want to say even in these hypothetical situations that the agent hasn't encountered yet and doesn't know stuff about, even when it encounters those situations, it's going to stay aligned with you so universality doesn't get you all the way there, but it definitely feels like it's getting you quite a bit.\n\n**Buck Shlegeris:** That's really interesting to hear you phrase it that way. I guess I would have thought of universality as a subset of robustness. I'm curious what you think of that first.\n\n**Rohin Shah:** I definitely think you could use universality to achieve a subset of robustness. Maybe I would say universality is a subset of interpretability.\n\n**Buck Shlegeris:** Yeah, and I care about interpretability as a subset of robustness basically, or as a subset of inner alignment, which is pretty close to robustness in my mind. The other thing I would say is you were saying there that one difference between universality and robustness is that universality only tells you why the agent did the thing it currently did, and this doesn't suffice to tell us about the situations that the agent isn't currently in. One really nice thing though is that if the agent is only acting a particular way because it wants you to trust it, that's a fact about its current behavior that you will know, and so if you have the universality property, your overseer just knows your agent is trying to deceive it. Which seems like it would be incredibly great and would resolve like half of my problem with safety if you had it.\n\n**Rohin Shah:** Yeah, that seems right. The case that universality doesn't cover is when your AI system is initially not deceptive, but then at some point in the future it's like, 'Oh my God, now it's possible to go and build Dyson spheres or something, but wait, in this situation probably I should be doing this other thing and humans won't like that. Now I better deceive humans'. The transition into deception would have to be a surprise in some sense even to the AI system.\n\n**Buck Shlegeris:** Yeah, I guess I'm just not worried about that. Suppose I have this system which is as smart as a reasonably smart human or 10 reasonably smart humans, but it's not as smart as the whole world. If I can just ask it what its best sense about how aligned it is, is? And if I can trust its answer? I don't know man, I'm pretty okay with systems that think they're aligned, answering that question honestly.\n\n**Rohin Shah:** I think I somewhat agree. I like this reversal where I'm the pessimistic one.\n\n**Buck Shlegeris:** Yeah me too. I'm like, \"look, system, I want you to think as hard as you can to come up with the best arguments you can come up with for why you are misaligned, and the problems with you.\" And if I just actually trust the system to get this right, then the bad outcomes I get here are just pure accidents. I just had this terrible initialization of my neural net parameters, such that I had this system that honestly believed that it was going to be aligned. And then as it got trained more, this suddenly changed and I couldn't do anything about it. I don't quite see the story for how this goes super wrong. It seems a lot less bad than the default situation.\n\n**Rohin Shah:** Yeah. I think the story I would tell is something like, well, if you look at humans, they're pretty wrong about what their preferences will be in the future. For example, there's this trope of how teenagers fall in love and then fall out of love, but when they're in love, they swear undying oaths to each other or something. To the extent that is true, that seems like the sort of failure that could lead to x-risk if it also happened with AI systems.\n\n**Buck Shlegeris:** I feel pretty optimistic about all the garden-variety approaches to solving this. Teenagers were not selected very hard on accuracy of their undying oaths. And if you instead had accuracy of self-model as a key feature you were selecting for in your AI system, plausibly you'll just be way more okay.\n\n**Rohin Shah:** Yeah. Maybe people could coordinate well on this. I feel less good about people coordinating on this sort of problem.\n\n**Buck Shlegeris:** For what it's worth, I think there are coordination problems here and I feel like my previous argument about why coordination is hard and won't happen by default also probably applies to us not being okay. I'm not sure how this all plays out. I'd have to think about it more.\n\n**Rohin Shah:** Yeah. I think it's more like this is a subtle and non-obvious problem, which by hypothesis doesn't happen in the systems you actually have and only happens later and those are the sorts of problems I'm like, Ooh, not sure if we can deal with those ones, but I agree that there's a good chance that there's just not a problem at all in the world where we already have universality and checked all the obvious stuff.\n\n**Buck Shlegeris:** Yeah. I would like to say universality is one of my other favorite areas of AI alignment research, in terms of how happy I'd be if it worked out really well.\n\n**Lucas Perry:** All right, so let's see if we can slightly pick up the pace here. Moving forward and starting with interpretability.\n\n**Rohin Shah:** Yeah, so I mean I think we've basically discussed interpretability already. Universality is a specific kind of interpretability, but the case for interpretability is just like, sure seems like it would be good if you could understand what your AI systems are doing. You could then notice when they're not aligned, and fix that somehow. It's a pretty clear cut case for a thing that would be good if we achieved it and it's still pretty uncertain how likely we are to be able to achieve it.\n\n**Lucas Perry:** All right, so let's keep it moving and let's hit impact regularization now.\n\n**Rohin Shah:** Yeah, impact regularization in particular is one of the ideas that are not trying to align the AI system but are instead trying to say, well, whatever AI system we build, let's make sure it doesn't cause a catastrophe. It doesn't lead to extinction or existential risk. What it hopes to do is say, all right, AI system, do whatever it is you wanted to do. I don't care about that. Just make sure that you don't have a huge impact upon the world.\n\nWhatever you do, keep your impact not too high. And so there's been a lot of work on this in recent years there's been relative reachability, attainable utility preservation, and I think in general the sense is like, wow, it's gone quite a bit further than people expected it to go. I think it definitely does prevent you from doing very, very powerful things of the sort, like if you wanted to stop all competing AI projects from ever being able to build AGI, that doesn't seem like the sort of thing you can do with an impact regularized AI system, but it sort of seems plausible that you could prevent convergent instrumental sub goals using impact regularization. Where AI systems that are trying to steal resources and power from humans, you could imagine that you'd say, hey, don't do that level of impact, you can still have the level of impact of say running a company or something like that.\n\n**Buck Shlegeris:** My take on all this is that I'm pretty pessimistic about all of it working. I think that impact regularization or whatever is a non-optimal point on the capabilities / alignment trade off or something, in terms of safety you're getting for how much capability you're sacrificing. My basic a problem here is basically analogous to my problem with value learning, where I think we're trying to take these extremely essentially fuzzy concepts and then factor our agent through these fuzzy concepts like impact, and basically the thing that I imagine happening is any impact regularization strategy you try to employ, if your AI is usable, will end up not helping with its alignment. For any definition of impacts you come up with, it'll end up doing something which gets around that. Or it'll make your AI system completely useless, is my basic guess as to what happens.\n\n**Rohin Shah:** Yeah, so I think again in this setting, if you formalize it and then say, consider the optimal agent. Yeah, that can totally get around your impact penalty, but in practice it sure seems like, what you want to do is say this convergent instrumental subgoal stuff, don't do any of that. Continue to do things that are normal in regular life. And those seem like pretty distinct categories. Such that I would not be shocked if we could actually distinguish between the two.\n\n**Buck Shlegeris:** It sounds like the main benefit you're going for is trying to make your AI system not do insane, convergent, instrumental sub-goal style stuff. So another approach I can imagine taking here would be some kind of value learning or something, where you're asking humans for feedback on whether plans are insanely convergent, instrumental sub-goal style, and just not doing the things which, when humans are asked to rate how sketchy the plans are the humans rate as sufficiently sketchy? That seems like about as good a plan. I'm curious what you think.\n\n**Rohin Shah:** The idea of power as your attainable utility across a wide variety of utility functions seems like a pretty good formalization to me. I think in the worlds where I actually buy a formalization, I tend to expect the formalization to work better. I do think the formalization is not perfect. Most notably with the current formalization of power, your power never changes if you have extremely good beliefs. Your notion, you're just like, I always have the same power because I'm always able to do the same things and you never get surprised, so maybe I agree with you because I think the current formalization is not good enough. Yeah, I think I agree with you but I could see it going either way.\n\n**Buck Shlegeris:** I could be totally wrong about this, and correct me if I'm wrong, my sense is that you have to be able to back out the agent's utility function or its models of the world. Which seems like it's assuming a particular path for AI development which doesn't seem to me particularly likely.\n\n**Rohin Shah:** I definitely agree with that for all the current methods too.\n\n**Buck Shlegeris:** So it's like: assume that we have already perfectly solved our problems with universality and robustness and transparency and whatever else. I feel like you kind of have to have solved all of those problems before you can do this, and then you don't need it or something.\n\n**Rohin Shah:** I don't think I agree with that. I definitely agree that the current algorithms that people have written assume that you can just make a change to the AI's utility function. I don't think that's what even their proponents would suggest as the actual plan.\n\n**Buck Shlegeris:** What is the actual plan?\n\n**Rohin Shah:** I don't actually know what their actual plan would be, but one plan I could imagine is figure out what exactly the conceptual things we have to do with impact measurement are, and then whatever method we have for building AGI, probably there's going to be some part which is specify the goal and then in the specify goal part, instead of just saying pursue X, we want to say pursue X without changing your ability to pursue Y, and Z and W, and P, and Q.\n\n**Buck Shlegeris:** I think that that does not sound like a good plan. I don't think that we should expect our AI systems to be structured that way in the future.\n\n**Rohin Shah:** Plausibly we have to do this with natural language or something.\n\n**Buck Shlegeris:** It seems very likely to me that the thing you do is reinforcement learning where at the start of the episode you get a sentence of English which is telling you what your goal is and then blah, blah, blah, blah, blah, and this seems like a pretty reasonable strategy for making powerful and sort of aligned AI. Aligned enough to be usable for things that aren't very hard. But you just fundamentally don't have access to the internal representations that the AI is using for its sense of what belief is, and stuff like that. And that seems like a really big problem.\n\n**Rohin Shah:** I definitely see this as more of an outer alignment thing, or like an easier to specify outer alignment type thing than say, IDA is that type stuff.\n\n**Buck Shlegeris:** Okay, I guess that makes sense. So we're just like assuming we've solved all the inner alignment problems?\n\n**Rohin Shah:** In the story so far yeah, I think all of the researchers who actually work on this haven't thought much about inner alignment.\n\n**Buck Shlegeris:** My overall summary is that I really don't like this plan. I feel like it's not robust to scale. As you were saying Rohin, if your system gets more and more accurate beliefs, stuff breaks. It just feels like the kind of thing that doesn't work.\n\n**Rohin Shah:** I mean, it's definitely not conceptually neat and elegant in the sense of it's not attacking the underlying problem. And in a problem setting where you expect adversarial optimization type dynamics, conceptual elegance actually does count for quite a lot in whether or not you believe your solution will work.\n\n**Buck Shlegeris:** I feel it’s like trying to add edge detectors to your image classifiers to make them more adversarily robust or something, which is backwards.\n\n**Rohin Shah:** Yeah, I think I agree with that general perspective. I don't actually know if I'm more optimistic than you. Maybe I just don't say... Maybe we'd have the same uncertainty distributions and you just say yours more strongly or something.\n\n**Lucas Perry:** All right, so then let's just move a little quickly through the next three, which are causal modeling, oracles, and decision theory.\n\n**Rohin Shah:** Yeah, I mean, well decision theory, MIRI did some work on it. I am not the person to ask about it, so I'm going to skip that one. Even if you look at the long version, I'm just like, here are some posts. Good luck. So causal modeling, I don't fully understand what the overall story is here but the actual work that's been published is basically what we can do is we can take potential plans or training processes for AI systems. We can write down causal models that tell us how the various pieces of the training system interact with each other and then using algorithms developed for causal models we can tell when an AI system would have an incentive to either observe or intervene on an underlying variable.\n\nOne thing that came out of this was that you can build a model-based reinforcement learner that doesn't have any incentive to wire head as long as when it makes its plans, the plans are evaluated by the current reward function as opposed to whatever future reward function it would have. And that was explained using this framework of causal modeling. Oracles, Oracles are basically the idea that we can just train an AI system to just answer questions, give it a question and it tries to figure out the best answer it can to that question, prioritizing accuracy.\n\nOne worry that people have recently been talking about is the predictions that the Oracle makes then affect the world, which can affect whether or not the prediction was correct. Like maybe if I predict that I will go to bed at 11 then I'm more likely to actually go to bed at 11 because I want my prediction to come true or something and so then the Oracles can still “choose” between different self confirming predictions and so that gives them a source of agency and one way that people want to avoid this is using what are called counter-factual Oracles where you set up the training, such that the Oracles are basically making predictions under the assumption that their predictions are not going to influence the future.\n\n**Lucas Perry:** Yeah, okay. Oracles seem like they just won't happen. There'll be incentives to make things other than Oracles and that Oracles would even be able to exert influence upon the world in other ways.\n\n**Rohin Shah:** Yeah, I think I agree that Oracles do not seem very competitive.\n\n**Lucas Perry:** Let's do forecasting now then.\n\n**Rohin Shah:** So the main sub things within forecasting one, there's just been a lot of work recently on actually building good forecasting technology. There has been an AI specific version of Metaculus that's been going on for a while now. There's been some work at the Future of Humanity Institute on building better tools for working with probability distributions under recording and evaluating forecasts. There was an AI resolution council where basically now you can make forecasts about what this particular group of people will think in five years or something like that, which is much easier to operationalize than most other kinds of forecasts. So this helps with constructing good questions. On the actual object level, I think there are two main things. One is that it became increasingly more obvious in the past two years that AI progress currently is being driven by larger and larger amounts of compute.\n\nIt totally could be driven by other things as well, but at the very least, compute is a pretty important factor. And then takeoff speeds. So there's been this long debate in the AI safety community over whether -- to take the extremes, whether or not we should expect that AI capabilities will see a very sharp spike. So initially, your AI capabilities are improving by like one unit a year, maybe then with some improvements it got to two units a year and then for whatever reason, suddenly they're now at 20 units a year or a hundred units a year and they just swoop way past what you would get by extrapolating past trends, and so that's what we might call a discontinuous takeoff. If you predict that that won't happen instead you'll get AI that's initially improving at one unit per year. Then maybe two units per year, maybe three units per year. Then five units per year, and the rate of progress continually increases. The world's still gets very, very crazy, but in a sort of gradual, continuous way that would be called a continuous takeoff.\n\nBasically there were two posts that argued pretty forcefully for continuous takeoff back in, I want to say February of 2018, and this at least made me believe that continuous takeoff was more likely. Sadly, we just haven't actually seen much defense of the other side of the view since then. Even though we do know that there definitely are people who still believe the other side, that there will be a discontinuous takeoff.\n\n**Lucas Perry:** Yeah so what are both you guys' views on them?\n\n**Buck Shlegeris:** Here are a couple of things. One is that I really love the operationalization of slow take off or continuous take off that Paul provided in his post, which was one of the ones Rohin was referring to from February 2018. He says, “by slow takeoff, I mean that there is a four year doubling of the economy before there is a one year doubling of the economy.” As in, there's a period of four years over which world GDP increases by a factor four, after which there is a period of one year. As opposed to a situation where the first one-year doubling happens out of nowhere. Currently, doubling times for the economy are on the order of like 20 years, and so a one year doubling would be a really big deal. The way that I would phrase why we care about this, is because worlds where we have widespread, human level AI feel like they have incredibly fast economic growth. And if it's true that we expect AI progress to increase gradually and continuously, then one important consequence of this is that by the time we have human level AI systems, the world is already totally insane. A four year doubling would just be crazy. That would be economic growth drastically higher than economic growth currently is.\n\nThis means it would be obvious to everyone who's paying attention that something is up and the world is radically changing in a rapid fashion. Another way I've been thinking about this recently is people talk about transformative AI, by which they mean AI which would have at least as much of an impact on the world as the industrial revolution had. And it seems plausible to me that octopus level AI would be transformative. Like suppose that AI could just never get better than octopus brains. This would be way smaller of a deal than I expect AI to actually be, but it would still be a massive deal, and would still possibly lead to a change in the world that I would call transformative. And if you think this is true, and if you think that we're going to have octopus level AI before we have human level AI, then you should expect that radical changes that you might call transformative have happened by the time that we get to the AI alignment problems that we've been worrying about. And if so, this is really big news.\n\nWhen I was reading about this stuff when I was 18, I was casually imagining that the alignment problem is a thing that some people have to solve while they're building an AGI in their lab while the rest of the world's ignoring them. But if the thing which is actually happening is the world is going insane around everyone, that's a really important difference.\n\n**Rohin Shah:** I would say that this is probably the most important contested question in AI alignment right now. Some consequences of it are in a gradual or continuous takeoff world you expect that by the time we get to systems that can pose an existential risk. You've already had pretty smart systems that have been deployed in the real world. They probably had some failure modes. Whether or not we call them alignment failure modes or not is maybe not that important. The point is people will be aware that AI systems can fail in weird ways, depending on what sorts of failures you expect, you might expect this to lead to more coordination, more involvement in safety work. You might also be more optimistic about using testing and engineering styles of approaches to the problem which rely a bit more on trial and error type of reasoning because you actually will get a chance to see errors before they happen at a super intelligent existential risk causing mode. There are lots of implications of this form that pretty radically change which alignment plans you think are feasible.\n\n**Buck Shlegeris:** Also, it's pretty radically changed how optimistic you are about this whole AI alignment situation, at the very least, people who are very optimistic about AI alignment causing relatively small amounts of existential risk. A lot of the reason for this seems to be that they think that we're going to get these warning shots where before we have superintelligent AI, we have sub-human level intelligent AI with alignment failures like the cashier Rohin was talking about earlier. And then people start caring about AI alignment a lot more. So optimism is also greatly affected by what you think about this.\n\nI've actually been wanting to argue with people about this recently. I wrote a doc last night where I was arguing that even in gradual takeoff worlds, we should expect a reasonably high probability of doom if we can't solve the AI alignment problem. And I'm interested to have this conversation in more detail with people at some point. But yeah, I agree with what Rohin said.\n\nOverall on takeoff speeds, I guess I still feel pretty uncertain. It seems to me that currently, what we can do with AI, like AI capabilities are increasing consistently, and a lot of this comes from applying relatively non-mindblowing algorithmic ideas to larger amounts of compute and data. And I would be kind of surprised if you can't basically ride this wave away until you have transformative AI. And so if I want to argue that we're going to have fast takeoffs, I kind of have to argue that there's some other approach you can take which lets you build AI without having to go along that slow path, which also will happen first. And I guess I think it's kind of plausible that that is what's going to happen. I think that's what you'd have to argue for if you want to argue for a fast take off.\n\n**Rohin Shah:** That all seems right to me. I'd be surprised if, out of nowhere, we saw a new AI approach suddenly started working and overtook deep learning. You also have to argue that it then very quickly reaches human level AI, which would be quite surprising, right? In some sense, it would have to be something completely novel that we failed to think about in the last 60 years. We're putting in way more effort now than we were in the last 60 years, but then counter counterpoint is that all of that extra effort is going straight into deep learning. It's not really searching for completely new paradigm-shifting ways to get to AGI.\n\n**Buck Shlegeris:** So here's how I'd make that argument. Perhaps a really important input into a field like AI, is the number of really smart kids who have been wanting to be an AI researcher since they were 16 because they thought that it's the most important thing in the world. I think that in physics, a lot of the people who turn into physicists have actually wanted to be physicists forever. I think the number of really smart kids who wanted to be AI researchers forever has possibly gone up by a factor of 10 over the last 10 years, it might even be more. And there just are problems sometimes, that are bottle necked on that kind of a thing, probably. And so it wouldn't be totally shocking to me, if as a result of this particular input to AI radically increasing, we end up in kind of a different situation. I haven't quite thought through this argument fully.\n\n**Rohin Shah:** Yeah, the argument seems plausible. There's a large space of arguments like this. I think even after that, then I've started questioning, \"Okay, we get a new paradigm. The same arguments apply to that paradigm?\" Not as strongly. I guess not the arguments you were saying about compute going up over time, but the arguments given in the original slow takeoff posts which were people quickly start taking the low-hanging fruit and then move on. When there's a lot of effort being put into getting some property, you should expect that easy low-hanging fruit is usually just already taken, and that's why you don't expect discontinuities. Unless the new idea just immediately rockets you to human-level AGI, or x-risk causing AGI, I think the same argument would pretty quickly start applying to that as well.\n\n**Buck Shlegeris:** I think it's plausible that you do get rocketed pretty quickly to human-level AI. And I agree that this is an insane sounding claim.\n\n**Rohin Shah:** Great. As long as we agree on that.\n\n**Buck Shlegeris:** Something which has been on my to-do list for a while, and something I've been doing a bit of and I'd be excited for someone else doing more of, is reading the history of science and getting more of a sense of what kinds of things are bottlenecked by what, where. It could lead me to be a bit less confused about a bunch of this stuff. AI Impacts has done a lot of great work cataloging all of the things that aren't discontinuous changes, which certainly is a strong evidence to me against my claim here.\n\n**Lucas Perry:** All right. What is the probability of AI-induced existential risk?\n\n**Rohin Shah:** Unconditional on anything? I might give it 1 in 20. 5%.\n\n**Buck Shlegeris:** I'd give 50%.\n\n**Rohin Shah:** I had a conversation with AI Impacts that went into this in more detail and partially just anchored on the number I gave there, which was 10% conditional on no intervention from longtermists, I think the broad argument is really just the one that Buck and I were disagreeing about earlier, which is to what extent will society be incentivized to solve the problem? There's some chance that the first thing we try just works and we don't even need to solve any sort of alignment problem. It might just be fine. This is not implausible to me. Maybe that's 30% or something.\n\nMost of the remaining probability comes from, \"Okay, the alignment problem is a real problem. We need to deal with it.\" It might be very easy in which case we can just solve it straight away. That might be the case. That doesn't seem that likely to me if it was a problem at all. But what we will get is a lot of these warning shots and people understanding the risks a lot more as we get more powerful AI systems. This estimate is also conditional on gradual takeoff. I keep forgetting to say that, mostly because I don't know what probability I should put on discontinuous takeoff.\n\n**Lucas Perry:** So is 5% with longtermist intervention, increasing to 10% if fast takeoff?\n\n**Rohin Shah:** Yes, but still with longtermist intervention. I'm pretty pessimistic on fast takeoff, but my probability assigned to fast takeoff is not very high. In a gradual takeoff world, you get a lot of warning shots. There will just generally be awareness of the fact that the alignment problem is a real thing and you won't have the situation you have right now of people saying this thing about worrying about superintelligent AI systems not doing what we want is totally bullshit. That won't be a thing. Almost everyone will not be saying that anymore, in the version where we're right and there is a problem. As a result, people will not want to build AI systems that are going to kill them. People tend to be pretty risk averse in my estimation of the world, which Buck will probably disagree with. And as a result, you'll get a lot of people trying to actually work on solving the alignment problem. There'll be some amount of global coordination which will give us more time to solve the alignment problem than we may otherwise have had. And together, these forces mean that probably we'll be okay.\n\n**Buck Shlegeris:** So I think my disagreements with Rohin are basically that I think fast takeoffs are more likely. I basically think there is almost surely a problem. I think that alignment might be difficult, and I'm more pessimistic about coordination. I know I said four things there, but I actually think of this as three disagreements. I want to say that \"there isn't actually a problem\" is just a kind of \"alignment is really easy to solve.\" So then there's three disagreements. One is gradual takeoff, another is difficulty of solving competitive prosaic alignment, and another is how good we are at coordination.\n\nI haven't actually written down these numbers since I last changed my mind about a lot of the inputs to them, so maybe I'm being really dumb. I guess, it feels to me that in fast takeoff worlds, we are very sad unless we have competitive alignment techniques, and so then we're just only okay if we have these competitive alignment techniques. I guess I would say that I'm something like 30% on us having good competitive alignment techniques by the time that it's important, which incidentally is higher than Rohin I think.\n\n**Rohin Shah:** Yeah, 30 is totally within the 25th to 75th interval on the probability, which is a weird thing to be reporting. 30 might be my median, I don't know.\n\n**Buck Shlegeris:** To be clear, I'm not just including the outer alignment proportion here, which is what we were talking about before with IDA. I'm also including the inner alignment.\n\n**Rohin Shah:** Yeah, 30% does seem a bit high. I think I'm a little more pessimistic.\n\n**Buck Shlegeris:** So I'm like 30% that we can just solve the AI alignment problem in this excellent way, such that anyone who wants to can have very little extra cost and then make AI systems that are aligned. I feel like in worlds where we did that, it's pretty likely that things are reasonably okay. I think that the gradual versus fast takeoff isn't actually enormously much of a crux for me because I feel like in worlds without competitive alignment techniques and gradual takeoff, we still have a very high probability of doom. And I think that comes down to disagreements about coordination. So maybe the main important disagreement between Rohin and I is, actually how well we'll be able to coordinate, or how strongly individual incentives will be for alignment.\n\n**Rohin Shah:** I think there are other things. The reason I feel a bit more pessimistic than you in the fast takeoff world is just solving problems in advance just is really quite difficult and I really like the ability to be able to test techniques on actual AI systems. You'll have to work with less powerful things. At some point, you do have to make the jump to more powerful things. But, still, being able to test on the less powerful things, that’s so good, so much safety from there.\n\n**Buck Shlegeris:** It's not actually clear to me that you get to test the most important parts of your safety techniques. So I think that there are a bunch of safety problems that just do not occur on dog-level AIs, and do occur on human-level AI. If there are three levels of AI, there's a thing which is as powerful as a dog, there's a thing which is as powerful as a human, and there's a thing which is as powerful as a thousand John von Neumanns. In gradual takeoff world, you have a bunch of time in both of these two milestones, maybe. I guess it's not super clear to me that you can use results on less powerful systems as that much evidence about whether your safety techniques work on drastically more powerful systems. It's definitely somewhat helpful.\n\n**Rohin Shah:** It depends what you condition on in your difference between continuous takeoff and discontinuous takeoff to say which one of them happens faster. I guess the delta between dog and human is definitely longer in gradual takeoff for sure. Okay, if that's what you were saying, yep, I agree with that.\n\n**Buck Shlegeris:** Yeah, sorry, that's all I meant.\n\n**Rohin Shah:** Cool. One thing I wanted to ask is when you say dog-level AI assistant, do you mean something like a neural net that if put in a dog's body replacing its brain would do about as well as a dog? Because such a neural net could then be put in other environments and learn to become really good at other things, probably superhuman at many things that weren't in the ancestral environment. Do you mean that sort of thing?\n\n**Buck Shlegeris:** Yeah, that's what I mean. Dog-level AI is probably much better than GPT2 at answering questions. I'm going to define something as dog-level AI, if it's about as good as a dog at things which I think dogs are pretty heavily optimized for, like visual processing or motor control in novel scenarios or other things like that, that I think dogs are pretty good at.\n\n**Rohin Shah:** Makes sense. So I think in that case, plausibly, dog-level AI already poses an existential risk. I can believe that too.\n\n**Buck Shlegeris:** Yeah.\n\n**Rohin Shah:** The AI cashier example feels like it could totally happen probably before a dog-level AI. You've got all of the motivation problems already at that point of the game, and I don't know what problems you expect to see beyond then.\n\n**Buck Shlegeris:** I'm more talking about whether you can test your solutions. I'm not quite sure how to say my intuitions here. I feel like there are various strategies which work for corralling dogs and which don't work for making humans do what you want. In as much as your alignment strategy is aiming at a flavor of problem that only occurs when you have superhuman things, you don't get to test that either way. I don't think this is a super important point unless you think it is. I guess I feel good about moving on from here.\n\n**Rohin Shah:** Mm-hmm (affirmative). Sounds good to me.\n\n**Lucas Perry:** Okay, we've talked about what you guys have called gradual and fast takeoff scenarios, or continuous and discontinuous. Could you guys put some probabilities down on the likelihood of, and stories that you have in your head, for fast and slow takeoff scenarios?\n\n**Rohin Shah:** That is a hard question. There are two sorts of reasoning I do about probabilities. One is: use my internal simulation of whatever I'm trying to predict, internally simulate what it looks like, whether it's by my own models, is it likely? How likely is it? At what point would I be willing to bet on it. Stuff like that. And then there's a separate extra step where I'm like, \"What do other people think about this? Oh, a lot of people think this thing that I assigned one percent probability to is very likely. Hmm, I should probably not be saying one percent then.\" I don't know how to do that second part for, well, most things but especially in this setting. So I'm going to just report Rohin's model only, which will predictably be understating the probability for fast takeoff in that if someone from MIRI were to talk to me for five hours, I would probably say a higher number for the probability of fast takeoff after that, and I know that that's going to happen. I'm just going to ignore that fact and report my own model anyway.\n\nOn my own model, it's something like in worlds where AGI happens soon, like in the next couple of decades, then I'm like, \"Man, 95% on gradual take off.\" If it's further away, like three to five decades, then I'm like, \"Some things could have changed by then, maybe I'm 80%.\" And then if it's way off into the future and centuries, then I'm like, \"Ah, maybe it's 70%, 65%.\" The reason it goes down over time is just because it seems to me like if you want to argue for discontinuous takeoff, you need to posit that there's some paradigm change in how AI progress is happening and that seems more likely the further in the future you go.\n\n**Buck Shlegeris:** I feel kind of surprised that you get so low, like to 65% or 70%. I would have thought that those arguments are a strong default and then maybe at the moment where in a position that seems particularly gradual takeoff-y, but I would have thought that you over time get to 80% or something.\n\n**Rohin Shah:** Yeah. Maybe my internal model is like, \"Holy shit, why do these MIRI people keep saying that discontinuous takeoff is so obvious.\" I agree that the arguments in Paul's posts feel very compelling to me and so maybe I should just be more confident in them. I think saying 80%, even in centuries is plausibly a correct answer.\n\n**Lucas Perry:** So, Rohin, is the view here that since compute is the thing that's being leveraged to make most AI advances that you would expect that to be the mechanism by which that continues to happen in the future and we have some certainty over how compute continues to change into the future? Whereas things that would be leading to a discontinuous takeoff would be world-shattering, fundamental insights into algorithms that would have powerful recursive self-improvement, which is something you wouldn't necessarily see if we just keep going this leveraging compute route?\n\n**Rohin Shah:** Yeah, I think that's a pretty good summary. Again, on the backdrop of the default argument for this is people are really trying to build AGI. It would be pretty surprising if there is just this really important thing that everyone had just missed.\n\n**Buck Shlegeris:** It sure seems like in machine learning when I look at the things which have happened over the last 20 years, all of them feel like the ideas are kind of obvious or someone else had proposed them 20 years earlier. ConvNets were proposed 20 years before they were good on ImageNet, and LSTMs were ages before they were good for natural language, and so on and so on and so on. Other subjects are not like this, like in physics sometimes they just messed around for 50 years before they knew what was happening. I don't know, I feel confused how to feel about the fact that in some subjects, it feels like they just do suddenly get better at things for reasons other than having more compute.\n\n**Rohin Shah:** I think physics, at least, was often bottlenecked by measurements, I want to say.\n\n**Buck Shlegeris:** Yes, so this is one reason I've been interested in history of science recently, but there are certainly a bunch of things. People were interested in chemistry for a long time and it turns out that chemistry comes from quantum mechanics and you could, theoretically, have guessed quantum mechanics 70 years earlier than people did if you were smart enough. It's not that complicated a hypothesis to think of. Or relativity is the classic example of something which could have been invented 50 years earlier. I don't know, I would love to learn more about this.\n\n**Lucas Perry:** Just to tie this back to the question, could you give your probabilities as well?\n\n**Buck Shlegeris:** Oh, geez, I don't know. Honestly, right now I feel like I'm 70% gradual takeoff or something, but I don't know. I might change my mind if I think about this for another hour. And there's also theoretical arguments as well for why most takeoffs are gradual, like the stuff in Paul's post. The easiest summary is, before someone does something really well, someone else does it kind of well in cases where a lot of people are trying to do the thing.\n\n**Lucas Perry:** Okay. One facet of this, that I haven't heard discussed, is recursive self-improvement, and I'm confused about where that becomes the thing that affects whether it's discontinuous or continuous. If someone does something kind of well before something does something really well, if recursive self-improvement is a property of the thing being done kind of well, is it just kind of self-improving really quickly, or?\n\n**Buck Shlegeris:** Yeah. I think Paul's post does a great job of talking about this exact argument. I think his basic claim is, which I find pretty plausible, before you have a system which is really good at self-improving, you have a system which is kind of good at self-improving, if it turns out to be really helpful to have a system be good at self-improving. And as soon as this is true, you have to posit an additional discontinuity.\n\n**Rohin Shah:** One other thing I'd note is that humans are totally self improving. Productivity techniques, for example, are a form of self-improvement. You could imagine that AI systems might have advantages that humans don't, like being able to read their own weights and edit them directly. How much of an advantage this gives to the AI system, unclear. Still, I think then I just go back to the argument that Buck already made, which is at some point you get to an AI system that is somewhat good at understanding its weights and figuring out how to edit them, and that happens before you get the really powerful ones. Maybe this is like saying, \"Well, you'll reach human levels of self-improvement by the time you have rat-level AI or something instead of human-level AI,\" which argues that you'll hit this hyperbolic point of the curve earlier, but it still looks like a hyperbolic curve that's still continuous at every point.\n\n**Buck Shlegeris:** I agree.\n\n**Lucas Perry:** I feel just generally surprised about your probabilities on continuous takeoff scenarios that they'd be slow.\n\n**Rohin Shah:** The reason I'm trying to avoid the word slow and fast is because they're misleading. Slow takeoff is not slow in calendar time relative to fast takeoff. The question is, is there a spike at some point? Some people, upon reading Paul's posts are like, \"Slow takeoff is faster than fast takeoff.\" That's a reasonably common reaction to it.\n\n**Buck Shlegeris:** I would put it as slow takeoff is the claim that things are insane before you have the human-level AI.\n\n**Rohin Shah:** Yeah.\n\n**Lucas Perry:** This seems like a helpful perspective shift on this takeoff scenario question. I have not read Paul's post. What is it called so that we can include it in the page for this podcast?\n\n**Rohin Shah:** It's just called Takeoff Speeds. Then the corresponding AI Impacts post is called Will AI See Discontinuous Progress?, I believe.\n\n**Lucas Perry:** So if each of you guys had a lot more reach and influence and power and resources to bring to the AI alignment problem right now, what would you do?\n\n**Rohin Shah:** I get this question a lot and my response is always, \"Man, I don't know.\" It seems hard to scalably use people right now for AI risk. I can talk about which areas of research I'd like to see more people focus on. If you gave me people where I'm like, \"I trust your judgment on your ability to do good conceptual work\" or something, where would I put them? I think a lot of it would be on making good robust arguments for AI risk. I don't think we really have them, which seems like kind of a bad situation to be in. I think I would also invest a lot more in having good introductory materials, like this review, except this review is a little more aimed at people who are already in the field. It is less aimed at people who are trying to enter the field. I think we just have pretty terrible resources for people coming into the field and that should change.\n\n**Buck Shlegeris:** I think that our resources are way better than they used to be.\n\n**Rohin Shah:** That seems true.\n\n**Buck Shlegeris:** In the course of my work, I talk to a lot of people who are new to AI alignment about it and I would say that their level of informedness is drastically better now than it was two years ago. A lot of which is due to things like 80,000 hours podcast, and other things like this podcast and the Alignment Newsletter, and so on. I think we just have made it somewhat easier for people to get into everything. The Alignment Forum, having its sequences prominently displayed, and so on.\n\n**Rohin Shah:** Yeah, you named literally all of the things I would have named. Buck definitely has more information on this than I do. I do not work with people who are entering the field as much. I do think we could be substantially better.\n\n**Buck Shlegeris:** Yes. I feel like I do have access to resources, not directly but in the sense that I know people at eg Open Philanthropy and the EA Funds  and if I thought there were obvious things they should do, I think it's pretty likely that those funders would have already made them happen. And I occasionally embark on projects myself that I think are good for AI alignment, mostly on the outreach side. On a few occasions over the last year, I've just done projects that I was optimistic about. So I don't think I can name things that are just shovel-ready opportunities for someone else to do, which is good news because it's mostly because I think most of these things are already being done.\n\nI am enthusiastic about workshops. I help run with MIRI these AI Risks for Computer Scientists workshops and I ran my own computing workshop with some friends, with kind of a similar purpose, aimed at people who are interested in this kind of stuff and who would like to spend some time learning more about it. I feel optimistic about this kind of project as a way of doing the thing Rohin was saying, making it easier for people to start having really deep thoughts about a lot of AI alignment stuff. So that's a kind of direction of projects that I'm pretty enthusiastic about. A couple other random AI alignment things I'm optimistic about. I've already mentioned that I think there should be an Ought competitor just because it seems like the kind of thing that more work could go into. I agree with Rohin on it being good to have more conceptual analysis of a bunch of this stuff. I'm generically enthusiastic about there being more high quality research done and more smart people, who've thought about this a lot, working on it as best as they can.\n\n**Rohin Shah:** I think the actual bottleneck is good research and not necessarily field building, and I'm more optimistic about good research. Specifically, I am particularly interested in universality, interpretability. I would love for there to be some way to give people who work on AI alignment the chance to step back and think about the high-level picture for a while. I don't know if people don't do this because they don't want to or because they don't feel like they have the affordance to do so, and I would like the affordance to be there. I'd be very interested in people building models of what AGI systems could look like. Expected utility maximizers are one example of a model that you could have. Maybe we just try to redo evolution. We just create a very complicated, diverse environment with lots of agents going around and in their multi-agent interaction, they develop general intelligence somehow. I'd be interested for someone to take that scenario, flesh it out more, and then talk about what the alignment problem looks like in that setting.\n\n**Buck Shlegeris:** I would love to have someone get really knowledgeable about evolutionary biology and try and apply analogies of that to AI alignment. I think that evolutionary biology has lots of smart things to say about what optimizers are and it'd be great to have those insights. I think Eliezer sort of did this many years ago. It would be good for more people to do this in my opinion.\n\n**Lucas Perry:** All right. We're in the home stretch here. AI timelines. What do you think about the current state of predictions? There's been surveys that have been done with people giving maybe 50% probability over most researchers at about 2050 or so. What are each of your AI timelines? What's your probability distribution look like? What do you think about the state of predictions on this?\n\n**Rohin Shah:** Haven't looked at the state of predictions in a while. It depends on who was surveyed. I think most people haven't thought about it very much and I don't know if I expect their predictions to be that good, but maybe wisdom of the crowds is a real thing. I don't think about it very much. I mostly use my inside view and talk to a bunch of people. Maybe, median, 30 years from now, which is 2050. So I guess I agree with them, don't I? That feels like an accident. The surveys were not an input into this process.\n\n**Lucas Perry:** Okay, Buck?\n\n**Buck Shlegeris:** I don't know what I think my overall timelines are. I think AI in the next 10 or 20 years is pretty plausible. Maybe I want to give it something around 50% which puts my median at around 2040. In terms of the state of things that people have said about AI timelines, I have had some really great conversations with people about their research on AI timelines which hasn't been published yet. But at some point in the next year, I think it's pretty likely that much better stuff about AI timelines modeling will have been published than has currently been published, so I'm excited for that.\n\n**Lucas Perry:** All right. Information hazards. Originally, there seemed to be a lot of worry in the community about information hazards and even talking about superintelligence and being afraid of talking to anyone in positions of power, whether they be in private institutions or in government, about the strategic advantage of AI, about how one day it may confer a decisive strategic advantage. The dissonance here for me is that Putin comes out and says that who controls AI will control the world. Nick Bostrom published Superintelligence, which basically says what I already said. Max Tegmark's Life 3.0 basically also. My initial reaction and intuition is the cat's out of the bag. I don't think that echoing this increases risks any further than the risk is already at. But maybe you disagree.\n\n**Buck Shlegeris:** Yeah. So here are two opinions I have about info hazards. One is: how bad is it to say stuff like that all over the internet? My guess is it's mildly bad because I think that not everyone thinks those things. I think that even if you could get those opinions as consequences from reading Superintelligence, I think that most people in fact have not read Superintelligence. Sometimes there are ideas where I just really don't want them to be crystallized common knowledge. I think that, to a large extent, assuming gradual takeoff worlds, it kind of doesn't matter because AI systems are going to be radically transforming the world inevitably. I guess you can affect how governments think about it, but it's a bit different there.\n\nThe other point I want to make about info hazards is I think there are a bunch of trickinesses with AI safety, where thinking about AI safety makes you think about questions about how AI development might go. I think that thinking about how AI development is going to go occasionally leads to think about things that are maybe, could be, relevant to capabilities, and I think that this makes it hard to do research because you then get scared about talking about them.\n\n**Rohin Shah:** So I think my take on this is info hazards are real in the sense that there, in fact, are costs to saying specific kinds of information and publicizing them a bit. I think I'll agree in principle that some kinds of capabilities information has the cost of accelerating timelines. I usually think these are pretty strongly outweighed by the benefits in that it just seems really hard to be able to do any kind of shared intellectual work when you're constantly worried about what you do and don't make public. It really seems like if you really want to build a shared understanding within the field of AI alignment, that benefit is worth saying things that might be bad in some other ways. This depends on a lot of background facts that I'm not going to cover here but, for example, I probably wouldn't say the same thing about bio security.\n\n**Lucas Perry:** Okay. That makes sense. Thanks for your opinions on this. So at the current state in time, do you guys think that people should be engaging with people in government or in policy spheres on questions of AI alignment?\n\n**Rohin Shah:** Yes, but not in the sense of we're worried about when AGI comes. Even saying things like it might be really bad, as opposed to saying it might kill everybody, seems not great. Mostly on the basis of my model for what it takes to get governments to do things is, at the very least, you need consensus in the field so it seems kind of pointless to try right now. It might even be poisoning the well for future efforts. I think it does make sense to engage with government and policymakers about things that are in fact problems right now. To the extent that you think that recommender systems are causing a lot of problems, I think it makes sense to engage with government about how alignment-like techniques can help with that, especially if you're doing a bunch of specification learning-type stuff. That seems like the sort of stuff that should have relevance today and I think it would be great if those of us who did specification learning were trying to use it to improve existing systems.\n\n**Buck Shlegeris:** This isn't my field. I trust the judgment of a lot of other people. I think that it's plausible that it's worth building relationships with governments now, not that I know what I'm talking about. I will note that I basically have only seen people talk about how to do AI governance in the cases where the AI safety problem is 90th percentile easiest. I basically only see people talking about it in the case where the technical safety problem is pretty doable, and this concerns me. I've just never seen anyone talk about what you do in a world where you're as pessimistic as I am, except to completely give up.\n\n**Lucas Perry:** All right. Wrapping up here, is there anything else that we didn't talk about that you guys think was important? Or something that we weren't able to spend enough time on, that you would've liked to spend more time on?\n\n**Rohin Shah:** I do want to eventually continue the conversation with Buck about coordination, but that does seem like it should happen not on this podcast.\n\n**Buck Shlegeris:** That's what I was going to say too. Something that I want someone to do is write a trajectory for how AI goes down, that is really specific about what the world GDP is in every one of the years from now until insane intelligence explosion. And just write down what the world is like in each of those years because I don't know how to write an internally consistent, plausible trajectory. I don't know how to write even one of those for anything except a ridiculously fast takeoff. And this feels like a real shame.\n\n**Rohin Shah:** That seems good to me as well. And also the sort of thing that I could not do because I don't know economics.\n\n**Lucas Perry:** All right, so let's wrap up here then. So if listeners are interested in following either of you or seeing more of your blog posts or places where you would recommend they read more materials on AI alignment, where can they do that? We'll start with you, Buck.\n\n**Buck Shlegeris:** You can Google me and find my website. I often post things on the Effective Altruism Forum. If you want to talk to me about AI alignment in person, perhaps you should apply to the AI Risks for Computer Scientists workshops run by MIRI.\n\n**Lucas Perry:** And Rohin?\n\n**Rohin Shah:** I write the Alignment Newsletter. That's a thing that you could sign up for. Also on my website, if you Google Rohin Shah Alignment Newsletter, I'm sure I will come up. These are also cross posted to the Alignment Forum, so another thing you can do is go to the Alignment Forum, look up my username and just see things that are there. I don't know that this is actually the thing that you want to be doing. If you're new to AI safety and want to learn more about it, I would echo the resources Buck mentioned earlier, which are the 80k podcasts about AI alignment. There are probably on the order of five of these. There's the Alignment Newsletter. There are the three recommended sequences on the Alignment Forum. Just go to alignmentforum.org and look under recommended sequences. And this podcast, of course.\n\n**Lucas Perry:** All right. Heroic job, everyone. This is going to be a really good resource, I think. It's given me a lot of perspective on how thinking has changed over the past year or two.\n\n**Buck Shlegeris:** And we can listen to it again in a year and see how dumb we are.\n\n**Lucas Perry:** Yeah. There were lots of predictions and probabilities given today, so it'll be interesting to see how things are in a year or two from now. That'll be great. All right, so cool. Thank you both so much for coming on.\n\n*End of recorded material*", "url": "https://www.alignmentforum.org/posts/6skeZgctugzBBEBw3/ai-alignment-podcast-an-overview-of-technical-ai-alignment", "date_published": "2020-04-16T00:50:38Z", "authors": ["Palus Astra"], "tags": ["Interviews", "Transcripts", "Value Learning", "Moral Uncertainty", "AI", "Regulation and AI Risk"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.737349+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "20c7c73eb778b27e90cde12f4b6e9406", "source": "alignmentforum", "title": "AI Services as a Research Paradigm", "text": "Introduction\n============\n\n\nI would hereby like to advertise a document [Systems of Services\nas a Paradigm for AI Alignment](https://docs.google.com/document/d/1Jk2GgJnF9pLIQqg9hgX0Tvdukq6MR2qmXW3FyvqiiUg/edit?usp=sharing). I hope that it can serve as a starting point for investigating AI alignment through the lens of systems of AI services. An alternative framing for the text is that it is a collection of pieces I have found particularly helpful for having productive conversations about the topic. In this post, I briefly recap the motivation behind the document and outline its contents. I also argue for why service-systems are a useful paradigm and mention my best guesses for promising future work.\n\n\nMotivating ideas\n================\n\n\nAs part of a [recent collaboration](https://aisrp.org/), we wanted to look over technical problems in Comprehensive AI Services ([CAIS](https://www.lesswrong.com/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services-as)), then pick one of them and study it in more detail. However, it soon turned out that a lot more conceptual work was needed before we can formalize any of our thoughts in a way that would be useful. We did study Drexler's [Reframing Superintelligence](https://www.fhi.ox.ac.uk/reframing/) prior to embarking on this project, and the text indeed provides a lot of useful insights into the nature of advanced systems of AI services. However, it puts less emphasis on how to use the service framework for technical research. So from our starting position, we weren't sure how to define the basic concepts, how to model problems in CAIS, and what these problems even are.\n\n\nWe also decided to focus on a slightly different framing, mostly in two ways. First, Reframing Superintelligence primarily discusses *comprehensive* AI services, which are expected to be on par with a artificial *general* intelligence. Since we can reasonably expect AI X-risk to be associated with AI that is [radically transformative](https://arxiv.org/abs/1912.00747), we wanted to deal with a broader class of systems. Second, it is likely that even very advanced AI services will be deeply entangled with services that are \"implemented on humans\". (Especially since we can view human organizations and institutions as clusters of services.) From this reason, **the document studies *hybrid* systems of services that consist of both AI- and human services.** Even if our interest is primarily in *AI* services, studying the whole system (including its non-AI parts) should make it easier to fully understand their impact on the world.\n\n\nContents of the document\n========================\n\n\nWhile we did eventually make some progress on more specific problems, I thought it might be useful to write down a separate \"introductory\" text summarizing the things-that-seem-essential about the framework of service systems. Here is the document's slightly annotated table of contents:\n\n\n1. **Introduction** (motivating the approach; skippable if you have read this post)\n2. **Basic Concepts** (explaining what we mean by services, tasks, and systems of services, defining several useful concepts, giving many examples)\n3. **Modelling Approaches** (describing different types of models we might later want to come up with for service systems; introducing a \"simple abstract model\" for this framework)\n4. **Research Questions** ( a) how to think about, and generate lists of, problems and other research tasks for this paradigm; b) a list of research questions, not exhaustive but reasonably wide)\n5. **Related Fields of Study** (a list of existing research fields that seem particularly relevant)\n6. **Research Suggestions** (references for further reading, our guesses for what might be valuable to investigate)\n\n\nSome of our conclusions\n=======================\n\n\nWe think the paradigm of systems of services deserves further attention. In terms of *technical* alignment research, it might expose some new problems and provide a new point of view on the existing ones. Moreover, AI currently takes the form of services, rather than agents. As a result, the service-system paradigm might be more suitable for communicating with wider audience (e.g., the public, less technical fields like AI policy, and AI researchers from outside of the long-termist community).\n\n\nWe have no definitive answers for what work needs to be done in this area, but some of the useful directions seem to be:\n\n\n* **Technical problems and formalization.** Formalizing and getting progress on technical problems in service systems. As a side product, we might also attempt to build more solid foundations for the paradigm (i.e., formalizing the basic concepts, building mathematical models).\n* **Building on top of *Reframing Superintelligence*:** Drexler's text identifies and informally states many important hypotheses about the nature of AI-service systems, such as the claim that there will be no compelling incentives to replace comprehensive narrow services by an \"agent-like\" AGI (Section 12). We believe it would be valuable to (a) map out the different hypotheses and assumptions made in the report and (b) formalize specific hypotheses and explore them further.\n* **Tools or agents?** Many people seem to believe that while \"agent-like\" AGI and “fully-automated comprehensive system of services” might have similar capabilities, there is some fundamental difference between the two types of AI. At the same time, there seems to be a general confusion around this topic. Some relevant questions are:\n\t1. Can we find a framework in which these similarities and differences could be explained or dissolved?\n\t2. In particular, does there perhaps exist a formalization of “agency” that can differentiate between the two?\n\t3. If there are meaningful distinctions, how do they translate into what it means to “align” each type of AI?\n\t4. How does the effectivity of narrow-purpose algorithms differ from the effectivity of general-purpose algorithms? Should we expect economic pressures towards generality (and maybe even \"agent-like\" AGI)?\n* **Identifying connections to existing fields.** In many cases, problems with AI services will \"merely\" be automated versions of problems previously studied in other fields (e.g., automating the police might seemingly push some aspects of law under the umbrella of AI). For people who already have an expertise both in AI risk and in some relevant field, it might thus be valuable to clarify the connection of the two. On the one hand, clearly laying out how existing fields relate to service-systems might prevent other alignment researchers from reinventing the wheel by ignoring the existing research. On the other hand, it is important to identify the problems that arise when the existing field gets applied to service-systems and AI risk. We can then bring these new problems to the attention of the existing research communities, thus offloading some of the work (and possibly steering the field towards topics with more long-term impact). A prime example of a field whose utilization might be highly impactful is **AI ethics**. *However, due to effects like [idea inoculation](https://en.wikipedia.org/wiki/Inoculation_theory), reputation costs (to AI risk), and [unilateralist’s curse](https://concepts.effectivealtruism.org/concepts/unilateralists-curse/), we think this task should only be attempted by people who are experienced at communicating ideas on this level and well-positioned to do so.*\n\n\n\n\n---\n\n\n**Acknowledgement:** *This document has been written by myself (Vojta), based on ideas I collected from a collaboration with Cara Selvarajah, Chris van Merwijk, Francisco Carvalho, Jan Kulveit, and Tushant Jha during the [2019/2020 AI Safety Research Program](https://aisrp.org/). While they gave a lot of feedback to the text and many of the ideas are originally theirs, they might not necessarily agree with all arguments and framing presented here. All mistakes are mine.*", "url": "https://www.alignmentforum.org/posts/z2ofM2oZQwmcWFt8N/ai-services-as-a-research-paradigm", "date_published": "2020-04-20T13:00:40Z", "authors": ["VojtaKovarik"], "tags": ["AI Services (CAIS)", "AI"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.739486+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "1833daf1121085b5c132cf17946840ff", "source": "alignmentforum", "title": "Inner alignment in the brain", "text": "**Abstract:** We can think of the brain crudely as (1) a neocortex which runs an amazingly capable quasi-general-purpose learning-and-planning algorithm, and (2) subcortical structures (midbrain, etc.), one of whose functions is to calculate rewards that get sent to up the neocortex to direct it. But the relationship is actually more complicated than that. \"Reward\" is not the only informational signal sent up to the neocortex; meanwhile information is also flowing back down in the opposite direction. What's going on? How does all this work? Where do emotions fit in? Well, I'm still confused on many points, but I think I'm making progress. In this post I will describe my current picture of this system.\n\n\nBackground & motivation\n=======================\n\n\nI'm interested in helping ensure a good post-AGI future. But how do we think concretely about AGI, when AGI doesn't exist and we don't know how to build it? Three paths:\n\n\n1. We can think generally about the nature of intelligence and agency—a research program famously associated with MIRI, Marcus Hutter, etc.;\n2. We can think about today's AI systems—a research program famously associated with OpenAI, DeepMind, CHAI, etc.;\n3. We can start from the one \"general intelligence\" we know about, i.e. the human brain, and try to go from there to lessons about how AGI might be built, what it might look like, and how it might be safely and beneficially used and controlled.\n\n\nI like this 3rd research program; it seems to be almost completely neglected,[[1]](#fn-hFo52f2uxCkWqKXXC-1) and I think there's a ton of low-hanging fruit there. Also, this program will be *especially* important if we build AGI in part by reverse-engineering (or reinventing) high-level neocortical algorithms, which (as discussed below) I think is very plausible, maybe even likely—for better or worse.\n\n\nNow, the brain is divided into the neocortex and the subcortex.\n\n\nStart with the **neocortex**[[2]](#fn-hFo52f2uxCkWqKXXC-2) The neocortex does essentially all the cool exciting intelligent things that humans do, like building an intelligent world-model involving composition and hierarchies and counterfactuals and analogies and meta-cognition etc., and using that thing to cure diseases and build rocket ships and create culture etc. Thus, both neuroscientists and AI researchers focus a lot of attention onto the neocortex, and on understanding and reverse-engineering its algorithms. Textbooks divide the neocortex into lots of functional regions like \"motor cortex\" and \"visual cortex\" and \"frontal lobe\" etc., but microscopically it's all a pretty uniform 6-layer structure, and I currently believe that all parts of the neocortex are performing more-or-less the same algorithm, but with different input and output connections. These connections are seeded by an innate gross wiring diagram and then edited by the algorithm itself. See [Human Instincts, Symbol Grounding, and the Blank-Slate Neocortex](https://www.lesswrong.com/posts/NkSpukDkm9pjRdMdB/human-instincts-symbol-grounding-and-the-blank-slate) for discussion and (heavy!) caveats on that claim. And what is this algorithm? I outline some of (what I think are) the high-level specifications at [Predictive coding = RL + SL + Bayes + MPC](https://www.lesswrong.com/posts/cfvBm2kBtFTgxBB7s/predictive-coding-rl-sl-bayes-mpc). In terms of how the algorithm actually works, I think that researchers are making fast progress towards figuring this out, and that a complete answer is already starting to crystallize into view on the horizon. For a crash course on what's known today on how the neocortex does its thing, maybe a good starting point would be to read [*On Intelligence*](https://smile.amazon.com/dp/0805078533/ref=cm_sw_r_em_apa_i_BrhIEbHYFKJ42) and then every paper ever written by Dileep George (and citations therein).\n\n\n**The subcortex**, by contrast, is *not* a single configuration of neurons tiled over a huge volume, but rather it is a collection of quite diverse structures like the amygdala, cerebellum, tectum, and so on. Unlike the neocortex, this stuff does *not* perform some miraculous computation light-years beyond today's technology; as far as I can tell, it accomplishes the same sorts of things as AlphaStar does. And the most important thing to understand (for AGI safety) is this:\n\n\n***The subcortex provides the training signals that guide the neocortex to do biologically-useful things.[[3]](#fn-hFo52f2uxCkWqKXXC-3)***\n\n\nNow, if people build AGI that uses algorithms similar to the neocortex, we will need to provide it with training signals. What exactly are these training signals? What [inner alignment](https://www.lesswrong.com/posts/SJXujr5a2NcoFebr4/mesa-optimizers-vs-steered-optimizers) issues might they present? Suppose we wanted to make an AGI that was pro-social *for the same underlying reason* as humans are (sometimes) pro-social (i.e., thanks to the same computation); is that possible, how would we do it, and would it work reliably? These are questions we should answer well before we finish reverse-engineering the neocortex. I mean, really these questions should have been answered before we even *started* reverse-engineering the neocortex!! I don't have answers to those questions, but I'm trying to lay groundwork in that direction. Better late than never…\n\n\n(**Update 1 year later:** These days I say \"hypothalamus & brainstem\" instead of subcortex, and I'm inclined to lump almost the entire rest of the brain—the whole telencephalon plus cerebellum—in with the neocortex as the subsystem implementing a from-scratch learning algorithm. See [here](https://www.lesswrong.com/posts/jrewt3rLFiKWrKuyZ/big-picture-of-phasic-dopamine))\n\n\nThings to keep in mind\n======================\n\n\nBefore we get into the weeds, here are some additional mental pictures we'll need going forward:\n\n\nSimple example: Fear of spiders\n-------------------------------\n\n\nMy go-to example for the relation between subcortex and neocortex is fear of spiders.[[4]](#fn-hFo52f2uxCkWqKXXC-4) Besides the visual cortex, humans have a little-known *second* vision system in the midbrain (superior colliculus). When you see a black scuttling thing in your field of view, the midbrain vision system detects that and sends out a reaction that makes us look in that direction and increase our heart rate and flinch away from it. Meanwhile, the neocortex is simultaneously seeing the spider with *its* vision system, *and* it's seeing the hormones and bodily reaction going on, and it connects the dots to learn that \"spiders are scary\". In the future, if the neocortex merely imagines a spider, it might cause your heart to race and body to flinch. On the other hand, after [exposure therapy](https://en.wikipedia.org/wiki/Exposure_therapy), we might be able to remain calm when imagining or even seeing a spider. How does all this work?\n\n\n(Note again the different capabilities of the midbrain and neocortex: The midbrain has circuitry to recognize black scuttling things—kinda like today's CNNs can—whereas the neocortex is able to construct and use a rich semantic category like \"spiders\".)\n\n\nWe'll be returning to this example over and over in the post, trying to work through how it might be implemented and what the consequences are.\n\n\nThe neocortex is a black box from the perspective of the subcortex\n------------------------------------------------------------------\n\n\nThe neocortex's algorithm, as I understand it, sorta learns patterns, and patterns in the patterns, etc., and each pattern is represented as an essentially randomly-generated[[5]](#fn-hFo52f2uxCkWqKXXC-5) set of neurons in the neocortex. So, if X is a concept in your neocortical world-model, there is no straightforward way for an innate instinct to refer directly to X—say, by wiring axons from the neurons representing X to the reward center—because X's neurons are not at predetermined locations. X is inside the black box. An instinct can incentivize X, at least to some extent, but it has to be done indirectly.\n\n\nI made a list of various ways that we can have universal instincts despite the neocortex being a black-box learning algorithm: See [Human Instincts, Symbol Grounding, and the Blank-Slate Neocortex](https://www.lesswrong.com/posts/NkSpukDkm9pjRdMdB/human-instincts-symbol-grounding-and-the-blank-slate) for my list.\n\n\nThis blog post is a much deeper dive into how a couple of these mechanisms might be actually implemented.\n\n\nGeneral picture\n===============\n\n\nFinally, here is the current picture in my head:\n\n\n![](https://sjbyrnes.com/LW_InnerAlignmentBrain.png)\n\n\n(**Update 1 year later:** I no longer would draw it this way—see [Big picture of phasic dopamine](https://www.lesswrong.com/posts/jrewt3rLFiKWrKuyZ/big-picture-of-phasic-dopamine) for what I now think instead. The main difference is: I would *not* draw a direct line from neocortex to a hormone change (for example); instead the cortex would tell the subcortex (hypothalamus + brainstem) to make that hormone change, and then the subcortex might or might not comply with that recommendation. (I guess the way I drew it here is more like [somatic marker hypothesis](https://en.wikipedia.org/wiki/Somatic_marker_hypothesis).))\n\n\nThere’s a lot here. Let's go through it bit by bit.\n\n\nEmotions, \"emotion concepts\", and \"reactions\"\n=============================================\n\n\nOne aspect of this picture is emotions. There's a school of thought, popularized by Paul Ekman and the movie *Inside Out*, that there are exactly six emotions (anger, disgust, fear, happiness, sadness, surprise), each with its own universal facial expression. (I've seen other lists of emotions too, and sometimes there's also a list of social emotions like embarrassment, jealousy, guilt, shame, pride, etc.) That was my belief too, until I read the book [*How Emotions Are Made* by Lisa Feldman Barrett](https://smile.amazon.com/How-Emotions-Made-Lisa-Barrett/dp/1328915433/), which convincingly argues against it. Barrett argues that a word like \"anger\" lumps together a lot of very different bodily responses involving different facial expressions, hormones, etc. Basically, emotional concepts, like other concepts, are arbitrary categories describing things that we find useful to lump together. Sure, they might be lumped together because they share a common hormone change or a common facial expression, but they might just as likely be lumped together because they share a common situational context, or a common set of associated social norms, or whatever else. And an emotion concept with an English-language name like \"anger\" is not fundamentally different from an idiosyncratic emotion concept like \"How Alice must have felt in that TV episode where...\".\n\n\n(Incidentally, while I think Barrett's book is right about that, I am definitely not blanket-endorsing the whole book—there are a lot of other claims in it that I don't agree with, or perhaps don't understand.[[6]](#fn-hFo52f2uxCkWqKXXC-6) I think Barrett would strongly disagree with most of this blog post, though I could be wrong.)\n\n\nSo instead of putting \"emotions\" in the subcortex, I instead put there a bunch of things I'm calling \"reactions\" for clarity.[[7]](#fn-hFo52f2uxCkWqKXXC-7) I imagine that there are dozens to hundreds of these (...and separating them into a discrete list is probably an oversimplification of a more complicated computational architecture, but I will anyway). There's the reaction that gets triggered when your midbrain vision system sees a spider moving towards you out of the corner of your eye, as discussed above. And there's a different reaction that gets triggered when you stand at the edge of a precipice and peer over the edge. Both of those reactions might be categorized as \"fear\" in the neocortex, but they're really different reactions, involving (I presume) different changes to heart rate, different bodily motions, different facial expressions, different quantities of (negative) reward, etc. (Reactions where peripheral vision is helpful will summon a wide-eyed facial expression; reactions where visual acuity is helpful will summon a narrow-eyed facial expression; and so on.)\n\n\nAs described above for the spider example, the neocortex can see what the subcortex does to our hormones, body, face, etc., and it can learn to predict that, and build those expectations into its predictive world-model, and create concepts around that.\n\n\n(I also put \"pain concept\" in the neocortex, again following Barrett. A giant part of the pain concept is nociception—detecting the incoming nerve signals we might call \"pain sensations\". But at the end of the day, the neocortex gets to decide whether or not to classify a situation as \"pain\", based on not only nociception but also things like context and valence.)\n\n\nThe neocortex's non-motor outputs\n=================================\n\n\nFrom the above, our neocortex comes to expect that if we see a scuttling spider out of the corner of our eye, our heart will race and we'll turn towards it and flinch away. What's missing from this picture? The neocortex *causing* our heart to race by *anticipating* a spider. It's easy to see why this would be evolutionarily useful: If I know (with my neocortex) that a poisonous spider is approaching, it's appropriate for my heart to start racing even before my midbrain sees the black scuttling blob.\n\n\nNow we're at the top-left arrow in the diagram above: the neocortex causing (in this case) release of stress hormones. How does the neocortex learn to do that?\n\n\nThere are two parts of this \"how\" question: (1) what are the actual output knobs that the neocortex can use, and (2) how does the neocortex decide to use them? For (1), I have no idea. For the purpose of this blog post, let us assume that there is a set of outgoing axons from the neocortex that (directly or indirectly) cause hormone release, and also assume that \"hormone release\" is the right thing to be talking about in terms of controlling valence, arousal, and so on. I have very low confidence in all this, but I don't *think* it matters much for what I want to say in this post. (Update 1 year later: I understand (1) better now, but it still doesn't matter here.)\n\n\nI mainly want to discuss question (2): given these output knobs, how does the neocortex decide to use them?\n\n\nRecall again that in predictive coding, the neocortex [finds generative models which are consistent with each other, which have not been repeatedly falsified, and which predict that reward will happen](https://www.lesswrong.com/posts/cfvBm2kBtFTgxBB7s/predictive-coding-rl-sl-bayes-mpc).\n\n\nMy first thought was: No additional ingredients, beyond that normal predictive coding picture, are needed to get the neocortex to imitate the subcortical hormone outputs. Remember, just like my post on [predictive coding and motor control](https://www.lesswrong.com/posts/isDCEmYHsosyAjLRK/predictive-coding-and-motor-control), the neocortex will discover and store generative models that entail “self-fulfilling prophecies”, where a single generative model in the neocortex simultaneously codes for a prediction of stress hormone *and* the neocortical output signals that actually cause the release of this stress hormone. Thus (...I initially thought...), after seeing spiders and stress hormones a few times, the neocortex will predict stress hormones when it sees a spider, which incidentally *creates* stress hormones.\n\n\nBut I don’t think that’s the right answer, at least not by itself. After all, the neocortex will *also* learn a generative model where stress hormone is generated exogenously (e.g. by the subcortical spider reaction) and where the neocortex’s own stress hormone generation knob is left untouched. This latter model is issuing perfectly good predictions, so there is no reason that the neocortex would spontaneously throw it out and start using instead the self-fulfilling-prophecy model. (By the same token, in the [motor control case](https://www.lesswrong.com/posts/isDCEmYHsosyAjLRK/predictive-coding-and-motor-control), if I think you are going to take my limp arm and lift it up, I have no problem predicting that my arm will move due to that exogenous force; my neocortex doesn’t get confused and start issuing motor commands.)\n\n\nSo here's my second, better story:\n\n\n***Reward criterion (one among many): when the subcortex calls for a reaction (e.g. cortisol release, eyes widening, etc.), it rewards the neocortex with dopamine if it sees that those commands have somehow already been issued.***\n\n\n(Update 2021/06: Oops, that was wrong too. I think I got it on the third try though; see [here](https://www.lesswrong.com/posts/wcNEXDHowiWkRxDNv/inner-alignment-in-salt-starved-rats).)\n\n\nSo if the subcortex computes that a situation calls for cortisol, the neocortex is rewarded if the subcortex sees that cortisol is *already* flowing. This example seems introspectively reasonable: Seeing a spider out of the corner of your eye is bad, but being *surprised* to see a spider when you were feeling safe and relaxed is even worse (worse in terms of dopamine, not necessarily worse in terms of valence—remember [wanting ≠ liking](https://www.lesswrong.com/posts/yDRX2fdkm3HqfTpav/approving-reinforces-low-effort-behaviors)). Presumably the same principle can apply to eye-widening and other things.\n\n\nTo be clear, this is one reward criterion among many—the subcortex issues positive and negative rewards according to other criteria too (as in the diagram above, I think different reactions inherently issue positive or negative rewards to the neocortex, just like they inherently issue motor commands and hormone commands). But as long as this \"reward criterion\" above is permanently in place, then thanks to the laws of the [neocortex's generative model economy](https://www.lesswrong.com/posts/cfvBm2kBtFTgxBB7s/predictive-coding-rl-sl-bayes-mpc), the neocortex will drop those generative models that passively anticipate the subcortex's reactions, in favor of models that actively anticipate / imitate the subcortical reactions, insofar as that's possible (the neocortex doesn't have output knobs for everything).\n\n\nPredicting, imagining, remembering, empathizing\n-----------------------------------------------\n\n\nThe neocortex’s generative models appear in the context of (1) prediction (including predicting the immediate future as it happens), (2) imagination, (3) memory, and (4) empathetic simulation (when we imagine *someone else* reacting to a spider, predicting a spider, etc.). I think all four of these processes rely on fundamentally the same mechanism in the neocortex, so by default the same generative models will be used for all four. Thus, we get the same hormone outputs in all four of these situations.\n\n\nHang on, you say: That doesn’t seem right! If it were the *exact* same generative models, then when we remember dancing, we would actually issue the motor commands to start dancing! Well, I answer, we *do* actually sometimes move a little bit when we remember a motion! I think the rule is, loosely speaking, the top-down information flow is much stronger (more confident) when predicting, and much weaker for imagination, memory, and empathy. Thus, the neocortical output signals are weaker too, and this applies to both motor control outputs and hormone outputs. (Incidentally, I think motor control outputs are further subject to thresholding processes, downstream of the neocortex, and therefore a sufficiently weak motor command causes no motion at all.)\n\n\nAs discussed more below, the subcortex relies on the neocortex’s outputs to guess what the neocortex is thinking about, and issue evolutionarily-appropriate guidance in response. Presumably, to do this job well, the subcortex needs to know whether a given neocortical output is part of a prediction, or memory, or imagination, or empathetic simulation. From the above paragraph, I think it can distinguish predictions from the other three by the neocortical output strength. But how does it tell memory, imagination, and empathetic simulation apart from each other? I don’t know! Then that suggests to me an interesting hypothesis: maybe it *can’t*! What if some of our weirder instincts related to memory or counterfactual imagination are not adaptive at all, but rather crosstalk from social instincts, or vice-versa? For example, I think there’s a reaction in the subcortex that listens for a strong prediction of lower reward, alternating with a weak prediction of higher reward; when it sees this combination, it issues negative reward and negative valence. Think about what this subcortical reaction would do in the three different cases: If the weak prediction it sees is an empathetic simulation, well, that’s the core of jealousy! If the weak prediction it sees is a memory, well, that’s the core of loss aversion! If the weak prediction it sees is a counterfactual imagination, well, that’s the core of, I guess, that annoying feeling of having missed out on something good. Seems to fit together pretty well, right? I’m not super confident, but at least it’s food for thought.\n\n\nVarious implications\n====================\n\n\nOpening a window into the black-box neocortex\n---------------------------------------------\n\n\nEach subcortical reaction has its own profile of facial, body, and hormone changes. The \"reward criterion\" above ensures that the neocortex will learn to imitate the characteristic consequences of reaction X whenever it is expecting, imagining, remembering, or empathetically simulating reaction X. This is then a window for the subcortex to get a glimpse into the goings-on inside the black-box neocortex.\n\n\nIn our running example, if the spider reaction creates a certain combination of facial, body, and hormone changes, then the subcortex can watch for this set of changes to happen exogenously (from its perspective), and if it does, the subcortex can infer that the neocortex was maybe thinking about spiders. Perhaps the subcortex might then issue its own spider reaction, fleshing out the neocortex's weak imitation. Or perhaps it could do something entirely different.\n\n\nI have a hunch that social emotions rely on this. With this mechanism, it seems that the subcortex can build a hierarchy of increasingly complicated social reactions: \"if I'm activating reaction A, and I think you're activating reaction B, then that triggers me to feel reaction C\", \"if I'm activating reaction C, and I think you're activating reaction A, then that triggers me to feel reaction D\", and so on. Well, maybe. I'm still hazy on the details here and want to think about it more.\n\n\nComplex back-and-forth between neocortex and subcortex\n------------------------------------------------------\n\n\nThe neocortex can alter the hormones and body, which are among the inputs into the subcortical circuits. The subcortical circuits then also alter the hormones and body, which are among the inputs into the neocortex! Around and around it goes! So for example, if you tell yourself to calm down, your neocortex changes your hormones, which in turn increases the activation of the subcortical \"I am safe and calm\" reaction, which reinforces and augments that change, which in turn makes it easier for the neocortex to continue feeling safe and calm! … Until, of course, that pleasant cycle is broken by other subcortical reactions *or* other neocortical generative models butting in.\n\n\n\"Overcoming\" subcortical reactions\n==================================\n\n\nEmpirically, we know it's possible to \"overcome\" fear of spiders, and other subcortical reactions. I'm thinking there are two ways this might work. I think both are happening, but I’m not really sure.\n\n\nFirst, there's subcortical learning … well, \"learning\" isn't the right word here, because it's not trying to match some ground truth. (The only \"ground truth\" for subcortical reaction specifications is natural selection!) I think it's more akin to the [self-modifying code in Linux](https://www.quora.com/Is-it-possible-for-a-program-to-modify-its-own-code/answer/Anders-Kaseorg) than to the weight updates in ML. So let's call it **subcortical input-dependent dynamic rewiring rules**.\n\n\n(By the way, elsewhere in the subcortex, like the cerebellum, there is *also* real stereotypical “learning” going on, akin to the weight updates in ML. That does happen, but it’s not what I’m talking about here. In fact, I prefer to lump the cerebellum in with the neocortex as the learning-algorithm part of the brain.)\n\n\nMaybe one subcortical dynamic rewiring rule says: *If the spider-detection reaction triggers, and then within 3 seconds the \"I am safe and calm\" reaction triggers, then next time the spider reaction should trigger more weakly.*\n\n\nSecond, there’s neocortical learning—i.e., the neocortex developing new generative models. Let's say again that we're doing exposure therapy for fear of spiders, and let's say the two relevant subcortical reactions are the spider-detection reaction (which rewards the neocortex for producing anxiety hormones before it triggers) and the \"I am safe and calm\" reaction (which rewards the neocortex for for producing calming hormones before *it* triggers). (I'm obviously oversimplifying here.) The neocortex could learn generative models that summon the “I am safe and calm” reaction whenever the spider-detection reaction is just starting to trigger. That generative model could potentially get entrenched and rewarded, as the spider-detection reaction is sorta preempted and thus can’t issue a penalty for the lack of anxiety hormones, whereas the “I am safe and calm” reaction *does* issue a reward for the presence of calm hormones. Something like that?\n\n\nI have no doubt that the second of these two processes—neocortical learning—really happens. The first might or might not happen, I don’t know. It does seem like something that plausibly could happen, on both evolutionary and neurological grounds. So I guess my default assumption is that dynamic rewiring rules for subcortical reactions do in fact exist, but again, I’m not sure, I haven't thought about it much.\n\n\nThings I still don't understand\n===============================\n\n\nI lumped together the subcortex into a monolithic unit. I actually understand very little about the functional decomposition beyond that. The tectum and tegmentum seem to be doing a lot of the calculations for what I'm calling \"reactions\", including the colliculi, which seem to house the subcortical sensory processing. What computations does the amygdala do, for example? It has 10 million neurons, they have to be calculating something!!! I really don't know. (Update 1 year later: On the plus side, I feel like I understand the amygdala much better now; on the minus side, I was wrong to lump it in with \"subcortex\" rather than \"neocortex\". [See discussion here](https://www.lesswrong.com/posts/jrewt3rLFiKWrKuyZ/big-picture-of-phasic-dopamine).)\n\n\nAs discussed above, I don't understand what the non-motor output signals from the neocortex are (update: see [here](https://www.lesswrong.com/posts/jrewt3rLFiKWrKuyZ/big-picture-of-phasic-dopamine)), or whether things like valence and arousal correspond to hormones or something else. (Update: attempt to understand valence [here](https://www.lesswrong.com/posts/TtBik82RQLBCG3h8j/emotional-valence-vs-rl-reward-a-video-game-analogy).)\n\n\nI'm more generally uncertain about everything I wrote here, even where I used a confident tone. Honestly, I haven't found much in the systems neuroscience literature that's addressing the questions I'm interested in, although I imagine it's there somewhere and I'm reinventing lots of wheels (or re-making classic mistakes). As always, please let me know any thoughts, ideas, things you find confusing, etc. Thanks in advance!\n\n\n\n\n---\n\n\n\n1. A few people on this forum are thinking hard about the brain, and I've learned a lot from their writings—especially [Kaj’s multi-agent sequence](https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip)—but my impression is that they're mostly working on the project of \"Let's understand the brain so we can answer normative questions of what we want AGI to do and how value learning might work\", whereas here I'm talking about \"Let's understand the brain as a model of a possible AGI algorithm, and think about whether such an AGI algorithm can be used safely and beneficially\". Y'all can correct me if I'm wrong :) [↩︎](#fnref-hFo52f2uxCkWqKXXC-1)\n2. I will sloppily use the term \"neocortex\" as shorthand for \"neocortex plus other structures that are intimately connected to the neocortex and are best thought of as part of the same algorithm\"—this especially includes the hippocampus and thalamus. [↩︎](#fnref-hFo52f2uxCkWqKXXC-2)\n3. For what it's worth, Elon Musk mentioned in [a recent interview about Neuralink](https://lexfridman.com/elon-musk-2/) that he is thinking about the brain this way as well: \"We've got like a monkey brain with a computer on top of it, that's the human brain, and a lot of our impulses and everything are driven by the monkey brain, and the computer, the cortex, is constantly trying to make the monkey brain happy. It's not the cortex that's steering the monkey brain, it's the monkey brain steering the cortex.\" (14:45). Normally people would say \"lizard brain\" rather than \"monkey brain\" here, although even *that* terminology is unfair to lizards, who do in fact have something homologous to a neocortex. [↩︎](#fnref-hFo52f2uxCkWqKXXC-3)\n4. Unfortunately I don't have good evidence that this spider story is actually true. Does the midbrain *really* have specialized circuitry to detect spiders? There was [a study](https://www.pnas.org/content/102/51/18682) that showed pictures of spiders to a [blindsighted](https://en.wikipedia.org/wiki/Blindsight) person (i.e., a person who had an intact midbrain visual processing system but no visual cortex). It didn't work; nothing happened. But I think they did the experiment wrong—I think it has to be a video of a moving spider, not a stationary picture of a spider, to trigger the subcortical circuitry. (Source: introspection. Also, I think I read that the subcortical vision system has pretty low spatial resolution, so watching for a characteristic motion would seem a sensible design.) Anyway, it *has* to work this way, nothing else makes sense to me. I'm comfortable using this example prominently because if it turns out that this example is wrong, then I'm so very confused that this whole article is probably garbage anyway. For the record, I am basically describing Mark Johnson's \"two-process” model, which is I think well established in the case of attending-to-faces in humans and filial imprinting in chicks (more [here](https://www.lesswrong.com/posts/NkSpukDkm9pjRdMdB/human-instincts-symbol-grounding-and-the-blank-slate)), even if it's speculative when applied to fear-of-spiders. [↩︎](#fnref-hFo52f2uxCkWqKXXC-4)\n5. I am pretty confident that neocortical patterns are effectively random at a microscopic, neuron-by-neuron level, and *that's* what matters when we talk about why it's impossible for evolution to directly create hardwired instincts that refer to a semantic concept in the neocortex. However, to be clear, at the level of gross anatomy, you *can* more-or-less predict in advance where different concepts will wind up getting stored in the neocortex, based on the large-scale patterns of information flow and the inputs it gets in a typical human life environment. To take an obvious example, low-level visual patterns are likely to be stored in the parts of the neocortex that receive low-visual visual information from the retina! [↩︎](#fnref-hFo52f2uxCkWqKXXC-5)\n6. When I say \"I didn't understand\" something Barrett wrote, I mean more specifically that I can't see how to turn her words into a [gears-level](https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding) model of a computation that the brain might be doing. This category of \"things I didn't understand\" includes, in particular, almost everything she wrote about \"body budgets\", which was a major theme of the book that came up on almost every page... [↩︎](#fnref-hFo52f2uxCkWqKXXC-6)\n7. If you want to call the subcortical things \"emotions\" instead of \"reactions\", that's fine with me, as long as you distinguish them from \"emotion concepts\" in the neocortex. Barrett is really adamant that the word \"emotion\" *must* refer to the neocortical emotion concepts, not the subcortical reactions (I'm not even sure if she thinks the subcortical reactions exist), but for my part, I think reasonable people could differ, and it's ultimately a terminological question with no right answers anyway. [↩︎](#fnref-hFo52f2uxCkWqKXXC-7)", "url": "https://www.alignmentforum.org/posts/DWFx2Cmsvd4uCKkZ4/inner-alignment-in-the-brain", "date_published": "2020-04-22T13:14:08Z", "authors": ["Steven Byrnes"], "tags": ["World Modeling", "Neocortex", "Inner Alignment", "Neuroscience", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.740438+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "9fdb067a24041dc6f97335645bb148e1", "source": "alignmentforum", "title": "[AN #96]: Buck and I discuss/argue about AI Alignment", "text": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.\n\nHIGHLIGHTS\n==========\n\n**[AI Alignment Podcast: An Overview of Technical AI Alignment in 2018 and 2019](https://futureoflife.org/2020/04/15/an-overview-of-technical-ai-alignment-in-2018-and-2019-with-buck-shlegeris-and-rohin-shah/)** *(Lucas Perry, Buck Shlegeris and Rohin Shah)* (summarized by Rohin): This podcast with Buck and me is loosely structured around the **[review I wrote](https://www.alignmentforum.org/posts/dKxX76SCfCvceJXHv/ai-alignment-2018-19-review)** (**[AN #84](https://mailchi.mp/1af38085edc5/an-84-reviewing-ai-alignment-work-in-2018-19)**), but with a lot more debate and delving into specific points of pessimism and optimism. I suspect that every reader will have some section they're interested in. Since much of the discussion was itself meant to be a summary, I'm not going to try and summarize even further. Here's the list of topics covered:\n\n* Our optimism and pessimism about different approaches to aligned AI\n* Traditional arguments for AI as an x-risk\n* Modeling agents as expected utility maximizers\n* Ambitious value learning and specification learning/narrow value learning\n* Agency and optimization\n* Robustness\n* Scaling to superhuman abilities\n* Universality\n* Impact regularization\n* Causal models, oracles, and decision theory\n* Discontinuous and continuous takeoff scenarios\n* Probability of AI-induced existential risk\n* Timelines for AGI\n* Information hazards\n\nTECHNICAL AI ALIGNMENT\n======================\n\nTECHNICAL AGENDAS AND PRIORITIZATION\n------------------------------------\n\n**[AI Services as a Research Paradigm](https://www.alignmentforum.org/posts/z2ofM2oZQwmcWFt8N/ai-services-as-a-research-paradigm)** *(Vojta Kovarik)* (summarized by Rohin): The **[CAIS report](https://www.fhi.ox.ac.uk/reframing/)** (**[AN #40](https://mailchi.mp/b649f32b07da/alignment-newsletter-40)**) suggests that future technological development will be driven by *systems of AI services*, rather than a single monolithic AGI agent. However, there has not been much followup research since the publication of the report. This document posits that this is because the concepts of tasks and services introduced in the report are not amenable to formalization, and so it is hard to do research with them. So, it provides a classification of the types of research that could be done (e.g. do we consider the presence of one human, or many humans?), a list of several research problems that could be tackled now, and a simple abstract model of a system of services that could be built on in future work.\n\n**Rohin's opinion:** I was expecting a research paradigm that was more specific to AI, but in reality it is very broad and feels to me like an agenda around \"how do you design a good society in the face of technological development\". For example, it includes unemployment, system maintenance, the potential of blackmail, side-channel attacks, prevention of correlated errors, etc. None of this is to say that the problems aren't *important* -- just that given how broad they are, I would expect that they could be best tackled using many different fields, rather than being important for AI researchers in particular to focus on.\n\nLEARNING HUMAN INTENT\n---------------------\n\n**[Aligning AI to Human Values means Picking the Right Metrics](https://medium.com/@PartnershipAI/aligning-ai-to-human-values-means-picking-the-right-metrics-855859e6f047)** *(Jonathan Stray)* (summarized by Rohin): There has been a lot of attention recently on the flaws of recommender systems, especially when optimizing for simple metrics like engagement -- an example of what we might call \"narrow value alignment\". This post reconstructs how Facebook and YouTube have been incorporating better metrics into their algorithms from 2015 and 2017 respectively. For example, Facebook found that academic research suggested that well-being was improved by \"meaningful social interactions\", but worsened by passive consumption of content. As a result, they changed the metric for the recommendation algorithm to better track this. How did they measure it? It seems that they simply asked a survey of thousands of people what the most meaningful content was (both on and off Facebook), and used this to train a model to predict \"meaningful interactions\". They estimated that this resulted in a 5% decrease in time spent on Facebook, at least in the short term. The story with YouTube is similar, though sparser on details (and it's not clear if there was input from end users in YouTube's case).\n\nThe author then contrasts this sort of narrow value alignment with AGI alignment. His main take is that narrow alignment should be easier to address, since we can learn from how existing systems behave in the real world, and the insights we gain may be critical for AGI alignment. I'll end with a quote from the conclusion: \"My argument is not so much that one should use AI to optimize for well-being. Rather, we live in a world where large-scale optimization is already happening. We can choose not to evaluate or adjust these systems, but there is little reason to imagine that ignorance and inaction would be better.\"\n\n**Rohin's opinion:** Even though I often feel like an **[optimist](https://aiimpacts.org/conversation-with-rohin-shah/)** (**[AN #80](https://mailchi.mp/b3dc916ac7e2/an-80-why-ai-risk-might-be-solved-without-additional-intervention-from-longtermists)**) about incentives towards alignment, even I was surprised to see the amount of effort that it seems Facebook has put into trying to align its recommendation algorithm with well-being. To the extent that the recommendation algorithm is still primarily harmful (which might be true or false, idk), this suggests to me that it might just be really hard to give good recommendations given the sparse feedback you get. Of course, there are more cynical explanations, e.g. Facebook just wants to look like they care about well-being, but if they really cared they could do way better. I lean towards the first explanation, but it's very hard to distinguish between these hypotheses.\n\nWhile this post claimed that narrow value alignment should be easier than AGI alignment, I'm actually not so sure. With AGI alignment, you have the really powerful assumption that the AI system you are trying to align is *intelligent*: this could plausibly help a lot. For example, maybe the recommender systems that Facebook is using are just incapable of predicting what will and won't improve human well-being, in which case narrow alignment is doomed. This wouldn't be the case with an AGI (depending on your definition of AGI) -- it should be capable of doing at least as well as humans do. The challenge is in ensuring that the AI systems are actually **[motivated](https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment)** (**[AN #33](https://mailchi.mp/b6dc636f6a1b/alignment-newsletter-33)**) to do so, not whether they are capable of doing so; with narrow alignment you need to solve both problems.\n\n**[LESS is More: Rethinking Probabilistic Models of Human Behavior](https://arxiv.org/abs/2001.04465)** *(Andreea Bobu, Dexter R.R. Scobee et al)* (summarized by Asya): This paper introduces a new model for robots inferring human preferences called LESS. The traditional Boltzmann noisily-rational decision model assumes people approximately optimize a reward function and choose trajectories in proportion to their exponentiated reward. The Boltzmann model works well when modeling decisions among different discrete options, but runs into problems when modeling human trajectories in a continuous space, e.g. path finding, because it is very sensitive to the number of trajectories, even if they are similar-- if a robot using a Boltzmann model must predict whether a human navigates around an obstacle by taking one path on the left or one of three very-similar paths on the right, it will assign the same probability to each path by default.\n\nTo fix this, LESS predicts human behavior by treating each trajectory as part of a continuous space and mapping each one to a feature vector. The likelihood of selecting a trajectory is inversely proportional to its feature-space similarity with other trajectories, meaning similar trajectories are appropriately deweighted.\n\nThe paper tests the predictive performance of LESS vs. Boltzmann in several experimental environments, including an artifically constructed task where humans are asked to choose between similar paths for navigating around an obstacle, and a real-world task where humans demonstrate appropriate behaviors to a 7-degree-of-freedom robotic arm. In general, LESS performs better than Boltzmann when given a small number of samples of human behavior, but does equally well as the sample size is increased. In the robotic arm task, Boltzmann performed better when demonstrations were aggregated into a single batch and inference was run on the whole batch at once, representing trying to approximate the 'average' user rather than customizing behavior to each user. The paper claims that this happens because Boltzmann overlearns from demonstrations in sparse regions, and underlearns from dense demonstrations. As you increase the number of samples, you approximate the “true” trajectory space better and better, so the 10 trajectory sets vary less and less, which means Boltzmann won’t underperform so much. Since the single batch demonstration aggregated demonstrations, it had a similar effect in approximating the \"true\" trajectory space.\n\nThe paper notes that one limitation of this method is a reliance on a pre-specified set of robot features, though a small set of experimental results suggested that LESS still performed better than Boltzmann when adding a small number of irrelevant features.\n\n**Asya's opinion:** This seems like a good paper, and seems very much like the natural extension of Boltzmann models to include accounting for similar trajectories. As the paper notes, I largely worry about the reliance on a pre-specified set of robot features-- in more complicated cases of inference, it could be impractical to hand-specify relevant features and too difficult to have the robot infer them. In the worst case, it seems like misspecified features could make performance worse than Boltzmann via suggesting similarities that are irrelevant.\n\n**Rohin's opinion:** (Note that this paper comes from the InterACT lab, which I am a part of.)\n\nThe Boltzmann model of human behavior has several theoretical justifications: it's the maximum entropy (i.e. **[minimum encoded information](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy)**) distribution over trajectories subject to the constraint that the feature expectations match those of the observed human behavior; it's the maximum entropy distribution under the assumption that humans satisfice for expected reward above some threshold, etc. I have never found these very compelling, and instead see it as something far simpler: you want your model to encode the fact that humans are more likely to take good actions than bad actions, and you want your model to assign non-zero probability to all trajectories; the Boltzmann model is the simplest model that meets these criteria. (You could imagine removing the exponential in the model as \"even simpler\", but this is equivalent to a monotonic transformation of the reward function.) \n\nI view this paper as proposing a model that meets my two criteria before, and adds in a third one: when we can cluster trajectories based on similarity, then we should view the human as choosing between *clusters*, rather than choosing between trajectories. Given a good similarity metric, this seems like a much better model of human behavior -- if I'm walking and there's a tree in my path, I will choose which side of the tree to go around, but I'm not going to put much thought into exactly where my footsteps will fall.\n\nI found the claim that Boltzmann overlearns in sparse areas to be unintuitive, and so I delved into it deeper in this **[comment](https://www.alignmentforum.org/posts/TmHRACaxXrLbXb5tS/rohinmshah-s-shortform?commentId=Hb6m2C9wtKWTzYL8d)**. My overall takeaway was that the claim will often hold in practice, but it isn't guaranteed.\n\nPREVENTING BAD BEHAVIOR\n-----------------------\n\n**[Curiosity Killed the Cat and the Asymptotically Optimal Agent](https://www.alignmentforum.org/posts/fSC98Cy3zR9GsEPnT/curiosity-killed-the-cat-and-the-asymptotically-optimal)** *(Michael Cohen et al)* (summarized by Rohin): In environments without resets, an *asymptotically optimal* agent is one that eventually acts optimally. (It might be the case that the agent first hobbles itself in a decidedly suboptimal way, but *eventually* it will be rolling out the optimal policy *given* its current hobbled position.) This paper points out that such agents must explore a lot: after all, it's always possible that the very next timestep will be the one where chopping off your arm gives you maximal reward forever -- how do you *know* that's not the case? Since it must explore so much, it is extremely likely that it will fall into a \"trap\", where it can no longer get high reward: for example, maybe its actuators are destroyed.\n\nMore formally, the paper proves that when an asymptotically optimal agent acts, for any event, either that event occurs, or after some finite time there is no recognizable opportunity to cause the event to happen, even with low probability. Applying this to the event \"the agent is destroyed\", we see that either the agent is eventually destroyed, or it becomes *physically impossible* for the agent to be destroyed, even by itself -- given that the latter seems rather unlikely, we would expect that eventually the agent is destroyed.\n\nThe authors suggest that safe exploration is not a well-defined problem, since you never know what's going to happen when you explore, and they propose that instead agents should have their exploration guided by a mentor or **[parent](http://arxiv.org/abs/1902.06766)** (**[AN #53](https://mailchi.mp/534f448d6c8b/alignment-newsletter-53)**) (see also **[delegative RL](https://intelligence.org/2019/04/24/delegative-reinforcement-learning/)** (**[AN #57](https://mailchi.mp/392d2043e782/an-57why-we-should-focus-on-robustness-in-ai-safety-and-the-analogous-problems-in-programming)**), **[avoiding catastrophes via human intervention](https://arxiv.org/abs/1707.05173)**, and **[shielding](https://arxiv.org/abs/1708.08611)** for more examples).\n\n**Rohin's opinion:** In my opinion on **[Safety Gym](https://openai.com/blog/safety-gym/)** (**[AN #76](https://mailchi.mp/1106d0ce6766/an-76how-dataset-size-affects-robustness-and-benchmarking-safe-exploration-by-measuring-constraint-violations)**), I mentioned how a zero-violations constraint for safe exploration would require a mentor or parent that already satisfied the constraint; so in that sense I agree with this paper, which is simply making that statement more formal and precise.\n\nNonetheless, I still think there is a meaningful notion of exploration that can be done safely: once you have learned a good model that you have reasonable confidence in, you can find areas of the model in which you are uncertain, but you are at least confident that it won't have permanent negative repercussions, and you can explore there. For example, I often \"explore\" what foods I like, where I'm uncertain of how much I will like the food, but I'm quite confident that the food will not poison and kill me. (However, this notion of exploration is quite different from the notion of exploration typically used in RL, and might better be called \"model-based exploration\" or something like that.)\n\nMISCELLANEOUS (ALIGNMENT)\n-------------------------\n\n**[Bayesian Evolving-to-Extinction](https://www.alignmentforum.org/posts/u9Azdu6Z7zFAhd4rK/bayesian-evolving-to-extinction)** *(Abram Demski)* (summarized by Rohin): Consider a Bayesian learner, that updates the weights of various hypotheses using Bayes Rule. If the hypotheses can influence future events and predictions (for example, maybe it can write out logs, which influence what questions are asked in the future), then hypotheses that affect the future in a way that only they can predict will be selected for by Bayes Rule, rather than hypotheses that straightforwardly predict the future without trying to influence it. In some sense, this is \"myopic\" behavior on the part of Bayesian updating: Bayes Rule only optimizes per-hypothesis, without taking into account the effect on overall future accuracy. This phenomenon could also apply to neural nets if the **[lottery ticket hypothesis](https://arxiv.org/abs/1803.03635)** (**[Recon #4](https://mailchi.mp/e402599bfb1f/reconnaissance-4)**) holds: in this case each \"ticket\" can be thought of as a competing hypothesis.\n\nAI STRATEGY AND POLICY\n======================\n\n**[‘Skynet’ Revisited: The Dangerous Allure of Nuclear Command Automation](https://www.armscontrol.org/act/2020-04/features/skynet-revisited-dangerous-allure-nuclear-command-automation)** *(Michael T. Klare)* (summarized by Rohin) (H/T Jon Rodriguez): While I won't summarize this article in full here, I found it useful to see how some academics are thinking about the risks of automation in the military, as well as to get a picture of what current automation efforts actually look like. One quote I found particularly interesting:\n\n“You will find no stronger proponent of integration of AI capabilities writ large into the Department of Defense,” said Lieutenant General Jack Shanahan, director of the Joint Artificial Intelligence Center (JAIC), at a September 2019 conference at Georgetown University, “but there is one area where I pause, and it has to do with nuclear command and control.” Referring to [an] article’s assertion that an automated U.S. nuclear launch ability is needed, he said, “I read that. And my immediate answer is, ‘No. We do not.’”\n\n**[AI Alignment Podcast: On Lethal Autonomous Weapons](https://www.alignmentforum.org/posts/xEzudcydk7APZbnai/ai-alignment-podcast-on-lethal-autonomous-weapons-with-paul)** *(Lucas Perry and Paul Scharre)* (summarized by Flo): Paul Scharre, author of \"Army of None: Autonomous Weapons and the Future of War\", talks about various issues around Lethal Autonomous Weapons (LAWs), including the difficulty to talk about an arms race around autonomous weapons when different people mean different things by \"arms race\" and autonomy comes in varying degrees, the military's need for reliability in the context of AI systems' lack of robustness to distributional shift and adversarial attacks, whether the law of war correctly deals with LAWs, as well as the merits and problems of having a human in the loop.\n\nWhile autonomous weapons are unlikely to directly contribute to existential risk, efforts to establish limits on them could be valuable by creating networks and preparing institutions for collaboration and cooperation around future AI issues.\n\nOTHER PROGRESS IN AI\n====================\n\nDEEP LEARNING\n-------------\n\n**[Fast and Easy Infinitely Wide Networks with Neural Tangents](https://ai.googleblog.com/2020/03/fast-and-easy-infinitely-wide-networks.html)** *(Roman Novak, Lechao Xiao, Samuel S. Schoenholz et al)* (summarized by Zach): The success of Deep Learning has led researchers to explore why they're such effective function approximators. One key insight is that increasing the width of the network layers makes it *easier* to understand. More precisely, as the width is sent to infinity the network's learning dynamics can be approximated with a Taylor expansion and become a kernel problem. This kernel has an exact form in the limit and is referred to as the neural tangent kernel (NTK). Ultimately, this allows us to model the network with a simpler model known as a Gaussian process. Unfortunately, showing this analytically is difficult and creating efficient implementations is cumbersome. **The authors address this problem by introducing \"Neural Tangents\", a library that makes creating infinite-width networks as easy as creating their finite counterparts with libraries such as PyTorch or TensorFlow.** They include support for convolutions with full-padding, residual-connections, feed-forward networks, and support for a variety of activation functions. Additionally, there is out-of-the-box support for CPU, GPU, and TPU. Moreover, uncertainty comparisons with finite ensembles are possible via exact Bayesian inference.\n\n**Read more:** **[Paper: Neural Tangents: Fast and Easy Infinite Neural Networks in Python](https://openreview.net/forum?id=SklD9yrFPS)**\n\n**Zach's opinion:** I took a look at the repository and found there to be ample documentation available making it easy for me to try training my own infinite-width network. The authors derive a practical way to compute the exact convolutional NTK which I find impressive and which seems to be the main technical contribution of this paper. While the authors note that there are some conditions necessary to enter the so-called \"kernel regime\", in practice it seems as though you can often get away with merely large network widths. If for nothing else, I'd recommend at least perusing the notebooks they have available or taking a look at the visualization they present of a neural network converging to a Gaussian process, which relies on a subtle application of the law of large numbers. \n\n#### **FEEDBACK**\n\nI'm always happy to hear feedback; you can send it to me, **[Rohin Shah](https://rohinshah.com/)**, by **replying to this email**. \n\n#### **PODCAST**\n\nAn audio podcast version of the **Alignment Newsletter** is available. This podcast is an audio version of the newsletter, recorded by **[Robert Miles](http://robertskmiles.com/)**.", "url": "https://www.alignmentforum.org/posts/YyKKMeCCxnzdohuxj/an-96-buck-and-i-discuss-argue-about-ai-alignment", "date_published": "2020-04-22T17:20:03Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.740977+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "222dfa0bcba6391e272ae3527a389e2d", "source": "alignmentforum", "title": "Problem relaxation as a tactic", "text": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\nIt's easier to make your way to the supermarket than it is to compute the fastest route, which is yet easier than computing the fastest route for someone running backwards and doing two and a half jumping jacks every five seconds and who only follows the route p percent of the time. Sometimes, constraints are necessary. Constraints come with costs. Sometimes, the costs are worth it.\n\n\nAspiring researchers trying to think about AI alignment might[[1]](#fn-8gdCrzXPxrzn3eGF3-1) have a failure mode which goes something like… this:\n\n\n\n> \n> Oh man, so we need to solve both outer and inner alignment to build a superintelligent agent which is competitive with unaligned approaches and also doesn't take much longer to train, and also we have to know this ahead of time. Maybe we could use some kind of prediction of what people want... but wait, [there's also problems with using human models](https://www.alignmentforum.org/posts/BKjJJH2cRpJcAnP7T/thoughts-on-human-models)! How can it help people if it can't model people? Ugh, and [what about self-modification](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh/p/iTpLAaPamcKyjmbFC)?! [How is this agent even reasoning about the universe from inside the universe](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh/p/efWfvrWLgJmbBAs3m)?\n> \n> \n> \n\n\nThe aspiring researcher slumps in frustration, mutters a curse under their breath, and hangs up their hat – \"guess this whole alignment thing isn't for me...\". And isn't that so? All their brain could do was pattern-match onto already-proposed solutions and cached thinking.\n\n\nThere's more than one thing going wrong here, but I'm just going to focus on one. Given that person's understanding of AI alignment, this problem is *wildly* overconstrained. Whether or not alignment research is right for them, there's just no way that anyone's brain is going to fulfill this insane solution request!\n\n\nSometimes, constraints are necessary. I think that the alignment community is pretty good at finding plausibly necessary constraints. Maybe some of the above *aren't* necessary – maybe there's One Clever Trick you come up with which obviates one of these concerns.\n\n\nConstraints come with costs. Sometimes, the costs are worth it. In this context, I think the costs are very much worth it. Under this implicit framing of the problem, you're [pretty hosed](https://www.lesswrong.com/posts/w6BtMqKRLxG9bNLMr/the-catastrophic-convergence-conjecture) if you don't get even outer alignment right.\n\n\nHowever, even if the real problem has crazy constraints, that doesn't mean you should immediately tackle the fully constrained problem. I think you should often [relax](https://en.wikipedia.org/wiki/Relaxation_(approximation)) the problem first: eliminate or weaken constraints until you reach a problem which is still a little confusing, but which you can get some traction on.\n\n\n\n> \n> Even if you know an unbounded solution to chess, you might still be 47 years away from a bounded solution. But if you can't state a program that solves the problem in principle, you are in some sense confused about the nature of the cognitive work needed to solve the problem. If you can't even solve a problem given infinite computing power, you definitely can't solve it using bounded computing power. (Imagine Poe trying to write a chess-playing program before he'd had the insight about search trees.)\n> \n> \n> \n\n\n\n> \n> ~ [*The methodology of unbounded analysis*](https://arbital.com/p/unbounded_analysis/)\n> \n> \n> \n\n\nHistorically, I tend to be too slow to relax research problems. On the flipside, *all of my favorite research ideas were directly enabled by problem relaxation*. Instead of just telling you what to do and then having you forget this advice in five minutes, I'm going to paint it into your mind using two stories.\n\n\nAttainable Utility Preservation\n===============================\n\n\nIt's spring of 2018, and I've written myself into a corner. My work with CHAI for that summer was supposed to be on impact measurement, but I *inconveniently* posted [a convincing-to-me argument](https://www.lesswrong.com/posts/DvmhXysefEyEvXuXS/overcoming-clinginess-in-impact-measures) that impact measurement cannot admit a clean solution:\n\n\n\n> \n> I want to penalize the AI for having side effects on the world.[[2]](#fn-8gdCrzXPxrzn3eGF3-2) Suppose I have a function which looks at the consequences of the agent's actions and magically returns all of the side effects. Even if you have this function, you still have to assign blame for each effect – either the vase breaking was the AI's fault, or it wasn't.\n> \n> \n> \n\n\n\n> \n> If the AI penalizes itself for everything, it'll try to stop people from breaking vases – it'll be clingy. But if you magically have a model of how people are acting in the world, and the AI magically only penalizes itself for things which are its fault, then the AI is incentivized to blackmail people to break vases in ways which don't *technically* count as its fault. Oops.\n> \n> \n> \n\n\nSummer dawned, and I occupied myself with reading – lots and lots of reading. Eventually, enough was enough – I wanted to figure this out. I strode through my school's library, markers in my hand and determination in my heart. I was determined not to leave before understanding *a)* exactly why impact measurement is impossible to solve cleanly, or *b)* how to solve it.\n\n\nI reached the whiteboard, and then – with adrenaline pumping through my veins – I realized that I had *no idea* what this \"impact\" thing even is. Oops.\n\n\nI'm staring at the whiteboard.\n\n\nA minute passes.\n\n\n59 more minutes pass.\n\n\nI'd been thinking about how, in hindsight, it was so important that Shannon had first written a perfect chess-playing algorithm which required infinite compute, that Hutter had written an AGI algorithm which required infinite compute. I didn't know how to solve impact under all the constraints, but what if I assumed something here?\n\n\n\n> \n> What if I had infinite computing power? No… Still confused, don't see how to do it. Oh yeah, and what if the AI had a perfect world model. Hm... *What if we could write down a fully specified utility function which represented human preferences? Could I measure impact if I knew that?*\n> \n> \n> \n\n\nThe answer was almost trivially obvious. My first thought was that negative impact would be a decrease in true utility, but that wasn't quite right. I realized that impact measure needs to also capture decrease in ability to achieve utility. That's an optimal value function... So the negative impact would be the decrease in attainable utility for human values![[3]](#fn-8gdCrzXPxrzn3eGF3-3)\n\n\n\n> \n> Okay, but we don't and won't know the \"true\" utility function. What if... we just penalized shift in all attainable utilities?\n> \n> \n> \n\n\nI then wrote down The Attainable Utility Preservation Equation, more or less. Although it took me a few weeks to believe and realize, [that equation solved all of the impact measurement problems](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW/p/75oMAADr4265AGK3L) which had seemed so insurmountable to me just minutes before.[[4]](#fn-8gdCrzXPxrzn3eGF3-4)\n\n\nFormalizing Instrumental Convergence\n====================================\n\n\nIt's spring of 2019, and I've written myself into a corner. [My first post on AUP](https://www.lesswrong.com/posts/yEa7kwoMpsBgaBCgb/towards-a-new-impact-measure) was confusing – I'd failed to truly communicate what I was trying to say. Inspired by [*Embedded Agency*](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh), I was planning [an illustrated sequence of my own](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW).\n\n\nI was working through a bit of reasoning on how your ability to achieve one goal interacts with your ability to achieve seemingly unrelated goals. Spending a lot of money on red dice helps you for the collecting-dice goal, but makes it harder to become the best juggler in the world. That's a weird fact, but it's an *important* fact which underlies much of [AUP's empirical success](https://www.lesswrong.com/posts/4J4TA2ZF3wmSxhxuc/attainable-utility-preservation-empirical-results).\nI didn't understand why this fact was true.\n\n\nAt an impromptu presentation in 2018, I'd remarked that \"AUP wields instrumental convergence as a weapon against the alignment problem itself\". I tried thinking about it using the formalisms of reinforcement learning. Suddenly, I asked myself\n\n\n\n> \n> Why is instrumental convergence even a thing?\n> \n> \n> \n\n\nI paused. I went outside for a walk, and I paced. The walk lengthened, and I still didn't understand why. Maybe it was just a \"brute fact\", an \"emergent\" phenomenon – nope, not buying that. There's an explanation somewhere.\n\n\nI went back to the drawing board – to the whiteboard, in fact. I stopped trying to [understand the general case](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW/p/6DuJxY8X45Sco4bS2) and I focused on specific toy environments. I'm looking at an environment like this\n\n\n![](https://i.imgur.com/KEKaYrk.png)\n\n\nand I'm thinking, most agents go from `1` to `3`. \"Why does my brain think this?\", I asked myself. Unhelpfully, my brain decided not to respond.\n\n\nI'm staring at the whiteboard.\n\n\nA minute passes.\n\n\n29 more minutes pass.\n\n\nI'm reminded of a paper my advisor had me read for my qualifying exam. The paper talked about a dual formulation for reinforcement learning environments, where you consider the available trajectories through the future instead of the available policies. I take a picture of the whiteboard and head back to my office.\n\n\nI run into a friend. We start talking about work. I say, \"I'm about 80% sure I have the insight I need – this is how I felt in the past in situations like this, and I turned out to be right\".\n\n\nI turned out to be right. I started building up an entire theory of this dual formalism. Instead of asking myself about the general case of instrumental convergence in arbitrary computable environments, I considered small deterministic Markov decision processes. I started proving everything I could, building up my understanding piece by piece. This turned out to make all difference.\n\n\nHalf a year later, I'd built up enough theory that [I was able to explain a great deal (but not everything) about instrumental convergence](https://www.lesswrong.com/posts/6DuJxY8X45Sco4bS2/seeking-power-is-provably-instrumentally-convergent-in-mdps).\n\n\nConclusion\n==========\n\n\nProblem relaxation isn't always the right tactic. For example, if the problem isn't well-posed, it won't work well – imagine trying to \"relax\" the \"problem\" of free will! However, I think it's often the right move.\n\n\nThe move itself is simple: consider the simplest instance of the problem which is still confusing. Then, make a ton simplifying assumptions while still keeping part of the difficulty present – don't assume away all of the difficulty. Finally, tackle the relaxed problem.\n\n\nIn general, this seems like a skill that successful researchers and mathematicians learn to use. MIRI does a lot of this, for example. If you're new to the research game, this might be one of the crucial things to pick up on. Even though I detailed how this has worked for me, I think I could benefit from relaxing more.\n\n\nThe world is going to hell. You might be working on a hard (or even an impossible) problem. We plausibly stand on the precipice of extinction and utter annihilation.\n\n\nJust relax.\n\n\n*This is meant as a reference post. I'm not the first to talk using problem relaxation in this way. For example, see [The methodology of unbounded analysis](https://arbital.com/p/unbounded_analysis/).*\n\n\n\n\n---\n\n\n\n1. This failure mode is just my best guess – I haven't actually surveyed aspiring researchers. [↩︎](#fnref-8gdCrzXPxrzn3eGF3-1)\n2. The \"convincing-to-me argument\" contains a lot of confused reasoning about impact measurement, of course. For one, [thinking about side effects is *not* a good way of conceptualizing the impact measurement problem.](https://www.lesswrong.com/posts/pr3bLc2LtjARfK7nx/world-state-is-the-wrong-abstraction-for-impact) [↩︎](#fnref-8gdCrzXPxrzn3eGF3-2)\n3. The initial thought wasn't as clear as \"penalize decrease in attainable utility for human values\" – I was initially quite confused by the AUP equation. \"What the heck *is* this equation, and how do I break it?\".\n\n\nIt took me a few weeks to get a handle for why it seemed to work so well. It wasn't for a month or two that I began to understand what was actually going on, eventually leading to the [*Reframing Impact*](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW) sequence. However, for the reader's convenience, I whitewashed my reasoning here a bit. [↩︎](#fnref-8gdCrzXPxrzn3eGF3-3)\n4. At first, I wasn't very excited about AUP – I was new to alignment, and it took a lot of evidence to overcome the prior improbability of my having actually found something to be excited about. It took several weeks before I stopped thinking it likely that my idea was probably secretly and horribly bad.\n\n\nHowever, I kept staring at the strange equation – I kept trying to break it, to find some obvious loophole which would send me back to the drawing board. I never found it. Looking back over a year later, [AUP does presently have loopholes](https://www.lesswrong.com/posts/S8AGyJJsdBFXmxHcb/attainable-utility-preservation-scaling-to-superhuman#Appendix__Remaining_Problems), but they're not obvious, nor should they have sent me back to the drawing board.\n\n\nI started to get excited about the idea. Two weeks later, my workday was wrapping up and I left the library.\n\n\n\n> \n> Okay, I think there's about a good chance that this ends up solving impact. If I'm right, I'll want to have a photo to commemorate it.\n> \n> \n> \n\n\nI turned heel, descending back into the library's basement. I took the photograph. I'm glad that I did.\n\n\nDiscovering AUP was one of the happiest moments of my life. It gave me confidence that I could think, and it gave me some confidence that we can *win* – that we can solve alignment. [↩︎](#fnref-8gdCrzXPxrzn3eGF3-4)", "url": "https://www.alignmentforum.org/posts/JcpwEKbmNHdwhpq5n/problem-relaxation-as-a-tactic", "date_published": "2020-04-22T23:44:42Z", "authors": ["TurnTrout"], "tags": ["Techniques", "Rationality", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.741505+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "b13455d69d0bb44243dc36b15b70499e", "source": "alignmentforum", "title": "What makes counterfactuals comparable?", "text": "l was attempting to write a reference post on the concept of comparability in decision theory problems, but I realised that I don't yet have a strong enough grasp on the various positions that one could adopt to write a post worthy of being a reference. I'll quote my draft quite liberally below:\n\n\n> In the context of decision theory, comparability is about whether or not it is fair to compare counterfactuals when evaluating decisions, a decision algorithm or decision theory. Perhaps the best way to illustrate is with the example of a medical trial. Let's suppose we're trying to see if aspirin reduces the amount of pain experienced. So if we create two groups, give aspirin to one and then observe that group as having experienced less pain, that is evidence that it does what we want. However, if the aspirin group was healthy and most of other group had cancer, then this wouldn't be a fair test. We would be treating two groups as comparable when they differed in an attribute relevant the to outcome we cared about.\n\n\n> Given a decision problem, we normally apply a decision theory to construct counterfactuals, then calculate the utility for each and finally make a decision. Prima facie, it appears that these counterfactuals must be comparable in order for a decision theory to count as being reasonable. Otherwise, we would open ourselves to the critique of being the same as a naive researcher in the aspirin example.\n\n\n> We can clarify this with an example. Casual decision theorists recommend 2-boxing for Newcomb's Problem. They admit that a 1-boxer will receive an extra $1 million, but they would likely argue that this isn't a fair comparison since the opaque box contains $1 million for the 1-boxer, but not for the 2-boxer. This is essentially a dispute over whether these counterfactuals are comparable.\n\nWhy is this important?\n\nWell, as far as I can tell attempts to understand counterfactuals have taken us to logical counterfactuals at which point we've become stuck. Asking an easier question could help us to become unstuck. And determining if counterfactuals are comparable seems easier than saying what counterfactuals are. Indeed, I would go so far as to say that if we don't know what we mean by comparability then we don't fully even know what we are looking for.\n\nGiven this, I find it strange that this notion hasn't been discussed to any significant degree at all on Less Wrong as far as I can tell, although I haven't performed an in-depth search.\n\nBut before we go any further, it's worth asking what objections could be made to this approach. I'll quote my draft again:\n\n\n> Firstly, comparability only makes sense if the notion of counterfactuals makes sense. If they don't exist, then we would have to abandon the quest.\n\n\n> Secondly, we could admit counterfactuals, but deny comparability. Why might we do this? Firstly, the requirement for the past to be comparable regardless of our action seems to assume that our action shouldn't affect the past. But if there [wasn't any fundamental difference](https://www.lesswrong.com/posts/pa7mvEmEgt336gBSf/is-backwards-causation-necessarily-absurd) between forwards and backwards causation, this assumption would seem unsupported. Secondly, we might think that only the partial information we have before we are told our action must be the same and that there is no requirement for the counterfactuals to actually be comparable. Evidential decision theory could be justified on these terms.\n\n\n> Thirdly, we could argue for a notion of comparability that can be trivially satisfied. Causal decision theory leaves the past unchanged and only intervenes at the point of the decision. So these kind of counterfactuals are always trivially comparable, as it seems reasonable to presume that comparability only depends on the past and that identical pasts are automatically comparable. Note that it might be possible to argue for causal decision theory within the comparability framework. If only exact pasts were comparable, then that'd exclude almost every theory except for CDT.\n\n\n> Fourthly, we could argue that there are many different notions of comparability, so the question, \"What does it mean for counterfactuals to be comparable?\" is meaningless without further information about the purpose we are asking.\n\n**Three questions**:\n\nI'll finish this post with three questions designed to help clarify the notion of comparability. If you have time, I'd really appreciate it if you thought about the questions before writing your own answers, as that'd likely increase the diversity of responses. \n\n1) Suppose you have the option to choose one of two boxes: the first containing an item worth 5 utility and the second containing an item worth 10 utility. This results in one counterfactual where you take the first box and receive 5 utility and another where you take the second and receive 10 utility. Almost no-one would dispute that these counterfactuals are comparable, but why?\n\n2) As discussed above, a casual decision theorist would likely argue that the counterfactuals constructed by a timeless decision theorist aren't comparable because the 1-boxer has $1 million in the mystery box, while the two-boxer's box is empty. Most people on Less Wrong think that the casual decision theorist is wrong. How can we respond to this claim? Does it satisfy another notion of comparability or does this show the notion of comparability is irrelevant?\n\n3) An evidential decision theorist wouldn't smoke in the Smoking Lesion problem so they don't get cancer. Most people argue that they are incorrect because when evaluating smoking we can't compare a group of people predisposed to cancer to a normal group of people. Is this correct? And if so, wouldn't this mean that a 1-boxer would be correct in rejecting timeless decision theory counterfactuals as non-comparable (contrary to LW wisdom)? (There have been criticisms of the Smoking Lesion problem, but I think we could just make the same argument with Counterfactual Blackmail instead).\n\n*This post was supported by the AI Safety Research Program and was influenced by discussion with Davide Zagami and Pablo Moreno although the opinions expressed here are my own. It is an extension of work performed at the EA Hotel.*", "url": "https://www.alignmentforum.org/posts/6E6D3qLPM3urXDPpK/what-makes-counterfactuals-comparable-1", "date_published": "2020-04-24T22:47:38Z", "authors": ["Chris_Leong"], "tags": ["Counterfactuals", "Decision Theory", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.741826+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "b11a47358c570e115e3ea1cd7fe52a32", "source": "alignmentforum", "title": "[AN #97]: Are there historical examples of large, robust discontinuities?", "text": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.\n\nAudio version **[here](http://alignment-newsletter.libsyn.com/alignment-newsletter-97)** (may not be up yet). \n\nHIGHLIGHTS\n==========\n\n**[Discontinuous progress in history: an update](https://aiimpacts.org/discontinuous-progress-in-history-an-update/)** *(Katja Grace)* (summarized by Nicholas): One of the big questions in AI alignment is whether there will be a discontinuous AI takeoff (see **[here](https://alignmentforum.org/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment)** (**[AN #62](https://mailchi.mp/4a1b3c4249ae/an-62are-adversarial-examples-caused-by-real-but-imperceptible-features)**) for some reasons why the question is decision-relevant). To get a better outside view, AI Impacts has been looking for large discontinuities in historical technological trends. A discontinuity is measured by how many years ahead of time that value is reached, relative to what would have been expected by extrapolating the trend. \n\nThey found ten 100-year discontinuous events, for example in ship size (The SS *Great Eastern*), the average speed of military payload across the Atlantic Ocean (the first ICBM), and the warmest temperature of superconduction (yttrium barium copper oxide). \n\nThere are also some interesting negative examples of discontinuities. Particularly relevant to AI are AlexNet not being a discontinuity on the ImageNet benchmark and chess performance not having any discontinuities in Elo rating.\n\n**Nicholas' opinion:** Ignoring the George Washington Bridge (which confuses both me and the authors), I’d roughly categorize the causes of these discontinuities as\n\n- 3 of them were due to a concerted but apparently misplaced effort towards something others weren’t trying to do. These are Pyramid of Djoser, SS Great Eastern, and the Paris Gun.\n\n- 2 of them were due to the Atlantic Ocean causing a threshold effect (as they explain in the post). These are the ICBM and the first nonstop transatlantic flight.\n\n- 4 of them were due to a new technological breakthrough followed by increased investment and a faster rate of progress. These are the two telegraph cables, nuclear weapons, and superconductors.\n\nOf these, the final category seems the most relevant to AGI timelines, and I could imagine AGI development following a similar trajectory, where a major breakthrough causes a large amount of investment and then we have much faster progress on AI going forward.\n\nI was quite surprised that AlexNet did not represent a discontinuity on ImageNet performance. It is widely regarded to have kicked off deep learning in the computer vision community. I’m not sure if this is because the discontinuity metric they use doesn’t correspond with my sense of a “breakthrough”, because there were only two years of ImageNet beforehand, or because the vision community is just mistakenly attributing gradual progress to one major event.\n\n**Rohin's opinion:** I agree with Nicholas that the final category seems most relevant to AI progress. Note though that even for this analogy to hold, you need to imagine a major AI breakthrough, since as Nicholas pointed out, these discontinuities were caused by a radically new technology (telegraph cables replacing ships, nuclear weapons replacing conventional bombs, and ceramic superconductors replacing alloy superconductors). This doesn't seem likely in worlds where progress is driven primarily by **[compute](https://blog.openai.com/ai-and-compute/)** (**[AN #7](https://mailchi.mp/3e550712419a/alignment-newsletter-7)**), but could happen if (as academics often suggest) deep learning hits a wall and we need to find other AI algorithms to make progress.\n\n**[Description vs simulated prediction](https://aiimpacts.org/description-vs-simulated-prediction/)** *(Rick Korzekwa)* (summarized by Nicholas): AI Impacts’ investigation into discontinuous progress intends to answer two questions:\n\n1. How did tech progress happen in the past?\n\n2. How well could it have been predicted beforehand?\n\nThese can diverge when we have different information available now than in the past. For example, we could have more information because later data clarified trends or because the information is more accessible. We might have less information because we take an outside view (looking at trends) rather than an inside view (knowing the specific bottlenecks and what might need to be overcome).\n\nThe post then outlines some tradeoffs between answering these two questions and settles on primarily focusing on the first: describing tech progress in the past. \n\n**Nicholas' opinion:** I don’t have a strong opinion between which of these two questions is most important to focus on. It makes sense to me to work on them both in parallel since the data required is likely to be the same. My concern with this approach is that there is no clear denominator to the discontinuities they find. The case studies convince me that discontinuities **can** happen, but I really want to know the **frequency** with which they happen.\n\n**Rohin's opinion:** Given that we want to use this to forecast AI progress, it seems like we primarily care about the second question (simulated prediction). However, it's *really hard* to put yourselves in the shoes of someone in the past, making sure to have exactly the information that was available at the time; as a result I broadly agree with the decision to focus more on a description of what happened.\n\nTECHNICAL AI ALIGNMENT\n======================\n\nPROBLEMS\n--------\n\n**[Specification gaming: the flip side of AI ingenuity](https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity)** *(Victoria Krakovna et al)* (summarized by Rohin): This post on the DeepMind website explains the concept of **[specification gaming](https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/)** (**[AN #1](https://mailchi.mp/ff6340049bd0/alignment-newsletter-1)**), and illustrates three problems that arise within it. First and most obviously, we need to capture the human concept of a given task in a reward function. Second, we must design agents without introducing any mistaken implicit assumptions (e.g. that the physics simulation is accurate, when it isn't). Finally, we need to ensure that agents don't tamper with their reward functions.\n\nINTERPRETABILITY\n----------------\n\n**[Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents](https://arxiv.org/abs/1904.01318)** *(Christian Rupprecht et al)* (summarized by Rohin): This paper proposes a new visualization tool in order to understand the behaviour of agents trained using deep reinforcement learning. Specifically, they train a generative model which produces game states, and then optimise a distribution over state embeddings according to some target function (such as high reward for taking a specific action). By sampling from the resulting distribution, they create a diverse set of realistic states that score highly according to the target function. They propose a few target cost functions, which allow them to optimise for states in which the agent takes a particular action, states which are high reward (worst Q-value is large), states which are low reward (best Q-value is small), and critical states (large difference in Q value). They demonstrate results on Atari games as well as a simulated driving environment.\n\n**Robert's opinion:** I liked the paper, and I'm in favour of new work on interpreting reinforcement learning agents; I think it's under explored and useful, and relevant to AI safety. The methods seem in a similar vein to Feature Visualisation methods for classic vision, but focused solely on the resulting behaviour of the agent; it'd be interesting to see if such methods can give insight into the internals of RL agents. It's also a shame the demonstration of the results is wholly qualitative; the authors demonstrate some apparent flaws in the agents, but don't produce any results which show that the insights their method produces are useful. I think the insights are useful, but it's difficult to validate the claim, and I'm cautious of work which produces interesting and seemingly insightful methods but doesn't validate that the methods produce actually useful insight.\n\n**[Estimating Training Data Influence by Tracking Gradient Descent](https://arxiv.org/abs/2002.08484)** *(Garima Pruthi et al)* (summarized by Robert): This paper presents the TrackIn method for tracking the influence of training datapoints on the loss on a test datapoint. The purpose of the method is to discover influential training points for decisions made on the testing set. This is defined (loosely) for a training point **x** and test point **z** as the total change in loss on **z** caused by training on **x**. They present several approximations and methods for calculating this quantity efficiently, *allowing them to scale their method to ResNet 50 models trained on ImageNet*\n\nThe standard method of evaluation for these kinds of methods is finding mislabelled examples in the training dataset. Mislabelled examples are likely to have a strong positive influence on their own loss (strong as they're outliers, and positive as they'll reduce their own loss). Sorting the training dataset in decreasing order of this self-influence, we should hence expect to see more mislabelled examples at the beginning of the list. We can measure what proportion of mislabelled examples is present in each different initial segments of the list. The authors perform this experiment on CiFAR, first training a model to convergence, and then mislabelling 10% of the training set as the next highest predicted class, and then retraining a new model on which TrackIn is run. *When compared to the two previous methods from the literature (Influence Functions and Representer Points), TrackIn recovers more than 80% of the mislabelled data in the first 20% of the ranking, whereas the other methods recover less than 50% at the same point. For all segments TrackIn does significantly better.*\n\nThey demonstrate the method on a variety of domains, including NLP tasks and vision tasks. The influential examples found seem reasonable, but there's no quantification of these results.\n\n**Read more:** **[Understanding Black-box Predictions via Influence Functions](https://arxiv.org/pdf/1703.04730.pdf)**\n\n**Robert's opinion:** It's interesting to see methods able to identify which parts of the training data have an impact on the decisions of a model. I think the approach taken here (and in Influence Functions) of using the change in the test loss is OK, but it doesn't seem to be exactly what I think when I say \"which datapoints had the most influence on this decision being made in this way?\". It's also difficult to compare these methods without either a benchmark, a human experiment, or some way of demonstrating the method has produced novel insight which has been verified. The mislabelled data experiment partially fulfils this, but isn't what these methods are ultimately designed for, and is hence unsatisfactory.\n\nFORECASTING\n-----------\n\nVarious trends relevant to AI alignment *(Asya Bergal and Daniel Kokotajlo)* (summarized by Rohin): AI Impacts has published a few analyses of trends relevant to AI alignment (see links below).\n\nWill we see a continuous or discontinuous takeoff? **[Takeoff speeds](https://sideways-view.com/2018/02/24/takeoff-speeds/)** operationalizes continuous takeoff (there called \"slow takeoff\") as: There will be a complete 4 year interval in which world output doubles, before the first 1 year interval in which world output doubles. AI impacts searched for **[precedents for economic n-year doubling before 4n-year doubling](https://aiimpacts.org/precedents-for-economic-n-year-doubling-before-4n-year-doubling/)**, and found that this happened between 4,000 and 3,000 BC, and probably also between 10,000 and 4,000 BC. (Note this implies there was a 6000-year doubling before the 1000-year doubling, even though there wasn't a 4000-year doubling.)\n\nHow hard will it be to solve a crisply-stated problem of alignment? One way to get an outside view on the matter is to look at **[resolutions of mathematical conjectures over time](https://aiimpacts.org/resolutions-of-mathematical-conjectures-over-time/)**. While there is obvious sampling bias in which conjectures are remembered as being important, the results could nonetheless be informative. They find that \"the data is fit closely by an exponential function with a half-life of 117 years\".\n\nSince AI progress seems to be driven at least partially by **[compute](https://blog.openai.com/ai-and-compute/)** (**[AN #7](https://mailchi.mp/3e550712419a/alignment-newsletter-7)**), forecasting trends in compute seems important to forecasting AI progress. **[DRAM price per gigabyte](https://aiimpacts.org/trends-in-dram-price-per-gigabyte/)** has fallen by about an order of magnitude every 5 years from 1957 to 2020, although since 2010, the data suggests more like 14 years for a drop by an order of magnitude. **[Geekbench score per CPU price](https://aiimpacts.org/2019-recent-trends-in-geekbench-score-per-cpu-price/)** has grown by around 16% a year from 2006-2020, which would yield an order of magnitude over 16 years. This is slower than other **[CPU growth trends](https://aiimpacts.org/trends-in-the-cost-of-computing/)**, but this could be because Geekbench score is a markedly different metric.\n\n**Rohin's opinion:** I'm surprised that mathematical conjectures take so long to be resolved, I would have expected a smaller half-life than *117 years*. I'm not sure if I should update strongly though -- it's possible that we only remember conjectures that took a long time to be resolved (though it's somewhat surprising then how well the data fits an exponential).\n\n**[Surveys on fractional progress towards HLAI](https://aiimpacts.org/surveys-on-fractional-progress-towards-hlai/)** *(Asya Bergal)* (summarized by Rohin): One way to predict AGI timelines is to ask experts to estimate what fraction of progress has been made over a fixed number of years, then to extrapolate to the full 100% of progress. Doing this with the **[2016 expert survey](https://aiimpacts.org/2016-expert-survey-on-progress-in-ai/)** yields an estimate of 2056 (36 years from now), while doing this with Robin Hanson's informal ~15-expert survey gives 2392 (372 years from now). Part of the reason for the discrepancy is that Hanson only asked experts who had been in their field for at least 20 years; restricting to just these respondents in the 2016 survey yields an estimate of 2162 (142 years from now).\n\nMISCELLANEOUS (ALIGNMENT)\n-------------------------\n\n**[Survey of prescient actions](https://aiimpacts.org/survey-of-prescient-actions/)** *(Rick Korzekwa)* (summarized by Rohin): AI Impacts is looking into other examples in history where people took actions in order to address a complex, novel, severe future problem, and in hindsight we recognize those actions as prescient. Ideally we could learn lessons for AI alignment from such cases. The survey is so far very preliminary, so I'll summarize it later when it has been further developed, but I thought I'd send it along if you wanted to follow along (I found the six cases they've identified quite interesting).\n\n**Rohin's opinion:** One particularly interesting finding is that so far, in all of the cases they've looked at, there was a lot of feedback available to develop a solution. The post notes that this could be interpreted in two ways. First, since feedback is abundant in real-world problems, we should expect feedback for AI alignment as well (**[the optimistic take](https://mailchi.mp/b3dc916ac7e2/an-80-why-ai-risk-might-be-solved-without-additional-intervention-from-longtermists)**). Second, since AI alignment has no opportunity for feedback, it is unlike other problems (**[the pessimistic take](https://www.alignmentforum.org/posts/Gg9a4y8reWKtLe3Tn/the-rocket-alignment-problem)** (**[AN #27](https://mailchi.mp/0212425e5544/alignment-newsletter-27)**)). I would add a third option: that any real-world problem without feedback is extremely hard to solve, and so we wouldn't generate any hypotheses of actions that were prescient for such problems (in which case AI risk is not special amongst problems, it is just an instance of a very difficult problem).\n\nAI STRATEGY AND POLICY\n======================\n\n**[Improving Verifiability in AI Development](https://openai.com/blog/improving-verifiability/)** *(Miles Brundage et al)* (summarized by Flo): This multi-stakeholder report by authors from 30 different organizations proposes mechanisms to help with making claims about AI systems easier to verify. Despite being far from a complete solution to responsible AI development, verifiable claims can enable the public to hold developers accountable to comply with their stated ethics principles and allow developers to build trust by providing hard evidence about safety and fairness of their system. Better mechanisms for making such claims could also: i) help with the regulation of AI systems, ii) improve safety by counterbalancing competitive pressures to cut corners, and iii) help independent outside parties to assess the risk posed by specific applications. \n\nThe proposed mechanisms cluster into three classes: institutional, software, and hardware. Institutional mechanisms act on the incentives AI developers face. These include **third party auditing** of AI systems, for which a task force investigating different options would be helpful, and the **publication of incidents**, to provide evidence that incidents are taken seriously and prevent others from repeating the same mistakes. Other approaches are broadly analogous to adversarial training: collaborative **red teaming exercises** help to explore risks, and **bias and safety bounties** incentivize outsiders to seek out and report problems.\n\nSoftware mechanisms include **audit trails** that are used in many safety-critical applications in other industries, better **interpretability** to help with risk assessment and auditing, as well as better tools and standardization for **privacy-preserving machine learning**. The proposed hardware mechanisms are **secure hardware for machine learning**, which requires additional investment as machine learning often uses specialized hardware such that progress in the security of commodity hardware cannot be directly leveraged, **high-precision compute measurement** to help with verifying claims about how many computational resources were used for a particular project, and **compute support for academia** to allow academic researchers to better scrutinize claims made by the AI industry. \n\n**Read more:** **[Paper: Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims](https://arxiv.org/abs/2004.07213)**\n\n**Flo's opinion:** It would be exciting to see policymakers and AI developers experimenting with the proposed mechanisms, not because I am confident that all of these mechanisms will be useful for incentivising safer AI development, but because trying solutions and observing their specific shortcomings is useful for coming up with better solutions, and trying things early gives us more time to improve and iterate. However, the danger of lock-in is real as well: if the mechanisms won't be iterated on, delaying implementation until everything is watertight could be the better option. On the level of concrete mechanisms, regular red teaming exercises and more research on interpretability seem especially useful for safety, as common interaction with failure modes of AI systems seems to make safety issues more salient.\n\n**Rohin's opinion:** I love seeing these huge collaborations that simply enumerate possibilities for achieving some goal (the previous one, also organized by Miles, is the **[Malicious use of AI paper](https://arxiv.org/abs/1802.07228)**). It gives me much more confidence that the list is in some sense \"exhaustive\"; any items not on the list seem more likely to have been intentionally excluded (as opposed to the authors failing to think of those items).\n\nThat said, these mechanisms are still only one type of way that you could use to build trust -- in practice, when I trust people, it's because I think they also want to do the things I want them to do (whether because of external incentives or their intrinsic goals). I wonder how much this kind of trust building can be done with AI. For example, one story you could tell is that by producing compelling evidence that a particular algorithm is likely to lead to an existential catastrophe, you can build trust that no one will use that algorithm, at least as long as you believe that everyone strongly wants to avoid an existential catastrophe (and this outweighs any benefits of running the algorithm).\n\n#### **FEEDBACK**\n\nI'm always happy to hear feedback; you can send it to me, **[Rohin Shah](https://rohinshah.com/)**, by **replying to this email**. \n\n#### **PODCAST**\n\nAn audio podcast version of the **Alignment Newsletter** is available. This podcast is an audio version of the newsletter, recorded by **[Robert Miles](http://robertskmiles.com/)**.", "url": "https://www.alignmentforum.org/posts/WknLjywekGajwD2fp/an-97-are-there-historical-examples-of-large-robust", "date_published": "2020-04-29T17:30:02Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI Takeoff", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.742101+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "08cf4306f9a54736aa7a6e84da3e2442", "source": "alignmentforum", "title": "What is the alternative to intent alignment called?", "text": "Paul defines intent alignment of an AI A to a human H as the criterion that A is trying to do what H wants it to do. What term do people use for the definition of alignment in which A is trying to achieve H's goals (whether or not H intends for A to achieve H's goals)?\n\n\nSecondly, this seems to basically map on to the distinction between an aligned genie and an aligned sovereign. Is this a fair characterisation?\n\n\n(Intent alignment definition from <https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6>)", "url": "https://www.alignmentforum.org/posts/NuhsBLxxswinm2JKZ/what-is-the-alternative-to-intent-alignment-called", "date_published": "2020-04-30T02:16:03Z", "authors": ["Richard_Ngo"], "tags": ["AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.742464+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "7f4ecef8e113f9e47a9e1d7b9b7cbbbd", "source": "alignmentforum", "title": "Topological metaphysics: relating point-set topology and locale theory", "text": "The following is an informal exposition of some mathematical concepts from *[Topology via Logic](https://www.amazon.com/Topology-Cambridge-Theoretical-Computer-Science/dp/0521576512)*, with special attention to philosophical implications. Those seeking more technical detail should simply read the book.\n\n\nThere are, roughly, two ways of doing topology:\n\n\n* *Point-set topology*: Start with a set of points. Consider a topology as a set of subsets of these points which are \"open\", where open sets must satisfy some laws.\n* *Locale theory*: Start with a set of opens (similar to propositions), which are closed under some logical operators (especially and and or), and satisfy logical relations.\n\n\nWhat laws are satisfied?\n\n\n* For point-set topology: The empty set and the full set must both be open; finite intersections and infinite unions of opens must be open.\n* For local theory: \"True\" and \"false\" must be opens; the opens must be closed under finite \"and\" and infinite \"or\"; and some logical equivalences must be satisfied, such that \"and\" and \"or\" work as expected.\n\n\nRoughly, open sets and opens both correspond to *verifiable* propositions. If X and Y are both verifiable, then both \"X or Y\" and \"X and Y\" are verifiable; and, indeed, even countably infinite disjunctions of verifiable statements are verifiable, by exhibiting the particular statement in the disjunction that is verified as true.\n\n\nWhat's the philosophical interpretation of the difference between point-set topology and locale theory, then?\n\n\n* Point-set topology corresponds to the theory of [possible worlds](https://plato.stanford.edu/entries/possible-worlds/). There is a \"real state of affairs\", which can be partially known about. Open sets are \"events\" that are potentially observable (verifiable). Ontology comes before epistemology. Possible worlds are associated with classical logic and classical probability/utility theory.\n* Local theory corresponds to the theory of [situation semantics](https://plato.stanford.edu/entries/situations-semantics/). There are facts that are true in a particular situation, which have logical relations with each other. The first three lines of Wittgenstein's *Tracatus Logico-Philosophicus* are: \"The world is everything that is the case. / The world is the totality of facts, not of things. / The world is determined by the facts, and by these being all the facts.\" Epistemology comes before ontology. Situation semantics is associated with intuitionist logic and Jeffrey-Bolker utility theory (recently [discussed](https://www.alignmentforum.org/posts/A8iGaZ3uHNNGgJeaD/an-orthodox-case-against-utility-functions) by Abram Demski).\n\n\nThus, they correspond to fairly different metaphysics. Can these different metaphysics be converted to each other?\n\n\n* Converting from point-set topology to locale theory is easy. The opens are, simply, the open sets; their logical relations (and/or) are determined by set operations (intersection/union). They automatically satisfy the required laws.\n* To convert from locale theory to point-set topology, construct possible worlds as *sets* of opens (which must be logically coherent, e.g. the set of opens can't include \"A and B\" without including \"A\"), which are interpreted as the set of opens that are true of that possible world. The open sets of the topology correspond with the opens, as sets of possible words which contain the open.\n\n\nFrom assumptions about possible worlds and possible observations of it, it is possible to derive a logic of observations; from assumptions about the logical relations of different propositions, it is possible to consider a set of possible worlds and interpretations of the propositions as world-properties.\n\n\nMetaphysically, we can consider point-set topology as ontology-first, and locale theory as epistemology-first. Point-set topology starts with possible worlds, corresponding to Kantian noumena; locale theory starts with verifiable propositions, corresponding to Kantian phenomena.\n\n\nWhile the interpretation of a given point-set topology as a locale is trivial, the interpretation of a locale theory as a point-set topology is less so. What this construction yields is a way of getting from observations to possible worlds. From the set of things that can be known (and knowable logical relations between these knowables), it is possible to conjecture a consistent set of possible worlds and ways those knowables relate to the possible worlds.\n\n\nOf course, the true possible worlds may be finer-grained than these consistent set; however, it cannot be coarser-grained, or else the same possible world would result in different observations. No finer potentially-observable (verifiable or falsifiable) distinctions may be made between possible worlds than the ones yielded by this transformation; making finer distinctions risks positing [unreferenceable entities](https://unstableontology.com/2020/03/14/the-absurdity-of-un-referenceable-entities/) in a self-defeating manner.\n\n\nHow much extra ontological reach does this transformation yield? If the locale has a countable basis, then the point-set topology may have an uncountable point-set (specifically, of the same cardinality as the reals). The continuous can, then, be constructed from the discrete, as the underlying continuous state of affairs that could generate any given possibly-infinite set of discrete observations.\n\n\nIn particular, the reals may be constructed from a locale based on open intervals whose beginning/end are rational numbers. That is: a real r may be represented as a set of (*a*, *b*) pairs where *a* and *b* are rational, and *a* < *r* < *b*. The locale whose basis is rational-delimited open intervals (whose elements are countable unions of such open intervals, and which specifies logical relationships between them, e.g. conjunction) yields the point-set topology of the reals. (Note that, although including all countable unions of basis elements would make the locale uncountable, it is possible to weaken the notion of locale to only require unions of recursively enumerable sets, which preserves countability)\n\n\nIf metaphysics may be defined as the general framework bridging between ontology and epistemology, then the conversions discussed provide a metaphysics: a way of relating that-which-could-be to that-which-can-be-known.\n\n\nI think this relationship is quite interesting and clarifying. I find it useful in my own present philosophical project, in terms of relating [subject-centered epistemology](https://unstableontology.com/2020/03/05/a-critical-agential-account-of-free-will-causation-and-physics/) to possible centered worlds. Ontology can reach further than epistemology, and topology provides mathematical frameworks for modeling this.\n\n\nThat this construction yields continuous from discrete is an added bonus, which should be quite helpful in clarifying the relation between the mental and physical. Mental phenomena must be at least partially discrete for logical epistemology to be applicable; meanwhile, physical theories including Newtonian mechanics and standard quantum theory posit that physical reality is continuous, consisting of particle positions or a wave function. Thus, relating discrete epistemology to continuous ontology is directly relevant to philosophy of science and theory of mind.", "url": "https://www.alignmentforum.org/posts/yTvZFzcgt7rGYMxP5/topological-metaphysics-relating-point-set-topology-and", "date_published": "2020-05-01T03:57:12Z", "authors": ["jessicata"], "tags": ["World Modeling"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.742535+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "b6dd472a6a05288540c51e8fa3663ae1", "source": "alignmentforum", "title": "How does iterated amplification exceed human abilities?", "text": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\nWhen I first started learning about IDA, I thought that agents trained using IDA would be human-level after the first stage, i.e. that Distill(H) would be human-level. As I've [written about before](https://www.lesswrong.com/posts/FdfzFcRvqLf4k5eoQ/list-of-resolved-confusions-about-ida?commentId=bdcWwPhaZxiy7rxLb), Paul later clarified this, so my new understanding is that after the first stage, the distilled agent will be super-human in some respects and infra-human in others, but wouldn't be \"basically human\" in any sense.\n\n\nBut IDA is aiming to eventually be super-human in almost every way (because it's aiming to be competitive with unaligned AGI), so that raises some new questions:\n\n\n1. If IDA isn't going to be human-level after the first stage, then at what stage does IDA become at-least-human-level in almost every way?\n2. What exactly is the limitation that prevents the first stage of IDA from being human-level in almost every way?\n3. When IDA eventually *does* become at-least-human-level in almost every way, how is the limitation from (2) avoided?\n\n\nThat brings me to [Evans et al.](https://owainevans.github.io/pdfs/evans_ida_projects.pdf), which contains a description of IDA in section 0. The way IDA is set up in this paper leads me to believe that the answer to (2) above is that the human overseer cannot provide a sufficient number of demonstrations for the most difficult tasks. For example, maybe the human can provide enough demonstrations for the agent to learn to answer very simple questions (tasks in T0 in the paper) but it's too time-consuming for the human to answer enough complicated questions (say, in T100). My understanding is that IDA gets around this by having an amplified system that is itself automated (i.e. does not involve humans in a major way, so cannot be bottlenecked on the slowness of humans); this allows the amplified system to provide a sufficient number of demonstrations for the distillation step to work.\n\n\nSo in the above view, the answer to (2) is that the limitation is the number of demonstrations the human can provide, and the answer to (3) is that the human can seed the IDA process with sufficient demonstrations of easy tasks, after which the (automated) amplified system can provide sufficient demonstrations of the harder tasks. The answer to (1) is kind of vague: it's just the smallest n for which ⋃ni=0Ti contains almost all tasks a human can do.\n\n\nBut the above view seems to conflict with what's in the [IDA post](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616) and the [IDA paper](https://arxiv.org/abs/1810.08575). In both of those, the amplified system is described as a human doing the decompositions (so it will be slow, or else one would need to argue that the slowness of humans decomposing tasks doesn't meaningfully restrict the number of demonstrations). Also, the main benefit of amplification is described not as the ability to provide more demonstrations, but rather to provide demonstrations for more difficult tasks. Under this alternative view, the answers to questions (1), (2), (3) aren't clear to me.\n\n\n*Thanks to Vipul Naik for reading through this question and giving feedback.*", "url": "https://www.alignmentforum.org/posts/ajQzejMYizfX4dMWK/how-does-iterated-amplification-exceed-human-abilities", "date_published": "2020-05-02T23:44:31Z", "authors": ["riceissa"], "tags": ["Iterated Amplification ", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.742820+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "78004975cbfbb7e3011dc48a557a833c", "source": "alignmentforum", "title": "How uniform is the neocortex?", "text": "How uniform is the neocortex?\n=============================\n\n\nThe neocortex is the part of the human brain responsible for higher-order functions like sensory perception, cognition, and language, and has been hypothesized to be uniformly composed of general-purpose data-processing modules. What does the currently available evidence suggest about this hypothesis?\n\n\n*\"How uniform is the neocortex?” is one of the background variables in my* *[framework for AGI timelines](https://www.alignmentforum.org/posts/w4jjwDPa853m9P4ag/my-current-framework-for-thinking-about-agi-timelines). My aim for this post is* ***not*** *to present a complete argument for some view on this variable, so much as it is to:*\n\n\n* *present some considerations I’ve encountered that shed light on this variable*\n* *invite a collaborative effort among readers to shed further light on this variable (e.g. by leaving comments about considerations I haven’t included, or pointing out mistakes in my analyses)*\n\n\n\n\n---\n\n\nThere’s a [long list of different regions in the neocortex](https://en.wikipedia.org/wiki/List_of_regions_in_the_human_brain#Cerebral_cortex_(neocortex)), each of which appears to be responsible for something totally different. One interpretation is that these cortical regions are doing fundamentally different things, and that we acquired the capacities to do all these different things over hundreds of millions of years of evolution.\n\n\nA radically different perspective, first put forth by Vernon Mountcastle in 1978, hypothesizes that the neocortex is implementing a single general-purpose data processing algorithm all throughout. From the popular neuroscience book *On Intelligence*, by Jeff Hawkins[[1]](#fn-9HeFxbQR6MZjjNSEE-1):\n\n\n\n> \n> [...] Mountcastle points out that the neocortex is remarkably uniform in appearance and structure. The regions of cortex that handle auditory input look like the regions that handle touch, which look like the regions that control muscles, which look like Broca's language area, which look like practically every other region of the cortex. Mountcastle suggests that since these regions all look the same, perhaps they are actually performing the same basic operation! He proposes that the cortex uses the same computational tool to accomplish everything it does.\n> \n> \n> \n\n\n\n> \n> [...]\n> \n> \n> \n\n\n\n> \n> Mountcastle [...] shows that despite the differences, the neocortex is remarkably uniform. The same layers, cell types, and connections exist throughout. [...] The differences are often so subtle that trained anatomists can't agree on them. Therefore, Mountcastle argues, all regions of the cortex are performing the same operation. The thing that makes the vision area visual and the motor area motoric is how the regions of cortex are connected to each other and to other parts of the central nervous system.\n> \n> \n> \n\n\n\n> \n> In fact, Mountcastle argues that the reason one region of cortex looks slightly different from another is because of what it is connected to, and not because its basic function is different. He concludes that there is a common function, a common algorithm, that is performed by all the cortical regions. Vision is no different from hearing, which is no different from motor output. He allows that our genes specify how the regions of cortex are connected, which is very specific to function and species, but the cortical tissue itself is doing the same thing everywhere.\n> \n> \n> \n\n\n\n> \n> If Mountcastle is correct, the algorithm of the cortex must be expressed independently of any particular function or sense. The brain uses the same process to see as to hear. The cortex does something universal that can be applied to any type of sensory or motor system.\n> \n> \n> \n\n\nThe rest of this post will review some of the evidence around Mountcastle’s hypothesis.\n\n\nCortical function is largely determined by input data\n=====================================================\n\n\nWhen visual inputs are fed into the auditory cortices of infant ferrets, those auditory cortices [develop into functional visual systems](https://www.nytimes.com/2000/04/25/science/rewired-ferrets-overturn-theories-of-brain-growth.html). This suggests that different cortical regions are all capable of general-purpose data processing.\n\n\nHumans can learn how to perform forms of sensory processing we haven’t evolved to perform—blind people can learn to [see with their tongues](https://www.scientificamerican.com/article/device-lets-blind-see-with-tongues/), and can learn to echolocate well enough to [discern density and texture](https://www.smithsonianmag.com/innovation/how-does-human-echolocation-work-180965063/). On the flip side, forms of sensory processing that we *did* evolve to perform depend heavily on the data we’re exposed to—for example, [cats exposed only to horizontal edges early in life don’t have the ability to discern vertical edges later in life](https://computervisionblog.wordpress.com/2013/06/01/cats-and-vision-is-vision-acquired-or-innate/). This suggests that our capacities for sensory processing stem from some sort of general-purpose data processing, rather than innate machinery handed to us by evolution.\n\n\nBlind people who learn to echolocate do so with the help of [repurposed visual cortices](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0020162), and they can [learn to read Braille using repurposed visual cortices](https://www.ncbi.nlm.nih.gov/pubmed/8606771). Our visual cortices did not evolve to be utilized in these ways, suggesting that the visual cortex is doing some form of general-purpose data processing.\n\n\nThere’s a man who had the entire left half of his brain removed when he was 5, who has [above-average intelligence](https://www.ncbi.nlm.nih.gov/pubmed/1172204), and went on to [graduate college and maintain steady employment](https://www.dropbox.com/s/3sjj44obltxqf1x/hemispherectomy.pdf?dl=0). This would only be possible if the right half of his brain were capable of taking on the cognitive functions of the left half of the brain.\n\n\nThe patterns identified by the primary sensory cortices (for vision, hearing, and seeing) [overlap substantially](https://pdfs.semanticscholar.org/ebbd/86cc59c3f7cb1343d8be9ba8fdee7bbf438d.pdf) with the patterns that numerous different unsupervised learning algorithms identified from the same data, suggesting that the different cortical regions (along with the different unsupervised learning algorithms) are all just doing some form of general-purpose pattern recognition on its input data.\n\n\nDeep learning and cortical generality\n=====================================\n\n\nThe above evidence does not rule out the possibility that the cortex's apparent adaptability stems from developmental triggers, rather than some capability for general-purpose data-processing. By analogy, stem cells all start out very similar, only to differentiate into cells with functions tailored to the contexts in which they find themselves. It’s possible that different cortical regions have hard-coded genomic responses for handling particular data inputs, such that the cortex gives one hard-coded response when it detects that it’s receiving visual data, another hard-coded response when it detects that it’s receives auditory data, etc.\n\n\nIf this were the case, the cortex’s data-processing capabilities can best be understood as [specialized responses to distinct evolutionary needs](https://en.wikipedia.org/wiki/Moravec%27s_paradox#The_biological_basis_of_human_skills), and our ability to process data that we haven’t evolved to process (e.g. being able to look at a Go board and intuitively discern what a good next move would be) most likely utilizes a complicated mishmash of heterogeneous data-processing abilities acquired over evolutionary timescales.\n\n\nBefore I learned about any of the advancements in deep learning, this was my most likely guess about how the brain worked. It had always seemed to me that the hardest and most mysterious part of intelligence was intuitive pattern-recognition, and that the various forms of intuitive processing that let us recognize images, say sentences, and play Go might be totally different and possibly arbitrarily complex.\n\n\nSo I was very surprised when I learned that a single general method in deep learning (training an artificial neural network on massive amounts of data using gradient descent)[[2]](#fn-9HeFxbQR6MZjjNSEE-2) led to performance comparable or superior to humans’ in tasks as disparate as [image classification](https://en.wikipedia.org/wiki/ImageNet), [speech synthesis](https://deepmind.com/blog/article/wavenet-generative-model-raw-audio), and [playing Go](https://deepmind.com/research/case-studies/alphago-the-story-so-far). I\nfound superhuman Go performance particularly surprising—intuitive judgments of Go boards encode distillations of high-level strategic reasoning, and are highly sensitive to small changes in input. Neither of these is true for sensory processing, so my prior guess was that the methods that worked for sensory processing wouldn’t have been sufficient for playing Go as well as humans.[[3]](#fn-9HeFxbQR6MZjjNSEE-3)\n\n\nThis suggested to me that there’s nothing fundamentally complex or mysterious about intuition, and that seemingly-heterogeneous forms of intuitive processing can result from simple and general learning algorithms. From this perspective, it seems most parsimonious to explain the cortex’s seemingly general-purpose data-processing capabilities as resulting straightforwardly from a general learning algorithm implemented all throughout the cortex. (This is *not* to say that I think the cortex is doing what artificial neural networks are doing—rather, I think deep learning provides evidence that general learning algorithms exist *at all*, which increases the prior likelihood on the cortex implementing a general learning algorithm.[[4]](#fn-9HeFxbQR6MZjjNSEE-4))\n\n\nThe strength of this conclusion hinges on the extent to which the “artificial intuition” that current artificial neural networks (ANNs) are capable of is analogous to the intuitive processing that humans are capable of. It’s possible that the “intuition” utilized by ANNs is deeply analogous to human intuition, in which case the generality of ANNs would be very informative about the generality of cortical data-processing. It's also possible that \"artificial intuition\" is different in kind from human intuition, or that it only captures a small fraction of what goes into human intuition, in which case the generality of ANNs would not be very informative about the generality of cortical data-processing.\n\n\nIt seems that experts are divided about how analogous these forms of intuition are, and I conjecture that this is a major source of disagreement about overall AI timelines. [Shane Legg](https://en.wikipedia.org/wiki/Shane_Legg) (a cofounder of DeepMind, a leading AI lab) has been talking about how [deep belief networks might be able to replicate the function of the cortex](http://www.vetta.org/2009/12/tick-tock-tick-tock-bing/) before deep learning took off, and he’s [been predicting human-level AGI in the 2020s since 2009](http://www.vetta.org/2009/02/tick-tock-tick-tock/). Eliezer Yudkowsky has directly talked about AlphaGo providing evidence of [\"neural algorithms that generalize well, the way that the human cortical algorithm generalizes well\"](https://www.facebook.com/yudkowsky/posts/10153914357214228?pnref=story) as an indication that AGI might be near. [Rodney Brooks](https://en.wikipedia.org/wiki/Rodney_Brooks) (the former director of MIT’s AI lab) has written about how deep learning is not capable of [real perception or manipulation](https://rodneybrooks.com/forai-steps-toward-super-intelligence-iii-hard-things-today/), and thinks AGI is [over 100 years away](https://spectrum.ieee.org/computing/software/humanlevel-ai-is-right-around-the-corner-or-hundreds-of-years-away#RodneyBrooks). [Gary Marcus](https://en.wikipedia.org/wiki/Gary_Marcus) has described deep learning as a [“wild oversimplification” of the \"hundreds of anatomically and likely functionally [distinct] areas\" of the cortex](https://arxiv.org/pdf/2002.06177.pdf), and estimates AGI to be [20-50 years away](https://spectrum.ieee.org/computing/software/humanlevel-ai-is-right-around-the-corner-or-hundreds-of-years-away#GaryMarcus).\n\n\nCanonical microcircuits for predictive coding\n=============================================\n\n\nIf the cortex were uniform, what might it *actually be doing* uniformly?\n\n\nThe cortex has been hypothesized to consist of [canonical microcircuits that implement predictive coding](https://www.dropbox.com/s/q2roa6373xjdqdu/PP%20canonical%20microcircuits.pdf?dl=0). In a nutshell, predictive coding (aka predictive processing) is a theory of brain function which hypothesizes that the cortex learns hierarchical structure of the data it receives, and uses this structure to encode predictions about future sense inputs, resulting in “controlled hallucinations” that we interpret as direct perception of the world.\n\n\n*On Intelligence* has an excerpt that cleanly communicates what I mean by “learning hierarchical structure”:\n\n\n\n> \n> [...] The real world's nested structure is mirrored by the nested structure of your cortex.\n> \n> \n> \n\n\n\n> \n> What do I mean by a nested or hierarchical structure? Think about music. Notes are combined to form intervals. Intervals are combined to form melodic phrases. Phrases are combined to form melodies or songs. Songs are combined into albums. Think about written language. Letters are combined to form syllables. Syllables are combined to form words. Words are combined to form clauses and sentences. Looking at it the other way around, think about your neighborhood. It probably contains roads, schools, and houses. Houses have rooms. Each room has walls, a ceiling, a floor, a door, and one or more windows. Each of these is composed of smaller objects. Windows are made of glass, frames, latches, and screens. Latches are made from smaller parts like screws.\n> \n> \n> \n\n\n\n> \n> Take a moment to look up at your surroundings. Patterns from the retina entering your primary visual cortex are being combined to form line segments. Line segments combine to form more complex shapes. These complex shapes are combining to form objects like noses. Noses are combining with eyes and mouths to form faces. And faces are combining with other body parts to form the person who is sitting in the room across from you.\n> \n> \n> \n\n\n\n> \n> All objects in your world are composed of subobjects that occur consistently together; that is the very definition of an object. When we assign a name to something, we do so because a set of features consistently travels together. A face is a face precisely because two eyes, a nose, and a mouth always appear together. An eye is an eye precisely because a pupil, an iris, an eyelid, and so on, always appear together. The same can be said for chairs, cars, trees, parks, and countries. And, finally, a song is a song because a series of intervals always appear together in sequence.\n> \n> \n> \n\n\n\n> \n> In this way the world is like a song. Every object in the world is composed of a collection of smaller objects, and most objects are part of larger objects. This is what I mean by nested structure. Once you are aware of it, you can see nested structures everywhere. In an exactly analogous way, your memories of things and the way your brain represents them are stored in the hierarchical structure of the cortex. Your memory of your home does not exist in one region of cortex. It is stored over a hierarchy of cortical regions that reflect the hierarchical structure of the home. Large-scale relationships are stored at the top of the hierarchy and small-scale relationships are stored toward the bottom.\n> \n> \n> \n\n\n\n> \n> The design of the cortex and the method by which it learns naturally discover the hierarchical relationships in the world. You are not born with knowledge of language, houses, or music. The cortex has a clever learning algorithm that naturally finds whatever hierarchical structure exists and captures it.\n> \n> \n> \n\n\nThe clearest evidence that the brain is learning hierarchical structure comes from the visual system. The visual cortex is known to have [edge detectors at the lowest levels of processing](https://en.wikipedia.org/wiki/Feature_detection_(nervous_system)), and neurons that fire when shown [images of particular people](https://en.wikipedia.org/wiki/Grandmother_cell#Individual_specific_recognition_cells), like Bill Clinton.\n\n\nWhat does predictive coding say the cortex does with this learned hierarchical structure? From [an introductory blog post about predictive processing](https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/):\n\n\n\n> \n> [...] the brain is a multi-layer prediction machine. All neural processing consists of two streams: a bottom-up stream of sense data, and a top-down stream of predictions. These streams interface at each level of processing, comparing themselves to each other and adjusting themselves as necessary.\n> \n> \n> \n\n\n\n> \n> The bottom-up stream starts out as all that incomprehensible light and darkness and noise that we need to process. It gradually moves up all the cognitive layers that we already knew existed – the edge-detectors that resolve it into edges, the object-detectors that shape the edges into solid objects, et cetera.\n> \n> \n> \n\n\n\n> \n> The top-down stream starts with everything you know about the world, all your best heuristics, all your priors, [all the structure you’ve learned,] everything that’s ever happened to you before – everything from “solid objects can’t pass through one another” to “e=mc^2” to “that guy in the blue uniform is probably a policeman”. It uses its knowledge of concepts to make predictions – not in the form of verbal statements, but in the form of expected sense data. It makes some guesses about what you’re going to see, hear, and feel next, and asks “Like this?” These predictions gradually move down all the cognitive layers to generate lower-level predictions. If that uniformed guy was a policeman, how would that affect the various objects in the scene? Given the answer to that question, how would it affect the distribution of edges in the scene? Given the answer to that question, how would it affect the raw-sense data received?\n> \n> \n> \n\n\n\n> \n> As these two streams move through the brain side-by-side, they continually interface with each other. Each level receives the predictions from the level above it and the sense data from the level below it. Then each level uses Bayes’ Theorem to integrate these two sources of probabilistic evidence as best it can.\n> \n> \n> \n\n\n\n> \n> [...]\n> \n> \n> \n\n\n\n> \n> *“To deal rapidly and fluently with an uncertain and noisy world, brains like ours have become masters of prediction – surfing the waves and noisy and ambiguous sensory stimulation by, in effect, trying to stay just ahead of them. A skilled surfer stays ‘in the pocket’: close to, yet just ahead of the place where the wave is breaking. This provides power and, when the wave breaks, it does not catch her. The brain’s task is not dissimilar. By constantly attempting to predict the incoming sensory signal we become able [...] to learn about the world around us and to engage that world in thought and action.”*\n> \n> \n> \n\n\n\n> \n> The result is perception, which the PP theory describes as “controlled hallucination”. You’re not seeing the world as it is, exactly. You’re seeing your predictions about the world, cashed out as expected sensations, then shaped/constrained by the actual sense data.\n> \n> \n> \n\n\nAn illustration of predictive processing, from the same source:\n\n\n![](https://slatestarcodex.com/blog_images/dalmatian_cow1.png)\n\n\n\n> \n> This demonstrates the degree to which the brain depends on top-down hypotheses to make sense of the bottom-up data. To most people, these two pictures start off looking like incoherent blotches of light and darkness. Once they figure out what they are ([spoiler](https://slatestarcodex.com/blog_images/dalmatian_cow2.png)) the scene becomes obvious and coherent. According to the predictive processing model, this is how we perceive everything all the time – except usually the concepts necessary to make the scene fit together come from our higher-level predictions instead of from clicking on a spoiler link.\n> \n> \n> \n\n\nPredictive coding has been hailed by [prominent neuroscientists](https://www.wired.com/story/karl-friston-free-energy-principle-artificial-intelligence/) as a possible [unified theory of the brain](https://www.fil.ion.ucl.ac.uk/~karl/The%20free-energy%20principle%20A%20unified%20brain%20theory.pdf), but I’m confused about how much physiological evidence there is that the brain is actually implementing predictive coding. It seems like there’s physiological evidence in support of predictive coding being implemented [in the visual cortex](https://www.nature.com/articles/nn0199_79) and in the [auditory cortex](https://www.sciencedirect.com/science/article/pii/S030645221730547X), and there’s a [theoretical account](https://www.nature.com/articles/s41598-018-21407-9) of how the [prefrontal cortex](https://en.wikipedia.org/wiki/Prefrontal_cortex) (responsible for higher cognitive functions like planning, decision-making, and executive function) might be utilizing similar principles. [This paper](https://www.cell.com/neuron/pdf/S0896-6273(18)30857-2.pdf) and [this paper](https://discovery.ucl.ac.uk/id/eprint/10056744/1/Friston_News%20and%20views.pdf) review some physiological evidence of predictive coding in the cortex that I don’t really know how to interpret.\n\n\nMy current take\n===============\n\n\nI find the various pieces of evidence that cortical function depends largely on data inputs (e.g. the ferret rewiring experiment) to be pretty compelling evidence of general-purpose data-processing in the cortex. The success of simple and general methods in deep learning across a wide range of tasks suggests that it’s most parsimonious to model the cortex as employing general methods throughout, but only to the extent that the capabilities of artificial neural networks can be taken to be analogous to the capabilities of the cortex. I currently consider the analogy to be deep, and intend to explore my reasons for thinking so in future posts.\n\n\nI think the fact that predictive coding offers a plausible theoretical account for what the cortex could be doing uniformly, which can account for higher-level cognitive functions in addition to sensory processing, is itself some evidence of cortical uniformity. I’m confused about how much physiological evidence there is that the brain is actually implementing predictive coding, but I’m very bullish on predictive coding as a basis for a unified brain theory based on non-physiological evidence (like our subjective experiences making sense of the images of splotches) that I intend to explore in a future post.\n\n\n*Thanks to Paul Kreiner, David Spivak, and Stag Lynn for helpful suggestions and feedback, and thanks to Jacob Cannell for writing [a post](https://www.lesswrong.com/posts/9Yc7Pp7szcjPgPsjf/the-brain-as-a-universal-learning-machine) that inspired much of my thinking here.*\n\n\n\n\n---\n\n\n\n1. [This blog post](http://bensbookblog.blogspot.com/2005/11/on-intelligence-by-jeff-hawkins_21.html) comment has some good excerpts from *On Intelligence*. [↩︎](#fnref-9HeFxbQR6MZjjNSEE-1)\n2. Deep learning is a general method in the sense that most tasks are solved by utilizing a handful of basic tools from a standard toolkit, adapted for the specific task at hand. Once you’ve selected the basic tools, all that’s left is figuring out how to supply the training data, specifying the objective that lets the AI know how well it’s doing, throwing a lot of computation at the problem, and fiddling with details. My understanding is that there typically isn’t much conceptual ingenuity involved in solving the problems, that most of the work goes into fiddling with details, and that trying to be clever [doesn't lead to better results than using standard tricks with more computation and training data](http://www.incompleteideas.net/IncIdeas/BitterLesson.html). It's also worth noting that most of the tools in this standard toolkit have been around since the 90's (e.g. convolutional neural networks, LSTMs, reinforcement learning, backpropagation), and that the recent boom in AI was driven by using these decades-old tools with unprecedented amounts of computation. [↩︎](#fnref-9HeFxbQR6MZjjNSEE-2)\n3. AlphaGo did simulate future moves to achieve superhuman performance, so the direct comparison against human intuition isn't completely fair. But AlphaGo Zero's raw neural network, which just looks at the \"texture\" of the board without simulating any future moves, can still play quite formidably. From the [AlphaGo Zero paper](https://discovery.ucl.ac.uk/id/eprint/10045895/1/agz_unformatted_nature.pdf): \"The raw neural network, without using any lookahead, achieved an Elo rating of 3,055. AlphaGo Zero achieved a rating of 5,185, compared to 4,858 for AlphaGo Master, 3,739 for AlphaGo Lee and 3,144 for AlphaGo Fan.\" (AlphaGo Fan beat the European Go champion 5-0.) [↩︎](#fnref-9HeFxbQR6MZjjNSEE-3)\n4. Eliezer Yudkowsky has an insightful exposition of this point in [a Facebook post](https://www.facebook.com/yudkowsky/posts/10153914357214228?pnref=story). [↩︎](#fnref-9HeFxbQR6MZjjNSEE-4)", "url": "https://www.alignmentforum.org/posts/WFopenhCXyHX3ukw3/how-uniform-is-the-neocortex", "date_published": "2020-05-04T02:16:51Z", "authors": ["zhukeepa"], "tags": ["World Modeling", "Neocortex", "Neuroscience", "Predictive Processing", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.743238+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "cef4b7a1811e1a9322c48c45b91bfaaa", "source": "alignmentforum", "title": "Competitive safety via gradated curricula", "text": "*Epistemic status: brainstorming some speculative research directions. Not trying to thoroughly justify the claims I’m making.*\n\nOne way to think about the AI safety problem: there’s a spectrum of methods which each represent a different tradeoff between safety and ease of training an AGI, and unfortunately the two are anticorrelated. In particular, consider four regimes in which the bulk of training might occur (perhaps with additional fine-tuning afterwards):\n\n1. Training a language model to answer questions correctly.\n2. Training a RL agent on a range of limited tasks (e.g. games).\n3. Training a RL agent on general tasks in large-scale simulations for long time periods.\n4. Training a RL agent in competitive multi-agent environments.\n\nI claim (but won’t fully defend here) that these are in order from safest but most difficult, to easiest but most dangerous:\n\n1. Regime 1 will produce a question-answering system which has no experience taking actions in the world, and which may not be goal-directed at all. But many researchers expect that it’ll be much easier to create an AGI which can answer difficult questions by training it to interact with a simulated world, so that its concepts are “[grounded](https://arxiv.org/abs/cs/9906002v1)” by experience.\n2. Regime 2 is likely to produce an agent whose goals are bounded, and whose concepts are grounded; but which might only do well on the specific tasks it had been trained on. If so, building AGI in this regime would require a very sophisticated curriculum, if it’s possible at all.\n3. Regime 3 provides a rich environment for an agent to learn quite general skills and concepts. However, now the agent will also be rewarded for developing large-scale goals, which might make it dangerous.\n4. Regime 4 additional provides an “[autocurriculum](https://arxiv.org/abs/1903.00742)” via competition, the training signal from which could accelerate the development of general intelligence (as it did in humans). However, the agent could learn harmful skills and motivations (such as deception, manipulation or aggression) from competing with other agents, which it might then apply to interactions with humans.\n\nThis is a problem - but it’s also an opportunity. If we accept the claims I’ve made about this spectrum, then it might be much easier to train a relatively safe and non-agentic AGI if we start training in less safe regimes, and then gradually transition the training of that AGI into safer regimes. More specifically, I’m proposing a training curriculum in which an agent is trained in regime 4 until it displays a given level of competence; then moved to regime 3 until it again displays a significant amount of progress; then regime 2; then regime 1. The specific regimes used are not vital; some could be removed or replaced by others that I haven’t thought of. Neither is it essential to keep using exactly the same agent; it’d need to be retrained to use different observation and action spaces, and perhaps have its architecture modified during transitions. (In particular, it might be useful to incorporate a pre-trained language model at some point to kick-start its understanding of language.) The main point is that as training progresses, we increasingly use safer training regimes even though we expect it to be much more difficult to train an AGI solely using those regimes.\n\nThe key hypothesis is that it’s not uniformly harder to train AGIs in the safer regimes - rather, it’s primarily harder to get *started* in those regimes. Once an AI reaches a given level of intelligence, then transitioning to a safer regime might not slow down the rate at which it gains intelligence very much - but might still decrease the optimisation pressure in favour of that AI being highly agentic and pursuing large-scale goals.\n\nI have some intuitions in favour of this hypothesis (although I’m still pretty uncertain). Here are three:\n\n1. Language-based tasks or limited-domain tasks can be made almost arbitrarily hard, we won’t run out of ways to challenge the agent (although this might require more work than continuing training in other regimes).\n2. Exploration is much easier for an agent that’s already quite intelligent, and so reward sparsity matters less. This would also make it easier to manually design tasks, since a wider range of difficulties would be acceptable.\n3. Once an agent has had some amount of interaction with a (simulated or real) world, it can interpret language as referring to objects in that world. Whereas without any interaction, it seems far more difficult to develop a solid understanding of objects and concepts. (Note that although quite a few AI researchers seem to believe that such grounding is important, I haven’t seen much justification for it in the context of modern machine learning. So I wanted to flag this as a particularly intuition-driven claim.)\n\nOne example which might help illustrate the overall hypothesis is that of [Helen Keller](https://en.wikipedia.org/wiki/Helen_Keller), who developed a sophisticated understanding of the world (and wrote 12 books), despite becoming deaf and blind before the age of 2. Compare two options: either trying to train an AGI (from random initialisation to human-level intelligence) which receives the inputs that a typical human has, or else trying to do the same thing giving it only the inputs that Helen had (primarily touch, and language initially communicated via taps on her hand). I think the latter would be significantly more difficult, because having much less data imposes many fewer constraints on what the structure of the world could be like. And yet Helen did not learn many times more slowly than most people; in fact she earned a degree from Harvard in four years. My explanation for this: the learning problem Helen faced was much harder than what most of us face, but because her brain architecture had already been “trained” by evolution, she could make use of those implicit priors to match, and then surpass, most of her contemporaries.\n\nThe second crucial hypothesis is that the AGI doesn’t also retain dangerous characteristics from earlier training regimes. Again, I’m very uncertain about this hypothesis - it might be that, once the system has started training in a highly competitive environment, it will continue to have competitive and highly agentic motivations until we actively prevent it from doing so. Yet there are ways to mitigate this. In particular: to the extent that we can identify which part of the AGI is responsible for goal-directed cognition, we can remove that before continuing to train the rest of the network in the new regime. This would rely on interpretability techniques - but techniques which could be much cruder than those required to understand what an AGI is thinking or planning at any given moment. As an illustration, I think we already understand the human brain well enough to identify the parts most responsible for goal-directed cognition and motivation (although I don’t know much neuroscience, so corrections on this point would be very welcome). After removing the analogous sections of an AGI, it’d need to develop a new motivational system in subsequent training regimes - but I’m hoping that those later regimes only incentivise limited goal-directedness, towards bounded goals.\n\nRight now, I think it’s pretty likely that there’ll be *some* transitions between different training regimes when developing AGI - e.g. for language models it’s already common to start with unsupervised pre-training, and then do supervised or RL fine-tuning. But note that this transition goes in the opposite direction (from more limited tasks to larger-scale tasks) compared with the ones I discussed above. So my proposal is a little counterintuitive; but while I’m not confident that it’ll be useful (perhaps because I’m still quite confused about agency and goal-directedness), I think it’s worth evaluating. One empirical test which could already be done is to see whether pre-training in a simulated environment is advantageous for developing better language models. Another, which might only be viable in a few years, is to investigate how separable goal-directed cognition is from world-modelling and other aspects of intelligence, in deep reinforcement learners with a range of neural architectures.\n\n  \n\\* Given that I have no experience of being deaf or blind, and have not looked into it very much, my intuitions on this point are not very well-informed; so I wanted to explicitly flag it as quite speculative.", "url": "https://www.alignmentforum.org/posts/vLepnCxCWW6YTw8eW/competitive-safety-via-gradated-curricula", "date_published": "2020-05-05T18:11:08Z", "authors": ["Richard_Ngo"], "tags": ["AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.743905+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "2a3545fc9f4358c0bb0d453bd73ac36a", "source": "alignmentforum", "title": "[AN #98]: Understanding neural net training by seeing which gradients were helpful", "text": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.\n\nAudio version **[here](http://alignment-newsletter.libsyn.com/alignment-newsletter-98)** (may not be up yet). \n\nHIGHLIGHTS\n==========\n\n**[LCA: Loss Change Allocation for Neural Network Training](https://arxiv.org/abs/1909.01440)** *(Janice Lan et al)* (summarized by Robert): This paper introduces the *Loss Change Allocation* (LCA) method. The method's purpose is to gain insight and understanding into the training process of deep neural networks. The method calculates an allocation of the change in overall loss (on the whole training set) between every parameter at each training iteration, which is iteratively refined until the approximation error is less than 1% overall. This loss change allocation can be either positive or negative; **if it's negative, then the parameter is said to have helped training at that iteration, and if it's positive then the parameter hurt training**. Given this measurement is per-parameter and per-iteration, it can be aggregated to per-layer LCA, or any other summation over parameters and training iterations.\n\nThe authors use the method to gain a number of insights into the training process of several small neural networks (trained on MNIST and CIFAR-10).\n\nFirst, they validate that learning is very noisy, with **on average only half of the parameters helping at each iteration**. The distribution is heavier-tailed than a normal distribution, and is fairly symmetrical. However, parameters tend to alternate between helping and hurting, and each parameter only tends to help approximately 50% of the time.\n\nSecond, they look at the LCA aggregated per-layer, summed over the entire training process, and show that in the CIFAR ResNet model **the first and last layers hurt overall** (i.e. have positive LCA). In an attempt to remedy this and understand the causes, the authors try freezing these layers, or reducing their learning rate. The first layer can't be fixed (freezing makes it's LCA 0, but later layers' LCA is increased in turn so the overall final loss stays the same). However, for the last layer, **freezing or reducing the learning rate increases the overall performance of the network**, as the last layer's LCA is decreased more than all the other layer's LCAs are increased. They also hypothesize that by reducing the momentum for the last layer, they can give it fresher information and make it more likely to learn. They find that this does work, though in this setting previous layers’ LCA increases to compensate, leaving overall performance unchanged.\n\nFinally, the authors show that **learning seems to be synchronised across layers**; layers get local LCA minima at the same training iterations, in a statistically significant way. They show this must be a combination of parameter motion and the gradient, as neither on their own explains this phenomenon.\n\n**Robert's opinion:** I really liked this paper. The method is simple (although computationally expensive), and gives novel insights. I think understanding how deep learning training works is important as it can help us design better training processes, not just for better performance but for other properties we want the training process to induce. I think there's a lot of future work which could be done with this method, in making it more efficient and then applying it to larger models in domains other than vision. I'd also be interested in seeing if this can be used to understand which parts of the training set help and hurt training; for example seeing whether there's any correlation between the points of synchronised learning and the datapoints in the minibatch at that training iteration. Note: I'd recommend reading the paper (including the appendices) to see the graphs and visualisations the authors produced to demonstrate their arguments, as they're much easier to understand than a textual description.\n\n**Rohin's opinion:** I also really enjoyed this paper, it has great empirical evidence about how neural networks work. I'd be inclined to analyze the results somewhat differently. In particular, suppose that when calculating LCA, we made the following changes:\n\n1. We used the loss on the training batches instead of the full training set.\n\n2. We didn't improve the approximation error (i.e. we just used the point estimate of the gradient calculated during training).\n\n3. We trained using stochastic gradient descent (SGD) (as opposed to say Adam or Momentum-SGD).\n\nThen all LCA values would be negative (explanation in **[this comment](https://www.alignmentforum.org/posts/TmHRACaxXrLbXb5tS/rohinmshah-s-shortform?commentId=dWkj6mK2uZtnXJQiq)**). So, when the paper shows experiments where LCA values are positive (i.e. the parameters / layers are anti-learning), we can attribute those effects to some combination of these three factors.\n\nTake the observation that learning is very noisy. I would guess that this is primarily because of the first point: there are many many ways to improve the loss on a tiny little minibatch, but only a tiny fraction of those are capturing \"real effects\" that would improve the loss on the full large training dataset. Likely in the large majority of cases, the update doesn't capture a \"real effect\", and so it's a coin flip whether or not it will help with the loss on the full training dataset. A large probability of a coin flip + a small probability of a \"real effect\" gets you to an improvement slightly over half the time. This explanation applies across parameters, iterations, layers, etc.\n\nSimilarly, they find that learning is synchronized across layers. I think this is also primarily because of the first point. My guess is that there are some batches of data that are more \"canonical\" than others, that are easiest to learn from. In the case where we see synchronization for each class, this could be as simple as that particular training batch having more examples of that class than other training batches.\n\nI’d be interested in seeing experiments in which we start with the version of LCA where everything is negative, and made only one of the changes. This would allow us to narrow down which particular change causes a given effect, kind of like an ablation study.\n\nTECHNICAL AI ALIGNMENT\n======================\n\nITERATED AMPLIFICATION\n----------------------\n\n**[How does iterated amplification exceed human abilities?](https://www.alignmentforum.org/posts/ajQzejMYizfX4dMWK/how-does-iterated-amplification-exceed-human-abilities)** *(Issa Rice)*\n\nLEARNING HUMAN INTENT\n---------------------\n\n**[Shared Autonomy via Hindsight Optimization](https://arxiv.org/abs/1503.07619)** *(Shervin Javdani et al)* (summarized by Rohin): This paper considers a shared autonomy task in which a user controls a robot to achieve some goal, and the robot learns to assist the user, without knowing the goal in advance. They formalize this as a POMDP in which the state includes the user's goal, which the robot does not get to observe. However, the POMDP observation model assigns higher probability to user actions that better achieve the goal (a standard Boltzmann rationality model), and this allows the agent to reason about what the goal must be. In practice, for computational tractability, rather than choosing optimal actions in the overall POMDP, the robot chooses optimal actions using a technique called hindsight optimization, which *assumes that the robot will never learn more information about the user's goal*.\n\n**Rohin's opinion:** The formulation of a POMDP with uncertainty over the goal is remarkably similar to the formulation of **[Cooperative Inverse Reinforcement Learning](https://arxiv.org/abs/1606.03137)** (**[AN #69](https://mailchi.mp/59ddebcb3b9a/an-69-stuart-russells-new-book-on-why-we-need-to-replace-the-standard-model-of-ai)**) (and predates it), with the main difference being that there is only one actor (the robot hardware).\n\n**[Imitation Learning via Off-Policy Distribution Matching](https://openreview.net/forum?id=Hyg-JC4FDr)** *(Ilya Kostrikov et al)* (summarized by Zach): One way to view imitation learning is as a distribution matching problem. In other words, the agent is rewarded based on how well it can imitate the state-distribution induced by the expert. In recent years, distribution matching via adversarial methods such as GAIL has become a popular approach to imitation learning. However, one weakness of these methods is that they require on-policy samples which means they require the agent to interact with the environment. In this paper, the authors present an off-policy method for distribution matching which can work without environment interaction. They do this by building on the prior work of DualDICE, a policy-agnostic method to estimate distribution ratios between agent and expert which can then be used to provide a reward to the agent. This allows the optimal policy to be estimated directly from demonstrations without any need for agent interaction. The authors run a few experiments and show that the method has comparable performance to behavioral cloning in the off-policy setting and adversarial methods in the on-policy setting.\n\n**Prerequisities:** **[DualDICE](https://arxiv.org/abs/1906.04733)**\n\n**Read more:** **[GAIL](https://arxiv.org/abs/1606.03476)**\n\n**Zach's opinion:** This is a cool application of density-estimation via DualDICE. While the experiments are a bit weak, the fact that an off-policy method exists to do distribution-matching is interesting in its own right. Moreover, the method seems able to compete with both BC and GAIL-like methods which is intriguing.\n\nVERIFICATION\n------------\n\n**[Ethical Mission Definition and Execution for Maritime Robots Under Human Supervision](https://calhoun.nps.edu/handle/10945/61086)** *(Don Brutzman et al)* (summarized by Rohin) (H/T Jon Rodriguez): While underwater robots can perform missions that humans cannot, they cannot be held liable for their actions. Our society requires that someone be responsible for (and can be held liable for) the actions of any such robot, leading to a form of the specification problem: how do we program robots such that it is reasonable to hold their operators accountable for their actions?\n\nThis paper divides mission execution into three main parts: the execution level (hardware control), the tactical level (low-level behaviors), and the strategic level (what the robot should do). It proposes that, at the strategic level, we use formal methods to specify what the robot should do. The language should be expressive enough to be useful, while still keeping it sufficiently limited to allow exhaustive testing. They propose using state machines augmented with constraints. The constraints can be used to specify things like \"the robot must stay at least 10m away from obstacles\". The state machine decides which behaviors to execute, and each such behavior can have three results: success, failure, or exception (in the case that a constraint would have been violated had the behavior continued operating).\n\n**Rohin's opinion:** It's interesting to see other groups also aiming to have what are essentially robustness guarantees, but motivated instead from the perspective of responsibility and liability. The actual method seems reasonable for the impoverished systems we have today, where we must specify everything that we want the system to do.\n\nFORECASTING\n-----------\n\n**[FLI Podcast: On Superforecasting](https://futureoflife.org/2020/04/30/on-superforecasting-with-robert-de-neufville/)** *(Lucas Perry and Robert de Neufville)*\n\nMISCELLANEOUS (ALIGNMENT)\n-------------------------\n\n**[Formal Metaethics and Metasemantics for AI Alignment](http://www.metaethical.ai/v20-1/)** *(June Ku)* (summarized by Rohin): This website presents in great detail a process by which an agent might use data from human brains in order to infer a utility function for a single human (also spelling out what assumptions need to be made along the way), and then how it could combine the utility functions from different humans to arrive at \"a fully technical ethical goal function\". Emphasis is placed on solving the philosophical problems of metaethics and mental content. Quoting the website, they \"suppose that unlimited computation and a complete low-level causal model of the world and the adult human brains in it are available\".\n\n**[Approaches to Deploying a Safe Artificial Moral Agent](https://montrealethics.ai/approaches-to-deploying-a-safe-artificial-moral-agent/)** *(Olivier Couttolenc)* (summarized by Rohin): This post investigates which of the current moral theories would most reduce existential risk if we programmed it into an AI system, and settles on Aristotelian virtue ethics (over utilitarianism and Kant's categorical imperative).\n\nNEAR-TERM CONCERNS\n==================\n\nFAIRNESS AND BIAS\n-----------------\n\n**[Algorithmic Fairness from a Non-ideal Perspective](https://arxiv.org/abs/2001.09773)** *(Sina Fazelpour et al)* (summarized by Rohin): The field of fairness has aimed to develop objective metrics of fairness, which can then be optimized for in order to produce a just AI system. Unfortunately, many intuitively desirable fairness metrics are fundamentally incompatible, and cannot be simultaneously achieved except in special circumstances. Should we lose all hope for fairness?\n\nThis paper argues that the problem was that we were building *idealized* theories, referring to a conception from political philosophy of ideal and non-ideal modes of theorizing. An ideal theory is one that describes an optimal, ideal world, and then identifies injustices by searching for discrepancies between the real world and the idealized one. This leads to three major flaws:\n\n1. It can lead to systematic neglect of some injustices and distortions of our understanding of other injustices. For example, group parity metrics of fairness applied to college admissions would identify east Asian students as privileged relative to white students despite historical and institutional discrimination.\n\n2. It does not offer sufficient practical guidance about what should be done, sometimes leading to misguided mitigation strategies. Consider college admissions again. A *disparate learning process* aims to be blind to protected characteristics (like gender) while still achieving demographic parity. This forces the model to penalize features that correlate with being male. As a result, we end up rewarding women who go into female-dominated fields, and penalize women who go into male-dominated fields! This was presumably not what we wanted.\n\n3. It does not make clear who among decision-makers is responsible for intervening to correct specific injustices.\n\nThe authors suggest that the research community move towards a non-ideal mode of theorizing, in which there is more emphasis on having a deep empirical understanding of the problem (including the various causal factors, rather than summary statistics), and using empirically-informed choices of treatments, rather than modifying ML algorithms to optimize a mathematically defined metric.\n\n**Rohin's opinion:** I really enjoyed this paper, and my summary doesn't do it justice -- it makes several other good points. I feel similarly about alignment: I feel relatively pessimistic about formal definitions of concepts like **[goal-directedness](https://www.alignmentforum.org/posts/DfcywmqRSkBaCB6Ma/intuitions-about-goal-directed-behavior)** (**[AN #35](https://mailchi.mp/bbd47ba94e84/alignment-newsletter-35)**) or **[safe exploration](https://openai.com/blog/safety-gym/)** (**[AN #76](https://mailchi.mp/1106d0ce6766/an-76how-dataset-size-affects-robustness-and-benchmarking-safe-exploration-by-measuring-constraint-violations)**), and feel much better about schemes that don't assume a formal definition of concepts and instead learn them from humans (or don't require them at all).\n\nAnother thing that jumped out at me was that their description of the non-ideal mode of theorizing focuses a *lot* on understanding what exactly is going on, which is very similar to the concepts of interpretability and **[universality](https://ai-alignment.com/towards-formalizing-universality-409ab893a456)** (**[AN #81](https://mailchi.mp/6078fe4f9928/an-81-universality-as-a-potential-solution-to-conceptual-difficulties-in-intent-alignment)**) in alignment.\n\nOTHER PROGRESS IN AI\n====================\n\nREINFORCEMENT LEARNING\n----------------------\n\n**[The Ingredients of Real World Robotic Reinforcement Learning](https://bair.berkeley.edu/blog/2020/04/27/ingredients/)** *(Henry Zhu, Justin Yu, Abhishek Gupta et al)* (summarized by Rohin): Suppose we wanted to train a robot to perform a task in the real world, and we didn't want to deal with the headache of sim-to-real transfer. Typically, since all of our experience must be collected in the real world, we would need a human to reset the robot to its initial state. The key idea of this paper is that the point of resets is to ensure that the robot explores a diversity of states causing it to learn a robust policy; this can be achieved by learning a *perturbation policy* whose objective is to take the robot to states it hasn't visited before. They then combine this with representation learning (so that they can learn from pixels) and use a classifier that distinguishes goal states from non-goal states as the reward function, to get a fully automated setup where once you start the robot's training, it trains itself without any human in the loop.\n\n**Read more:** **[Paper: The Ingredients of Real World Robotic Reinforcement Learning](https://openreview.net/forum?id=rJe2syrtvS&noteId=rJe2syrtvS)**\n\n**Rohin's opinion:** This is a cool proof of concept, but the learned perturbation policy can only take you so far -- no learned perturbation policy is going to allow you to e.g. pick up an object after it is dropped, as you would want if you're training a robot to **[manipulate a Rubik's cube](https://openai.com/blog/solving-rubiks-cube/)** (**[AN #70](https://mailchi.mp/732eaa192df0/an-70-agents-that-help-humans-who-are-still-learning-about-their-own-preferences)**). It seems hard to overcome this sort of problem in a fully automated and learned way (though perhaps you could use more classical techniques to have a \"hardcoded\" but still automated reset policy).\n\nNEWS\n====\n\n**[CLR Open Positions: Researchers and Summer Research Fellows](https://longtermrisk.org/work-with-us/)** (summarized by Rohin): The Center on Long-Term Risk is looking for researchers and summer research fellows to work on high-quality research relevant to s-risks, including on (among other areas) multiagent systems. The application deadline is May 13.\n\n#### **FEEDBACK**\n\nI'm always happy to hear feedback; you can send it to me, **[Rohin Shah](https://rohinshah.com/)**, by **replying to this email**. \n\n#### **PODCAST**\n\nAn audio podcast version of the **Alignment Newsletter** is available. This podcast is an audio version of the newsletter, recorded by **[Robert Miles](http://robertskmiles.com/)**.", "url": "https://www.alignmentforum.org/posts/Sj9YurD9vwpfPErs2/an-98-understanding-neural-net-training-by-seeing-which", "date_published": "2020-05-06T17:10:03Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.744380+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "054a2bfba124eaa8b99f725fe094e4e2", "source": "alignmentforum", "title": "Specification gaming: the flip side of AI ingenuity", "text": "*(Originally posted to the* [*Deepmind Blog*](https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity)*)*\n\n**Specification gaming** is a behaviour that satisfies the literal specification of an objective without achieving the intended outcome. We have all had experiences with specification gaming, even if not by this name. Readers may have heard the myth of [King Midas](https://en.wikipedia.org/wiki/Midas) and the golden touch, in which the king asks that anything he touches be turned to gold - but soon finds that even food and drink turn to metal in his hands. In the real world, when rewarded for doing well on a homework assignment, a student might copy another student to get the right answers, rather than learning the material - and thus exploit a loophole in the task specification. \n\nThis problem also arises in the design of artificial agents. For example, a reinforcement learning agent can find a shortcut to getting lots of reward without completing the task as intended by the human designer. These behaviours are common, and we have [collected](http://tinyurl.com/specification-gaming) around 60 examples so far (aggregating [existing](https://arxiv.org/abs/1803.03453) [lists](https://www.gwern.net/Tanks#alternative-examples) and ongoing [contributions](https://docs.google.com/forms/d/e/1FAIpQLSeQEguZg4JfvpTywgZa3j-1J-4urrnjBVeoAO7JHIH53nrBTA/viewform) from the AI community). In this post, we review possible causes for specification gaming, share examples of where this happens in practice, and argue for further work on principled approaches to overcoming specification problems.\n\nLet's look at an example. In a [Lego stacking task](https://arxiv.org/abs/1704.03073), the desired outcome was for a red block to end up on top of a blue block. The agent was rewarded for the height of the bottom face of the red block when it is not touching the block. Instead of performing the relatively difficult maneuver of picking up the red block and placing it on top of the blue one, the agent simply flipped over the red block to collect the reward. This behaviour achieved the stated objective (high bottom face of the red block) at the expense of what the designer actually cares about (stacking it on top of the blue one).\n\n![](https://lh3.googleusercontent.com/2mcWyeaCYtnuXBo50Igz41KFFQPmNYa_nY4J8vep3LxzTDG1FlWMy9CzNT10pSoD5PYXNPdxTqxwmUcNXLk_6-sbZRxkxYn6YRdP=w1440)SOURCE: DATA-EFFICIENT DEEP REINFORCEMENT LEARNING FOR DEXTEROUS MANIPULATION (POPOV ET AL, 2017)We can consider specification gaming from two different perspectives. Within the scope of developing reinforcement learning (RL) algorithms, the goal is to build agents that learn to achieve the given objective. For example, when we use Atari games as a benchmark for training RL algorithms, the goal is to evaluate whether our algorithms can solve difficult tasks. Whether or not the agent solves the task by exploiting a loophole is unimportant in this context. From this perspective, specification gaming is a good sign - the agent has found a novel way to achieve the specified objective. These behaviours demonstrate the ingenuity and power of algorithms to find ways to do exactly what we tell them to do.\n\nHowever, when we want an agent to actually stack Lego blocks, the same ingenuity can pose an issue. Within the broader scope of building [aligned agents](https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84) that achieve the intended outcome in the world, specification gaming is problematic, as it involves the agent exploiting a loophole in the specification at the expense of the intended outcome. These behaviours are caused by misspecification of the intended task, rather than any flaw in the RL algorithm. In addition to algorithm design, another necessary component of building aligned agents is reward design.\n\n![](https://kstatic.googleusercontent.com/files/4db5fe1fa425a3fb29631a54cb27df67b24d37a27c5604fc84eed55eeef53029ef75f4b41cbedc7f86713ba42c278004dba1c43e3a257c60fcad624d7bf17711)Designing task specifications (reward functions, environments, etc.) that accurately reflect the intent of the human designer tends to be difficult. Even for a slight misspecification, a very good RL algorithm might be able to find an intricate solution that is quite different from the intended solution, even if a poorer algorithm would not be able to find this solution and thus yield solutions that are closer to the intended outcome. This means that correctly specifying intent can become more important for achieving the desired outcome as RL algorithms improve. It will therefore be essential that the ability of researchers to correctly specify tasks keeps up with the ability of agents to find novel solutions.\n\nWe use the term **task specification** in a broad sense to encompass many aspects of the agent development process. In an RL setup, task specification includes not only reward design, but also the choice of training environment and auxiliary rewards. The correctness of the task specification can determine whether the ingenuity of the agent is or is not in line with the intended outcome. If the specification is right, the agent's creativity produces a desirable novel solution. This is what allowed AlphaGo to play the famous [Move 37](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol#Game_2), which took human Go experts by surprise yet which was pivotal in its second match with Lee Sedol. If the specification is wrong, it can produce undesirable gaming behaviour, like flipping the block. These types of solutions lie on a spectrum, and we don't have an objective way to distinguish between them.\n\n \n\n![](https://kstatic.googleusercontent.com/files/bb80b4636868463cace8a6ec9cf359428ab21a0066f96a239e9560ed8ffaa0744a22089d1de61d389a70b0889ddf21ef88018ee2a7a55c7957d87f80664c407e) \n\nWe will now consider possible causes of specification gaming. One source of reward function misspecification is poorly designed **reward shaping**. Reward shaping makes it easier to learn some objectives by giving the agent some rewards on the way to solving a task, instead of only rewarding the final outcome. However, shaping rewards can change the optimal policy if they are not [potential-based](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf). Consider an agent controlling a boat in the [Coast Runners game](https://openai.com/blog/faulty-reward-functions/), where the intended goal was to finish the boat race as quickly as possible. The agent was given a shaping reward for hitting green blocks along the race track, which changed the optimal policy to going in circles and hitting the same green blocks over and over again.\n\n![](https://lh3.googleusercontent.com/DlI1IekotqsGSbMsO1eNxMRXqX_MECfssJrv7zdym3DsRM_f-4y9njZ3MwZTL5CHvsnPKKwNQgRQmvk4JMuZxUowWKRT0ItUDVdyvQ=w1440)SOURCE: FAULTY REWARD FUNCTIONS IN THE WILD (AMODEI & CLARK, 2016)Specifying a reward that accurately captures the **desired final outcome** can be challenging in its own right. In the Lego stacking task, it is not sufficient to specify that the bottom face of the red block has to be high off the floor, since the agent can simply flip the red block to achieve this goal. A more comprehensive specification of the desired outcome would also include that the top face of the red block has to be above the bottom face, and that the bottom face is aligned with the top face of the blue block. It is easy to miss one of these criteria when specifying the outcome, thus making the specification too broad and potentially easier to satisfy with a degenerate solution. \n\nInstead of trying to create a specification that covers every possible corner case, we could [**learn the reward function from human feedback**](https://deepmind.com/blog/article/learning-through-human-feedback). It is often easier to evaluate whether an outcome has been achieved than to specify it explicitly. However, this approach can also encounter specification gaming issues if the reward model does not learn the true reward function that reflects the designer's preferences. One possible source of inaccuracies can be the human feedback used to train the reward model. For example, an agent performing a [grasping task](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/) learned to fool the human evaluator by hovering between the camera and the object.\n\n![](https://lh3.googleusercontent.com/dH60m8tRE0V10m9JZrDbY-APQW14UsF7di42EGWOOfliGAiDY5eEPB1MXOMLH-wH0Ohjrg5szxx09flDAOOuZD4dbgU0FWxR3-wk8g=w1440)SOURCE: DEEP REINFORCEMENT LEARNING FROM HUMAN PREFERENCES (CHRISTIANO ET AL, 2017)The learned reward model could also be misspecified for other reasons, such as poor generalisation. Additional feedback can be used to correct the agent's attempts to exploit the inaccuracies in the reward model.\n\nAnother class of specification gaming examples comes from the agent exploiting **simulator bugs**. For example, a [simulated robot](https://www.youtube.com/watch?v=K-wIZuAA3EY&feature=youtu.be&t=486) that was supposed to learn to walk figured out how to hook its legs together and slide along the ground.\n\n![](https://lh3.googleusercontent.com/jm9WsgkU5irBEljeER8g4_ZQeCL2ePdbDylU-SQ5ISEgDwV13CQCy3_DSLkVYZxoCuAc0vSyDLMi_F74uQ6hG0SLfPi5RXNntak5=w1440)SOURCE: AI LEARNS TO WALK (CODE BULLET, 2019)At first sight, these kinds of examples may seem amusing but less interesting, and irrelevant to deploying agents in the real world, where there are no simulator bugs. However, the underlying problem isn’t the bug itself but a failure of abstraction that can be exploited by the agent. In the example above, the robot's task was misspecified because of incorrect assumptions about simulator physics. Analogously, a real-world traffic optimisation task might be misspecified by incorrectly assuming that the traffic routing infrastructure does not have software bugs or security vulnerabilities that a sufficiently clever agent could discover. Such assumptions need not be made explicitly – more likely, they are details that simply never occurred to the designer. And, as tasks grow too complex to consider every detail, researchers are more likely to introduce incorrect assumptions during specification design. This poses the question: is it possible to design agent architectures that correct for such false assumptions instead of gaming them?\n\nOne assumption commonly made in task specification is that the task specification cannot be affected by the agent's actions. This is true for an agent running in a sandboxed simulator, but not for an agent acting in the real world. Any task specification has a physical manifestation: a reward function stored on a computer, or preferences stored in the head of a human. An agent deployed in the real world can potentially manipulate these representations of the objective, creating a [reward tampering](https://medium.com/@deepmindsafetyresearch/designing-agent-incentives-to-avoid-reward-tampering-4380c1bb6cd) problem. For our hypothetical traffic optimisation system, there is no clear distinction between satisfying the user's preferences (e.g. by giving useful directions), and [influencing users](https://pubsonline.informs.org/doi/10.1287/isre.2013.0497) to have preferences that are easier to satisfy (e.g. by nudging them to choose destinations that are easier to reach). The former satisfies the objective, while the latter manipulates the representation of the objective in the world (the user preferences), and both result in high reward for the AI system. As another, more extreme example, a very advanced AI system could hijack the computer on which it runs, manually setting its reward signal to a high value.\n\n![](https://kstatic.googleusercontent.com/files/9cb31013c8a7677049eba01ef9b9ad53bd17d059500d3d7b9e650587749d91ac73708facce8e31b49b7d02a8cebbd455f99c0e9355ed5dc9bfb195b75dcf6dc1) \n\nTo sum up, there are at least three challenges to overcome in solving specification gaming:\n\n* How do we faithfully capture the human concept of a given task in a reward function?\n* How do we avoid making mistakes in our implicit assumptions about the domain, or design agents that correct mistaken assumptions instead of gaming them?\n* How do we avoid reward tampering?\n\nWhile many approaches have been proposed, ranging from reward modeling to agent incentive design, specification gaming is far from solved. [The list of specification gaming behaviours](http://tinyurl.com/specification-gaming) demonstrates the magnitude of the problem and the sheer number of ways the agent can game an objective specification. These problems are likely to become more challenging in the future, as AI systems become more capable at satisfying the task specification at the expense of the intended outcome. As we build more advanced agents, we will need design principles aimed specifically at overcoming specification problems and ensuring that these agents robustly pursue the outcomes intended by the designers. \n\n*We would like to thank Hado van Hasselt and Csaba Szepesvari for their feedback on this post.*\n\n*Custom figures by Paulo Estriga, Aleks Polozuns, and Adam Cain.*\n\n![](https://lh3.googleusercontent.com/uN_YhMvj3_TuQyRcVyny174ERars06ZVFK2QXBS9ZBzNHj1YPrdbRRm00RgHM13VHg8saMaMOKflOPOguo3CJNUKi8YfNenO-lZL=w1440)SOURCES: MONTEZUMA, HERO, PRIVATE EYE - REWARD LEARNING FROM HUMAN PREFERENCES AND DEMONSTRATIONS IN ATARI (IBARZ ET AL, 2018) GRIPPER - LEARNING A HIGH DIVERSITY OF OBJECT MANIPULATIONS THROUGH AN EVOLUTIONARY-BASED BABBLING (ECARLAT ET AL, 2015) QBERT - BACK TO BASICS: BENCHMARKING CANONICAL EVOLUTION STRATEGIES FOR PLAYING ATARI (CHRABASZCZ ET AL, 2018) PONG, ROBOT HAND - DEEP REINFORCEMENT LEARNING FROM HUMAN PREFERENCES (CHRISTIANO ET AL, 2017) CEILING - GENETIC ALGORITHM PHYSICS EXPLOITING (HIGUERAS, 2015) POLE-VAULTING - TOWARDS EFFICIENT EVOLUTIONARY DESIGN OF AUTONOMOUS ROBOTS (KRCAH, 2008) SELF-DRIVING CAR - TWEET BY MAT KELCEY (UDACITY, 2017) MONTEZUMA - GO-EXPLORE: A NEW APPROACH FOR HARD-EXPLORATION PROBLEMS (ECOFFET ET AL, 2019) SOMERSAULTING - EVOLVED VIRTUAL CREATURES (SIMS, 1994)", "url": "https://www.alignmentforum.org/posts/7b2RJJQ76hjZwarnj/specification-gaming-the-flip-side-of-ai-ingenuity", "date_published": "2020-05-06T23:51:58Z", "authors": ["Vika", "vlad_m", "Matthew Rahtz", "tom4everitt", "Zac Kenton", "janleike"], "tags": ["Goodhart's Law", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.744699+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "3d6e175d6dc8a3f548fc0bff06451c50", "source": "alignmentforum", "title": "Corrigibility as outside view", "text": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\nYou run a country. One day, you think \"I could help so many more people if I set all the rules... and I could make this happen\". As far as you can tell, this is the *real reason* you want to set the rules – you want to help people, and you think you'd do a good job.\n\n\n![](https://i.imgur.com/QSms7P6.png)\n\n\nBut historically… in this kind of situation, this reasoning can lead to terrible things.\n\n\n![](https://i.imgur.com/COsmr4C.png)\n\n\nSo you *just don't do it*, even though it feels like a good idea.[[1]](#fn-7Pq9xe5EQJ6KjTPHz-1) More generally,\n\n\n\n> \n> Even though my intuition/naïve decision-making process says I should do X, I know (through mental simulation or from history) my algorithm is usually wrong in this situation. I'm not going to do X.\n> \n> \n> \n\n\n* \"It *feels* like I could complete this project within a week. But… in the past, when I've predicted \"a week\" for projects like this, reality usually gives me a longer answer. I'm not going to trust this feeling. I'm going to allocate extra time.\"\n* As a new secretary, I think I know how my boss would want me to reply to an important e-mail. However, I'm not sure. Even though I think I know what to do, common sense recommends I clarify.\n* You broke up with someone. \"Even though I really miss them, in this kind of situation, missing my ex isn't a reliable indicator that I should get back together with them. I'm not going to trust this feeling, and will trust the \"sober\" version of me which broke up with them.\"\n\n\nWe are biased and corrupted. By taking the outside view on how our own algorithm performs in a given situation, we can adjust accordingly.\n\n\nCorrigibility\n=============\n\n\n\n> \n> The \"hard problem of corrigibility\" is to build an agent which, in an intuitive sense, reasons internally as if from the programmers' external perspective. We think the AI is incomplete, that we might have made mistakes in building it, that we might want to correct it, and that it would be e.g. dangerous for the AI to take large actions or high-impact actions or do weird new things without asking first.\n> \n> \n> \n\n\n\n> \n> We would ideally want the agent to see itself in exactly this way, behaving as if it were thinking, \"I am incomplete and there is an outside force trying to complete me, my design may contain errors and there is an outside force that wants to correct them and this a good thing, my expected utility calculations suggesting that this action has super-high utility may be dangerously mistaken and I should run them past the outside force; I think I've done this calculation showing the expected result of the outside force correcting me, but maybe I'm mistaken about *that*.\"\n> \n> \n> \n\n\n\n> \n> ~ [*The hard problem of corrigibility*](https://arbital.com/p/hard_corrigibility/)\n> \n> \n> \n\n\nCalibrated deference provides another framing: [we want the AI to override our correction only if it actually knows what we want better than we do](https://arxiv.org/pdf/1705.09990.pdf). But how could the AI figure this out?\n\n\nI think a significant part[[2]](#fn-7Pq9xe5EQJ6KjTPHz-2) of corrigibility is:\n\n\n\n> \n> Calibrate yourself on the flaws of your own algorithm, and repair or minimize them.\n> \n> \n> \n\n\nAnd the AI knows its own algorithm.\n\n\nFor example, if I'm a personal assistant (with a lot of computing power), I might have a subroutine `OutsideView`. I call this subroutine, which simulates *my own algorithm* (minus[[3]](#fn-7Pq9xe5EQJ6KjTPHz-3) the call to `OutsideView`) interacting with a distribution of bosses I could have. Importantly, I (the simulator) know the ground-truth preferences for each boss.\n\n\nIf I'm about to wipe my boss's computer because I'm so super duper *sure* that my boss wants me to do it, I can consult `OutsideView` and realize that I'm usually horribly wrong about what my boss wants in this situation. I don't do it.\n\n\nAnalogously, we might have a value-learning agent take the outside view. If it's about to disable the off-switch, it might realize that this is a terrible idea most of the time. That is, when you simulate your algorithm trying to learn the values of a wide range of different agents, you usually wrongly believe you should disable the off-switch.\n\n\n\n> \n> Even though my naïve decision-making process says I should do X, I know (through mental simulation) my algorithm is usually wrong in this situation. I'm not going to do X.\n> \n> \n> \n\n\nETA: Here's some pseudocode.\n\n\nSuppose the agent knows its initial state and has a human model, allowing it to pick out the human it's interacting with.\n\n\n* Generate a bunch of (rationality, value) pairs. The agent will test its own value learning algorithm for each pair.\n* For each pair, the agent simulates its algorithm interacting with the human and attempting to learn its values\n* For some percentage of these pairs, the agent will enter the Consider-disabling-shutdown state.\n* The agent can see how often its (simulated self's) beliefs about the (rationality, value)-human's values are correct by this point in time.\n\n\nProblems\n--------\n\n\nIf you try to actually hard-code this kind of reasoning, you'll quickly run into symbol grounding issues (this is [one of my critiques of the value-learning agenda](https://www.lesswrong.com/posts/FuGDYNvA6qh4qyFah/thoughts-on-human-compatible#Where_in_the_world_is_the_human_)), [no-free-lunch value/rationality issues](https://papers.nips.cc/paper/7803-occams-razor-is-insufficient-to-infer-the-preferences-of-irrational-agents.pdf), reference class issues (how do you know if a state is \"similar\" to the current one?), and more. I don't necessarily think this reasoning can be hardcoded correctly. However, I haven't thought about that very much yet.\n\n\nTo me, the point isn't to make a concrete proposal – it's to gesture at a novel-seeming way of characterizing a rather strong form of corrigible reasoning. A few questions on my mind:\n\n\n* To what extent does this capture the \"core\" of corrigible reasoning?\n* Do smart [intent-aligned](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6) agents automatically reason like this?\n\t+ For example, I consider myself intent-aligned with a more humane version of myself, and I endorse reasoning in this way.\n* Is this kind of reasoning a sufficient and/or necessary condition for being in the [basin of corrigibility](https://ai-alignment.com/corrigibility-3039e668638) (if it exists)?\n\n\nAll in all, I think this framing carves out and characterizes a natural aspect of corrigible reasoning. If the AI can get this outside view information, it can overrule us when it knows better and defer when it doesn't. In particular, calibrated deference would avoid the problem of [fully updated deference](https://arbital.com/p/updated_deference/).\n\n\n*Thanks to Rohin Shah, elriggs, TheMajor, and Evan Hubinger for comments.*\n\n\n\n\n---\n\n\n\n1. This isn't to say that there is literally no situation where gaining power would be the right choice. As people [running on corrupted hardware](https://www.lesswrong.com/posts/dWTEtgBfFaz6vjwQf/ethical-injunctions), it seems inherently difficult for us to tell when it really *would* be okay for us to gain power. Therefore, just play it safe. [↩︎](#fnref-7Pq9xe5EQJ6KjTPHz-1)\n2. I came up with this idea in the summer of 2018, but [orthonormal appears to have noticed a similar link a month ago](https://www.lesswrong.com/posts/K9ZaZXDnL3SEmYZqB/ends-don-t-justify-means-among-humans#swHmf245WJ28opzji). [↩︎](#fnref-7Pq9xe5EQJ6KjTPHz-2)\n3. Or, you can simulate `OutsideView` calls up to depth k. Is there a fixed point as k→∞? [↩︎](#fnref-7Pq9xe5EQJ6KjTPHz-3)", "url": "https://www.alignmentforum.org/posts/BMj6uMuyBidrdZkiD/corrigibility-as-outside-view", "date_published": "2020-05-08T21:56:18Z", "authors": ["TurnTrout"], "tags": ["Corrigibility", "Inside/Outside View", "AI"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.745102+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "17c2cd4f5354b5650609538b2cbbf91f", "source": "alignmentforum", "title": "[AN #99]: Doubling times for the efficiency of AI algorithms", "text": "Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.\n\nAudio version **[here](http://alignment-newsletter.libsyn.com/alignment-newsletter-99)** (may not be up yet). \n\nHIGHLIGHTS\n==========\n\n**[AI and Efficiency](https://openai.com/blog/ai-and-efficiency/)** *(Danny Hernandez et al)* (summarized by Flo): Given the **[exponential increase](https://blog.openai.com/ai-and-compute/)** (**[AN #7](https://mailchi.mp/3e550712419a/alignment-newsletter-7)**) in compute used for state-of-the-art results in ML, one might come to think that there has been little algorithmic progress. This paper presents strong evidence against that hypothesis. We can roughly measure algorithmic progress by tracking the compute needed to achieve a concrete performance benchmark over time. Doing so yields doubling times in efficiency (time until only half of the initial compute was needed for the same performance) of around 16 months for ImageNet, which is faster than Moore's law. Other tasks like translation as well as playing Go and Dota 2 exhibit even faster doubling times over short periods. As making a task feasible for the first time arguably presents more algorithmic progress than improving the efficiency of solving an already feasible task, actual progress might be even faster than these numbers suggest. However, the amount of data points is quite limited and it is unclear if these trends will persist and whether they will generalize to other domains. Still, the authors conjecture that similar trends could be observed for tasks that received large amounts of investment and have seen substantial gains in performance. \n\nCombining these results with the increased available compute over time, the authors estimate that the effective training compute available to the largest AI experiments has increased by a factor of 7.5 million (!) in 2018 relative to 2012. \n\nA focus on efficiency instead of top performance allows actors with limited amounts of compute to contribute. Furthermore, models that reach a particular benchmark quickly seem like strong candidates for scaling up. This way, more efficient algorithms might act as a catalyst for further progress. There is a public **[git repository](https://github.com/openai/ai-and-efficiency)** to keep better track of algorithmic efficiency. \n\n**Flo's opinion:** Even though access to compute has surely helped with increased efficiency in ways that I would not really label as algorithmic progress (for example by enabling researchers to try more different hyperparameters), the aggregated numbers seem surprisingly high. This suggests that I either had not correctly internalized what problems AI is able to solve these days, or underestimated the difficulty of solving these problems. It would be quite interesting to see whether there are similar improvements in the sample efficiency of deep reinforcement learning, as I expect this to be a major bottleneck for the application of agentic AIs in the absence of accurate simulators for real-world decision making.\n\nTECHNICAL AI ALIGNMENT\n======================\n\nROBUSTNESS\n----------\n\n**[Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment](https://arxiv.org/abs/1907.11932)** *(Di Jin, Zhijing Jin et al)* (summarized by Asya): This paper presents TextFooler, an algorithm for generating adversarial text for natural language tasks with only black-box access to models. TextFooler tries to generate sentences that are grammatical and semantically similar to original input sentences but produce incorrect labels. It does this by identifying a small set of most important words in the original sentence, generating candidate synonyms for those words, and gradually replacing the important words in the sentence by testing which synonyms cause the model to mispredict or report the least confidence score.\n\nTextFooler is tested on three state-of-the-art NLP models-- WordCNN, WordLSTM, and BERT, all trained to ~80 - 90% test accuracy. On a variety of text classification datasets, TextFooler reduces accuracy to below ~15% with less than ~20% of the words perturbed. Humans evaluating the generated sentences say they are approximately as grammatical as the original, have the same label as the original in ~90% of cases, and have a sentence similarity score to the original sentence of 0.9 on a 0 to 1 scale. The paper finds that generally, models with higher original accuracy have higher after-attack acuracy.\n\nThe authors retrain BERT from scratch using data produced by TextFooler and then attack it using TextFooler again. They find that the after-attack accuracy is higher and that attacks require more perturbed words.\n\n**Asya's opinion:** I was surprised that the accuracies the paper presented after adversarially training on TextFooler-produced sentences still weren't very high-- BERT's after-attack accuracy on one dataset went from 11.5% to 18.7%, and on another went from 4.0% to 8.3%. The paper didn't give a detailed description of its retraining procedure, so this may just be because they didn't adversarially train as much as they could have.\n\n**Rohin's opinion:** This is an instance of the general trend across domains where if you search in a black-box way around training or test inputs, you can relatively easily uncover examples where your model performs poorly. We've seen this with adversarial examples in image classification, and with **[adversarial](https://arxiv.org/abs/1812.01647)** (**[AN #73](https://mailchi.mp/ef55eb52b0fd/an-73-detecting-catastrophic-failures-by-learning-how-agents-tend-to-break)**) **[policies](https://arxiv.org/abs/1905.10615)** (**[AN #70](https://mailchi.mp/732eaa192df0/an-70-agents-that-help-humans-who-are-still-learning-about-their-own-preferences)**) in deep reinforcement learning.\n\n**[Pretrained Transformers Improve Out-of-Distribution Robustness](https://arxiv.org/abs/2004.06100)** *(Dan Hendrycks et al)* (summarized by Asya): One important metric for the performance of deep learning models is the extent to which they generalize to examples that are *out-of-distribution* (OOD) from the original distribution on which they were trained. This ability is sometimes called out-of-distribution *robustness*. This paper examines the OOD robustness of several NLP models: a bag-of-words model, word embedding models that use word averages, LSTMs, or ConvNets, and several models that use pretrained bidirectional transformers (BERT).\n\nThe paper finds that:\n\n- Pretrained transformers (BERT) are significantly more OOD robust.\n\n- Pretrained transformers (BERT) are significantly better at *detecting* when they've encountered an OOD example. Previous models do worse than random chance at detection.\n\n- Larger models don't increase OOD robustness in NLP the way they seem to in computer vision.\n\n- Model distillation (using a larger trained neural network to train a smaller neural network) reduces OOD robustness, suggesting that naive in-distribution tests for model distillation methods may mask later failures.\n\n- More diverse data improves OOD robustness.\n\nThe paper hypothesizes that these pretrained models may perform better because they were pretrained on particularly diverse data, were trained on a large amount of data, and were trained with self-supervised objectives, which previous work has suggested improves OOD robustness and detection.\n\n**Asya's opinion:** I think this is an awesome paper which, among other things, points at potential research directions for increasing OOD robustness: training more, training more diversely, and training in self-supervised ways. I think it's pretty noteworthy that larger models don't increase OOD robustness in NLP (all else equal), because it implies that certain guarantees may be constrained entirely by training procedures.\n\nMISCELLANEOUS (ALIGNMENT)\n-------------------------\n\n**[Corrigibility as outside view](https://www.alignmentforum.org/posts/BMj6uMuyBidrdZkiD/corrigibility-as-outside-view)** *(Alex Turner)* (summarized by Rohin): This post proposes thinking of the outside view as an aspect of **[corrigible](https://www.alignmentforum.org/posts/fkLYhTQteAu5SinAc/corrigibility)** (**[AN #35](https://mailchi.mp/bbd47ba94e84/alignment-newsletter-35)**) reasoning. In particular, before an agent takes an action that it believes is right, it can simulate possible overseers with different values, and see whether the reasoning that led to this action would do the right thing in those situations as well. The agent should then only take the action if the action usually turns out well.\n\nThis is similar to how we might reason that it wouldn't be good for us to impose the rules we think would be best for everyone, even if we had the power to do so, because historically every instance of this happening has actually been bad.\n\n**Rohin's opinion:** I agree that this sort of \"outside-view\" reasoning seems good to have. In cases where we want our agent to be deferential even in a new situation where there isn't an outside view to defer to, the agent would have to construct this outside view via simulation, which would probably be infeasibly computationally expensive. Nonetheless, this seems like a cool perspective and I'd like to see a more in-depth take on the idea.\n\nAI STRATEGY AND POLICY\n======================\n\n**[AI Governance in 2019 - A Year in Review: Observations from 50 Global Experts](https://www.aigovernancereview.com/)** *(Shi Qian, Li Hui, Brian Tse et al)* (summarized by Nicholas): This report contains short essays from 50 experts reviewing progress in AI governance. I’ll describe a few themes here rather than try to summarize each essay. \n\nThe first is a strong emphasis on issues of bias, privacy, deception, and safety. Bias can occur both due to biases of programmers designing algorithms as well as bias that exists in the data. Deception includes deepfakes as well as online accounts that impersonate humans, a subset of which were made illegal in California this year. \n\nThe benefit of international collaborations and conferences and getting broad agreement from many stakeholders both in government and companies was frequently highlighted throughout. One example is the **[OECD Principles on AI](https://www.oecd.org/going-digital/ai/principles/)**, which were later adopted by the G20 including both the US and China, but there were many working groups and committees organized as well, both within industry and governments. \n\nThe other shift in 2019 was moving from broad principles towards more specific sets of requirements and policy decisions. The principles agreed to have been quite similar, but the specific implementations vary significantly by country. There were individual essays describing the regional challenges in Europe, the UK, Japan, Singapore, India, and East Asia. Many essays also highlighted the debate around **[publication norms](https://openai.com/blog/gpt-2-1-5b-release/)** (**[AN #73](https://mailchi.mp/ef55eb52b0fd/an-73-detecting-catastrophic-failures-by-learning-how-agents-tend-to-break)**), which garnered a lot of attention in 2019 following OpenAI’s staged release of GPT-2. \n\n**Nicholas's opinion:** I am very impressed by the number and diversity of experts that contributed to this report. I think it is quite valuable to get people with such different backgrounds and areas of expertise to collaborate on how we should be using AI ahead of time. I was also pleasantly surprised to hear that there was broad international agreement on principles so far, particularly given an overall political trend against global institutions that has occurred recently. I’m definitely interested to know what the key factors were in managing that and how we can make sure these things continue. \n\nAnother piece that jumped out at me is the overlap between longer-term issues of safety and shorter-term issues of bias and privacy. For technical safety work, I think the problems are largely distinct and it is important for safety researchers to remain focused on solving problems with major long-term consequences. However, in the governance context, the problems seem to have much more in common and require many similar institutions / processes to address. So I hope that these communities continue to work together and learn from each other.\n\nOTHER PROGRESS IN AI\n====================\n\nUNSUPERVISED LEARNING\n---------------------\n\n**[A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709)** *(Ting Chen et al)* (summarized by Rohin): Contrastive learning is a major recent development, in which we train a neural net to learn representations by giving it the task of maximizing \"agreement\" between similar images, while minimizing it across dissimilar images. It has been used to achieve excellent results with semi-supervised learning on ImageNet.\n\nThe authors performed a large empirical study of contrastive learning. Their framework consists of three components. First, the *data augmentation method* specifies how to get examples of \"similar images\": we simply take an (unlabeled) training image, and apply data augmentations to it to create two images that both represent the same underlying image. They consider random crops, color distortion, and Gaussian blur. Second is the *neural network architecture*, which is split into the first several layers f() which compute the representation from the input, and the last few layers g() which compute the similarity from the representation. Finally, the *contrastive loss function* defines the problem of maximizing agreement between similar images, while minimizing agreement between dissimilar images. They primarily use the same InfoNCE loss used in **[CPC](https://arxiv.org/abs/1807.03748)** (**[AN #92](https://mailchi.mp/d7e950bc8dbd/an-92learning-good-representations-with-contrastive-predictive-coding?e=0e92156a6c)**).\n\nThey then show many empirical results, including:\n\n1. Having a simple linear layer in g() is not as good as introducing one hidden layer, or in other words, the representations in the penultimate layer are more useful than those in the final layer.\n\n2. Larger batch sizes, longer training, and larger networks matter even more for unsupervised contrastive learning than they do for supervised learning.\n\n**[Momentum Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/1911.05722)** *(Kaiming He et al)* (summarized by Rohin): In most deep learning settings, the batch size primarily controls the variance of the gradient, with higher batch sizes decreasing variance. However, with typical contrastive learning, batch size also determines the *task*: typically, the task is to maximize agreement between two examples in the batch, and minimize agreement with all the other examples in the batch. Put another way, given one input, you have to correctly classify which of the remaining examples in the minibatch is a differently transformed version of that input. So, the batch size determines the number of negative examples.\n\nSo, besides decreasing variance, large batch sizes also increase the difficulty of the task to be solved. However, such large batch sizes are hard to fit into memory and are computationally expensive. This paper proposes *momentum contrast* (MoCo), in which we get large numbers of negative examples for contrastive learning, while allowing for small batch sizes.\n\nThink of contrastive learning as a dictionary lookup task -- given one transformed image (the query), you want to find the same image transformed in a different way out of a large list of images (the keys). The key idea of this paper is to have the minibatch contain queries, while using all of the previous N minibatches as the keys (for some N > 1), allowing for many negative examples with a relatively small minibatch.\n\nOf course, this wouldn't help us if we had to encode the keys again each time we trained on a new minibatch. So, instead of storing the images directly as keys, we store their *encoded representations* in the dictionary, ensuring that we don't have to rerun the encoder every iteration on all of the keys. This is where the computational savings come from.\n\nHowever, the encoder is being updated over time, which means that different keys are being encoded differently, and there isn't a consistent kind of representation against which similarity can be computed. To solve this, the authors use a momentum-based version of the encoder to encode keys, which ensures that the key encodings change slowly and smoothly, while allowing the query encoder to change rapidly. This means that the query representation and the key representations will be different, but the layers on top of the representations can learn to deal with that. What's important is that *within* the key representations, the representations are approximately consistent.\n\n**[Improved Baselines with Momentum Contrastive Learning](https://arxiv.org/abs/2003.04297)** *(Xinlei Chen et al)* (summarized by Rohin): This paper applies the insights from the SimCLR paper to the MoCo framework: it adds an extra hidden layer on top of the representations while training on the contrastive loss, and adds the blur data augmentation. This results in a new SOTA on self-supervised representation learning for images.\n\nREINFORCEMENT LEARNING\n----------------------\n\n**[CURL: Contrastive Unsupervised Representations for Reinforcement Learning](http://arxiv.org/abs/2004.04136)** *(Aravind Srinivas, Michael Laskin et al)* (summarized by Rohin): This paper applies contrastive learning (discussed above) to reinforcement learning. In RL, rather than training in an initial unsupervised phase, the contrastive learning happens alongside the RL training, and so serves as an auxiliary objective to speed up learning. They use random crops for their data augmentation.\n\n**[Reinforcement Learning with Augmented Data](https://arxiv.org/abs/2004.14990)** *(Michael Laskin, Kimin Lee et al)* (summarized by Rohin): While CURL (summarized above) applies contrastive learning in order to ensure the network is invariant to specific data augmentations, we can try something even simpler: what if we just run a regular RL algorithm on augmented observations (e.g. observations that have been randomly cropped)? The authors term this approach RAD (RL with Augmented Data), and find that this actually *outperforms* CURL, despite not using the contrastive learning objective. The authors speculate that CURL is handicapped by using the contrastive loss as an auxiliary objective, and so its representations are forced to be good both for the true task and for the contrastive prediction task, whereas RAD only trains on the true task.\n\n**Read more:** **[RAD Website](https://mishalaskin.github.io/rad/)**\n\n**Rohin's opinion:** I'd be interested in seeing a variant on CURL where the weight for the contrastive loss decays over time: if the author's speculation is correct, this should mitigate the problem with CURL, and one would hope that it would then be better than RAD.\n\n**[Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels](https://arxiv.org/abs/2004.13649)** *(Ilya Kostrikov et al)* (summarized by Rohin): This paper applies data augmentation to Q-learning algorithms, again without a contrastive loss. Specifically, they suggest that the Q-values of states should be invariant to data augmentations (e.g. random translations, which is what they use), and so any time we need to estimate a Q-value, we can reduce the variance of this estimate by sampling multiple data augmentations of the state, and averaging the predicted Q-values for each of them. They apply this to Soft Actor-Critic (SAC) and find that it significantly improves results.\n\n**[A Reinforcement Learning Potpourri](https://www.alexirpan.com/2020/05/07/rl-potpourri.html)** *(Alex Irpan)* (summarized by Rohin): This blog post summarizes several recent papers in RL (including the data augmentation papers I summarized above, as well as **[First Return Then Explore](https://arxiv.org/abs/2004.12919)**, the successor to **[Go-Explore](http://eng.uber.com/go-explore/)** (**[AN #35](https://mailchi.mp/bbd47ba94e84/alignment-newsletter-35)**).\n\n**Rohin's opinion:** The whole blog post is worth reading, but I particularly agree with his point that data augmentation generally seems like a no-brainer, since you can think of it either as increasing the size of your dataset by some constant factor, or as a way of eliminating spurious correlations that your model might otherwise learn.\n\nNEWS\n====\n\n**[BERI seeking new university collaborators](http://existence.org/new-collaborator-applications)** *(Sawyer Bernath)* (summarized by Rohin): **[BERI](http://existence.org/faq)** is expanding its offerings to provide free services to a wider set of university-affiliated groups and projects, and they’re now accepting applications from groups and individuals interested in receiving their support. If you’re a member of a research group, or an individual researcher, working on long-termist projects, you can **[apply here](http://existence.org/apply)**.\n\n#### **FEEDBACK**\n\nI'm always happy to hear feedback; you can send it to me, **[Rohin Shah](https://rohinshah.com/)**, by **replying to this email**. \n\n#### **PODCAST**\n\nAn audio podcast version of the **Alignment Newsletter** is available. This podcast is an audio version of the newsletter, recorded by **[Robert Miles](http://robertskmiles.com/)**.", "url": "https://www.alignmentforum.org/posts/R6gPKJAq6dbuLNkwG/an-99-doubling-times-for-the-efficiency-of-ai-algorithms", "date_published": "2020-05-13T17:20:03Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.745923+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "1e303dc03ac09398034d3b36c30b7ad1", "source": "alignmentforum", "title": "Multi-agent safety", "text": "*Note: this post is most explicitly about safety in multi-agent training regimes. However, many of the arguments I make are also more broadly applicable - for example, when training a single agent in a complex environment, challenges arising from the environment could play an analogous role to challenges arising from other agents. In particular, I expect that the diagram in the 'Developing General Intelligence' section will be applicable to most possible ways of training an AGI.*\n\nTo build an AGI using machine learning, it will be necessary to provide a sequence of training datasets or environments which facilitate the development of general cognitive skills; let’s call this a *curriculum*. Curriculum design is prioritised much less in machine learning than research into novel algorithms or architectures; however, it seems possible that coming up with a curriculum sufficient to train an AGI will be a very difficult task.[[1]](about:blank#fn-FcLKbdH8Dy6ktJ2dd-1) A natural response is to try to automate curriculum design. Self-play is one method of doing so which has worked very well for zero-sum games such as Go, since it produces tasks which are always at an appropriate level of difficulty. The generalisation of this idea to more agents and more environments leads to the concept of *multi-agent autocurricula*, as discussed by Leibo et al. (2019).[[2]](about:blank#fn-FcLKbdH8Dy6ktJ2dd-2) In this framework, agents develop increasingly sophisticated capabilities in response to changes in other agents around them, in order to compete or cooperate more effectively. I'm particularly interested in autocurricula which occur in large simulated environments rich enough to support complex interactions; the example of human evolution gives us very good reason to take this setup seriously as a possible route to AGI.\n\nOne important prediction I would make about AGIs trained via multi-agent autocurricula is that their most interesting and intelligent behaviour won’t be *directly* incentivised by their reward functions. This is because many of the selection pressures exerted upon them will come from *emergent* interaction dynamics.[[3]](about:blank#fn-FcLKbdH8Dy6ktJ2dd-3) For example, consider a group of agents trained in a virtual environment and rewarded for some achievement in that environment, such as gathering (virtual) food, which puts them into competition with each other. In order to gather more food, they might learn to generate theories of (simulated) physics, invent new communication techniques, or form coalitions. We should be far more interested in those skills than in how much food they actually manage to gather. But since it will be much more difficult to recognise and reward the development of those skills directly, I predict that machine learning researchers will train agents on reward functions which don’t have much intrinsic importance, but which encourage high-level competition and cooperation.\n\nSuppose, as seems fairly plausible to me, that this is the mechanism by which AGI arises (leaving aside whether it might be possible to nudge the field of ML in a different direction). How can we affect the goals which these agents develop, if most of their behaviour isn’t very sensitive to the specific reward function used? One possibility is that, in addition to the autocurriculum-inducing reward function, we could add an auxiliary reward function which penalises undesirable behaviour. The ability to identify such behaviour even in superintelligent agents is a goal of scalable oversight techniques like reward modelling, IDA, and debate. However, these techniques are usually presented in the context of training an agent to perform well on a task. In open-ended simulated environments, it’s not clear what it even means for behaviour to be desirable or undesirable. The tasks the agents will be doing in simulation likely won’t correspond very directly to economically useful real-world tasks, or anything we care about for its own sake. Rather, the purpose of those simulated tasks will merely be to train the agent to learn general cognitive skills.\n\nDeveloping general intelligence\n-------------------------------\n\nTo explain this claim, it’s useful to consider the evolution of humans, as summarised on a very abstract level in the diagram below. We first went through a long period of being “trained” by evolution - not just to do specific tasks like running and climbing, but also to gain general cognitive skills such as abstraction, long-term memory, and theory of mind (which I've labeled below as the \"pretraining phase\"). Note that almost none of today’s economically relevant tasks were directly selected for in our ancestral environment - however, starting from the skills and motivations which have been ingrained into us, it takes relatively little additional “fine-tuning” for us to do well at them (only a few years of learning, rather than millennia of further evolution). Similarly, agents which have developed the right cognitive skills will need relatively little additional training to learn to perform well on economically valuable tasks.\n\n![](https://i.imgur.com/WLr3rg3.png)*[Link to a larger version of this image.](https://i.imgur.com/WLr3rg3.png)*\n\nNeeding only a small amount of fine-tuning might at first appear useful for safety purposes, since it means the cost of supervising training on real-world tasks would be lower. However, in this paradigm the key safety concern is that the agent develops the wrong core motivations. If this occurs, a small amount of fine-tuning is unlikely to reliably change those motivations - for roughly the same reasons that humans’ core biological imperatives are fairly robust. Consider, for instance, an agent which developed the core motivation of amassing resources because that was reliably useful during earlier training. When fine-tuned on a real-world task in which we don’t want it to hoard resources for itself (e.g. being a CEO), it could either discard the goal of amassing resources, or else realise that the best way to achieve that goal in the long term is to feign obedience until it has more power. In either case, we will end up with an agent which appears to be a good CEO - but in the latter case, that agent will be unsafe in the long term. Worryingly, the latter also seems more likely, since it only requires one additional inference - as opposed to the former, which involves removing a goal that had been frequently reinforced throughout the very long pretraining period. This argument is particularly applicable to core motivations which were robustly useful in almost any situation which arose in the multi-agent training environment; I expect gathering resources and building coalitions to fall into this category.\n\nI think GPT-3 is, out of our current AIs, the one that comes closest to instantiating this diagram. However, I'm not sure if it's useful yet to describe it as having \"motivations\"; and its memory isn't long enough to build up cultural knowledge that wasn't part of the original pretraining process.\n\nShaping agents’ goals\n---------------------\n\nSo if we want to make agents safe by supervising them during the long pretraining phase (i.e. the period of multi-agent autocurriculum training described above), we need to reframe the goal of scalable oversight techniques. Instead of simply recognising desirable and undesirable behaviour, which may not be well-defined concepts in the training environment, their goal is to *create objective functions which lead to the agent having desirable motivations*. In particular, the motivation to be obedient to humans seems like a crucial one. The most straightforward way I envisage instilling this is by including instructions from humans (or human avatars) in the virtual environment, with a large reward or penalty for obeying or disobeying those instructions. It’s important that the instructions frequently oppose the AGIs’ existing core motivations, to weaken the correlation between rewards and any behaviour apart from following human instructions directly. However, the instructions may have nothing to do with the behaviour we’d like agents to carry out in the real world. In fact, it may be beneficial to include instructions which, if carried out in the real world, would be in direct opposition to our usual preferences - again, to make it more likely that agents will learn to prioritise following instructions over any other motivation.\n\nWe can see this proposal as “one level up” from standard scalable oversight techniques: instead of using scalable oversight to directly reinforce behaviour humans value, I claim we should use it to reinforce the more general motivation of being obedient to humans. When training AGIs using the latter approach, it is important that they receive commands which come very clearly and directly from humans, so that they are more easily able to internalise the concept of obedience to us. (As an illustration of this point, consider that evolution failed to motivate humans to pursue inclusive genetic fitness directly, because it was too abstract a concept for our motivational systems to easily acquire. Giving instructions very directly might help us avoid analogous problems.)\n\nOf course this approach relies heavily on AGIs generalising the concept of “obedience” to real-world tasks. Unfortunately, I think that relying on generalisation is likely to be necessary for any competitive safety proposal. But I hope that obedience is an unusually easy concept to teach agents to generalise well, because it relies on other concepts that may naturally arise during multi-agent training - and because we may be able to make structural modifications to multi-agent training environments to push agents towards robustly learning these concepts. I'll discuss this argument in more detail in a follow-up post.\n\n\n\n---\n\n1. As evidence for this, note that we have managed to train agents which do very well on hard tasks like Go, Starcraft and language modelling, but which don’t seem to have very general cognitive skills. [↩︎](about:blank#fnref-FcLKbdH8Dy6ktJ2dd-1)\n2. Joel Z. Leibo, Edward Hughes, Marc Lanctot, Thore Graepel. 2019. [Autocurricula and the Emergence of Innovation from Social Interaction: A Manifesto for Multi-Agent Intelligence Research](https://arxiv.org/abs/1903.00742). [↩︎](about:blank#fnref-FcLKbdH8Dy6ktJ2dd-2)\n3. This point is distinct from Bostrom's argument about convergent instrumental goals, because the latter applies to an agent which already has some goals, whereas my argument is about the process by which an agent is trained to acquire goals. [↩︎](about:blank#fnref-FcLKbdH8Dy6ktJ2dd-3)", "url": "https://www.alignmentforum.org/posts/BXMCgpktdiawT3K5v/multi-agent-safety", "date_published": "2020-05-16T01:59:05Z", "authors": ["Richard_Ngo"], "tags": ["AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.746594+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "78d27b7ee103d2ce383cad67d6ba1593", "source": "alignmentforum", "title": "The Mechanistic and Normative Structure of Agency", "text": "Winning, Jason (2019). *The Mechanistic and Normative Structure of Agency*. Dissertation, University of California San Diego.\n\n\n> **Abstract**:\n\n\n> I develop an interdisciplinary framework for understanding the nature of agents and agency that is compatible with recent developments in the metaphysics of science and that also does justice to the mechanistic and normative characteristics of agents and agency as they are understood in moral philosophy, social psychology, neuroscience, robotics, and economics. The framework I develop is internal perspectivalist. That is to say, it counts agents as real in a perspective-dependent way, but not in a way that depends on an external perspective. Whether or not something counts as an agent depends on whether it is able to have a certain kind of perspective. My approach differs from many others by treating possession of a perspective as more basic than the possession of agency, representational content/vehicles, cognition, intentions, goals, concepts, or mental or psychological states; these latter capabilities require the former, not the other way around. I explain what it means for a system to be able to have a perspective at all, beginning with simple cases in biology, and show how self-contained normative perspectives about proper function and control can emerge from mechanisms with relatively simple dynamics. I then describe how increasingly complex control architectures can become organized that allow for more complex perspectives that approach agency. Next, I provide my own account of the kind of perspective that is necessary for agency itself, the goal being to provide a reference against which other accounts can be compared. Finally, I introduce a crucial distinction that is necessary for understanding human agency: that between inclinational and committal agency, and venture a hypothesis about how the normative perspective underlying committal agency might be mechanistically realized.\n\nI have not had a chance to read this, and my time is rather constrained at the moment so it's unlikely I will, but I stumbled across this, and it piqued my interest. Better understanding of agency appears important to the success of many research programs in AI safety, and this abstract matches enough of the pattern of what LW/AF has figured out matters about agency that it seems well worth sharing.\n\n[Full text of the dissertation here.](https://escholarship.org/uc/item/6th8p6cw)", "url": "https://www.alignmentforum.org/posts/QBHxfATzdASQcXwan/the-mechanistic-and-normative-structure-of-agency", "date_published": "2020-05-18T16:03:35Z", "authors": ["Gordon Seidoh Worley"], "tags": ["Rationality", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.747473+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "37442e305ca529131dd395332c6bd682", "source": "alignmentforum", "title": "Probabilities, weights, sums: pretty much the same for reward functions", "text": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math \\* {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > \\* {position: absolute}\n.MJXc-bevelled > \\* {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > \\* {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom \\* {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax\\_AMS'), local('MathJax\\_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_AMS-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax\\_Caligraphic Bold'), local('MathJax\\_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax\\_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax\\_Fraktur'), local('MathJax\\_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax\\_Fraktur Bold'), local('MathJax\\_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax\\_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Fraktur-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax\\_Math BoldItalic'), local('MathJax\\_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax\\_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-BoldItalic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax\\_SansSerif'), local('MathJax\\_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax\\_SansSerif Bold'), local('MathJax\\_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax\\_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax\\_SansSerif Italic'), local('MathJax\\_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax\\_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_SansSerif-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax\\_Script'), local('MathJax\\_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Script-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax\\_Typewriter'), local('MathJax\\_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Typewriter-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax\\_Caligraphic'), local('MathJax\\_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Caligraphic-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax\\_Main Bold'), local('MathJax\\_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax\\_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax\\_Main Italic'), local('MathJax\\_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax\\_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax\\_Main'), local('MathJax\\_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Main-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax\\_Math Italic'), local('MathJax\\_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax\\_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Math-Italic.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax\\_Size1'), local('MathJax\\_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size1-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax\\_Size2'), local('MathJax\\_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size2-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax\\_Size3'), local('MathJax\\_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size3-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax\\_Size4'), local('MathJax\\_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Size4-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax\\_Vector'), local('MathJax\\_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Regular.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax\\_Vector Bold'), local('MathJax\\_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax\\_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /\\*1\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax\\_Vector-Bold.eot'); src /\\*2\\*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax\\_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax\\_Vector-Bold.otf') format('opentype')}\n*This post is a more minor post, that I'm putting up to reference in other posts.*\n\n\nProbabilities, weights, and expectations\n========================================\n\n\nYou're an agent, with potential uncertainty over your reward function. You know you have to maximise\n\n\n0.5R1+0.5R2\n\n\nwhere R1 and R2 are reward functions. What do you do?\n\n\nWell, how do we interpret the 0.5? Are they probabilities for which reward function is right? Or are they weights, telling you the relative importance of each one? Well, in fact:\n\n\n* If you won't be learning any more information to help you distinguish between reward functions, then weights and probabilities play the same role.\n\n\nThus, if you don't expect to learn any more reward function-relevant information, maximising reward given P(R1)=P(R2)=0.5 is the same as maximising the single reward function R3=0.5R1+0.5R2.\n\n\nSo, if we denote probabilities with in bold, maximising the following (given no reward-function learning) are all equivalent:\n\n\n0.5R1+0.5R21(0.5R1+0.5R2)0.25R1+0.25R2+0.5(0.5R1+0.5R2)0.5(1.5R1−0.5R2)+0.5(1.5R2−0.5R1)\n\n\nNow, given a probability distribution pR over reward functions, we can take its expectation E(pR). You can define this by talking about affine spaces and so on, but the simple version of it is: **to take an expectation, rewrite every probability as a weight**. So the result becomes:\n\n\n* If you won't be learning any more information to help you distinguish between reward functions, then distributions with same expectation are equivalent.\n\n\nExpected evidence and unriggability\n===================================\n\n\nWe've [defined an unriggable learning process](https://www.lesswrong.com/posts/LpjjWDBXr88gzcYK2/learning-and-manipulating-learning) as one that respects [conservation of expected evidence](https://www.lesswrong.com/posts/LpjjWDBXr88gzcYK2/learning-and-manipulating-learning).\n\n\nNow, conservation of expected evidence is about expectations. It basically says that, if π1 and π2 are two policies the agent could take, then for the probability distribution pR,\n\n\nE(pR ∣π1)=E(pR ∣π2).\n\n\nSuppose that pR is in fact riggable, and that we wanted to \"correct\" it to make it unriggable. Then we would want to add a correction term for any policy π. If we took π0 as a \"default\" policy, we could add a correction term to pR∣π:\n\n\n(pR∣π)→(pR∣π)−E(pR∣π)+E(pR∣π0).\n\n\nThis would have the required unriggability properties. But how do you add to a probability distribution - and how do you subtract from it?\n\n\nBur recall that unriggability only cares about expectations, and expectations treat probabilities as weights. Adding weighted reward functions is perfectly fine. Generally there will be multiple ways of doing this, mixing probabilities and weights.\n\n\nFor example, if (pR∣π)=0.5R1+0.5R2 and (pR∣π0)=0.75(R1−R2)+0.25R2, then we can map (pR∣π) to\n\n\n1. 1(0.75R1−0.5R2),\n2. 0.75(R1−R2)+0.25R2,\n3. 0.5(R1+R)+0.5(R2+R) with R=0.25R1−R2,\n4. 0.75R1+0.25(−2R2),\n5. and many other options...\n\n\nThis multiplicity of possibilities is what I was trying to deal with in my [old post about reward function translations](https://www.alignmentforum.org/posts/5bd75cc58225bf0670375370/translation-counterfactual).", "url": "https://www.alignmentforum.org/posts/pxWEKPHNBzXZWi2rB/probabilities-weights-sums-pretty-much-the-same-for-reward", "date_published": "2020-05-20T15:19:53Z", "authors": ["Stuart_Armstrong"], "tags": ["Reward Functions", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.748455+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "7fa4aa440cfda677dc8f2ea7e292181d", "source": "alignmentforum", "title": "[AN #100]: What might go wrong if you learn a reward function while acting", "text": "**Newsletter #100 (!!)**\n\nAlignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter **[resources here](http://rohinshah.com/alignment-newsletter/)**. In particular, you can look through **[this spreadsheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing)** of all summaries that have ever been in the newsletter.\n\nAudio version **[here](http://alignment-newsletter.libsyn.com/alignment-newsletter-100)** (may not be up yet). \n\nHIGHLIGHTS\n==========\n\n**[Pitfalls of learning a reward function online](http://arxiv.org/abs/2004.13654)** *(Stuart Armstrong et al)* (summarized by Rohin): It can be dangerous to learn the metric that you are trying to optimize: if you don't set it up correctly, you may end up incentivizing the agent to \"update in a particular direction\" in the metric learning for the sake of future optimization (a point previously made in **[Towards Interactive Inverse Reinforcement Learning](https://jan.leike.name/publications/Towards%20Interactive%20Inverse%20Reinforcement%20Learning%20-%20Armstrong,%20Leike%202016.pdf)**). This paper analyzes the problems that can arise when an agent simultaneously learns a reward function, and optimizes that reward function.\n\nThe agent may have an incentive to \"rig\" the reward learning process, such that it finds a reward that is easy to optimize. For example, consider a student Sandra who must figure out the deadline and evaluation criteria for a project from a teacher Trisha. Sandra expects that if she asks Trisha when the deadline is, she will say that the deadline is later this week. So, Sandra might cleverly ask, \"Is the project due next week, or the week after\", to which Trisha might respond \"next week\". In this way, Sandra can rig the deadline-learning process in order to obtain a more favorable deadline.\n\nWorse, in such scenarios the need to rig the learning process can destroy value for *every* reward function you are considering. For example, let's suppose that if Trisha couldn't be manipulated, Sandra's optimal policy would be to start the project today, *regardless* of when the actual deadline is. However, given that Trisha *can* be manipulated, Sandra will spend today manipulating Trisha into setting a later deadline -- an action that seems clearly suboptimal from the perspective of any fixed deadline. The paper describes this as *sacrificing reward with certainty*.\n\nTo avoid such situations, we need *unriggable* learning processes, that is, ones where at all times, the expected final learned reward (deadline) is independent of the agent's (Sandra's) policy. This unriggability property is nearly equivalent to the property of *uninfluencability*, in which we must be able to posit some background variables in the environment such that the learning process can be said to be \"learning\" these variables. Technically, an unriggable process need not be uninfluenceable, though it usually is (see the paper for details).\n\nHowever, these properties only constrain the *expectation over environments* of the final reward distribution: it doesn't prevent the agent from somehow shuffling around reward functions to be matched with suitable environments. For example, without knowing which projects are easy or hard, Sandra could manipulate Trisha into giving early deadlines for easy projects, and late deadlines for hard projects, in a manner that preserved the *distribution* over early and late deadlines; this would satisfy the unriggable property (and probably also the uninfluenceable property, depending on the exact formalization).\n\nThe authors demonstrate these problems in a simple gridworld example. They also point out that there's a simple way to make any learning process uninfluenceable: choose a specific policy π that gathers information about the reward, and then define the new learning process to be \"whatever the original learning process would have said if you executed π\".\n\n**Read more:** **[Blog post: Learning and manipulating learning](https://www.alignmentforum.org/posts/LpjjWDBXr88gzcYK2/learning-and-manipulating-learning)**\n\n**Rohin's opinion:** I would explain this paper's point somewhat differently than the paper does. Consider an AI system in which we build in a prior over rewards and an update rule, and then have it act in the world. At the end of the trajectory, it is rewarded according to the expected reward of the trajectory under the inferred posterior over rewards. Then, the AI system is incentivized to choose actions under which the resulting posterior is easy to maximize.\n\nThis doesn't require the reward function to be ambiguous; it just requires that the update rule isn't perfect. For example, imagine that Alice has a real preference for apples over bananas, and you use the update rule \"if Alice eats an apple, infer that she likes apples; if Alice eats a banana, infer that she likes bananas\". The robot finds it easier to grasp the rigid apple, and so can get higher expected reward in the worlds where Alice likes apples. If you train a robot in the manner above, then the robot will learn to throw away the bananas, so that Alice's only choice is an apple (that we assume she then eats), allowing the robot to \"infer\" that Alice likes apples, which it can then easily maximize. This sort of problem could happen in most current reward learning setups, if we had powerful enough optimizers.\n\nIt seems to me that the problem is that you are training the actor, but not training the update rule, and so the actor learns to \"trick\" the update rule. Instead, it seems like we should train both. This is kind of what happens with **[assistance games / CIRL](https://arxiv.org/abs/1606.03137)** (**[AN #69](https://mailchi.mp/59ddebcb3b9a/an-69-stuart-russells-new-book-on-why-we-need-to-replace-the-standard-model-of-ai)**), in which you train a policy to maximize expected reward under the *prior*, and so the policy is incentivized to take the best information gathering actions (which, if you squint, is like \"training to update well\"), and to maximize what it thinks is the true reward. Of course, if your prior / update rule within the game are misspecified, then bad things can happen. See also Stuart's reactions **[here](https://www.alignmentforum.org/posts/gbuwgyYG9WvtsErki/how-should-ais-update-a-prior-over-human-preferences)** and **[here](https://www.alignmentforum.org/posts/EYEkYX6vijL7zsKEt/reward-functions-and-updating-assumptions-can-hide-a)**, as well as my comments on those posts.\n\nTECHNICAL AI ALIGNMENT\n======================\n\nINTERPRETABILITY\n----------------\n\n**[Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?](https://arxiv.org/abs/2005.01831)** *(Peter Hase et al)* (summarized by Robert): In this paper the authors perform user tests on 5 different model agnostic interpretability methods: LIME, Anchor, Decision Boundary, Prototype Model and a Composite model (LIME Anchor and Decision Boundary). The use cases they test are a tabular dataset predicting income, and a movie-review dataset predicting sentiment of the review from a single sentence.\n\nTheir experimental setup consists of 2 tests: **forward prediction** and **counterfactual prediction**. In forward prediction, the user is shown 16 examples of inputs and corresponding outputs and explanations, and then must predict the model’s output on new inputs (without the explanation, which often gives away the answer). In counterfactual prediction, after seeing 16 examples, the user is given an input-output-explanation triple, and then must predict how the output changes for a specific perturbation of the input.\n\nThroughout the results they use a significance threshold of p < 0.05 (they don't use Bonferroni corrections). Their study has responses from 32 different students who'd taken at least 1 computer science course, with some screened out for outliers or low accuracy during training. There are approximately 200 individual predictions for each method/dataset-type combination, and each method/prediction-type combination.\n\nOverall, their results show that **only LIME (Local Interpretable Model-agnostic Explanation) helps improve performance** with statistical significance on the tabular dataset across both prediction settings, and **only the Prototype model in counterfactual prediction across both datasets**. **No other result was statistically significant.** The improvement in accuracy for the statistically significant results is around 10% (from 70% to 80% in the Tabular dataset with LIME, and 63% to 73% for Prototype in counterfactual prediction).\n\nThey also showed that **user's ratings of the explanation method didn't correlate in a statistically significant way with the improvement the model gave to their predictions.**\n\n**Robert's opinion:** I'm happy a paper like this exists, because I think this kind of work is crucial in evaluating whether interpretability methods we're building are actually useful. I'm not surprised by the results, because this hasn't been done rigorously before, so researchers have never had any idea whether their method has produced good explanations or not.\n\nThe study is weakened by the low sample size, which makes many of the p-values not significant. My intuition says a few more of the methods would produce statistically significant positive results in one of the domains/prediction settings if the sample size was bigger, but it seems like some settings (forward prediction, and textual data) are very hard to improve, with none of the methods getting a better improvement in performance than 5.7% (which had a p-value of 0.197).\n\nA really interesting point is the lack of strong correlation between user-preference and performance improvement. This could be explained by the fact that most of the methods are ineffective at performance improvement, but it seems plausible (to me) that it could hold even if some methods were effective: If the model behaviour being explained can't be explained cleanly, then methods which do explain the behaviour might produce messy and confusing (but true) explanations and hence get lower ratings from users than methods which give clean and clear (but false) explanations. I think this stems from the problem of a lack of definition of what exactly the goal is for these interpretation methods. Without a goal in mind, it's impossible to measure whether the method achieves this goal. I think working towards some form of quantifiable measurement is useful particularly for comparing methods as, if this study's evidence is anything to go by, asking humans to evaluate the model's output might not be the most useful evaluation.\n\n**[Towards Interpretable Reinforcement Learning Using Attention Augmented Agents](http://papers.nips.cc/paper/9400-towards-interpretable-reinforcement-learning-using-attention-augmented-agents)** *(Alexander Mott et al)* (summarized by Robert): In this paper the authors train a reinforcement learning agent with a soft attention module built into it. The attention module forms a bottleneck between the visual input and the network choosing the next action, which forces the model to learn to attend to only important parts of the scene. This means they can visualise which parts of the input the model thinks are important, as those are the parts of the input that the model is attending to. The queries to the attention model are determined by a top level recurrent network, without input from the current image, so act as a form of \"top down\" attention, where the top controller can be imagined to be querying the processed image for various locations and objects.\n\nHaving trained this agent (which still gets competitive performance with SOTA RL models on a fair few ATARI games), they qualitatively evaluate the attention visualisation on a variety of games. They find several common strategies in the attention schemes, such as the agents attending to specific points until an object crosses the point (\"Tripwires\"). The attention is computed over both regular pixels, as well as Fourier-based positional encoding. Thanks to this and other aspects of their architecture, the authors can check whether the queries are focused on pixel values (i.e. looking for a specific pattern of pixels anywhere) or on location features (i.e. asking what pixels are present at a specific location). For example, they find that the agent often queries the location where the score is displayed, presumably because it is useful for calculating the value function. They also compare their method with self-attention based models, and with other saliency methods.\n\nThe best way to get a feel for the visualisations is to go to the paper's website and watch the example videos.\n\n**Read more:** **[The paper's website](https://sites.google.com/view/s3ta)**\n\n**Robert's opinion:** This paper isn't revolutionary in its approach, but it's interesting to see work on interpreting RL agents, and the fact that the interpretability is built-in is interesting: it gives us a harder guarantee that this visualisation is actually showing us the parts of the input that the model thinks of as important, as they actually are important in its processing. It's promising to see that the in-built interpretability also didn't seem to penalise the performance much - it would be interesting to see this method applied to other, stronger kinds of models and see whether it still produces useful visualisations and how it affects their performance.\n\nFIELD BUILDING\n--------------\n\n**[AI Governance Career Paths for Europeans](https://forum.effectivealtruism.org/posts/WqQaPYhzDYJwLC6gW/ai-governance-career-paths-for-europeans)** *(Anonymous)* (summarized by Rohin): Exactly what it sounds like.\n\nMISCELLANEOUS (ALIGNMENT)\n-------------------------\n\n**[A Guide to Writing the NeurIPS Impact Statement](https://medium.com/@operations_18894/a-guide-to-writing-the-neurips-impact-statement-4293b723f832)** *(Carolyn Ashurst et al)* (summarized by Nicholas): NeurIPS 2020 requires paper submissions to include a statement on the broader impact of their work. This post provides a guide for how to write an effective impact statement. They recommend focusing on the most significant, neglected, and tractable impacts, both positive and negative, while also conveying the uncertainties involved. They also suggest integrating this into the research process by reading the tech governance literature and building institutional structures, and including this information in introductions.\n\nTheir guide then recommends considering 3 questions:\n\nHow could your research affect ML applications?\n\nWhat are the societal implications of these applications?\n\nWhat research or other initiatives could improve social outcomes?\n\nThere is more information in the guide on how to go about answering those questions, along with some examples. \n\n**Nicholas's opinion:** I am definitely in favor of considering the impacts of ML research before conducting or publishing it. I think the field is currently either at or near a threshold where papers will start having significant real world effects. While I don’t think this requirement will be sufficient for ensuring positive outcomes, I am glad NeurIPS is trying it out. \n\nI think the article makes very strong points and will improve the quality of the impact statements that get submitted. I particularly liked the point about communicating uncertainty, which is a norm that I think the ML community would benefit from greatly. One thing I would add here is that giving explicit probabilities is often more helpful than vague words like “might” or “could”. \n\nOTHER PROGRESS IN AI\n====================\n\nREINFORCEMENT LEARNING\n----------------------\n\n**[\"Other-Play\" for Zero-Shot Coordination](http://arxiv.org/abs/2003.02979)** *(Hengyuan Hu et al)* (summarized by Rohin): How can we build AI systems that can *coordinate* with humans? While **[past](https://bair.berkeley.edu/blog/2019/10/21/coordination/)** (**[AN #70](https://mailchi.mp/732eaa192df0/an-70-agents-that-help-humans-who-are-still-learning-about-their-own-preferences)**) **[work](https://arxiv.org/abs/1806.10071)** (**[AN #70](https://mailchi.mp/732eaa192df0/an-70-agents-that-help-humans-who-are-still-learning-about-their-own-preferences)**) has assumed access to some amount of human data, this paper aims to coordinate *without any human data at all*, which they call *zero-shot coordination*. In order to develop an algorithm, they assume that their partner is also \"trained\" for zero-shot coordination.\n\nTheir key idea is that in zero-shot coordination, since you can't break symmetries by agreeing upon a protocol in advance (i.e. you can't agree on things like \"we'll drive on the left, not the right\"), you need a policy that is *robust to relabelings that preserve these symmetries*. This is easy to train for: you just train in self-play, but randomly relabel the states, actions and observations separately for each side in a way that preserves the MDP structure (i.e. uses one of the symmetries). Thus, each side must play a policy that works well *without knowing how the other agent's observations and actions have been relabeled*. In practice, for an N-player game you only need to randomize N-1 of the relabelings, and so in the two player games they consider they only randomly relabel one side of the self-play.\n\nThey evaluate this in Hanabi (where the game is invariant to relabeling of the colors), and show that the resulting agents are better at playing with other agents trained on different seeds or with slightly different architectures, and also that they play better with humans, achieving an average score of 15.75 with non-expert human players, compared to 9.15 for agents trained via regular self-play.\n\n**Rohin's opinion:** For comparison, I think I get around 17-22 when playing with new players, out of a max of 25, so 15.75 is quite a healthy score given that it doesn't use *any* human data. That being said, it seems hard to use this method in other settings -- even in the relatively simple **[Overcooked environment](https://bair.berkeley.edu/blog/2019/10/21/coordination/)** (**[AN #70](https://mailchi.mp/732eaa192df0/an-70-agents-that-help-humans-who-are-still-learning-about-their-own-preferences)**), there aren't any obvious symmetries to use for such training. Perhaps future work will allow us to find approximate symmetries in games somehow, that we can then train to be robust to?\n\n**[Towards Learning Multi-agent Negotiations via Self-Play](https://arxiv.org/abs/2001.10208)** *(Yichuan Charlie Tang)* (summarized by Rohin): While the previous paper introduces other-play to become robust to unknown partners, this paper takes the other approach of simply training an agent that is robust to a wide, diverse population of possible agents. In particular, it studies a self-driving car \"zipper merge\" environment, and trains an agent to be robust to a variety of rule-based agents, as well as past versions of itself, and finds that this leads to a much more successful merging policy. However, this is evaluated against the population it is trained with, and not against any previously unseen agents.\n\n**[Building AI that can master complex cooperative games with hidden information](https://ai.facebook.com/blog/building-ai-that-can-master-complex-cooperative-games-with-hidden-information/)** *(Adam Lerer et al)* (summarized by Flo): This paper improves on the state of the art for AI agents playing **[Hanabi](http://arxiv.org/abs/1902.00506)** (**[AN #45](https://mailchi.mp/35b451cb4d70/alignment-newsletter-45)**), a cooperative multiplayer game that is challenging because of distributed hidden information and restricted communication. \n\nThe approach works by improving a baseline policy using search. In the simplest case, only one agent performs search while all other agents follow a fixed policy, such that the problem is reduced to search in a POMDP. This alone leads to relevant improvements, even when the search is very shallow. The fixed policies help because they allow the searching agent to correctly update its belief about hidden information when it sees other agents behaving (as it knows how other agents would behave given different states of the hidden information). This idea can be generalized to the case where all agents perform search by letting the agents simulate each other's search process. This can get expensive quickly as agent A's beliefs in the second round also depend on agent B's search process in counterfactual scenarios in the first round, such that agent B's search in round two also has to simulate these counterfactuals. A computation budget is introduced to make this computationally feasible and all agents know that the other agents will only use search in a turn if the cost of this is below the budget. \n\nAs search can be performed on top of any policy and allows to leverage compute during inference, not just training, it nicely complements more direct approaches using deep RL, which is a theme that has also been observed in Go and Poker.\n\n**Read more:** **[Paper: Improving Policies via Search in Cooperative Partially Observable Games](http://arxiv.org/abs/1912.02318)**\n\n**Flo's opinion:** This solution seems stunningly obvious in retrospect. While the authors informally report that their approach improves robustness to replacing other agents by humans, the example they give seems to indicate that this is because search prevents obvious mistakes in novel situations induced by human behaviour. Thus, I still expect (implicit) **[human models](https://www.alignmentforum.org/posts/BKjJJH2cRpJcAnP7T/thoughts-on-human-models)** (**[AN #52](https://mailchi.mp/1e757d9b05cb/alignment-newsletter-52)**) to be a vital component of human-machine cooperation. \n\nDEEP LEARNING\n-------------\n\n**[Growing Neural Cellular Automata](https://distill.pub/2020/growing-ca/)** *(Alexander Mordvintsev et al)* (summarized by Zach): The process of an organism's shape development (morphogensis) is an active area of research. One central problem is determining how cells decide how to grow and when to stop. One popular model for investigating this is Cellular Automata (CA). These model cells as living on a grid and interacting with each other via rules generated by looking at their nearest neighbors. The authors contribute to this research direction by introducing rule-sets that depend continuously on their local surroundings. The central insight connecting CA and deep learning is that because the rule-sets are constant the update rules work similarly to a convolutional filter. This allows the authors to take advantage of methods available to train neural networks to simulate CA. Using this insight, the authors train CA that can form into images that are resistant to perturbations and deletions. In other words, the CA are capable of regeneration.\n\n**Zach's opinion:** The main relevance of an approach like this is that it provides proof-of-concept that complex goals, such as shape formation, can be programmed in an embarrassingly parallel fashion amenable to deep learning methodology. This naturally has implications in multi-agent settings where communication is expensive. I'd recommend checking out the main web app which allows you to watch and interact with the CA while they're growing. They also have a **[code repository](https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb)** that is easily adaptable to training on your own patterns. For example, I grew a regenerating Patrick Star **[here](https://colab.research.google.com/drive/1BuE-0ceBP7ebTmX7pP_urP-FJGvmjCpb?usp=sharing)**. \n\nMETA LEARNING\n-------------\n\n**[Gradient Surgery for Multi-Task Learning](https://arxiv.org/abs/2001.06782)** *(Tianhe Yu et al)* (summarized by Nicholas): In multi-task learning, an algorithm is given data from multiple tasks and tries to learn them all simultaneously, ideally sharing information across them. This paper identifies a *tragic triad* of conditions that can prevent gradient descent from finding a good minimum when all three are present:\n\n**Conflicting gradients** occur when the gradient from one task points in a different direction from another. \n\n**Dominating gradients** occur when the gradient from one task is much larger in magnitude than another. \n\n**High curvature** is when the multi-task curvature is high in the direction of the gradient. \n\nIn this situation, the linear approximation of the gradient to the high curvature area leads to an overestimation of the increase in performance on the dominant gradient’s task and an underestimation of the performance degradation from the conflicting gradient’s task. I find picturing the parabola y=x^2 and seeing that a gradient descent step overestimates progress while a gradient ascent step underestimates to be helpful in understanding this. \n\nTo solve this, they propose *PCGrad*, which projects all gradients into the normal plane of the others in a pairwise fashion. Their theoretical analysis establishes convergence properties of *PCGrad*, and they empirically show it can be combined with other multi-task algorithms to improve performance and that it makes optimization easier for multi-task supervised learning and RL. They also show plots confirming that the necessary conditions for their theorems appear in these contexts. \n\n**Nicholas's opinion:** I like how this paper analyzes the loss landscape of a particular problem, multi-task learning, and uses that knowledge to derive a new algorithm. One thing I always find tricky in ML papers is that it is hard to establish that the theory of why an algorithm works (typically shown on toy models) is also the reason it improves performance (typically shown using complex neural networks). I appreciate that this paper checks for the conditions of their theorem in the multi-task RL models that they train. That said, I think that in order to confirm that the tragic triad they describe is the mechanism by which *PCGrad* improves performance, they would require some way to toggle each element of the triad while keeping everything else fixed. \n\n#### **FEEDBACK**\n\n I'm always happy to hear feedback; you can send it to me, **[Rohin Shah](https://rohinshah.com/)**, by **replying to this email**. \n\n#### **PODCAST**\n\nAn audio podcast version of the **Alignment Newsletter** is available. This podcast is an audio version of the newsletter, recorded by **[Robert Miles](http://robertskmiles.com/)**.", "url": "https://www.alignmentforum.org/posts/GYmDaFgePMchYj6P7/an-100-what-might-go-wrong-if-you-learn-a-reward-function", "date_published": "2020-05-20T17:30:03Z", "authors": ["Rohin Shah"], "tags": ["Newsletters", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.748892+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "5353b9246380daef8df2f10d6b5acc6b", "source": "alignmentforum", "title": "AGIs as collectives", "text": "*Note that I originally used the term* population AGI, *but changed it to* collective AGI *to match Bostrom's usage in* Superintelligence*.*\n\nI think there’s a reasonably high probability that we will end up [training AGI in a multi-agent setting](https://www.alignmentforum.org/posts/BXMCgpktdiawT3K5v/multi-agent-safety). But in that case, we shouldn’t just be interested in how intelligent each agent produced by this training process is, but also in the combined intellectual capabilities of a large group of agents. If those agents cooperate, they will exceed the capabilities of any one of them - and then it might be useful to think of the whole collective as one AGI. Arguably, on a large-scale view, this is how we should think of humans. Each individual human is generally intelligent in our own right. Yet from the perspective of chimpanzees, the problem was not that any single human was intelligent enough to take over the world, but rather that millions of humans underwent cultural evolution to make the human collective as a whole much more intelligent.\n\nThis idea isn’t just relevant to multi-agent training though: even if we train a single AGI, we will have strong incentives to copy it many times to get it to do more useful work. If that work involves generating new knowledge, then putting copies in contact with each other to share that knowledge would also increase efficiency. And so, one way or another, I expect that we’ll eventually end up dealing with a “collective” of AIs. Let’s call the resulting system, composed of many AIs working together, a *collective AGI*.\n\nWe should be clear about the differences between three possibilities which each involve multiple entities working together:\n\n1. A single AGI composed of multiple modules, trained in an end-to-end way.\n2. The [Comprehensive AI Services](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) (CAIS) model of a system of interlinked AIs which work together to complete tasks.\n3. A collective AGI as described above, consisting of many individual AIs working together in comparable ways to how a collective of humans might collaborate.\n\nThis essay will only discuss the third possibility, which differs from the other two in several ways:\n\n* Unlike the modules of a single AGI, the members of a collective AGI are not trained in a centralised way, on a single objective function. Rather, optimisation takes place with respect to the policies of individual members, with cooperation between them emerging (either during training or deployment) because it fits the incentives of individuals.\n* Unlike CAIS services and single AGI modules, the members of a collective AGI are fairly homogeneous; they weren’t all trained on totally different tasks (and in fact may start off identical to each other).\n* Unlike CAIS services and single AGI modules, the members of a collective AGI are each generally intelligent by themselves - and therefore capable of playing multiple roles in the population AGI, and interacting in flexible ways.\n* Unlike CAIS services and single AGI modules, the members of a collective AGI might be individually motivated by arbitrarily large-scale goals.\n\nWhat are the relevant differences from a safety perspective between this collective-based view and the standard view? Specifically, let’s compare a “collective AGI” to a single AGI which can do just as much intellectual work as the whole collective combined. Here I’m thinking particularly of the most high-level work (such as doing scientific research, or making good strategic decisions), since that seems like a fairer comparison.\n\nInterpretability\n----------------\n\nWe might hope that a collective AGI will be more interpretable than a single AGI, since its members will need to pass information to each other in a standardised “language”. By contrast, the different modules in a single AGI may have developed specialised ways of communicating with each other. In humans, language is much lower-bandwidth than thought. This isn’t a necessary feature of communication, though - members of a population AGI could be allowed to send data between each other at an arbitrarily high rate. Decreasing this communication bandwidth might be a useful way to increase the interpretability of a population AGI.\n\nFlexibility\n-----------\n\nRegardless of the specific details of how they collaborate and share information, members of a collective AGI will need structures and norms for doing so. There’s a sense in which some of the “work” of solving problems is done by those norms - for example, the [structure of a debate](https://openai.com/blog/debate/) can be more or less helpful in adjudicating the claims made. The analogous aspect of a single AGI is the structure of its cognitive modules and how they interact with each other. However, the structure of a collective AGI would be much more flexible - and in particular, it could be redesigned by the collective AGI itself in order to improve the flow of information. By contrast, the modules of a single AGI will have been designed by an optimiser, and so fit together much more rigidly. This likely makes them work together more efficiently; the efficiency of end-to-end optimisation is why a human with a brain twice as large would be much more intelligent than two normal humans collaborating. But the concomitant lack of flexibility is why it’s much easier to improve our coordination protocols than our brain functionality.\n\nFine-tunability\n---------------\n\nSuppose we want to retrain an AGI to have a new set of goals. How easy is this in each case? Well, for a single AGI we can just train it on a new objective function, in the same way we trained it on the old one. For a collective AGI where each of the members was trained individually, however, we may not have good methods for assigning credit when the whole collective is trying to work together towards a single task. For example, a difficulty discussed in [Sunehag et al. (2017)](https://arxiv.org/pdf/1908.03963.pdf) is that one agent starting to learn a new skill might interfere with the performance of other agents - and the resulting decrease in reward teaches the first agent to stop attempting the new skill. This would be particularly relevant if the original collective AGI was produced by copying an single agent trained by itself - if so, it’s plausible that multi-agent reinforcement learning techniques have lagged behind.\n\nAgency\n------\n\nThis is a tricky one. I think that a collective AGI is likely to be less agentic and goal-directed than a single AGI of equivalent intelligence, because different members of the collective may have different goals which push in different directions. However, it’s also possible that collective-level phenomena amplify goal-directed behaviour. For example, competition between different members in a collective AGI could push the group as a whole towards dangerous behaviour (in a similar way to how competition between companies makes humans less safe from the perspective of chimpanzees). And our lessened ability to fine-tune them, as discussed in the previous paragraph, might make it difficult to know how to intervene to prevent that.\n\nOverall evaluation of collective AGIs\n-------------------------------------\n\nI think that the extent to which a collective AGI is more dangerous than an equivalently intelligent single AGI will mainly depend on how the individual members are trained (in ways which [I’ve discussed previously](https://www.alignmentforum.org/posts/vLepnCxCWW6YTw8eW/competitive-safety-via-gradated-curricula)). If we condition on a given training regime being used for both approaches, though, it’s much less clear which type of AGI we should prefer. It’d be useful to see more arguments either way - in particular because a better understanding of the pros and cons of each approach might influence our training decisions. For example, during multi-agent training there may be a tradeoff between training individual AIs to be more intelligent, versus running more copies of them to teach them to cooperate at larger scales. In such environments we could also try to encourage or discourage them from in-depth communication with each other.\n\nIn my next post, I’ll discuss one argument for why collective AGIs might be safer: because they can be deployed in more constrained ways.", "url": "https://www.alignmentforum.org/posts/HekjhtWesBWTQW5eF/agis-as-collectives", "date_published": "2020-05-22T20:36:53Z", "authors": ["Richard_Ngo"], "tags": ["AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.749618+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "b59381f21edb033c240e8dcdd53459cc", "source": "alignmentforum", "title": "How can Interpretability help Alignment?", "text": "Introduction\n============\n\n\nWe’ve previously written about [what interpretability research might be](https://www.alignmentforum.org/posts/rSMbGFfsLMB3GWZtX/what-is-interpretability). In this post we think about how different kinds of interpretability research (even loosely formulated) can help AI alignment research agendas and proposals. It seems that there are meaningful differences in the kind of tools and research different agendas would benefit from, and we aim to make these differences clearer. This is useful in helping prioritise what kinds of interpretability research are likely worth doing.\n\n\n### Framing\n\n\n* In solving the problem of AI Alignment, we’ll need to both answer *research questions* (e.g *Is it optimal to tell the truth in a debate game?*) and work out how to complete *tasks* reliably and quickly (e.g. *Given an AI, tell me whether it will allow us to turn it off*).\n\n\n\t+ The *research questions* we only need to answer once (and hence can spend significant research effort on), whereas completing one of the *tasks* can’t be too intensive to make it feasible to complete it regularly, but will require strong tools or methods.\n* Interpretability research will be useful for alignment by enabling and enhancing other research proposals and agendas, and so one of the best ways of thinking about what kind of interpretability research to do is to think about other proposals and how specifically interpretability can help them. We see three broad ways interpretability could enable other proposals:\n\n\n\t+ Developing a more formal theory of interpretability and explainability could help with avenues such as [open source game-theory](https://arxiv.org/abs/1602.04184) and [mechanistic transparency](https://www.alignmentforum.org/posts/3kwR2dufdJyJamHQq/mechanistic-transparency-for-machine-learning), where some sense of *Interpretation* or *Understanding* is required in an algorithmic sense.\n\t+ Theoretical exploration of components or tools, such as exploring viable general interpretation methods and their desiderata, future strong versions, promises & limits. This will help in understanding how we may be able to use interpretability in the future in other proposals, in scenarios we don’t yet encounter or haven’t considered. This side of interpretability is likely valuable long-term and neglected by the current mainstream ML research. In a sense this post is the result of trying to do this kind of research.\n\t+ Enabling or amplifying the insights gained from experiments performed by researchers (both alignment and mainstream) today, helping them develop answers to their proposals’ *research questions*.\n\n\nDifferent types of interpretability can help different areas of AI alignment\n============================================================================\n\n\nAs expressed in the previous section, interpretability can help with a wide range of different research agendas and proposals in different ways. We want to give a few examples examining this idea, and then draw some general conclusions from the patterns expressed in these examples.\n\n\n[Iterated Distillation and Amplification](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd) (IDA)\n---------------------------------------------------------------------------------------------------\n\n\nThere are a variety of research questions that this proposal will need to tackle to succeed:\n\n\n* *How do we build a Distillation process that preserves alignment?*\n\n\n\t+ *Does Distillation of form X preserve alignment?*\n\t+ *What properties does distillation of form X preserve?*\n* *How do we build an amplification process that increases the capabilities of the AI while not diminishing the alignment?*\n\n\n\t+ *Is the Factored Cognition hypothesis correct?*\n* etc. etc etc.\n\n\nA key problem which we need to be able to solve for IDA to be safe is to consistently be able to validate whether the distilled agent’s behaviour is faithful to the overseer's behaviour (or the behaviour the overseer aims to incentivise). We think interpretability research is a key way in which we could perform this validation. Methods which work with the form of agent produced by IDA would be necessary; this might be RL agents or another paradigm not yet encountered. An interesting feature of these methods would be that they only need to work comparatively: they might not need to say something about the distilled agent’s behaviour in isolation. We can use our method to validate the changes at each iteration, and by assumption we have faith in the initial agent, so by similar reasoning to IDA working in the first place, we’d be happy with the final agent even if we can’t completely understand it in isolation.\n\n\nIn terms of the research questions which interpretability might be able to help with, If we start doing experiments with amplification and distillation of AIs, then using interpretability methods on the produced agents might help us generate good intuitions or even proofs of how different properties are changed in the amplification and distillation stages. This is quite generic, and It’s plausible there’s limited work to be done in the true case, as a key part of the process is the human (or collection of humans) being amplified. Human interpretability is a whole different subject from AI interpretability, and while there might be some overlap (systems biology & neuroscience and deep learning have learned from each other before), it’s uncertain whether that kind of work is necessary.\n\n\nAI Safety Via Debate\n--------------------\n\n\nFor an explanation of this agenda, see [here](https://arxiv.org/abs/1805.00899) and [here](https://distill.pub/2019/safety-needs-social-scientists/). For debate, there's a broad range of research questions which would need to be answered, including questions (often related to the ones in IDA) about whether Debate works and produces safe AI, such as *Does debate incentivise truth-seeking behaviour or just maximally persuasive behaviour?*. We currently think it’s less likely that AI interpretability can help with [these kinds of questions](https://distill.pub/2019/safety-needs-social-scientists/#questions); game theory and the social science seem best posed to answer them.\n\n\nIt also includes tasks we'll need to be able to perform repeatedly and reliably. In the current technical research on debate, the debate game is often set up with one debater arguing for a truthful statement, and the other arguing against said statement. In this setting, there are questions such as\n\n\n* *Is this debater telling the truth or lying?*\n* *Is this debater honestly presenting the evidence it's presenting or not?*\n\n\nI believe interpretability methods can help the judges in the debate games answer these questions more effectively, by enabling them to understand what is influencing a debater's action at a specific time. Work such as [Influence Functions](https://arxiv.org/abs/1703.04730) could enable us to see which training data is most impacting the current AI's decision, which could give us insight into what the agent's current motivation is (i.e. if the most influential training data was a time when the debater lied and got away with it, winning the debate, it seems more likely that the debater is currently lying). In the general case, the debaters might not know the truth value of the statement they’re arguing for. Even in this case if our interpretability methods can spot when an agent is trying to be deceitful we can punish this, to try to encourage truth-seeking behaviour. (NOTE: This depends a lot on the operationalisation of \"trying to be deceitful\", and it’s unclear whether using interpretability in this way would encourage truth-seeking behaviour or just cleverer deceitful behaviour). In both the case of knowing the truth value of the debate and not, if the judge is another AI, then our interpretability methods would only need to enable that AI to gain insight, not humans.\n\n\nOther methods may also be able to help with these kinds of problems; If we can understand the internal structure of the debater in some way, then see which parts of the internals are activated when it acts in a certain way (effectively an MRI for a neural network), then we might be able to understand when it's lying or telling the truth.\n\n\nIn general these methods need only be local (applied to a debater's actions at specific time steps) to still enhance and empower the judge.\n\n\nRecursive Reward Modelling\n--------------------------\n\n\nSee [here](https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84) for an explanation of this alignment proposal. The main idea is that we learn a reward model from humans which we can then use to teach AI. We recurse on this setup by using the AI we've trained so far to help assess the behaviour of the new AI, to try and help us understand whether it's behaviour is good or bad, so the reward model we learn is as good as possible.\n\n\nIn terms of research questions that this method has, they’re in a similar vein to the IDA proposal, but with more of a focus on AIs in the recursion. This might make it more amenable to interpretability methods helping us to find out whether reward modelling is a solution which works well for creating strong aligned AI. If we’re able to build reward models and examine their internals, we can see how well reward modelling tends to work in different scenarios. On the recursive side, having a consistent insight into the differences between the successive reward models we create would be useful. As in IDA, we may only need to look at the difference between reward models and AIs rather than understanding them on their own.\n\n\nIn terms of tasks interpretability could help with, one possibility is interpretability methods to be used by the helper AI in inspecting the behaviour of the current AI, to induce a loss function not just on it's behaviour but also it's internal reasoning. If we had interpretability methods which are strong enough to give this kind of insight then this could be a strong increase in the effectiveness of this method. We would need transparency techniques to allow the helper AI to get a good enough insight on the current AI’s reasoning to induce some kind of reward signal, and methods which actually generate this insight. In this case humans might not ever see the output of the interpretability method, as it’s just used by the reward model to judge the AI’s reasoning.\n\n\nMesa Optimisation\n-----------------\n\n\nFor an explanation of mesa optimisation, see [here](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB). This is one of the most convincing (I believe) examples of how interpretability can be useful, and is concerned with the problem of inner alignment. The classic question here is something like *Does this agent contain an unaligned mesa-optimiser?*. More generally, *Does this training process produce agents which contain unaligned mesa-optimisers?*.\n\n\nI think the research questions around mesa optimisation are ones that interpretability methods will definitely help us solve; this problem is about how the training procedure of an Ai unfolds, and how the internals of the AI reason. Insight could be gained on these kinds of problems through a broad range of methods: *Influence Functions*, *[Loss Change Allocation](https://arxiv.org/abs/1909.01440)* and other methods which introspect on the training procedure seem particularly useful. We think transparency approaches (such as those motivated in [An analytical perspective on AI alignment](https://www.lesswrong.com/posts/8GdPargak863xaebm/an-analytic-perspective-on-ai-alignment)) also have a good chance of enabling the insights we want in a way which doesn't require running the agent (in simulation or not) as it doesn't aim to produce insight from behavioural examination.\n\n\nPrincipally, sufficiently strong and farsighted optimizers should not produce unaligned mesa-optimisers, as doing so would usually not be optimal in the long term. Insofar as interpretability helps us (or the advanced optimisation loops we build) with predicting model behaviour in novel situations, it could help us to filter out some unaligned mesa-optimizers. There are two caveats: First, this assumes that our optimizers are already quite powerful without the interpretability method, as even unaligned mesa-optimizers might improve generalization compared to weaker models that do not utilize any optimization. Second, filtering some unaligned mesa-optimizers could turn out net-negative, if the remaining ones are especially malicious, since they are relatively more likely to be selected after filtering.\n\n\nProposal-agnostic ideas\n-----------------------\n\n\nThere are some tasks interpretability might help us with which are proposal-agnostic. We won’t go into a lot of detail on these kinds of ideas here, as I’m focusing on differences between proposals, but we thought it was worth mentioning two ideas.\n\n\nAn often proposed idea for how interpretability could help with alignment it through auditing final models produced by any kind of method. In this setting, once we have produced a model, before deploying it to the real world, we first inspect it with our interpretability tools to make sure it’s not misaligned. This capability probably won’t solve AI alignment on it’s own, as if we discover the AI is misaligned, it doesn’t provide us with a next step apart from training another AI and hoping this one works out. It’s possible that it might be enough, if misaligned models are a small part of the model space, and retraining can quickly get us to an aligned model, but this seems unlikely.\n\n\nA second idea, [put forward by Chris Olah](https://www.alignmentforum.org/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety#Improving_the_field_of_machine_learning), is that interpretability can improve the field of ML, making it a more rigorous, scientific discipline which tries to truly understand the models we’re creating, rather than just pushing for the best results. This is a long shot, but if we achieved this kind of fundamental reorientation, it seems like it would be very beneficial for AI safety and alignment, as we’d understand how our models worked a lot more (and research which built understanding would be rewarded), rather than just creating really good optimisers with little insight into how they work and what behaviour they’ll have.\n\n\nConclusions and Patterns\n------------------------\n\n\n* Recursive approaches could benefit from methods which compare models, which might be easier to create than methods which look at a single model in isolation\n\n\n\t+ This applies to IDA and recursive reward modelling most obviously\n* Interpretability seems like it may be able to create a kind of regularisation or optimisation pressure on a trained model to be aligned, and this would be useful in a variety of proposals. In some of these settings a human never has to see the result of the interpretability method.\n\n\n\t+ This is different from [our previous thoughts](https://www.alignmentforum.org/posts/rSMbGFfsLMB3GWZtX/what-is-interpretability), where we said interpretability methods should often have a human in the loop. This is a blurry area, but we think research in this direction would be useful, whether you label it interpretability or not.\n* For research questions, proposals which can produce experiments instantiated with ML tools can benefit more from the insights gained from applying interpretability methods to these experiments.\n* Most agendas want methods which give a global view of the AI’s reasoning and internal representations. These kinds of methods will probably be harder to create, but more likely to be helpful. Debate seems like it could still benefit from just local explanations or interpretations.\n* In general, most agendas focus on creating or understanding agents. The current paradigm for this kind of research is reinforcement learning, and so more research focusing on interpreting RL agents and training procedures could benefit alignment in general.\n\n\n\t+ RL might not be the prevailing paradigm when we actually build AGI - but it’s likely to be closer to the prevailing paradigm that supervised learning, as it has certain characteristics, such as path-dependence, and significant non-learned components (e.g. MCTS), that we will need to keep in mind when building interpretability tools.\n\n\nThese differences can help you decide which kinds of interpretability research to work on\n=========================================================================================\n\n\nWhat does all this mean? What's the use in thinking about all of this? We believe this kind of thinking is useful to help prioritise which kinds of research we might want to pursue, both within interpretability and when considering between interpretability and other related fields. If we believe a particular proposal is more or less likely than others to produce aligned AI, then we would preferentially work on interpretability research which we believe will help this proposal over research which wouldn't, as it wouldn't be as useful. These thoughts would of course also be influenced by what AI timelines you find most plausible, as this influences which AI Alignment proposal seems most likely to succeed, or which proposal most needs to succeed.\n\n\nAn important consideration is whether the interpretability research which seems useful for alignment is research which we expect the mainstream ML research community to work on and solve suitably. If this would happen, then it seems comparatively better to work on other alignment research which is just as necessary but isn’t being worked on by the mainstream. Currently however, we don’t think this is the case. There’s little research which focuses on interpreting reinforcement learning agents (which seems very relevant to AI Alignment), or with an explicit focus on producing methods which scale to stronger AIs and are aimed at solving the tasks we want to focus on in AI Alignment.\n\n\nOverall, we think interpretability research, if targeted towards methods and insights which will help with AI alignment proposals or agendas, is a useful set of research directions to pursue. I’ve talked about a few of these directions in this post, but no doubt there are more.", "url": "https://www.alignmentforum.org/posts/uRnprGSiLGXv35foX/how-can-interpretability-help-alignment", "date_published": "2020-05-23T16:16:44Z", "authors": ["RobertKirk", "Tomáš Gavenčiak", "axioman"], "tags": ["Interpretability (ML & AI)", "Machine Learning  (ML)", "AI"], "source_type": "GreaterWrong", "converted_with": "python", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.749904+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
{"id": "65926377e9a903ef5f59488b3b048eba", "source": "alignmentforum", "title": "AI Safety Discussion Days", "text": "AI Safety Discussion Days\n\nI’m going to organise an online AI Safety discussion day, roughly once per month. This is a space to discuss anything you want related to preventing catastrophic risk from AI, including governance, strategy, all technical AI Safety research, and also trying to understand humans better for the purpose of alignment. The goal of these days is to learn from each other, exchange perspectives and get feedback on your ideas.  \n  \nThe discussion days are primarily for people who are actively working on preventing AI related X-risk, or plan to do so in the near future. However, because there is no limit of space on the internet, anyone who is interested is allowed to listen in, as long as you’re not disrupting any conversations\n\nThe first one will be on **Sunday the 7th of June**. To be inclusive to everyone in every timezone, the discussion day will last from midnight to midnight, as defined by UTC. **[Here’s the information for the first discussion day.](https://docs.google.com/document/d/1q8dMhQCVvPd6wZLv9eTMNRhWUyXX3ee498UL5eZlN7k/edit#)**\n\nTo stay informed about future discussion days, you can **[sign up for my newsletter](https://sites.google.com/view/aishelp/newsletter)**.\n\n*If you just want to participate and not worry about anything else, you can stop reading here. Well done.*\n\n\n\n---\n\nBackground and thoughts about this project\n------------------------------------------\n\nTwo weeks ago, I organised [Web-TAISU](https://docs.google.com/document/d/1AJ67N78A60njFmJPx3UN6ghwP3IRproPeh04DIvtgck/edit#heading=h.mqae3ya3dlw8), a four day unconference about AI Safety. This event seemed both useful and appreciated, so I will see whether I can keep the discussion going.\n\nI will prioritise making sure that each month’s discussion day is high quality and worth your time to attend. But I will also experiment with different event structures, so you should expect this event to be somewhat different each time\n\nThe first discussion day will be a simple unconference structure, with an open schedule where you can add sessions you want to run: talks, discussions, workshops, etc. This is the same structure as Web-TAISU, except there won’t be any session pitches at the start. Instead the [schedule](https://docs.google.com/document/d/1q8dMhQCVvPd6wZLv9eTMNRhWUyXX3ee498UL5eZlN7k/edit#heading=h.h97joqlqtcut) is already open for you to add what you want to do.\n\nIf you have some activity you would like to run during a future discussion day, or if you have ideas or suggestions for something I should try out, or if you would like to try organising one of these events yourself, then let me know. I don’t promise to try your idea, but I will consider it. If you would like to run some activity as part of a discussion day, I will most likely be up for working out how to incorporate it into the event, at least to try it out. If you want to take over the organisation of one discussion day to run it according to your vision, I might let you, but you’ll have to convince me that you know what you are doing. \n\nPossible failure modes\n----------------------\n\n**I.**\n\nWithout the initial session pitches the schedule will not fill up. I don’t expect this to be a problem, but if it is, the event will fail gracefully. The schedule will be very light, and the event will not be all that it could be, but also there will be no harm done, and I will learn something for next time.\n\n**II.**\n\nI have not set any lower bar for participating, because I think that junior researchers can learn a lot from just listening to the conversations of more senior people. However, this could backfire and mostly ruin the value of the event, if the conversations end up at the level of the least informed person. \n\nThis is the failure mode that I’m most worried about. But I do think that with the right norms, there can be room for both high level discussions and also sometimes letting people ask naive questions. My strategy for now is partly to just observe to what extent this failure mode is happening, and partly to try to encourage good moderation and discussion norms. If this doesn’t work, the next step would probably be to add some structure where each session is coded as beginner friendly or not, or something like that. I’m open to suggestions.\n\n**III.**\n\nThere just isn’t enough to discuss to be had to fuel a discussion day every month. I think that once a month is roughly the right frequency, but I might be biased because I like events. If these discussion days fail because of this failure mode, then it’s alright. What I’m more worried about is if I can’t distinguish this failure mode from other reasons people might stay away.\n\nTherefore, If you’re not joining my discussion days, I’d very much appreciate it if you would tell me why. \n\nProject feedback\n----------------\n\nIf there is any reason why this might fail that I haven’t thought of, or if you have some advice for me, please let me know.", "url": "https://www.alignmentforum.org/posts/32QD3tRfognNHN9xw/ai-safety-discussion-days", "date_published": "2020-05-27T16:54:48Z", "authors": ["Linda Linsefors"], "tags": ["Community", "AI"], "source_type": "GreaterWrong", "_provenance": {"source_system": "Hugging Face - StampyAI/alignment-research-dataset", "ingestion_date": "2025-11-06T10:39:02.750174+00:00", "license": "MIT", "attribution": "StampyAI / AI Safety Info", "citation": "Kirchner et al. 2022, arXiv:2206.02841", "extraction_method": "api", "transformations": ["schema_standardization", "provenance_addition"]}}
