# Public Communication & Logs Strategy

**Version**: 1.0.0
**Last Updated**: 2025-11-09
**Status**: Planning

---

## Overview

This document outlines the strategy for:
1. Consolidating pipeline logs into public-facing updates
2. Creating a public development blog
3. Automating dataset release announcements
4. Maintaining transparency while protecting operational details

---

## Goals

### Primary Goals
- **Transparency**: Share data updates publicly
- **Traceability**: Document all data transformations
- **Accessibility**: Make logs understandable to non-technical users
- **Automation**: Generate updates automatically from pipeline runs

### Secondary Goals
- **Community Engagement**: Build trust through openness
- **Reproducibility**: Enable others to understand our process
- **Quality Signal**: Demonstrate data quality practices

---

## Components

### 1. Public Development Blog

**Location**: `docs/PUBLIC_DEVBLOG.md` (new file)

**Purpose**: Human-readable updates on data pipeline runs, new datasets, and changes

**Content Template**:
```markdown
## [YYYY-MM-DD] Pipeline Run: [Description]

### Summary
- Events added: N
- Events updated: N
- Data quality: X% validated
- Processing time: N minutes

### Changes
- New data source: [Source Name]
- Schema updates: [Changes]
- Pipeline improvements: [Improvements]

### Data Availability
- Total events: N
- Date range: YYYY to YYYY
- Categories: [List]

### Technical Details
- Pipeline commit: [SHA]
- Workflow run: [Link]
- Logs: [Link to artifacts]
```

**Automation**: Generated by GitHub Actions workflow after each pipeline run

---

### 2. Logs Consolidation

**Current State**: Logs in multiple locations
- `logs/alignment_extraction/` - Extraction logs
- `logs/alignment_validation/` - Validation logs
- GitHub Actions artifacts - Pipeline logs
- Git commit messages - Change history

**Proposed Consolidation**:

```
logs/
‚îú‚îÄ‚îÄ consolidated/               # NEW: Human-readable summaries
‚îÇ   ‚îî‚îÄ‚îÄ YYYY-MM-DD_summary.md  # Daily summary of all operations
‚îú‚îÄ‚îÄ pipeline/                   # Pipeline execution logs
‚îÇ   ‚îî‚îÄ‚îÄ YYYY-MM-DD_HH-MM-SS/
‚îÇ       ‚îú‚îÄ‚îÄ validation.log
‚îÇ       ‚îú‚îÄ‚îÄ cleaning.log
‚îÇ       ‚îú‚îÄ‚îÄ enrichment.log
‚îÇ       ‚îú‚îÄ‚îÄ transformation.log
‚îÇ       ‚îî‚îÄ‚îÄ manifest.log
‚îú‚îÄ‚îÄ extraction/                 # Source data extraction
‚îÇ   ‚îî‚îÄ‚îÄ alignment_research/
‚îÇ       ‚îî‚îÄ‚îÄ YYYY-MM-DD.log
‚îî‚îÄ‚îÄ validation/                 # Schema validation
    ‚îî‚îÄ‚îÄ alignment_research/
        ‚îî‚îÄ‚îÄ YYYY-MM-DD.log
```

**Consolidation Script**: `scripts/logging/consolidate_logs.py`
- Reads all logs from pipeline run
- Extracts key metrics and events
- Generates human-readable summary
- Appends to PUBLIC_DEVBLOG.md

---

### 3. Dataset Release Announcements

**Trigger**: When serveable zone is updated with significant changes

**Announcement Template** (`RELEASES.md`):
```markdown
# v[VERSION] - [YYYY-MM-DD]

## Dataset: [Name]

**Records**: N total (M new, P updated)
**Date Range**: YYYY to YYYY
**Quality**: X% validated, Y% enriched

### What's New
- [Key change 1]
- [Key change 2]
- [Key change 3]

### Download
- All events: `data/serveable/api/timeline_events/all_events.json`
- By year: `data/serveable/api/timeline_events/by_year/`
- Manifest: `data/serveable/MANIFEST.json`

### Integration
See [QUICK_START_INTEGRATION.md](docs/QUICK_START_INTEGRATION.md) for integration guide.

### Technical Details
- Pipeline: [Workflow run link]
- Commit: [SHA]
- Schema: event_v1.json
- Formats: JSON
```

**Automation**: Generated by GitHub Actions on serveable zone changes

---

### 4. GitHub Releases

**When to Create**:
- Major dataset additions (e.g., new 1,000+ events)
- Schema version changes
- Breaking changes to data format
- Quarterly milestones

**Release Assets**:
- Complete serveable zone as `.tar.gz`
- MANIFEST.json
- CHANGELOG excerpt
- Integration guide PDF

**Semantic Versioning**:
- Major: Breaking schema changes
- Minor: New datasets or significant additions
- Patch: Data quality improvements, bug fixes

Example: `v1.2.3`
- `v1.0.0` - Initial public release
- `v1.1.0` - Added alignment research events
- `v1.1.1` - Fixed ASCII compliance issues

---

## Automation Workflows

### Workflow 1: Daily Log Consolidation

**File**: `.github/workflows/consolidate-logs.yml`

```yaml
name: Consolidate Daily Logs

on:
  schedule:
    - cron: '0 4 * * *'  # Daily at 4am UTC
  workflow_dispatch:

jobs:
  consolidate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Run log consolidation
        run: |
          python scripts/logging/consolidate_logs.py \
            --date yesterday \
            --output logs/consolidated/

      - name: Update public devblog
        run: |
          python scripts/logging/update_devblog.py \
            --summary logs/consolidated/$(date -d yesterday +%Y-%m-%d)_summary.md

      - name: Commit consolidated logs
        run: |
          git add logs/consolidated/ docs/PUBLIC_DEVBLOG.md
          git commit -m "chore: consolidate logs for $(date -d yesterday +%Y-%m-%d)"
          git push
```

---

### Workflow 2: Dataset Release Announcements

**File**: `.github/workflows/announce-dataset-release.yml`

```yaml
name: Announce Dataset Release

on:
  push:
    branches:
      - main
    paths:
      - 'data/serveable/**'

jobs:
  announce:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 2  # Need previous commit to compare

      - name: Check for significant changes
        id: check_changes
        run: |
          # Count changed JSON files in serveable zone
          CHANGED=$(git diff HEAD~1 HEAD --name-only | grep 'data/serveable.*\.json' | wc -l)

          if [ $CHANGED -gt 0 ]; then
            echo "significant=true" >> $GITHUB_OUTPUT
          else
            echo "significant=false" >> $GITHUB_OUTPUT
          fi

      - name: Generate release announcement
        if: steps.check_changes.outputs.significant == 'true'
        run: |
          python scripts/publishing/generate_release_announcement.py \
            --manifest data/serveable/MANIFEST.json \
            --output RELEASES.md

      - name: Create GitHub Release (if version changed)
        if: steps.check_changes.outputs.significant == 'true'
        run: |
          # Check if MANIFEST version changed
          NEW_VERSION=$(jq -r '.version' data/serveable/MANIFEST.json)

          # Create release
          gh release create "v${NEW_VERSION}" \
            --title "Dataset Release v${NEW_VERSION}" \
            --notes-file RELEASES.md \
            data/serveable/MANIFEST.json
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

---

### Workflow 3: Weekly Public Summary

**File**: `.github/workflows/weekly-public-summary.yml`

```yaml
name: Weekly Public Summary

on:
  schedule:
    - cron: '0 12 * * 1'  # Mondays at 12pm UTC
  workflow_dispatch:

jobs:
  summarize:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Generate weekly summary
        run: |
          python scripts/logging/generate_weekly_summary.py \
            --start-date "last Monday" \
            --end-date "this Sunday" \
            --output docs/PUBLIC_DEVBLOG.md

      - name: Post to discussions (optional)
        run: |
          # Could post summary to GitHub Discussions
          # for community engagement
          echo "Weekly summary generated"

      - name: Commit summary
        run: |
          git add docs/PUBLIC_DEVBLOG.md
          git commit -m "docs: weekly summary for $(date +%Y-%m-%d)"
          git push
```

---

## Scripts to Implement

### 1. `scripts/logging/consolidate_logs.py`

**Purpose**: Consolidate all logs from a date into human-readable summary

**Key Functions**:
- `parse_extraction_log(path)` -> metrics dict
- `parse_validation_log(path)` -> validation results
- `parse_pipeline_logs(dir)` -> pipeline stats
- `generate_summary(metrics)` -> markdown text
- `save_summary(text, output_path)`

**Output**:
```markdown
# Daily Summary: 2025-11-09

## Operations
- Alignment research extraction: 50 new records
- Validation: 100% pass rate (1,000/1,000)
- Cleaning: 988 ASCII conversions
- Enrichment: 4,040 fields added
- Transformation: 1,000 events generated

## Data Quality
- Schema validation: PASS
- ASCII compliance: PASS
- Duplicate detection: 0 duplicates found
- Source attribution: 100% complete

## Pipeline Performance
- Total execution time: 12m 34s
- Records processed: 1,000
- Processing rate: 80 records/second

## Outputs
- Serveable events: 1,028 total
- Events by year: 2020 (400), 2021 (350), 2022 (250)
- Categories: technical_research_breakthrough (526), policy_development (424), ...
```

---

### 2. `scripts/logging/update_devblog.py`

**Purpose**: Append daily summary to public devblog

**Key Functions**:
- `load_summary(summary_path)` -> summary text
- `load_devblog(devblog_path)` -> current content
- `append_entry(devblog, summary, date)` -> updated content
- `save_devblog(content, devblog_path)`

**Behavior**:
- Prepends new entry to top of devblog
- Maintains chronological order (newest first)
- Limits total entries (keep last 30 days, archive older)

---

### 3. `scripts/publishing/generate_release_announcement.py`

**Purpose**: Generate release announcement from manifest changes

**Key Functions**:
- `load_manifest(path)` -> manifest dict
- `compare_manifests(old, new)` -> diff dict
- `generate_announcement(diff)` -> markdown text
- `save_announcement(text, output_path)`

**Output**: Formatted markdown for RELEASES.md and GitHub releases

---

### 4. `scripts/logging/generate_weekly_summary.py`

**Purpose**: Aggregate daily summaries into weekly report

**Key Functions**:
- `find_daily_summaries(start_date, end_date)` -> list of paths
- `aggregate_metrics(summaries)` -> weekly metrics
- `generate_weekly_report(metrics)` -> markdown text
- `append_to_devblog(report, devblog_path)`

**Output**: Weekly summary with:
- Total operations for week
- Data quality trends
- New datasets added
- Performance metrics
- Highlights and milestones

---

## Public Communication Channels

### 1. Repository README
- **Audience**: GitHub visitors
- **Content**: High-level overview, quick start
- **Update Frequency**: On major changes

### 2. docs/PUBLIC_DEVBLOG.md
- **Audience**: Technical users, contributors
- **Content**: Detailed pipeline runs, changes
- **Update Frequency**: Daily (automated)

### 3. RELEASES.md
- **Audience**: Data consumers
- **Content**: Dataset release notes
- **Update Frequency**: On significant changes

### 4. GitHub Releases
- **Audience**: All users
- **Content**: Versioned datasets with assets
- **Update Frequency**: Semantic versioning triggers

### 5. GitHub Discussions (Future)
- **Audience**: Community
- **Content**: Announcements, Q&A, feedback
- **Update Frequency**: Weekly summaries, as needed

---

## Privacy & Security Considerations

### What to Share Publicly
‚úÖ Data quality metrics
‚úÖ Record counts and statistics
‚úÖ Schema versions
‚úÖ Pipeline execution times
‚úÖ Error rates (aggregate)
‚úÖ Dataset descriptions

### What to Keep Private
‚ùå API keys and credentials
‚ùå Internal infrastructure details
‚ùå Specific error messages with paths
‚ùå Performance bottlenecks (until fixed)
‚ùå Unreleased features in development

---

## Implementation Roadmap

### Phase 1: Foundation (Week 1)
- [ ] Create docs/PUBLIC_DEVBLOG.md
- [ ] Create RELEASES.md
- [ ] Implement consolidate_logs.py
- [ ] Implement update_devblog.py
- [ ] Test manual log consolidation

### Phase 2: Automation (Week 2)
- [ ] Create consolidate-logs workflow
- [ ] Create announce-dataset-release workflow
- [ ] Test automated daily consolidation
- [ ] Test automated release announcements

### Phase 3: Enhancement (Week 3)
- [ ] Implement generate_weekly_summary.py
- [ ] Create weekly-public-summary workflow
- [ ] Add GitHub Discussions integration
- [ ] Create release asset packaging

### Phase 4: Refinement (Week 4)
- [ ] Gather feedback on public communications
- [ ] Refine log consolidation logic
- [ ] Improve summary readability
- [ ] Document public communication process

---

## Success Metrics

### Transparency Metrics
- [ ] 100% of pipeline runs have public summary
- [ ] All dataset releases announced within 24 hours
- [ ] Weekly summaries published consistently

### Engagement Metrics
- [ ] GitHub stars increase
- [ ] Issues/discussions from community
- [ ] External citations of dataset

### Quality Metrics
- [ ] Zero sensitive data leaked in logs
- [ ] Summaries accurately reflect operations
- [ ] Release notes match actual changes

---

## Future Enhancements

### Possible Additions
- **RSS Feed**: For automated notifications
- **Email Digest**: Weekly summary via email
- **Changelog Generator**: Automated from commits
- **Data Portal**: Web UI for browsing datasets
- **API Documentation**: Auto-generated from code
- **Tutorial Videos**: Screencast integration guides

### Community Features
- **Contribution Guide**: How to add data sources
- **Data Quality Dashboard**: Real-time metrics
- **Discussion Forum**: GitHub Discussions
- **Issue Templates**: Standardized requests

---

## Related Documentation

- [DATA_PUBLISHING_STRATEGY.md](DATA_PUBLISHING_STRATEGY.md) - Public data strategy
- [RUNBOOK.md](RUNBOOK.md) - Operations procedures
- [LINEAGE.md](LINEAGE.md) - Data provenance tracking
- [DEVBLOG.md](../DEVBLOG.md) - Internal development blog

---

## Maintenance

**Review Frequency**: Monthly
**Owned By**: pdoom-data maintainers
**Last Review**: 2025-11-09
**Next Review**: 2025-12-09

---

**Status**: ‚úÖ Strategy documented, üîÑ Implementation in progress
